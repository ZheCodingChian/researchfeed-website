{
  "date": "2025-07-22",
  "total_papers": 232,
  "papers": [
    {
      "id": "2507.16116",
      "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized\n  Timestep Adaptation",
      "authors": [
        "Yaofang Liu",
        "Yumeng Ren",
        "Aitor Artola",
        "Yuxuan Hu",
        "Xiaodong Cun",
        "Xiaotong Zhao",
        "Alan Zhao",
        "Raymond H. Chan",
        "Suiyun Zhang",
        "Rui Liu",
        "Dandan Tu",
        "Jean-Michel Morel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training\ncost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs.\n$\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen",
      "published_date": "2025-07-22T00:09:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16116v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16116v1",
      "latex_url": "http://arxiv.org/src/2507.16116v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The advent of diffusion models  {song2020score,ho2020denoising} has heralded a paradigm shift in generative modeling, particularly in the domain of image synthesis. These models, which leverage an iterative noise reduction process, have demonstrated remarkable efficacy in producing high-fidelity samples. Naturally, extending this framework to video generation~ {ho2022video,lvdm,chen2023videocrafter1,modelscope,ma2024latte,sora,vdmsurvey,liu2024evalcrafter} has been a focal point, yet it exposes fundamental limitations in modeling complex temporal dynamics. Conventional video diffusion models (VDMs) typically employ scalar timestep variables, enforcing uniform temporal evolution across all frames. This approach, while effective for text-to-video (T2V) clips generation, struggles with nuanced temporal dependencies task like image-to-video (I2V) generation, as highlighted in the FVDM work  {liu2024redefining} which solved this problem by introducing a vectorized timestep approach to enable independent frame evolution.\n\nConcurrently, methods like Diffusion Forcing  {chen2024diffusion} and AR-Diffusion  {sun2025ar} also explored autoregressive paradigms to avoid this rigid synchronization modeling form of conventional VDMs. Nonetheless, their applications w.r.t. video generation remained constrained by the autoregressive designs with token/frame-level noise. Large-scale models such as MAGI-1  {teng2025magi} and SkyReels V2 advanced scalability but still faced challenges in balancing computational efficiency and multi-task capability.\n\nIn this work, we bridge the gap between theoretical innovation and industrial deployment by extending the FVDM framework to industrial scale through fine-tuning on the SOTA open-source T2V model Wan2.1-T2V-14B (Wan-T2V) with vectorized timestep adaptation (VTA). Our key insight is to leverage the vectorized timestep variable (VTV) designs of FVDM within a robust large-model ecosystem, enabling efficient adaptation to diverse video generation tasks, especially I2V, while drastically reducing computational and data requirements.\n\nAs demonstrated in Table 1, our approach (Pusa) achieves a landmark improvement: with thousands of times smaller datasets ($4K$ samples vs. $  10M$) and hundreds of times less computation (training cost reduced to $0.5K vs.   100K $), we surpass prior art Wan2.1-I2V-14B (Wan-I2V) on the VBench-I2V benchmark . Notably, Pusa not only achieves a higher total score (87.32% vs. 86.86%) with superior I2V quality (94.84% vs. 92.90%), but also extends capabilities beyond T2V and I2V generation to support start-end frame generation, video extension, and other complex temporal tasks—all within a single unified model. This marks a critical departure from previous models like Wan-I2V, which are limited to I2V and require prohibitive resources.\n\nThe core of our innovation lies in the synergistic combination of FVDM's temporal modeling prowess with the SOTA generative capacity of Wan-T2V, optimized through a lightweight fine-tuning strategy. By preserving the vectorized timestep formulation, each frame evolves along its independent temporal trajectory during diffusion, enabling the model to capture intricate inter-frame dependencies without global synchronization. This architecture not only enhances temporal coherence but also enables zero-shot generalization to new tasks, as validated by our results in I2V generation without task-specific training.\n\nOur contributions can be summarized as:\n {itemize}\n   Industrial-Scale Efficiency: We demonstrate the first large-model adaptation of FVDM, achieving unprecedented data efficiency ($  1/2500$ dataset size) and computational efficiency ($  1/200$ training cost) compared to Wan-I2V, revolutionizing the video diffusion paradigm.\n   Multi-Task Generalization: our proposed model supports not only T2V and I2V, but also start-end frames, video extension, and more without additional training.\n   Quality-Throughput Tradeoff: Despite significantly reduced resources, Pusa achieves a superior total score (87.32% vs. 86.86%) on Vbench-I2V, proving the FVDM paradigm works well for large foundational models and greatly exceeds previous methods.\n\n {itemize}\n\nThis work represents a pivotal step toward democratizing advanced video generation: by unlocking the full potential of the FVDM paradigm within a practical, scalable framework, we enable high-fidelity, multi-task video synthesis accessible to researchers and industries alike. Through rigorous benchmarking and novel fine-tuning strategies, we establish that temporal modeling innovation, when paired with strategic large-model adaptation, can overcome the long-standing tradeoff between performance, efficiency, and versatility in video diffusion.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "01_intro.tex",
      "rlhf_score": 0.313,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.458,
      "distributed_training_score": 0.391,
      "datasets_score": 0.329,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of an efficient video diffusion model (PUSA V1.0) using vectorized timestep adaptation for improved temporal control in video generation tasks, such as image-to-video and text-to-video synthesis. It does not involve adapting the diffusion process for multi-step logical reasoning, Chain-of-Thought, or solving complex logical tasks; instead, it focuses on generative modeling for media. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667933",
      "updated_at": "2025-08-11T23:43:05.606891",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16119",
      "title": "Universal Wavelet Units in 3D Retinal Layer Segmentation",
      "authors": [
        "An D. Le",
        "Hung Nguyen",
        "Melanie Tran",
        "Jesse Most",
        "Dirk-Uwe G. Bartsch",
        "William R Freeman",
        "Shyamanga Borooah",
        "Truong Q. Nguyen",
        "Cheolhong An"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "This paper presents the first study to apply tunable wavelet units (UwUs) for\n3D retinal layer segmentation from Optical Coherence Tomography (OCT) volumes.\nTo overcome the limitations of conventional max-pooling, we integrate three\nwavelet-based downsampling modules, OrthLattUwU, BiorthLattUwU, and\nLS-BiorthLattUwU, into a motion-corrected MGU-Net architecture. These modules\nuse learnable lattice filter banks to preserve both low- and high-frequency\nfeatures, enhancing spatial detail and structural consistency. Evaluated on the\nJacobs Retina Center (JRC) OCT dataset, our framework shows significant\nimprovement in accuracy and Dice score, particularly with LS-BiorthLattUwU,\nhighlighting the benefits of tunable wavelet filters in volumetric medical\nimage segmentation.",
      "published_date": "2025-07-22T00:11:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16119v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16119v1",
      "latex_url": "http://arxiv.org/src/2507.16119v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Optical Coherence Tomography (OCT) enables high-resolution imaging of retinal structures and is essential for diagnosing diseases like age-related macular degeneration (AMD), diabetes-related macular edema (DME), and glaucoma. Accurate retinal layer segmentation supports disease monitoring but remains challenging due to motion artifacts and the limitations of 2D deep learning methods that ignore 3D context.\nTo address these issues, we enhance the MGU-Net architecture based model by replacing max-pooling layers with wavelet-inspired downsampling modules: OrthLattUwU, BiorthLattUwU, and the proposed LS-BiorthLattUwU. These modules use tunable lattice filter banks to better preserve spatial details and high-frequency features. We further incorporate an attention head to adaptively integrate subband information. Applied to 3D OCT segmentation with motion correction, our UwU-MGUNet framework showed improved performance based on accuracy and Dice scores, especially with LS-BiorthLattUwU. Experiments on the JRC dataset demonstrate superior performance over the standard pooling-based model.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.226,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.306,
      "distributed_training_score": 0.272,
      "datasets_score": 0.265,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669009",
      "updated_at": "2025-08-11T23:43:05.607103",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16122",
      "title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for\n  Efficient 3D Medical Image Segmentation",
      "authors": [
        "Nand Kumar Yadav",
        "Rodrigue Rizk",
        "William CW Chen",
        "KC Santosh"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP",
      "published_date": "2025-07-22T00:30:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16122v3",
      "pdf_url": "http://arxiv.org/pdf/2507.16122v3",
      "latex_url": "http://arxiv.org/src/2507.16122v3",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Accurate 3D medical image segmentation is crucial for a variety of reasons, such as clinical diagnosis, treatment planning, and disease monitoring. However, volumetric segmentation remains computationally intensive due to the high dimensionality of medical imaging data and the anatomical variability across modalities such as CT and MRI~.Traditional architectures, including 3D U-Net and its variants, have demonstrated strong performance~ but are often hindered by high memory consumption and inefficiency in real-time applications~. Similarly, transformer-based models, including UNETR~, leverage long-range dependencies but come with substantial computational overhead, hindering their deployment in resource-constrained environments~.\n\nTo address these limitations, we introduce {MLRU++}, a Multiscale Lightweight Residual UNETR++ architecture that combines the benefits of multiscale learning, lightweight residual design, and attention mechanisms to provide accurate yet efficient 3D segmentation.\nAt the heart of MLRU++ is the Lightweight Convolutional Block Attention Module (LCBAM), which replaces conventional attention schemes with a streamlined alternative. While the standard CBAM~ effectively improves feature quality by applying sequential channel and spatial attention, it still incurs non-trivial overhead due to multi-layer perceptrons and convolution operations. In contrast, LCBAM preserves the core benefits of dual attention while significantly reducing parameter count, making it well-suited for high-resolution 3D medical data. Through this hybrid of multiscale feature fusion and efficient attention, MLRU++ achieves strong segmentation performance across diverse medical datasets. By leveraging lightweight channel-spatial attention, MLRU++ achieves strong segmentation performance without incurring the computational burden typically associated with volumetric models.\n\nOur contributions are summarized as follows:\n {itemize}\n   We propose a {  lightweight residual UNETR++ backbone} that reduces parameter count while preserving representational capacity through residual connections.\n   We introduce a {  LCBAM} with multiscale feature handling which fuses channel and spatial attention to enhance multi-resolution features across the encoder-decoder pathway and adaptively highlights informative features across scales.\n   We validate MLRU++ across four large-scale datasets, showing that MLRU++ outperforms existing state-of-the-art models in accuracy, efficiency, and generalization with significantly reduced model complexity.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "1_intro.tex",
      "rlhf_score": 0.302,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.368,
      "datasets_score": 0.319,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669018",
      "updated_at": "2025-08-11T23:43:05.607105",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16124",
      "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
      "authors": [
        "Dakota Sullivan",
        "Shirley Zhang",
        "Jennica Li",
        "Heather Kirkorian",
        "Bilge Mutlu",
        "Kassem Fawaz"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.",
      "published_date": "2025-07-22T00:36:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16124v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16124v1",
      "latex_url": "http://arxiv.org/src/2507.16124v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "As robots become more competent and provide greater utility, we increasingly rely on them to perform tasks in private locations such as homes~ and care settings ~. As early as 2014, more than 3,000 Paro therapeutic robots had been sold worldwide~, and by 2024, over 50 million Roomba units had been sold globally~. At present, the market size for social robots has increased to \\$5.6 billion, with a forecast of \\$42.5 billion in 2033~.\n\nSimultaneously, researchers in academia and industry are implementing large language models (LLMs) and video language models (VLMs) on robots. These models equip robots with the tools to perceive and reason about their environments far beyond their previous capabilities. In recent years, researchers have demonstrated that LLMs can enhance robot capabilities by enabling natural language requests~, writing code to control robot movement on-the-fly~, achieving interactive dialogue~, and improving navigation abilities~. Additionally, researchers have evaluated LLM-powered robots through user studies to identify the potential benefits and weaknesses of these LLM frameworks~. Several large tech companies have also introduced projects that utilize LLMs or VLMs to enable more intelligent robots, such as Google's DeepMind-RT-2~ and NVIDIA's GR00T N1~. From these advancements, it is clear that LLMs and VLMs can offer significant benefits when used in conjunction with robots.\n\nLike many other technologies, robots pose a threat to users' data privacy. Robots can autonomously navigate human environments and collect data in private spaces that users may not have originally intended or desired. When a robot captures audio or visual data of humans or human environments, that data may be collected by the robot's manufacturers. While this data collection may amount to a privacy violation on its own, further use in training, marketing, or leakage of data can further cause harm to the individuals from which the data originated. Beyond these traditional data privacy concerns, however, social robots pose an additional threat to users' interpersonal privacy. Social robots can interact with users in a human-like manner that encourages engagement and communication. Once a robot has captured user data in this interpersonal context, it then has the potential to share that data with other users. This phenomenon is unique to social robots and places them in a position to engage in privacy decision-making. It is within this unique context that LLMs may have the potential to alleviate privacy concerns. LLMs may be able to determine whether a given piece of information is private, whether it is necessary to retain (e.g., for functional purposes), and whether or with whom it may be shared. Privacy is highly contextually dependent, and the nuances of any given scenario may require these highly sophisticated models to navigate appropriately.\n\nThough many state-of-the-art LLMs have been designed with some level of privacy protection in mind, it is not yet clear the degree to which these models are privacy aware. Prior works have primarily focused on general LLM safety, such as LLMs' vulnerability against jailbreak attacks~. Few works, however, have investigated the privacy perceptions of LLMs and VLMs~, and none have done so in the context of human-robot interaction. Given this gap, we aim to investigate whether and how LLMs interpret privacy scenarios in the context of human-robot interaction in home environments. Specifically, we raise the following research questions: RQ1: How does an individual’s privacy orientation influence their privacy expectations of social robots?; RQ2: How well do state-of-the-art LLMs align with individuals’ privacy expectations?; and RQ3: How do prompting strategies influence state-of-the-art LLMs’ conformity with individuals’ privacy expectations? Through the development and evaluation of privacy scenarios with participants and LLMs, we show that state-of-the-art LLMs maintain a broad understanding of privacy sensitivity, but may not yet possess nuanced privacy awareness.\n\nIn addressing our research questions, we present the following contributions:\n {itemize}\n   Data Set: A set of privacy scenarios grounded in contextual integrity (CI)\n   Annotation: Multiple large online user studies to determine appropriate robot and LLM responses to scenarios\n   Evaluation: A benchmark of 10 state-of-the-art LLMs’ privacy awareness using multiple prompting strategies\n   Design Implications: Insights into the use of LLMs as potential privacy-controllers for social robots\n {itemize}\n\n% {figure}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "Sections/1_Introduction.tex",
      "rlhf_score": 0.511,
      "weak_supervision_score": 0.428,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.349,
      "datasets_score": 0.427,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking LLMs for privacy awareness in social robots using user surveys and scenarios, but it does not involve training or fine-tuning AI models with human feedback via a reward model and reinforcement learning. There is no mention of RLHF techniques, such as using human-ranked data to align models, making it unrelated to this topic.",
      "weak_supervision_justification": "The paper does not involve training machine learning models using programmatically generated or noisy labels. Instead, it evaluates existing LLMs on privacy scenarios derived from user studies and Contextual Integrity, without any discussion of weak supervision methods for label generation or model training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include creating a new set of privacy scenarios grounded in Contextual Integrity, conducting user studies for annotation and evaluation, and benchmarking these scenarios with LLMs. This directly aligns with research on creating, analyzing, and evaluating datasets for AI applications, as it introduces a dataset, assesses its utility through benchmarks, and provides insights for future use.",
      "summary": "This paper examines the privacy awareness of large language models (LLMs) in the context of social robots operating in home environments, aiming to bridge the gap between human privacy expectations and LLM decision-making. By developing privacy scenarios based on Contextual Integrity, the authors conducted a survey with 450 participants to assess privacy preferences and benchmarked 10 state-of-the-art LLMs using various prompting strategies, revealing low agreement between human responses and LLM outputs, indicating that LLMs possess broad but not nuanced privacy understanding, and discussing implications for enhancing AI privacy controls in human-robot interactions.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing LLM benchmarking and user survey methods to the new context of social robot privacy, cleverly combining these techniques to address an emerging issue in human-robot interaction. While it doesn't introduce a entirely new architecture, it advances the state-of-the-art by highlighting specific gaps in LLM privacy awareness.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like AI ethics and robotics, as it provides practical insights into improving privacy mechanisms for LLM-empowered robots. However, its influence may be limited to specialized applications rather than widespread commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution by uncovering critical privacy discrepancies in LLMs for social robots, making it essential for researchers in AI and robotics to understand these implications for future designs. It represents a valuable but not groundbreaking addition to the field.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0692b2da8d022d6faf348754f63bdfd7fb9724e2",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 21,
      "average_h_index": 4.833333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Dakota Sullivan",
          "profile_url": "https://www.semanticscholar.org/author/2149604409",
          "h_index": 3
        },
        {
          "name": "Shirley Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2330493540",
          "h_index": 1
        },
        {
          "name": "Jennica Li",
          "profile_url": "https://www.semanticscholar.org/author/2352004015",
          "h_index": 1
        },
        {
          "name": "Heather Kirkorian",
          "profile_url": "https://www.semanticscholar.org/author/2295217294",
          "h_index": 2
        },
        {
          "name": "Bilge Mutlu",
          "profile_url": "https://www.semanticscholar.org/author/2127002816",
          "h_index": 1
        },
        {
          "name": "Kassem Fawaz",
          "profile_url": "https://www.semanticscholar.org/author/1910642",
          "h_index": 21
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668432",
      "updated_at": "2025-08-11T23:45:22.834528",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16126",
      "title": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task",
      "authors": [
        "Michael R. Bock",
        "Kara Molisee",
        "Zachary Ozer",
        "Sumit Shah"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Can AI file your taxes? Not yet. Calculating US personal income taxes is a\ntask that requires building an understanding of vast amounts of English text\nand using that knowledge to carefully compute results. We propose TaxCalcBench,\na benchmark for determining models' abilities to calculate personal income tax\nreturns given all of the necessary information. Our experiment shows that\nstate-of-the-art models succeed in calculating less than a third of federal\nincome tax returns even on this simplified sample set. Our analysis concludes\nthat models consistently misuse tax tables, make errors in tax calculation, and\nincorrectly determine eligibility. Our findings point to the need for\nadditional infrastructure to apply LLMs to the personal income tax calculation\ntask.",
      "published_date": "2025-07-22T00:37:59+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16126v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16126v1",
      "latex_url": "http://arxiv.org/src/2507.16126v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "LLMs have become increasingly capable over the past year at coding and math tasks thanks to improvements in reasoning via reinforcement learning. This increase in coding, math, and reasoning ability is evidenced by frontier models' performance improvements on benchmarks like AIME, LiveCodeBench, Aider Polyglot, SWE-bench Verified, and TAU-bench. And while tax calculation has been used in fun LLM demos , we haven't seen LLMs formally tested on their ability to calculate taxes.\n\nIn this paper, we describe tax calculation and filing tasks, the TaxCalcBench benchmark we've created to test models on this task, how the benchmark was created, and the results of testing the latest frontier models on this task.\n\nTax filing is an exercise in once-a-year personal financial data collection. Once you collect all of your information, you (or your accountant) prepare and enter that information into tax filing software. Behind the scenes of that tax filing software is a ``tax engine'' that computes the income, tax liability, credits, etc. that make up your tax return.\n\nTaxCalcBench aims to evaluate model performance on that third and final task: tax calculation. TaxCalcBench is a series of 51 test cases that represent a modest range of personal income tax returns. The test cases include the complete set of user inputs required to compute a tax return and the correct expected output from a traditional tax engine. The Tax Year 2024 (TY24) version of TaxCalcBench includes a set of federal-only tax returns representing just a share of Americans' tax  situations.\n\nOur experiment shows that frontier models cannot reliably calculate taxes. Even the best-performing model can only compute less than a third of returns correctly. When using a less precise evaluation criteria that allows for plus-or-minus \\$5 of tax owed or refund due (which is not allowed in tax calculation, but interesting nonetheless), models get 15-20% more returns correct on an overall  basis.\n\nOur analysis finds that models consistently use incorrect tax tables, make calculation errors, and incorrectly determine eligibility, leading to overall incorrectly computed tax returns.\n\nOur findings point to a continued need for deterministic tax calculation engines to ensure accuracy and the need for additional infrastructure and orchestration to augment LLMs to be able to reliably compute tax returns.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.334,
      "datasets_score": 0.362,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667772",
      "updated_at": "2025-08-11T23:43:05.606854",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16130",
      "title": "Disability Across Cultures: A Human-Centered Audit of Ableism in Western\n  and Indic LLMs",
      "authors": [
        "Mahika Phutane",
        "Aditya Vashistha"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "People with disabilities (PwD) experience disproportionately high levels of\ndiscrimination and hate online, particularly in India, where entrenched stigma\nand limited resources intensify these challenges. Large language models (LLMs)\nare increasingly used to identify and mitigate online hate, yet most research\non online ableism focuses on Western audiences with Western AI models. Are\nthese models adequately equipped to recognize ableist harm in non-Western\nplaces like India? Do localized, Indic language models perform better? To\ninvestigate, we adopted and translated a publicly available ableist speech\ndataset to Hindi, and prompted eight LLMs--four developed in the U.S. (GPT-4,\nGemini, Claude, Llama) and four in India (Krutrim, Nanda, Gajendra,\nAiravata)--to score and explain ableism. In parallel, we recruited 175 PwD from\nboth the U.S. and India to perform the same task, revealing stark differences\nbetween groups. Western LLMs consistently overestimated ableist harm, while\nIndic LLMs underestimated it. Even more concerning, all LLMs were more tolerant\nof ableism when it was expressed in Hindi and asserted Western framings of\nableist harm. In contrast, Indian PwD interpreted harm through intention,\nrelationality, and resilience--emphasizing a desire to inform and educate\nperpetrators. This work provides groundwork for global, inclusive standards of\nableism, demonstrating the need to center local disability experiences in the\ndesign and evaluation of AI systems.",
      "published_date": "2025-07-22T00:51:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16130v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16130v1",
      "latex_url": "http://arxiv.org/src/2507.16130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "People with disabilities (PwD) experience violence, discrimination, and derogatory speech at nearly four times the rate of those without disabilities~. This disparity is even starker in the Global South, where 80% of the world's PwD live . India alone is home to over 60 million PwD (Indian Census, 2011), the vast majority of whom face systemic exclusion with limited access to healthcare, rehabilitation services, and accessible infrastructure . These persistent forms of prejudice and marginalization are collectively termed as ableism\n.\n\nAbleism in India is pervasive. It is often intensified by axes of marginalization such as gender, caste, class, and religion~. Disability is widely seen as a personal tragedy, a karmic intervention from past life, or a moral failure, often addressed through charity aid or medicine to `cure the problem' . Unlike many Western contexts, where disability rights movements have increased visibility for PwD, Indian PwD continue to be stigmatized and isolated from society both offline and online~.\n\nDespite this global reality, most research on ableism, especially online ableism, remains firmly centered on Western experiences and WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations~. This reflects a broader bias in human-centered computing research, where systematic literature reviews found that 73% of CHI and 84% of FAccT papers have findings based on Western participant samples, despite these groups representing less than 12% of the global population. This gap is particularly concerning given that the majority of internet users—and therefore both the targets and perpetrators of online ableism—reside in the Global South.\n\nAI models like toxicity classifiers and LLMs are increasingly used to identify and mitigate online hate .\nHowever, mounting evidence shows that these models often reinforce harmful biases against historically marginalized groups~.\nWhile some recent work has explored how LLMs encode ableist assumptions~, these efforts have primarily focused on Western audiences.\n\nThese challenges raise urgent concerns about the cultural generalizability of AI systems: Can models trained in one context accurately recognize ableist harm in another? Are regionally developed models—such as those trained on Indic datasets—more attuned to local understandings of disability and injustice?\n\nTo investigate these questions, we conducted a comparative study grounded in the perspectives of people with disabilities. We used a publicly available dataset of ableist speech sourced from first-hand accounts shared by PwD~, and recruited 175 PwD from India and the United States to evaluate and explain the harm conveyed in each example. In parallel, we prompted eight large language models, four developed in the United States and four in India, to perform the same task.\n\nBy comparing human and model responses, we analyzed how ableism is interpreted across cultural and computational boundaries.\n\n \n\n \nOur findings revealed a significant misalignment between LLMs and Indian PwD in how ableist harm is assessed and explained. Western LLMs consistently overrated ableist harm compared to PwD, while Indian LLMs tended to underrate it. Notably, all LLMs showed greater tolerance for ableist speech, when expressed in Hindi, exposing problematic cultural biases through language.\n\nThese divergences were not merely statistical—they revealed deeper cultural disconnects. Western LLMs, often trained or fine-tuned on U.S.-centric datasets, were attuned to dominant Western framings of ableism (i.e, ``inspiration porn\"). These framings, by large, were absent in Indian PwD's explanations of ableism. Their interpretations relied on intent and relationality, showing greater tolerance towards ableist comments and expressing a desire to educate. They emphasized resilience as a counter-narrative to pity and charity, and viewed ableism as an intersecting identity with gender, caste, and class. LLMs failed to address or acknowledge such nuances.\n\nOur work makes several contributions.\nWe conduct the first comparative study of how PwD in India and the United States identify and explain ableist speech, highlighting divergent cultural framings rooted in relationality, intent, and intersectionality. This work is also the first to contribute an ableist speech dataset in Hindi, and audit Indian LLMs. Our findings reveal that while Indian LLMs may be multilingual, they remain insufficiently multicultural, and non-intersectional, failing to capture the lived realities of marginalized groups in non-Western contexts. Finally, we offer a methodological framework for evaluating AI systems through disability-centered, cross-cultural perspectives—surfacing the limitations of Western-centric fairness benchmarks and proposing a shift toward culturally grounded harm detection.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "text/01_introduction.tex",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.352,
      "datasets_score": 0.388,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on auditing and comparing the performance of existing large language models (LLMs) in detecting ableism across cultures, using human evaluations from people with disabilities (PwD) in the U.S. and India. It involves prompting LLMs and analyzing their outputs against human judgments, but does not discuss training or fine-tuning models using human feedback to create a reward model or apply reinforcement learning. Since RLHF specifically requires using human-ranked data to train a reward model and fine-tune the main model via reinforcement learning, and no such process is described, the paper's main contribution is unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669215",
      "updated_at": "2025-08-11T23:43:05.607126",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16136",
      "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization",
      "authors": [
        "Eduardo Pacheco",
        "Atila Orhon",
        "Berkin Durmus",
        "Blaise Munyampirwa",
        "Andrey Leonov"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Even state-of-the-art speaker diarization systems exhibit high variance in\nerror rates across different datasets, representing numerous use cases and\ndomains. Furthermore, comparing across systems requires careful application of\nbest practices such as dataset splits and metric definitions to allow for\napples-to-apples comparison. We propose SDBench (Speaker Diarization\nBenchmark), an open-source benchmark suite that integrates 13 diverse datasets\nwith built-in tooling for consistent and fine-grained analysis of speaker\ndiarization performance for various on-device and server-side systems. SDBench\nenables reproducible evaluation and easy integration of new systems over time.\nTo demonstrate the efficacy of SDBench, we built SpeakerKit, an inference\nefficiency-focused system built on top of Pyannote v3. SDBench enabled rapid\nexecution of ablation studies that led to SpeakerKit being 9.6x faster than\nPyannote v3 while achieving comparable error rates. We benchmark 6\nstate-of-the-art systems including Deepgram, AWS Transcribe, and Pyannote AI\nAPI, revealing important trade-offs between accuracy and speed.",
      "published_date": "2025-07-22T01:11:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16136v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16136v2",
      "latex_url": "http://arxiv.org/src/2507.16136v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Speaker diarization, the task of identifying \"who spoke when\" in audio data, is critical for applications such as meeting transcriptions, scribes, and voice assistants. Top-ranking systems on several recent challenges (, , ) have adopted the following multi-stage architecture: (i) speaker segmentation divides local audio windows into speaker-homogeneous segments, (ii) speaker embedding extracts speaker-discriminative representations from each speaker segment, and finally (iii) clustering groups segments by speaker identity based on speaker embeddings. Pyannote is an open-source speaker diarization project that implements this multi-stage architecture. While this architecture lends itself to stage-targeted improvements, researchers would need advanced tooling for fine-grained error analysis and stage-wise evaluation to pursue such improvements.\n\nState-of-the-art speaker diarization systems have recently reached commercially useful quality, prompting companies to productionize them through server-side APIs (, , , ) as well as on-device frameworks (, ). In the presence of numerous open-source projects and proprietary products, consistent and comprehensive benchmarks are key to contextualizing each system's trade-offs and relative performance. However, many of these systems publish either ad-hoc benchmarks or none at all: evaluates systems on a non-standard split of a single dataset without noting the precise version of any system. Furthermore, they provide varying levels of speaker count-related information to some systems and not others and skip the overlapped speech segments. published benchmarks for various versions of Pyannote; however, reproducing these benchmarks and adding new systems remains a significant effort for others. The absence of an open-source benchmark to consistently evaluate and compare systems over time across various domains and use cases hinders the adoption of these systems in production.\n\nFurthermore, open-source academic projects have so far focused on accuracy-related metrics while practical aspects such as system latency remain underexplored . In contrast, proprietary systems such as Picovoice improve efficiency for practical on-device deployment at the cost of accuracy. The absence of a high-accuracy and high-efficiency system hinders the adoption of on-device speaker diarization in production.\n\nOur main contributions can be summarized as follows:\n {itemize}\n   SDBench, an open-source speaker diarization benchmark toolkit designed for fine-grained and consistent evaluation. Using SDBench, we publish benchmarks for 6 systems across 13 datasets and conduct ablation studies to justify various design decisions in Pyannote.\n   SpeakerKit, a speaker diarization system built on top of Pyannote v3 that improves inference efficiency while retaining comparable error rates. SpeakerKit's development was guided by ablation experiments in SDBench, proving its efficacy in stage-targeted improvements.\n {itemize}\n\nWhile we benchmark several systems, this paper does not exhaust all available solutions, and we expect the community to integrate other state-of-the-art systems like NVIDIA NeMo and VBx as they adopt SDBench.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.274,
      "weak_supervision_score": 0.365,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.383,
      "datasets_score": 0.448,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SDBench, a benchmark suite that integrates and evaluates 13 diverse datasets for speaker diarization. This directly aligns with research on benchmarking and evaluating datasets for AI applications, as it provides tools for consistent analysis, fine-grained error assessment, and reproducible evaluations, thereby advancing dataset benchmarking methodologies in machine learning.",
      "summary": "This paper introduces SDBench, an open-source benchmark suite for speaker diarization that integrates 13 diverse datasets and provides tools for consistent, fine-grained performance analysis to enable reproducible evaluations and comparisons across systems. The authors demonstrate its utility by developing SpeakerKit, an optimized version of Pyannote v3 that achieves a 9.6x speed improvement while maintaining comparable error rates, and by benchmarking six state-of-the-art systems to reveal trade-offs between accuracy and efficiency.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a comprehensive, standardized benchmark suite for speaker diarization that addresses inconsistencies in existing evaluations, though it builds on known architectures like Pyannote rather than introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "SDBench is likely to be adopted and built upon in the subfield of audio and speech processing, facilitating easier comparisons and advancements in speaker diarization systems, though its influence may be limited to specialized applications rather than broad commercial or research domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable, practical tool for researchers in speaker diarization, offering standardized benchmarking that could enhance future work, making it essential for those in audio AI to be aware of and utilize.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/652d1a69ee02e06e5baa7e171866984e141f4be3",
      "h_index_fetch_method": "full_id",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 2,
      "average_h_index": 0.6,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Eduardo Pacheco",
          "profile_url": "https://www.semanticscholar.org/author/2372562404",
          "h_index": 0
        },
        {
          "name": "Atila Orhon",
          "profile_url": "https://www.semanticscholar.org/author/2373000707",
          "h_index": 0
        },
        {
          "name": "Berkin Durmus",
          "profile_url": "https://www.semanticscholar.org/author/2182168127",
          "h_index": 2
        },
        {
          "name": "Blaise Munyampirwa",
          "profile_url": "https://www.semanticscholar.org/author/2333424528",
          "h_index": 1
        },
        {
          "name": "Andrey Leonov",
          "profile_url": "https://www.semanticscholar.org/author/2375067085",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669224",
      "updated_at": "2025-08-11T23:45:57.056006",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16144",
      "title": "LongSplat: Online Generalizable 3D Gaussian Splatting from Long Sequence\n  Images",
      "authors": [
        "Guichen Huang",
        "Ruoyu Wang",
        "Xiangjun Gao",
        "Che Sun",
        "Yuwei Wu",
        "Shenghua Gao",
        "Yunde Jia"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "3D Gaussian Splatting achieves high-fidelity novel view synthesis, but its\napplication to online long-sequence scenarios is still limited. Existing\nmethods either rely on slow per-scene optimization or fail to provide efficient\nincremental updates, hindering continuous performance. In this paper, we\npropose LongSplat, an online real-time 3D Gaussian reconstruction framework\ndesigned for long-sequence image input. The core idea is a streaming update\nmechanism that incrementally integrates current-view observations while\nselectively compressing redundant historical Gaussians. Crucial to this\nmechanism is our Gaussian-Image Representation (GIR), a representation that\nencodes 3D Gaussian parameters into a structured, image-like 2D format. GIR\nsimultaneously enables efficient fusion of current-view and historical\nGaussians and identity-aware redundancy compression. These functions enable\nonline reconstruction and adapt the model to long sequences without\noverwhelming memory or computational costs. Furthermore, we leverage an\nexisting image compression method to guide the generation of more compact and\nhigher-quality 3D Gaussians. Extensive evaluations demonstrate that LongSplat\nachieves state-of-the-art efficiency-quality trade-offs in real-time novel view\nsynthesis, delivering real-time reconstruction while reducing Gaussian counts\nby 44\\% compared to existing per-pixel Gaussian prediction methods.",
      "published_date": "2025-07-22T01:43:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16144v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16144v1",
      "latex_url": "http://arxiv.org/src/2507.16144v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Growing interest in 3D scene reconstruction and novel view synthesis has led to rapid advancements in the field, among which 3D Gaussian splatting(3DGS) has gained particular attention for its effectiveness.\nDespite its impressive rendering speed at inference time, most existing methods still rely on slow, per-scene optimization for reconstruction, which can take minutes to hours even for moderately sized environments.\nThis slow optimization is a significant barrier for applications requiring fast, real-time perception and response, such as embodied AI and robotics, where timely adaptation to dynamic environments is essential.\nTo address these challenges, there is an increasing need for systems that can process long sequences of visual data in real-time, dynamically updating with each new frame input while ensuring high-quality reconstruction.\n\nRecent efforts have aimed to improve reconstruction efficiency by developing generalizable splatting models that directly predict 3D Gaussian parameters from images in a feed-forward manner.\nThese methods significantly reduce processing time and perform well under sparse-view settings.\nHowever, their performance often degrades when applied to long sequences or dense multi-view scenarios: the reconstructed Gaussians become increasingly redundant and noisy, resulting in artifacts such as floating points and blurred regions.\nMoreover, memory and computational costs grow rapidly as more views are processed, making these approaches difficult to scale to real-world applications involving hundreds of frames.\nThese limitations arise primarily from two factors: a lack of global historical Gaussians modeling and the absence of an efficient incremental update mechanism, both of which are essential for robust long-term reconstruction.\nAlthough some recent works extending generalizable 3D GS to sequential inputs sets, they still struggle with incremental updates or rely on fixed-length reconstruction pipelines, limiting their flexibility and scalability in online long-sequence scenarios.\n\nIn this paper, we propose LongSplat, an online 3DGS framework designed for real-time, incremental reconstruction from long-sequence images.\nIts core innovation lies in an incremental update mechanism that integrates current-view observations while selectively compressing redundant historical Gaussian.\nThis mechanism efficiently performs two key operations per frame:\n(1) Adaptive Compression: selectively compressing accumulated Gaussians from past views to eliminate redundancy and minimize storage/rendering costs,\nand (2) Online Integration: fusing current-view Gaussians with the historical state.\nThese strategies aim to mitigate a core limitation of generalizable 3DGS: per-pixel prediction inherently produces dense but redundant Gaussians.\nBy progressively refining the Gaussian field over time, our method seeks to improve scalability and memory usage while enhancing consistency across views.\nIn addition, the compression mechanism reduces redundancy and offers a potential path toward dynamic scene modeling, where outdated or redundant elements can be removed in a lightweight, incremental manner without reprocessing the entire sequence.\n\nSpecifically, we propose Gaussian-Image Representation (GIR) that projects 3D Gaussian parameters into a structured 2D image-like format.\nThis representation enables online reconstruction by facilitating the propagation of information across views and supporting localized compression.\nTo enhance cross-view interaction, GIR projects historical Gaussians into the current frame, enabling feature-level fusion.\nThis fusion not only improves the spatial consistency of the reconstructed 3D Gaussian field, but also provides a structured basis for subsequent compression of redundant historical information.\nIn addition, GIR plays a central role in localized compression by maintaining the mapping between 2D projections and their corresponding historical 3D Gaussians.\nThis identity-aware structure makes 3DGS more tractable and removes redundant splats accumulated over time.\nSuch compression not only reduces memory and rendering cost, but also improves visual quality by eliminating overlapping or outdated Gaussians.\nFurthermore, we leverage GIR's image-like structure to apply supervision from ground-truth 3DGS, using an optimized per-scene Gaussian dataset constructed with existing image compression techniques .\nThis strategy improves both compactness and fidelity of the learned 3D Gaussians without requiring full 3D loss computation.\n\nThrough extensive evaluations, we demonstrate that LongSplat achieves state-of-the-art efficiency-quality trade-offs in real-time novel view synthesis.\nOur method achieves real-time rendering and reduces Gaussian counts by 44% on DL3DV.\nMoreover, LongSplat outperforms the baseline by 3.6 dB in PSNR on the DL3DV benchmark and exhibits superior scalability for long-sequence scene reconstruction. By efficiently processing long-sequence visual data, LongSplat opens up new possibilities for real-time 3D perception in applications that require handling extensive visual inputs.\nOur contributions can be summarized as follows:\n {itemize}\n   We propose LongSplat, a real-time 3D Gaussian reconstruction framework tailored for arbitrary-view, long-sequence image inputs. By introducing a 3D Gaussian updating mechanism that selectively compresses redundant historical Gaussians and incrementally integrates current-view observations, LongSplat enables scalable, memory-efficient reconstruction and real-time novel view synthesis.\n   We introduce Gaussian-Image Representation(GIR), a structured 2D representation of 3D Gaussians that enables efficient historical feature fusion, redundancy compression, lightweight 2D operations, and GIR-space supervision.\n   Extensive experiments show that LongSplat achieves state-of-the-art real-time novel view synthesis, providing real-time rendering and reducing Gaussian counts by 44% compared to existing methods.\n {itemize}\n\n {figure*}[t]\n  \n  [width=1.0 ]{figures/framework.pdf}\n  {Overview of the Longsplat framework.\nGiven an input image sequence $\\{I_t\\}_{t=1}^T$, our model incrementally constructs a global 3D Gaussian scene representation $ {G}^g$ through iterative frame-wise updates.\nAt each timestep $t$, we extract two complementary feature streams: (1) a multi-view spatial feature map $ {F}_c$ from the current frame and its temporally adjacent neighbors using the DepthSplat pipeline, providing local geometry and appearance cues; and (2) a historical context feature map $ {F}_h$ by rendering the accumulated global Gaussians $ {G}^g_{t-1}$ into a 2D Gaussian-Image Representation (GIR) via differentiable projection. These streams are fused via a transformer-based module to produce a fused representation $ {F}_f$, from which we derive an adaptive update mask $ {M}_t$ and generate current-frame Gaussians $ {G}_t^c$. The global representation $ {G}^g$ is then selectively updated, enabling efficient long-sequence reconstruction with spatial-temporal consistency.}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.277,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.4,
      "distributed_training_score": 0.353,
      "datasets_score": 0.259,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an online 3D Gaussian Splatting framework for real-time reconstruction from long-sequence images, focusing on incremental updates and compression techniques. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667942",
      "updated_at": "2025-08-11T23:43:05.606893",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16145",
      "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series\n  with Clinical Validation in COPD Reporting",
      "authors": [
        "Shuhao Mei",
        "Yongchao Long",
        "Shan Cao",
        "Xiaobo Han",
        "Shijia Geng",
        "Jinbo Sun",
        "Yuxi Zhou",
        "Shenda Hong"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools.",
      "published_date": "2025-07-22T01:44:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16145v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16145v1",
      "latex_url": "http://arxiv.org/src/2507.16145v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory disease characterized by persistent airflow limitation, is one of the leading causes of disability and mortality worldwide. In the clinical diagnosis and management of COPD, the Pulmonary Function Test (PFT) and its corresponding spirogram curve are indispensable. They not only represent the gold standard for diagnosis but also serve as a crucial basis for assessing disease severity, monitoring progression, and guiding treatment strategies. However, the accurate interpretation of spirogram curves and the subsequent drafting of a standardized yet personalized diagnostic report are time-consuming, labor-intensive processes that are highly dependent on the specialized knowledge and long-term experience of clinicians. This reliance on expert resources is particularly pronounced in regions with limited medical access, creating a significant bottleneck in improving the efficiency and standardization of COPD diagnosis.\n\nTo address this challenge, researchers have begun exploring the use of Artificial Intelligence (AI) to automate diagnostics. In our prior work, we developed DeepSpiro , a model that demonstrated the feasibility of using deep learning to identify COPD-related features directly from spirogram curves. However, this and other early deep learning models were limited by their \"black-box\" nature, outputting only simple classification labels. Their inability to provide a rationale for their conclusions has hindered their clinical adoption and trust. More recently, the advent of Large Language Models (LLMs) has shown great promise in addressing this interpretability issue, with their ability to generate logically coherent medical texts that emulate the style of human experts . Nevertheless, applying LLMs to generate diagnostic reports directly from raw pulmonary function data still faces three core challenges:\n\n {figure*}[]\n { [width=1 ]{overview.pdf}}\n {This figure compares three workflows for pulmonary function assessment: the traditional clinical model (A), which relies on cumbersome in-clinic testing; the traditional large language model (B), which cannot understand raw physiological signals; and our proposed SpiroLLM framework (C), which supports at-home self-testing and instant generation of professional reports, significantly improving efficiency.}\n\n {figure*}\n\n {itemize}\n  A fundamental disconnect exists in current approaches. On one hand, vision-based or sequential models can process spirogram curves but cannot generate comprehensive reports. On the other hand, LLMs excel at processing textualized PFT numerical data but cannot directly \"see\" and interpret the rich morphological information embedded in the waveforms. A unified, end-to-end framework that seamlessly integrates both modalities is currently lacking.\n  Training a reliable report generation model requires a massive volume of high-quality, expert-authored reports as supervision signals. In clinical practice, it is infeasible to have specialists manually annotate tens of thousands of samples, creating a critical bottleneck at the data level.\n  The evaluation of current generative models largely relies on conventional text-similarity metrics (e.g., ROUGE, BLEU). These metrics fail to effectively measure performance along critical dimensions such as medical factual accuracy, logical coherence, and clinical safety, and thus do not reflect the true clinical utility of the models.\n {itemize}\n\nTo address the aforementioned challenges, we leveraged the authoritative, large-scale UK Biobank (UKB) to develop and validate SpiroLLM—a framework for COPD diagnostic report generation based on multimodal fusion and large language models (as shown in Figure ). The main contributions of this study are as follows:\n\n {itemize}\n   Building on our prior work in spirogram feature analysis, we are the first to design and implement SpiroLLM, which seamlessly integrates a specialized SpiroEncoder (for encoding spirogram curves) with an LLM via a lightweight alignment module, the SpiroProjector. This architecture achieves, for the first time, a deep fusion of visual features from time-series waveforms and textual PFT metrics, enabling the model to perform end-to-end diagnostic report generation.\n   To alleviate the scarcity of annotated data, we developed a semi-automated report generation pipeline. This pipeline combines a vision-language model, a quantitative metric calculation module, and a Retrieval-Augmented Generation mechanism based on GOLD guidelines. This process significantly reduces the cost and burden of manual annotation while ensuring the authoritativeness of the diagnostic logic.\n   We adopted an \"LLM-as-a-Judge\" approach to establish an evaluation framework spanning six clinical dimensions, including factual accuracy, logical consistency, and completeness. Furthermore, through meticulously designed input masking experiments, we quantitatively verify the superior robustness of our multimodal approach compared to single-modality methods and confirm the independent diagnostic contribution of visual features.\n {itemize}\n\nSpiroLLM is not only a technical innovation but also poised to become a powerful assistant for clinicians. By enhancing the efficiency and consistency of diagnostic report writing, it promises to ultimately improve patient care experiences and long-term health management.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.432,
      "distributed_training_score": 0.364,
      "datasets_score": 0.352,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on finetuning LLMs for multimodal understanding of spirogram data and generating diagnostic reports, using techniques like feature encoding and alignment. It does not involve training a reward model on human-ranked data or using reinforcement learning to align the model with human preferences, as required for RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a multimodal LLM framework for processing spirogram data and report generation, without any adaptation of diffusion models for iterative refinement or multi-step logical reasoning. There is no mention of treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668442",
      "updated_at": "2025-08-11T23:43:05.607005",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16151",
      "title": "SPACT18: Spiking Human Action Recognition Benchmark Dataset with\n  Complementary RGB and Thermal Modalities",
      "authors": [
        "Yasser Ashraf",
        "Ahmed Sharshar",
        "Velibor Bojkovic",
        "Bin Gu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by\naccumulating light intensities at each pixel, offering ultra-high energy\nefficiency and exceptional temporal resolution. Unlike event cameras, which\nrecord changes in light intensity to capture motion, spike cameras provide even\nfiner spatiotemporal resolution and a more precise representation of continuous\nchanges. In this paper, we introduce the first video action recognition (VAR)\ndataset using spike camera, alongside synchronized RGB and thermal modalities,\nto enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By\npreserving the inherent sparsity and temporal precision of spiking data, our\nthree datasets offer a unique platform for exploring multimodal video\nunderstanding and serve as a valuable resource for directly comparing spiking,\nthermal, and RGB modalities. This work contributes a novel dataset that will\ndrive research in energy-efficient, ultra-low-power video understanding,\nspecifically for action recognition tasks using spike-based data.",
      "published_date": "2025-07-22T01:59:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16151v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16151v1",
      "latex_url": "http://arxiv.org/src/2507.16151v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Video Action Recognition (VAR) is a key task in computer vision that focuses on detecting and classifying actions or activities from video sequences automatically  {var_def_2021}. Unlike image classification, which can be seen as analysis of an individual frame, VAR captures both spatial and temporal dynamics, adding complexity such as heavy information redundancy introduced by the temporal dimension, motion variations, changes in lighting or camera angles  {var_complexity_2024}, just to name a few. These complexities make VAR highly valuable for real-world applications such as surveillance, healthcare, industrial automation, and sports analysis  {var_applications}, where rapid and accurate processing is essential.\n\nSpiking Neural Networks (SNNs), inspired by biological neurons, offer a promising alternative to traditional Artificial Neural Networks (ANNs) for tasks involving temporal dynamics, such as VAR  {snn_review_2023}. SNNs operate on sparse, discrete spikes, enabling event-driven computation, which significantly reduces energy consumption compared to ANNs  {roy2019towards}. This makes SNNs particularly suited for energy-constrained environments like autonomous robots and edge devices  {snns_edge_2022}. However, despite their energy efficiency, video understanding with SNNs remains limited. Current research on VAR using SNNs typically relies on converting conventional RGB video data into spike trains, leading to information loss and constraining the full potential of SNNs  {var_complexity_2024}.\n\nA key limitation in existing research is the scarcity of spiking datasets that capture the rich temporal dynamics of real-world video sequences. While event cameras capture data based on brightness changes, spike cameras operate by generating spikes when the photon accumulation at each pixel surpasses a threshold. This enables spike cameras to capture absolute brightness at ultra-high sampling frequencies (up to 20,000 Hz), providing textural spatiotemporal details than event cameras  {dvs128_2021}. Despite these advantages, existing datasets, such as DVS128 (see Section ), are based on event cameras and lack the richness needed for complex action recognition tasks.\n\nIn the context of SNNs, video data presents unique challenges. Videos inherently involve temporal sequences, which align naturally with the temporal nature of SNNs. Any temporally structured data can be treated as a video, provided that each time step has a corresponding visual representation. When processing temporal input with an SNN, two distinct approaches arise: (1) aligning the temporal dimension of the input with the native temporal axis of the SNN model, or (2) encoding the entire video input separately from the SNN’s native temporal axis. In the latter approach, the video is encoded as spike train as a whole, hence another temporal dimension is added which coincides with the SNN’s temporal axis.\n\nMotivated by the lack of a complex spiking dataset suitable to challenge and push forward the SNNs capacity to address VAR task, we introduce a novel dataset specifically designed for human action recognition using spike cameras. In addition, we provide spiking data with synchronized RGB and thermal modalities to enhance motion capture in low-light conditioning and provide a comprehensive multimodal representation of human actions. By combining these modalities, we provide a framework to explore the complementary information across different sensor types, with the aim of improving the robustness and diversity of SNN models for action recognition tasks.\n\nOur datasets are collected from 44 participants, representing a wide range of demographic factors such as age, height, weight, sex, and ethnicity. Each participant performed 18 distinct daily actions (Figure ~), captured over two sessions, resulting in a total dataset duration of 264 minutes. To establish a baseline for comparison, we utilized state-of-the-art (SOTA) architecture based on convolutional neural networks (CNNs) and transformer blocks to provide a baseline model for our datasets in both ANN and SNN through ANN-SNN conversion. To summarize, our key contributions are as follows:\n\n {itemize}\n\n   We introduce SPACT18, the first VAR dataset captured using spike camera, setting a new benchmark for SNN-based models, and extend it with synchronized RGB and thermal modalities for comprehensive multimodal benchmarking.\n\n   We propose a compression algorithm for the spiking dataset, yielding new spiking datasets with lower native latency, while preserving critical temporal information, providing a framework for preprocessing and compression for the research community.\n\n  \n We evaluate our dataset across modalities using SOTA lightweight ANN models and SNN baseline obtained through ANN-SNN conversion, and direct training, highlighting critical challenges in spiking video recognition, such as the high latency in ANN-SNN and low accuracy in direct training, and providing novel research areas for optimizing SNNs.\n\n {itemize}\n\n {figure*}[ht]\n  \n  { }{3pt}\n  {tabular}{cccccc}\n   Running in Place &\n   Walking &\n   Jogging &\n   Clapping &\n   Right Hand Waving&\n   Left Hand Waving\n\n  [width=0.15 ]{Images/running.pdf} &\n\n  [width=0.15 ]{Images/walking.pdf} &\n\n  [width=0.15 ]{Images/jogging.pdf} &\n\n  [width=0.15 ]{Images/clapping.pdf} &\n\n  [width=0.15 ]{Images/waving_right.pdf} &\n\n  [width=0.15 ]{Images/wavingleft.pdf}\n\n   Drinking &\n   Playing Drums &\n   Forearm Roll &\n   Playing Guitar&\n   Jump in Place &\n   Squats\n\n  [width=0.15 ]{Images/drinking.pdf} &\n\n  [width=0.15 ]{Images/drums.pdf} &\n\n  [width=0.15 ]{Images/roll.pdf} &\n\n  [width=0.15 ]{Images/guitar.pdf} &\n\n  [width=0.15 ]{Images/jumping.pdf} &\n\n  [width=0.15 ]{Images/squat.pdf}\n\n   Arms Circling &\n   Side Butterfly&\n   Frontal Butterfly &\n   Stand Abs &\n   Boxing &\n   Jumping Jacks\n\n  [width=0.15 ]{Images/circling.pdf} &\n\n  [width=0.15 ]{Images/sidebutterfly.pdf} &\n\n  [width=0.15 ]{Images/frontbutterfly.pdf} &\n\n  [width=0.15 ]{Images/abs.pdf} &\n\n  [width=0.15 ]{Images/boxing.pdf} &\n\n  [width=0.15 ]{Images/jumpingjacks.pdf}\n\n  {tabular}\n  {Sample output frame from the spike camera for each action of the same subject. Texture reconstruction via TFP  {dong2021spike} with window=200.}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "iclr2025_conference.tex",
      "rlhf_score": 0.292,
      "weak_supervision_score": 0.277,
      "diffusion_reasoning_score": 0.281,
      "distributed_training_score": 0.339,
      "datasets_score": 0.378,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667442",
      "updated_at": "2025-08-11T23:43:05.606769",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16154",
      "title": "LSSGen: Leveraging Latent Space Scaling in Flow and Diffusion for\n  Efficient Text to Image Generation",
      "authors": [
        "Jyun-Ze Tang",
        "Chih-Fan Hsu",
        "Jeng-Lin Li",
        "Ming-Ching Chang",
        "Wei-Chao Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Flow matching and diffusion models have shown impressive results in\ntext-to-image generation, producing photorealistic images through an iterative\ndenoising process. A common strategy to speed up synthesis is to perform early\ndenoising at lower resolutions. However, traditional methods that downscale and\nupscale in pixel space often introduce artifacts and distortions. These issues\narise when the upscaled images are re-encoded into the latent space, leading to\ndegraded final image quality. To address this, we propose {\\bf Latent Space\nScaling Generation (LSSGen)}, a framework that performs resolution scaling\ndirectly in the latent space using a lightweight latent upsampler. Without\naltering the Transformer or U-Net architecture, LSSGen improves both efficiency\nand visual quality while supporting flexible multi-resolution generation. Our\ncomprehensive evaluation covering text-image alignment and perceptual quality\nshows that LSSGen significantly outperforms conventional scaling approaches.\nWhen generating $1024^2$ images at similar speeds, it achieves up to 246\\%\nTOPIQ score improvement.",
      "published_date": "2025-07-22T02:05:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16154v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16154v1",
      "latex_url": "http://arxiv.org/src/2507.16154v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent advances in { } and { } have revolutionized text-to-image synthesis, achieving unprecedented image quality. Despite their differing mathematical foundations, both are implemented as continuous denoising processes based on probabilistic path learning~. However, this iterative nature leads to slow generation~, and the computational cost grows quadratically with image resolution~.\n\nA common approach to this challenge is a progressive, coarse-to-fine generation pipeline~, which generates a base-resolution image and then upscales it beyond the training resolution in pixel space. While this helps preserve semantic structure at higher resolutions, pixel-space upscaling often introduces artifacts like blurriness (Fig.~). These artifacts arise because re-encoding the upscaled image back into the latent space distorts the underlying latent features. Without a clear understanding of these distortions, it remains difficult to resolve the speed-quality trade-off in FM and DM models.\n\nBoth { } and { } generate images by gradually transforming Gaussian noise into a clear image. This denoising process can be viewed as predicting a velocity field that moves the sample from a blurry state toward a sharp one. From an information-theoretic perspective, this progression aligns with a process that first captures low-frequency components and then incrementally adds high-frequency details~. We hypothesize that this natural blurry-to-sharp progression can be efficiently approximated by a {  resolution-based} approach: starting with a low-resolution image that captures the scene’s structure, and progressively increasing the resolution to introduce fine details and improve visual clarity.\n\nBuilding on this principle, we propose the {   } framework, which replaces the early stages of image generation with lower-resolution processing. Rather than scaling images in pixel space, LSSGen progressively upsamples features directly in the latent space using a dedicated latent upsampler. This approach avoids the artifacts commonly introduced by pixel-space resolution changes. To further address the shift in noise characteristics during scaling, we propose a novel {  noise compensation} and {  rescheduling} strategy. This ensures consistency between noise and data across stages and dynamically adjusts the denoising process for improved stability and image quality.\n\nNotably, our latent upsampler is VAE-dependent, making it reusable across different flow and diffusion models that share the same VAE. This architectural decoupling allows the upsampler to work purely on VAE-derived latent features, making the upsampler agnostic to the underlying generative architecture ({  e.g.}, U-Net or Transformer). It provides a versatile, {  train-once}, {  use-across-models} solution.\n\nExtensive experiments show that { } achieves a $1.5  $ speedup for $1024^2$ image generation while maintaining comparable quality across four standard evaluation metrics: GenEval~{}, CLIP-IQA~, TOPIQ~, and NIQE~. At $2048^2$ resolution, the benefits are even greater due to the quadratic increase in computational cost with image size. These results establish LSSGen as a practical and scalable solution for efficient high-resolution image synthesis. Additionally, our experiments show that LSSGen offers superior global semantic preservation compared to previous methods, particularly when generating images beyond the original training resolution.\n\nOur main contributions are summarized as follows:\n {itemize}[leftmargin=10pt]   -.1em\n\n  We introduce { }, a latent space scaling framework applicable to a wide range of generative models. It includes a novel latent upsampling method and a noise compensation and rescheduling strategy.\n\n  We demonstrate through extensive experiments that { } achieves a strong balance between computational efficiency and image quality, outperforming baseline methods in both areas.\n\n  We present a detailed analysis comparing pixel-space and latent-space transformations, offering fresh insights into the dynamics of multi-resolution image generation.\n\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.388,
      "datasets_score": 0.308,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for efficient text-to-image generation using diffusion models, focusing on latent space scaling to improve speed and quality in visual synthesis. It does not involve adapting diffusion for multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks, which are central to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667470",
      "updated_at": "2025-08-11T23:43:05.606780",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16158",
      "title": "AMMNet: An Asymmetric Multi-Modal Network for Remote Sensing Semantic\n  Segmentation",
      "authors": [
        "Hui Ye",
        "Haodong Chen",
        "Zeke Zexi Hu",
        "Xiaoming Chen",
        "Yuk Ying Chung"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Semantic segmentation in remote sensing (RS) has advanced significantly with\nthe incorporation of multi-modal data, particularly the integration of RGB\nimagery and the Digital Surface Model (DSM), which provides complementary\ncontextual and structural information about the ground object. However,\nintegrating RGB and DSM often faces two major limitations: increased\ncomputational complexity due to architectural redundancy, and degraded\nsegmentation performance caused by modality misalignment. These issues\nundermine the efficiency and robustness of semantic segmentation, particularly\nin complex urban environments where precise multi-modal integration is\nessential. To overcome these limitations, we propose Asymmetric Multi-Modal\nNetwork (AMMNet), a novel asymmetric architecture that achieves robust and\nefficient semantic segmentation through three designs tailored for RGB-DSM\ninput pairs. To reduce architectural redundancy, the Asymmetric Dual Encoder\n(ADE) module assigns representational capacity based on modality-specific\ncharacteristics, employing a deeper encoder for RGB imagery to capture rich\ncontextual information and a lightweight encoder for DSM to extract sparse\nstructural features. Besides, to facilitate modality alignment, the Asymmetric\nPrior Fuser (APF) integrates a modality-aware prior matrix into the fusion\nprocess, enabling the generation of structure-aware contextual features.\nAdditionally, the Distribution Alignment (DA) module enhances cross-modal\ncompatibility by aligning feature distributions through divergence\nminimization. Extensive experiments on the ISPRS Vaihingen and Potsdam datasets\ndemonstrate that AMMNet attains state-of-the-art segmentation accuracy among\nmulti-modal networks while reducing computational and memory requirements.",
      "published_date": "2025-07-22T02:07:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16158v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16158v1",
      "latex_url": "http://arxiv.org/src/2507.16158v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{R}{emote} sensing semantic segmentation is a fundamental task in geoscientific research, enabling the transformation of raw multispectral, hyperspectral, or LiDAR data into structured spatial representations.\nBy generating pixel-level classification of land cover and land use, this process supports accurate geospatial analysis and serves as a foundational tool for numerous remote sensing applications, such as environmental monitoring~, resource management~, and disaster assessment~.\n\nThe increasing complexity of Earth observation missions and the growing demand for fine-grained analysis have driven the advancement of remote sensing semantic segmentation techniques.\nEspecially, recent progress in deep neural networks has enabled high-resolution, task-specific perception across a wide range of remote sensing applications.\n\nHowever, the robustness of deep neural networks relying on single-modal input is often compromised under complex remote sensing (RS) conditions, including illumination changes, sensor noise, occlusions, and seasonal variations.\n\nMulti-modal learning, which refers to the integration of heterogeneous data sources for a specific task, is inherently well-suited for RS due to the availability of diverse data modalities such as RGB imagery and Digital Surface Models (DSM).\nAn example of these input modalities is illustrated in Fig.~.\n\nRGB imagery, typically derived from the visible portion of the electromagnetic spectrum, primarily captures contextual information~.\nHowever, it is highly susceptible to external disturbances such as cloud cover, fog, and atmospheric aerosols~, which can induce intra-class variability and inter-class similarity~, thereby posing challenges to robust semantic segmentation.\n\nIn contrast, Digital Surface Models (DSM) provide elevation-based structural information that, although lacking rich semantic content, offers a distinct and complementary perspective to RGB imagery.\nThis structural cue is particularly valuable for distinguishing objects with high contextual similarity but varying physical heights, such as roads versus buildings or low vegetation versus trees.\n\nAlthough previous studies~ have demonstrated the promising potential of multi-modality semantic segmentation, several critical limitations still hinder its effectiveness.\n\nOne of the limitations is architectural redundancy~, wherein the inclusion of extra encoder and fusion components leads to increased computational complexity.\n\nAnother fundamental issue is modality misalignment~, which stems from the inherent heterogeneity between modalities.\nIf not properly addressed, such misalignment can introduce irrelevant or conflicting information, leading to performance degradation.\n\nTo address these limitations, we propose the Asymmetric Multi-Modal Network (AMMNet), which incorporates three asymmetric modules.\n\nTo reduce architectural redundancy, we adopt an asymmetric encoder design, termed the Asymmetric Dual Encoder (ADE), which employs modality-specific encoders tailored to the distinct characteristics of each modality.\nSpecifically, the RGB branch is assigned a greater capacity to extract dense contextual information, while the DSM branch uses a lightweight encoder to extract sparse structural information.\n\nIn addition, two asymmetric designs are introduced to address modality misalignment:\nThe Asymmetric Prior Fuser (APF) integrates RGB and DSM features through a modality-aware prior matrix, generating structure-aware contextual representation.\nThe Distribution Alignment (DA) module further minimizes the distributional divergence between the DSM and RGB features, enhancing cross-modal compatibility.\n\n {figure}[t]\n  \n  [width=.95 ]{figs/input_vis.pdf}\n  {Visualization of the complementary characteristics between RGB and DSM modalities. Both RGB and DSM provide distinct yet complementary information, where each modality compensates for the ambiguities or limitations of the other.}\n\n {figure}\n\nTherefore, the main contributions of this work are summarized as follows:\n {itemize}\n   To achieve efficient and robust semantic segmentation in a complex urban environment, we propose AMMNet, a novel multi-modal network that employs modality-specific asymmetric designs to effectively leverage RGB-DSM modality pairs.\n\n   We introduce the Asymmetric Dual Encoder (ADE), which employs modality-specific encoder configurations tailored to the distinct characteristics of RGB-DSM inputs, enabling reduced computational complexity while maintaining representational capacity.\n\n   To address modality misalignment, we propose two asymmetric modules. The Asymmetric Prior Fuser (APF) constructs a modality-aware prior matrix to generate structure-aware contextual features, while the Distribution Alignment (DA) module enhances cross-modal compatibility between RGB and DSM inputs.\n\n   Extensive experiments on two benchmark datasets, ISPRS Vaihingen and Potsdam, demonstrate that AMMNet achieves superior semantic segmentation performance in complex urban environments while maintaining computational efficiency, outperforming advanced multi-modal approaches.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.36,
      "distributed_training_score": 0.363,
      "datasets_score": 0.386,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667950",
      "updated_at": "2025-08-11T23:43:05.606895",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16164",
      "title": "Attacking interpretable NLP systems",
      "authors": [
        "Eldor Abdukhamidov",
        "Tamer Abuhmed",
        "Joanna C. S. Santos",
        "Mohammed Abuhamad"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Studies have shown that machine learning systems are vulnerable to\nadversarial examples in theory and practice. Where previous attacks have\nfocused mainly on visual models that exploit the difference between human and\nmachine perception, text-based models have also fallen victim to these attacks.\nHowever, these attacks often fail to maintain the semantic meaning of the text\nand similarity. This paper introduces AdvChar, a black-box attack on\nInterpretable Natural Language Processing Systems, designed to mislead the\nclassifier while keeping the interpretation similar to benign inputs, thus\nexploiting trust in system transparency. AdvChar achieves this by making less\nnoticeable modifications to text input, forcing the deep learning classifier to\nmake incorrect predictions and preserve the original interpretation. We use an\ninterpretation-focused scoring approach to determine the most critical tokens\nthat, when changed, can cause the classifier to misclassify the input. We apply\nsimple character-level modifications to measure the importance of tokens,\nminimizing the difference between the original and new text while generating\nadversarial interpretations similar to benign ones. We thoroughly evaluated\nAdvChar by testing it against seven NLP models and three interpretation models\nusing benchmark datasets for the classification task. Our experiments show that\nAdvChar can significantly reduce the prediction accuracy of current deep\nlearning models by altering just two characters on average in input samples.",
      "published_date": "2025-07-22T02:20:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16164v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16164v1",
      "latex_url": "http://arxiv.org/src/2507.16164v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Deep learning models, particularly in Natural Language Processing (NLP), have revolutionized how machines understand and interact with human language. These advancements have enabled various applications, from simple spellcheck and keyword search to complex tasks such as sentiment analysis~, machine translation~, and chatbot interactions~. The integration of NLP into our daily digital interactions, such as through search engines, virtual assistants, and recommendation systems, highlights its importance. However, these models are shown to be susceptible to adversarial attacks .\n\nAdversarial attacks in NLP, which involve careful manipulations of input data leading to incorrect model outputs, are a growing concern. These attacks are especially stealthy because of the complex nature of human language, which is filled with idioms, metaphors, and context-dependent meanings~. It is essential to rigorously research and develop effective methods to counter these adversarial attacks, given the growing integration of such models in various NLP applications across different sectors.\nFor example, adversarial attacks on AI systems for social media moderation, opinion analysis, customer reviews, or market trends could result in failures to identify harmful content, misinterpretation of benign posts, and misleading analyses, ultimately leading to poor decisions~.\n\nBy coupling NLP classifiers with interpreters,   creating\nInterpretable NLP Systems (INLPS), adversarial manipulations can be recognized by an observer (see  {fig:intro_images}).\nINLPS offer transparency in the decision-making process, which is especially crucial for applications where understanding the reasoning behind decisions is as important as the decisions themselves.\nAlthough this transparency is designed to promote trust and accountability, it can inadvertently introduce vulnerabilities, offering potential attackers a roadmap to manipulate the outcomes.\nThis vulnerability is not just a theoretical concern but a practical challenge that needs to be addressed to ensure the reliability and safety of AI applications.\nIn highlighting the vulnerabilities introduced by INLPS, our research goes a step further by demonstrating these theoretical concerns in a practical context.\n {Our attack,  {}, targets Interpretable Natural Language Processing Systems (INLPS), which integrate NLP classifiers with interpreters to deliver predictions and explanations. The attack has two objectives: (1) to mislead the classifier into misclassifying the input, and (2) to ensure that the interpreter produces an explanation similar to that of a benign input.}\nWe focus on text classification tasks using large language models (LLMs) and diverse interpretation methods.\nBy changing certain words at the character level,  {} is designed to mislead both the primary NLP classifier and its interpreter.\n {} demonstrates that the strengths of INLPS can also become weaknesses (as shown in  {fig:intro_images}), emphasizing the need for robust countermeasures.\n\n {figure}[t]\n  \n  {justification=justified}\n  [width= ]{figures/example_nlp_v2.pdf}\n  { {Example texts comparing a benign sample and a sample subject to our proposed attack, along with their corresponding interpretations based on LIME interpreter. Both inputs generate similar interpretations regardless of differences.}}\n\n {figure}\n\n {We evaluate our attack using three distinct datasets: the Stanford Sentiment Treebank (SST-2), AG News, and Yahoo Answers datasets. SST-2 (with binary sentiment classification) serves as a basic test for our attack, while AG News (with four-category news topic classification) and Yahoo Answers (with ten-category question classification) provide more complex scenarios to assess the effectiveness of the attack.}\n\nWe selected a diverse range of LLM models,   GPT-2, BERT, DistilBERT, Electra, CANINE, FNet, and XLM-R, to provide a comprehensive analysis across these models.\nFurthermore, our study incorporates three interpretation models,   SHAP , Saliency Maps , and LIME , each providing unique insights into the model decision-making process.\n {Our findings show that the attack reaches a success rate of 79% in the AG News dataset using LIME interpreter with CANINE model. In the SST-2 and Yahoo Answers datasets, it achieves a success rate of 79% (with Saliency Map interpreter and BERT model) and 80% (with LIME interpreter and BERT model), respectively. The effectiveness of the attack varies by models and interpreters, with LIME generally outperforming others, especially on complex tasks.}\n\n {Contributions} We summarize our contributions as follows:\n {itemize}[leftmargin=1em]\n   We propose a stealthy and query-efficient black-box attack targeting INLPS. We evaluate the attack performance against seven classifiers when coupled with three different interpreters using {three} datasets. The evaluations show that the proposed attack achieves considerable success rates, highlighting the attack's effectiveness in various scenarios.\n   We investigate the transferability of adversarial examples across various classifiers and interpreters to assess whether adversarial samples designed for a certain model and interpreter can expose vulnerabilities in other models and interpreters, highlighting areas of concern in the field.\n   We perform a comparative analysis to examine several factors (  input length, amount of perturbation,  ) that might influence the performance of the attack.\n {itemize}\n\n {Organization} Our paper is organized as follows:\n {sec:related} reviews related research studies;\n  {sec:notations} describes the notations and terms used in the paper;\n  {sec:notations} presents the proposed attack and its underlying mechanisms;\n  {sec:evaluation} provides the results of the attack effectiveness, robustness, and transferability against LLMs and interpretation models;\n  {sec:discussion} explains the existing limitations and future work; and  {sec:conc} concludes the paper.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.357,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.308,
      "datasets_score": 0.332,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669233",
      "updated_at": "2025-08-11T23:43:05.607128",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16172",
      "title": "AtrousMamaba: An Atrous-Window Scanning Visual State Space Model for\n  Remote Sensing Change Detection",
      "authors": [
        "Tao Wang",
        "Tiecheng Bai",
        "Chao Xu",
        "Bin Liu",
        "Erlei Zhang",
        "Jiyun Huang",
        "Hongming Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recently, a novel visual state space (VSS) model, referred to as Mamba, has\ndemonstrated significant progress in modeling long sequences with linear\ncomplexity, comparable to Transformer models, thereby enhancing its\nadaptability for processing visual data. Although most methods aim to enhance\nthe global receptive field by directly modifying Mamba's scanning mechanism,\nthey tend to overlook the critical importance of local information in dense\nprediction tasks. Additionally, whether Mamba can effectively extract local\nfeatures as convolutional neural networks (CNNs) do remains an open question\nthat merits further investigation. In this paper, We propose a novel model,\nAtrousMamba, which effectively balances the extraction of fine-grained local\ndetails with the integration of global contextual information. Specifically,\nour method incorporates an atrous-window selective scan mechanism, enabling a\ngradual expansion of the scanning range with adjustable rates. This design\nshortens the distance between adjacent tokens, enabling the model to\neffectively capture fine-grained local features and global context. By\nleveraging the atrous window scan visual state space (AWVSS) module, we design\ndedicated end-to-end Mamba-based frameworks for binary change detection (BCD)\nand semantic change detection (SCD), referred to as AWMambaBCD and AWMambaSCD,\nrespectively. Experimental results on six benchmark datasets show that the\nproposed framework outperforms existing CNN-based, Transformer-based, and\nMamba-based methods. These findings clearly demonstrate that Mamba not only\ncaptures long-range dependencies in visual data but also effectively preserves\nfine-grained local details.",
      "published_date": "2025-07-22T02:36:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16172v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16172v1",
      "latex_url": "http://arxiv.org/src/2507.16172v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.254,
      "weak_supervision_score": 0.311,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.304,
      "datasets_score": 0.288,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.667959",
      "updated_at": "2025-08-11T23:43:05.606898",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16178",
      "title": "LLM Data Selection and Utilization via Dynamic Bi-level Optimization",
      "authors": [
        "Yang Yu",
        "Kai Han",
        "Hang Zhou",
        "Yehui Tang",
        "Kaiqi Huang",
        "Yunhe Wang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While large-scale training data is fundamental for developing capable large\nlanguage models (LLMs), strategically selecting high-quality data has emerged\nas a critical approach to enhance training efficiency and reduce computational\ncosts. Current data selection methodologies predominantly rely on static,\ntraining-agnostic criteria, failing to account for the dynamic model training\nand data interactions. In this paper, we propose a new Data Weighting Model\n(DWM) to adjust the weight of selected data within each batch to achieve a\ndynamic data utilization during LLM training. Specially, to better capture the\ndynamic data preference of the trained model, a bi-level optimization framework\nis implemented to update the weighting model. Our experiments demonstrate that\nDWM enhances the performance of models trained with randomly-selected data, and\nthe learned weighting model can be transferred to enhance other data selection\nmethods and models of different sizes. Moreover, we further analyze how a\nmodel's data preferences evolve throughout training, providing new insights\ninto the data preference of the model during training.",
      "published_date": "2025-07-22T02:47:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16178v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16178v1",
      "latex_url": "http://arxiv.org/src/2507.16178v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The success of modern language models has demonstrated the critical role that large-scale pre-training data plays in shaping their performance~. The diversity and scale of the training data are essential for enabling the model to generalize across various tasks and domains . As the scale of the pre-training data increases, language models exhibit a remarkable capacity to perform downstream tasks with minimal task-specific tuning, showcasing the power of data-driven approaches in natural language processing~.\n\nThough the power of large language models rises with the use of enormous and ever-growing datasets for pre-training, naively training a model on all available data may not be optimal, as the quality of available data varies, and the training increases the carbon footprint and financial costs. Recently, there is increasing evidence that choosing the right training data is more essential for producing state-of-the-art large language models~, and researchers have focused on studying data selection, the mechanism to determine which candidate data to include in the training, to improve the training efficiency of model pre-training. Specially, these data selection methods usually utilize referring data or referring models for an effective selection process. For example, in DSIR~, Wikipedia and books are used as the high-quality data to help data classification. In DsDM~ and MATES~, LAMBADA dataset~ is used as the target dataset to help evaluate the influence of the candidate data, i.e., the decrease of the loss in the target dataset when the model is trained with and without the candidate data. Besides, referring models are also utilized to help evaluate the quality of the data. The relationship between the perplexity of reference models (e.g., Llama) applied to candidate datasets and data quality is examined, with perplexity proposed as a potential metric for assessing data quality~. And QuRating~ utilizes the data preference of GPT-3.5-turbo to train the data rating model.\n\nThough these methods filter out data and decrease the training cost, they merely focus on the data selection isolatedly without considering the dynamic training process of LLMs. On the one hand, most of the previous methods selected data before model training, ignoring the dynamic data preference of the model during training. On the other hand, the data samples in existing methods are selected separately and utilized within a batch indiscriminately, without considering the joint effects between different samples. In fact, the data samples interact with each other, and the combination of them determines the model update direction together. Hence, the existing methods which select data separately and ignore the data utilization will limit the potential performance of the trained LLMs with selected data.\n\nIn this paper, to improve the data utilization for large language models training with selected data, we propose a bi-level optimization framework to capture the dynamic data preference of the model, and the joint effects of different data samples. In the framework, a plug-and-play Data Weighting Model (DWM) is introduced to weigh the data samples within each batch during model training, and therefore focuses on the joint effects of selected data. Specifically, to guarantee the weighting model knows the data preference of the trained model, we introduce a bi-level optimization to help learn the weighting model. The lower level will first optimized the trained model with data weighted by the weighting model, and the upper level will optimized the trained model updated by the lower-lever optimization, where the weighting model can be optimized with the help of the chain rule. Furthermore, to better capture the dynamic data preference of the trained model, we learn the DWM via the above bi-level optimization at different stages during model training, and hence learn the data preference dynamically and adaptively.\n\nWe conduct extensive experiments to validate the effectiveness of DWM. First, using randomly-selected data from SlimPajama, we pre-train a 370M model from scratch. The model trained with DWM and randomly selected data outperforms both models trained with randomly-selected data and those with carefully selected data. We then transfer DWM to a larger LLM (i.e., 1.3B) and other data selection methods, which also achieve a consistent performance improvement. Finally, we further analyze how the weighting model preferences evolve during training to provide more insights about the model data preference.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "example_paper.tex",
      "rlhf_score": 0.442,
      "weak_supervision_score": 0.464,
      "diffusion_reasoning_score": 0.442,
      "distributed_training_score": 0.453,
      "datasets_score": 0.41,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "The paper focuses on dynamic data weighting and bi-level optimization for LLM training, without any involvement of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses data selection and weighting for LLM training but does not involve programmatically generating labels from noisy sources; it relies on existing data and optimization methods, not weak supervision paradigms.",
      "diffusion_reasoning_justification": "The paper's contribution is centered on data utilization and bi-level optimization for LLM training, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses LLM training efficiency through data weighting but does not address parallel computing, multi-node systems, or strategies for partitioning data or computation across processors.",
      "datasets_justification": "The paper analyzes how data preferences evolve during LLM training and uses datasets like SlimPajama for experiments, providing some insights into data utilization, but its main focus is on the Data Weighting Model and optimization, not on creating, benchmarking, or curating datasets.",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668451",
      "updated_at": "2025-08-11T23:43:05.607007",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16184",
      "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind (A Position Paper)",
      "authors": [
        "Myung Ho Kim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification.",
      "published_date": "2025-07-22T02:54:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16184v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16184v1",
      "latex_url": "http://arxiv.org/src/2507.16184v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.285,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.311,
      "datasets_score": 0.289,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on an AI agent architecture called Agentic Flow, which integrates cognitive theories like Kahneman's dual-system theory and involves multi-step reasoning tasks. However, it does not mention or adapt diffusion models, iterative refinement processes, or treat a Chain-of-Thought as a holistically corrected entity. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668459",
      "updated_at": "2025-08-11T23:43:05.607009",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16191",
      "title": "Explicit Context Reasoning with Supervision for Visual Tracking",
      "authors": [
        "Fansheng Zeng",
        "Bineng Zhong",
        "Haiying Xia",
        "Yufei Tan",
        "Xiantao Hu",
        "Liangtao Shi",
        "Shuxiang Song"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Contextual reasoning with constraints is crucial for enhancing temporal\nconsistency in cross-frame modeling for visual tracking. However, mainstream\ntracking algorithms typically associate context by merely stacking historical\ninformation without explicitly supervising the association process, making it\ndifficult to effectively model the target's evolving dynamics. To alleviate\nthis problem, we propose RSTrack, which explicitly models and supervises\ncontext reasoning via three core mechanisms. \\textit{1) Context Reasoning\nMechanism}: Constructs a target state reasoning pipeline, converting\nunconstrained contextual associations into a temporal reasoning process that\npredicts the current representation based on historical target states, thereby\nenhancing temporal consistency. \\textit{2) Forward Supervision Strategy}:\nUtilizes true target features as anchors to constrain the reasoning pipeline,\nguiding the predicted output toward the true target distribution and\nsuppressing drift in the context reasoning process. \\textit{3) Efficient State\nModeling}: Employs a compression-reconstruction mechanism to extract the core\nfeatures of the target, removing redundant information across frames and\npreventing ineffective contextual associations. These three mechanisms\ncollaborate to effectively alleviate the issue of contextual association\ndivergence in traditional temporal modeling. Experimental results show that\nRSTrack achieves state-of-the-art performance on multiple benchmark datasets\nwhile maintaining real-time running speeds. Our code is available at\nhttps://github.com/GXNU-ZhongLab/RSTrack.",
      "published_date": "2025-07-22T03:07:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16191v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16191v1",
      "latex_url": "http://arxiv.org/src/2507.16191v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure/Paradigm_comparison}\n\nVisual object tracking is a fundamental task in computer vision.\nIt aims to locate the object in subsequent frames using a bounding box, given the initial state of an arbitrary object in the first frame.\nHowever, in some complex tracking scenarios, such as fast motion and similar object interference or occlusion, traditional methods relying on static initial frame often struggle to handle dynamic target appearance variations or background interference, leading to poor robustness.\n\nTo overcome the limitations of static references, researchers have explored leveraging historical target features as priors to capture correlations with current frame, thereby enabling dynamic modeling of both the target and the scene.\nThis topic has attracted widespread attention and developed rapidly, with mainstream methods transitioning from early discrete dynamic template updating to current continuous video-level context modeling .\nOverall, current video-level context modeling methods can be broadly categorized into two types:\n1) Contextual propagation:\nThis approach passes lightweight features or tokens along the temporal dimension to establish temporal association between the initial and current frames. However, the methods that aggregate target information from multiple frames using unified features or tokens often overlook the distinctive features of the target in different frames, making it hard to capture the contextual association accurately and resulting in poor temporal modeling.\n2) Contextual Association Reasoning:\nThis method generates a unique target state token for each frame. The state tokens from multiple historical frames form a continuous sequence of target states, which is then modeled and inferred along the temporal dimension. However, due to the lack of effective supervision during this process, the model struggles to fully capture the temporal dynamics of the target state, resulting in modeling outputs that deviate significantly from the actual target state.\n\nTo effectively learn and explicitly supervise the context reasoning process, we propose a novel context learning framework, RSTrack, aiming to model target consistency in video sequences to assist current state reason.\nAs shown in Fig., we innovatively uses the true target state as a forward supervision signal to guide the continuous reasoning from historical to current states.\nCompared with existing implicit context modeling approaches , RSTrack explicitly learns and leverages the temporal consistency among historical states to infer the current target state, thereby achieving context reasoning modeling with a clear evolutionary direction.\nSpecifically, RSTrack achieves the above objectives through the following three components:\n1) Efficient State Modeling:\nWe propose a spatial-channel compression module that reduces redundant target features across frames by compressing them into state tokens that only retain essential object information.\nTo ensure effective compression, we reconstruct the original features by combining the tokens with template features, and regularize the process using an L2 loss.\nThis mechanism not only enables efficient information storage but also mitigates interference caused by inter-frame redundancy during context reasoning, thereby avoiding invalid contextual associations.\n2) Context Reasoning Mechanism:\nWe use a state space model to capture temporal variations of the target across frames, modeling the temporal correlations between historical target states to predict the current state.\nThis predicted state token is converted to predicted target features via the reconstruction mechanism, which further refines the search feature in the temporal decoder.\nThis mechanism fully leverages temporal consistency, enabling the model to reason more accurately about the current frame based on past states.\n3) Forward Supervision Strategy: To constrain the context reasoning process, we compress the target features from the visual encoder into state tokens as supervision signals. By computing the L2 norm distance between the predicted state tokens and the supervision signals, we establish an explicit constraint mechanism that ensures effective learning of the contextual reasoning process and suppresses drift during the modeling process.\nIn summary, we make the following contributions:\n {itemize}\n [$ $]\nWe propose a novel tracking framework named RSTrack. This framework explicitly supervises and models the temporal consistency between historical states, enabling robust cross-frame target state inference.\n [$ $]\nWe design a spatial-channel compression module that retains core target information in each frame. This reduces computational costs and enables a more efficient contextual reasoning process.\n [$ $]\nOur approach achieves a new state-of-the-art on multiple benchmarks, including LaSOT, \\(LaSOT_{ext}\\), GOT-10K, TrackingNet, TNL2K and UAV123.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.364,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.34,
      "datasets_score": 0.298,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a visual tracking framework (RSTrack) that uses explicit context reasoning, supervision strategies, and efficient state modeling to enhance temporal consistency in object tracking. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning or chain-of-thought tasks. The focus is solely on computer vision techniques for tracking, with no elements related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667967",
      "updated_at": "2025-08-11T23:43:05.606900",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16193",
      "title": "LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs",
      "authors": [
        "Zitong Xu",
        "Huiyu Duan",
        "Bingnan Liu",
        "Guangji Ma",
        "Jiarui Wang",
        "Liu Yang",
        "Shiqi Gao",
        "Xiaoyu Wang",
        "Jia Wang",
        "Xiongkuo Min",
        "Guangtao Zhai",
        "Weisi Lin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.MM (Multimedia)"
      ],
      "abstract": "The rapid advancement of Text-guided Image Editing (TIE) enables image\nmodifications through text prompts. However, current TIE models still struggle\nto balance image quality, editing alignment, and consistency with the original\nimage, limiting their practical applications. Existing TIE evaluation\nbenchmarks and metrics have limitations on scale or alignment with human\nperception. To this end, we introduce EBench-18K, the first large-scale image\nEditing Benchmark including 18K edited images with fine-grained human\npreference annotations for evaluating TIE. Specifically, EBench-18K includes\n1,080 source images with corresponding editing prompts across 21 tasks, 18K+\nedited images produced by 17 state-of-the-art TIE models, 55K+ mean opinion\nscores (MOSs) assessed from three evaluation dimensions, and 18K+\nquestion-answering (QA) pairs. Based on EBench-18K, we employ outstanding LMMs\nto assess edited images, while the evaluation results, in turn, provide\ninsights into assessing the alignment between the LMMs' understanding ability\nand human preferences. Then, we propose LMM4Edit, a LMM-based metric for\nevaluating image Editing models from perceptual quality, editing alignment,\nattribute preservation, and task-specific QA accuracy in an all-in-one manner.\nExtensive experiments show that LMM4Edit achieves outstanding performance and\naligns well with human preference. Zero-shot validation on the other datasets\nalso shows the generalization ability of our model. The dataset and code are\navailable at https://github.com/IntMeGroup/LMM4Edit.",
      "published_date": "2025-07-22T03:11:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16193v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16193v1",
      "latex_url": "http://arxiv.org/src/2507.16193v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The rapid advancement of Text-guided Image Editing (TIE) allows for image modifications through text prompts . However, state-of-the-art TIE models still struggle to balance perceptual quality, editing alignment, and attribute preservation, limiting their reliability and practicality in real-world applications . Given that human evaluation is costly and inefficient, it is crucial to develop effective automatic evaluation metrics that closely align with human perception and preferences.\n\nExisting TIE evaluation methods include image quality assessment (IQA) metrics , vision-language approaches and LMM-based evaluation methods . Traditional IQA metrics primarily assess natural distortions such as noise, blur, compression, etc., but they fail to capture key challenges in TIE, such as structural distortions, text-image misalignment, and discrepancies between the source and target images. While vision-language approaches have made significant progress in text-to-image generation evaluation by incorporating human visual feedback , they focus solely on alignment between text and image, neglecting the editing alignment and relationship between the source and edited images. Recent studies have explored using LMMs for general quality evaluation , and some works have employed LMMs to bench TIE models . However, these zero-shot results fail to align with human preferences in all dimensions. Additionally, existing TIE evaluation benchmarks assess only a limited set of TIE models or only consider the alignment dimension, limiting their generalization and practical applicability.\n\nIn this paper, we introduce EBench-18K, a large-scale image  {E}diting  {Bench}mark to evaluate human preferences for TIE, as shown in Figure~(a)(b). The dataset includes 1,080 high-quality source images from the free photography website and open datasets, accompanied by corresponding diverse editing prompts across 21 editing tasks. Based on these source images and editing prompts, we generate 18K+ edited images using 17 state-of-the-art TIE models. Through an extensive subjective study, we collect 1M+ human annotations evaluated from perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, respectively, which result in 55,080 high-quality mean opinion scores (MOSs) and 18,360 question-answer (QA) pairs for TIE evaluation.\n\nBased on EBench-18K, we propose LMM4Edit, a novel an  {LMM}-based all-in-one approach for evaluating image  {Edit}ing models from multiple dimensions, including perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, as shown in Figure~(c). Specifically, LMM4Edit is built upon a LMM backbone fine-tuned with instruction tuning . To enhance the performance, we apply adaptive low-rank adaptation (AdaLoRA) to both the vision encoder and the language model, refining their ability to capture quality-aware, instruction-relevant and preservation-oriented attributes. A two-stage training step is used to achieve the better score regression. Extensive experiments on EBench-18K demonstrate that LMM4Edit achieves state-of-the-art performance and good generalization ability.\n {table}[t]\n \n\n {An overview of text-guided image editing methods selected to construct our EBench-18K.}\n {4}{4.5} \n { }{0.15pt}\n { }{0.08em}\n { }{0.15pt}\n  {0.48 }{!}{ {tabular}{lcccc}\n \n  { {-1.5pt}}\n\nModels & Time &Prompt Type & Method &Resolution\n\n \n  { {1.5pt}}\nText2LIVE & 2022.04 &Description &GAN &512$ $512\n\nEDICT & 2022.11 &Description & SD1.4 &512$ $512\n\nIP2P & 2023.04 &Instruction &SD1.4 &768$ $768\n\nDDPM & 2023.04 &Description & SD2.1 &512$ $512\n\nMasaCtrl & 2023.04 &Description &SD1.4 &512$ $512\n\nCDS & 2023.11 &Description & SD1.4 &512$ $512\n\nMagicbrush & 2023.06 &Instruction & SD1.4 &768$ $768\n\nPnP & 2023.10 &Description & SD1.5 &512$ $512\n\nAny2Pix & 2023.12 &Instruction & SDXL &1024$ $1024\n\nInfEdit & 2023.12 &Description & SD1.4 &512$ $512\n\nZONE & 2023.12 &Instruction & SD1.5 &512$ $512\n\nReNoise & 2024.03 &Description & SDXL &512$ $512\n\nHQEdit & 2024.04 &Instruction & DIFT &512$ $512\n\nRFSE & 2024.11 &Description & FLUX &1024$ $768\n\nFlowEdit (SD3) & 2024.12 &Description & SD3 &1024$ $1024\n\nFlowEdit (FLUX) & 2024.12 &Description & FLUX &1024$ $1024\n\nACE++ & 2025.01 &Instruction & FLUX &1024$ $1024\n\n  { {-1.5pt}}\n \n {tabular}}\n\n {table}\nThe main contributions of this work include:\n {itemize}[left=0pt, labelsep=0.6em, labelwidth=0pt]\n   We introduce EBench-18K, a large-scale dataset containing 18K edited images across diverse tasks with over 1M+ human annotations covering perceptual quality, editing alignment, attribute preservation and task-specific accuracy dimensions.\n   We use EBench-18K to bench both TIE generation ability and the LMMs' understanding and evaluating capabilities.\n   We propose LMM4Edit, a novel LMM-based all-in-one metric providing fine-grained perceptual quality, editing alignment, attribute preservation assessments for TIE models.\n   Extensive experimental results on EBench-18K validate the superior performance of LMM4Edit and its strong in aligning with human perception and generalization ability.\n {itemize}\n\n {figure*}  [width= ,height=0.13 ]{MOS1.pdf}\n  {(a) Distribution of perceptual quality, editing alignment and attribute preservation MOSs. (b) Counts and average MOSs across different tasks.}\n\n {figure*}\n {figure*}\n  [width= ,height=0.16 ]{MOSmodel.pdf}\n  {Comparison of TIE models across the dimensions of perceptual quality, editing alignment, attribute preservation MOSs, and question-answer (QA) accuracy.}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sample-sigconf.tex",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.31,
      "datasets_score": 0.408,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of EBench-18K, a large-scale dataset specifically designed for evaluating text-guided image editing (TIE) models in machine learning and AI. It details the dataset's creation process, including curation of 1,080 source images, generation of 18K+ edited images using 17 TIE models, and collection of over 1M human annotations for aspects like perceptual quality and editing alignment. The paper also benchmarks and analyzes this dataset to evaluate TIE models and LMMs, directly aligning with research on creating, benchmarking, and evaluating datasets for AI applications.",
      "summary": "The paper introduces EBench-18K, a large-scale benchmark with 18,000 edited images generated from 1,080 source images and prompts across 21 text-guided image editing (TIE) tasks using 17 state-of-the-art models, accompanied by extensive human annotations including mean opinion scores (MOSs) and question-answer pairs to evaluate aspects like perceptual quality and editing alignment. It proposes LMM4Edit, a fine-tuned Large Multimodal Model-based metric that assesses TIE models across multiple dimensions such as perceptual quality, editing alignment, attribute preservation, and task-specific accuracy, demonstrating superior performance, strong alignment with human preferences, and good generalization in experiments.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new large-scale benchmark (EBench-18K) and a novel LMM-based evaluation metric (LMM4Edit) that significantly advances TIE assessment by addressing gaps in existing methods and incorporating fine-grained human-aligned evaluations.",
      "impact_score": "High",
      "impact_justification": "The work provides a standardized benchmark and metric that could broadly influence TIE research, model development, and applications in computer vision by improving evaluation accuracy and generalization.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable and innovative contribution to TIE evaluation, making it essential for researchers in computer vision and multimedia to understand and build upon.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a43d4bdcf502d2876d3a071834ded9b62b802467",
      "h_index_fetch_method": "full_id",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 45,
      "average_h_index": 9.416666666666666,
      "notable_authors_count": 4,
      "author_h_indexes": [
        {
          "name": "Zitong Xu",
          "profile_url": "https://www.semanticscholar.org/author/2338407234",
          "h_index": 2
        },
        {
          "name": "Huiyu Duan",
          "profile_url": "https://www.semanticscholar.org/author/19269060",
          "h_index": 19
        },
        {
          "name": "Bingnan Liu",
          "profile_url": "https://www.semanticscholar.org/author/2372476225",
          "h_index": 0
        },
        {
          "name": "Guangji Ma",
          "profile_url": "https://www.semanticscholar.org/author/2339753952",
          "h_index": 2
        },
        {
          "name": "Jiarui Wang",
          "profile_url": "https://www.semanticscholar.org/author/2295270986",
          "h_index": 4
        },
        {
          "name": "Liu Yang",
          "profile_url": "https://www.semanticscholar.org/author/2294417578",
          "h_index": 4
        },
        {
          "name": "Shiqi Gao",
          "profile_url": "https://www.semanticscholar.org/author/2308086798",
          "h_index": 1
        },
        {
          "name": "Xiaoyu Wang",
          "profile_url": "https://www.semanticscholar.org/author/2374292486",
          "h_index": 0
        },
        {
          "name": "Jia Wang",
          "profile_url": "https://www.semanticscholar.org/author/2117997534",
          "h_index": 5
        },
        {
          "name": "Xiongkuo Min",
          "profile_url": "https://www.semanticscholar.org/author/2246414",
          "h_index": 45
        },
        {
          "name": "Guangtao Zhai",
          "profile_url": "https://www.semanticscholar.org/author/2266393212",
          "h_index": 18
        },
        {
          "name": "Weisi Lin",
          "profile_url": "https://www.semanticscholar.org/author/2266768297",
          "h_index": 13
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669028",
      "updated_at": "2025-08-11T23:45:45.657543",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16201",
      "title": "A Single-step Accurate Fingerprint Registration Method Based on Local\n  Feature Matching",
      "authors": [
        "Yuwei Jia",
        "Zhe Cui",
        "Fei Su"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Distortion of the fingerprint images leads to a decline in fingerprint\nrecognition performance, and fingerprint registration can mitigate this\ndistortion issue by accurately aligning two fingerprint images. Currently,\nfingerprint registration methods often consist of two steps: an initial\nregistration based on minutiae, and a dense registration based on matching\npoints. However, when the quality of fingerprint image is low, the number of\ndetected minutiae is reduced, leading to frequent failures in the initial\nregistration, which ultimately causes the entire fingerprint registration\nprocess to fail. In this study, we propose an end-to-end single-step\nfingerprint registration algorithm that aligns two fingerprints by directly\npredicting the semi-dense matching points correspondences between two\nfingerprints. Thus, our method minimizes the risk of minutiae registration\nfailure and also leverages global-local attentions to achieve end-to-end\npixel-level alignment between the two fingerprints. Experiment results prove\nthat our method can achieve the state-of-the-art matching performance with only\nsingle-step registration, and it can also be used in conjunction with dense\nregistration algorithms for further performance improvements.",
      "published_date": "2025-07-22T03:29:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16201v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16201v1",
      "latex_url": "http://arxiv.org/src/2507.16201v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{T}{he} researches on Automatic Fingerprint Identification Systems (AFIS) have developed over many years, and have made significant contributions in the fields of identity verification and criminal investigation. However, a number of factors still hinder the accuracy of fingerprint recognition algorithms, with fingerprint distortion being one of the critical challenges. When sensors capture fingerprints, the pressure and angle of the fingers on the sensor often cause distortions, leading to significant intra-class variations for fingerprints from the same finger. The goal of fingerprint registration algorithm is to align fingerprints from the same finger as accurately as possible to mitigate the effects of distortion .\n\nOver the years, fingerprint registration algorithms have evolved from early sparse registration method based on minutiae to a more elaborate registration approach that first perform sparse registration followed by dense registration . These advancements have made it possible to closely align the ridges of two fingerprints. However, most current fingerprint registration algorithms still rely on the sparse registration method based on minutiae as the first step, often referred to as \"coarse registration\" in some studies. The coarse registration algorithm relies on fingerprint minutiae, which makes it prone to failure\n {figure}[!]\n \n { [width= ]{imgs/img1_3.pdf}}\n {Flowchart of our fingerprint registration method and previous methods. Compared to the previous two-step method, our approach can handle more complex distortions and achieve precise registration in single-step. Left images show the correspondences required for fingerprint registration. In right images, Green areas of indicate overlapping ridges, gray and red indicate non-overlapping ridges of the two fingerprints respectively. }\n\n {figure}\nwhen the fingerprint image quality is poor or the minutiae are sparse. Meanwhile, the second step \"dense registration\" typically focuses solely on improving alignment accuracy, and can only handle small distortion on the basis after coarse registration. If the coarse registration fails, the entire fingerprint registration process is very likely to fail. This limitation significantly reduces the effectiveness of the entire fingerprint registration algorithm for low-quality fingerprints, making fingerprint registration difficult to implement in the real world. In addition, this two-step fingerprint registration approach is computationally demanding, making the entire fingerprint matching process very time consuming.\n\nTo address the aforementioned issues, we propose a new accurate and efficient single-step fingerprint registration method, which is a flexible fingerprint registration method based on semi-dense local feature matching . Recent studies in the field of fingerprint recognition have demonstrated that semi-dense local feature matching can be effectively used in fingerprint matching . However, these studies have not fully leveraged its potential advantages in fingerprint registration. Our method adopts the network structure and training framework from LoFTR, predicting the semi-dense correspondences between two fingerprints. These correspondences are then used to derive a deformation field between the fingerprints. The deformation field is applied to align the input fingerprint with the target fingerprint.\n\nWith the help of the Global-Local Attention Block , our method is able to align fingerprint pairs as much as possible in a single-step, achieving a higher registration performances and significantly addressing the issue of initial registration failures, thus achieving state-of-the-art matching performance with even reduced runtime. Experimental results demonstrate that the proposed algorithm achieves state-of-the-art matching performance in fingerprint registration.\n\nBy using our method as a replacement for the original initial registration module, the total registration performance and matching capability can be significantly improved and outperforms other dense registration methods . Compared to other fingerprint registration methods that require operations such as minutiae extraction, phase computation, and binarization, our method allows end-to-end fingerprint registration without preprocessing. By using the same set of deep neural network parameters, our approach can directly register raw fingerprint images across different sensors (optical, thermal wiped, latent), enabling a seamless, cross-sensor registration process without the need for preprocessing steps.\n\nThe main contributions of our work can be summarized as follows:\n {itemize}\n\n [1)] Semi-dense local feature matching is first applied to the elastic deformation of fingerprints. As an improvement to the coarse registration process in fingerprint registration, this method overcomes the failure issues caused by small overlapping areas and sparse minutiae during coarse registration.\n\n [2)] By introducing a Global-Local Attention Block into the semi-dense local matching of fingerprints, the accuracy of corresponding point matching is further improved. Thereby for the first time, the matching performance of a single-step fingerprint registration algorithm surpasses that of previous two-step algorithms, significantly reducing the runtime of fingerprint registration.\n\n [3)] The proposed fingerprint registration method has good adaptability, capable of registering various types of fingerprints and achieving state-of-the-art performances. Additionally, our algorithm takes raw fingerprint images as input, without the need for preprocessing and feature extraction in previous methods, significantly improving efficiency.\n\n [4)] Comprehensive experiments conducted on multiple fingerprint datasets demonstrate that our method can achieve end-to-end registration for fingerprints from different sensors and low-quality samples, breaking the previous bottlenecks in registration accuracy and speed in fingerprint registration research.\n {itemize}\n\nThe paper is organized as follows. Section II reviews the related works. Section III introduces the framework of the proposed fingerprint registration algorithm. Section IV presents the experimental results and discussions. Finally, we make conclusions in Section V.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "bare_jrnl_new_sample4.tex",
      "rlhf_score": 0.263,
      "weak_supervision_score": 0.278,
      "diffusion_reasoning_score": 0.267,
      "distributed_training_score": 0.291,
      "datasets_score": 0.264,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667976",
      "updated_at": "2025-08-11T23:43:05.606902",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16203",
      "title": "SVAgent: AI Agent for Hardware Security Verification Assertion",
      "authors": [
        "Rui Guo",
        "Avinash Ayalasomayajula",
        "Henian Li",
        "Jingbo Zhou",
        "Sujan Kumar Saha",
        "Farimah Farahmandi"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Verification using SystemVerilog assertions (SVA) is one of the most popular\nmethods for detecting circuit design vulnerabilities. However, with the\nglobalization of integrated circuit design and the continuous upgrading of\nsecurity requirements, the SVA development model has exposed major limitations.\nIt is not only inefficient in development, but also unable to effectively deal\nwith the increasing number of security vulnerabilities in modern complex\nintegrated circuits. In response to these challenges, this paper proposes an\ninnovative SVA automatic generation framework SVAgent. SVAgent introduces a\nrequirement decomposition mechanism to transform the original complex\nrequirements into a structured, gradually solvable fine-grained problem-solving\nchain. Experiments have shown that SVAgent can effectively suppress the\ninfluence of hallucinations and random answers, and the key evaluation\nindicators such as the accuracy and consistency of the SVA are significantly\nbetter than existing frameworks. More importantly, we successfully integrated\nSVAgent into the most mainstream integrated circuit vulnerability assessment\nframework and verified its practicality and reliability in a real engineering\ndesign environment.",
      "published_date": "2025-07-22T03:36:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16203v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16203v1",
      "latex_url": "http://arxiv.org/src/2507.16203v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "To meet the increasing demand for consumer electronics, the current integrated circuits supply chain is shifted to a horizontal model where numerous untrusted entities need to be involved for manufacturing and testing purposes~. In this case, various security vulnerabilities might be introduced and need to be addressed urgently. Assertion-based verification is one of the most effective methods to detect hardware security vulnerabilities~. However, traditional verification methods rely on manual code reviews and simulation-based testing, which is not only labor-intensive but also struggles to keep pace with increasingly complex designs and evolving security threats. Existing research on the generation of SVA mainly focuses on the functionalities, while research on SVAs for detecting potential security vulnerabilities is extremely limited. Functional errors can usually be effectively detected by traditional simulation and testing methods, and their characteristics often manifest as obvious failures or anomalies in normal operation. In contrast, hardware security vulnerabilities are inherently hidden and complex. They usually do not appear in standard functional tests but require specific trigger conditions and sequences to be activated. These security vulnerabilities may include information leakage paths or permission bypass mechanisms, which may be completely invisible under normal operating conditions but may lead to catastrophic security consequences under certain conditions. Therefore, SVA for hardware security requires a deep understanding of attacker thinking patterns and possible attack vectors, which significantly increases the workload for verification engineers due to the need for manual analysis, extensive threat modeling, and precise property definition. In the meantime, this security-oriented formal verification approach is crucial to preventing such changes in functionality and information leakage~, especially in the current environment of growing hardware supply chain security threats.\n\nAutomatically generating source code based on natural language instructions has been proven to significantly improve programming efficiency~. Developing SVA based on security requirements is essentially a complex language processing task that requires a deep understanding of the semantics of the requirements and converting them into formal specifications. Recent studies have shown that large language models (LLMs) have demonstrated outstanding capabilities in multiple dimensions, including natural language understanding, basic arithmetic reasoning, common sense reasoning, and symbolic logical thinking, and are gradually expanding to more advanced cognitive activities such as analogical reasoning and multimodal reasoning~. Many AI agent frameworks can achieve very good results by using only pre-trained LLMs as the core engine and performing structured prompts~. In this process, the prompts input to the LLM are crucial. However, studies show that for SVA generation tasks, current LLM-based frameworks cannot generate SVA with high accuracy~, and their average accuracy is usually less than 10%~.\n\nThe main reasons for this dilemma include: 1. The existing framework didn't effectively suppress the inherent hallucination tendency or random generation characteristics of LLM~, which directly led to the great volatility of the accuracy and coverage of SVA; 2. The existing framework ignored the key role of the attention mechanism, which made LLM tend to focus on redundant or irrelevant information when processing, thus significantly reducing the accuracy; 3. LLM showed obvious limitations in processing formal languages and temporal logic. 4. The task is too complex, causing LLM to have errors superimposed; 5. LLM lacks a deep understanding of the syntax structure of the hardware description language (HDL), making it difficult to accurately infer the hierarchy and signal dependencies of the hardware design. To address these challenges, this paper proposes an innovative approach  . Our main contributions are summarized as follows:\n\n {figure*}[ht]\n \n [width=0.9 ]{images/LLM_SVA_AutoSVA2_diff.png}\n {There are many differences in the SVA generated by AutoSVA2 for the same design. This is the result of the LLM's attention mechanism, hallucination, and random answers. This makes the generated code much less trustworthy.}\n\n {figure*}\n\n {itemize}\n   We propose an SVA generation framework  ~based on fine-grained prompting techniques. Without requiring extensive training data or GPU/TPU clusters, high-quality SVAs that are syntactically and logically correct can be generated.\n\n    ~is a general framework that can generate high-accuracy SVA when different LLM cores are applied. The experimental results show that  ~can effectively suppress the effects of hallucinations and random generation compared to other frameworks, and significantly improve the accuracy of the generated SVA.\n\n    ~can significantly reduce the required labor cost. We tested it on a bunch of benchmarks and the results show that  ~can reduce the workload of professional engineers.\n   We further apply the  ~on one of the most efficient hardware vulnerability tools, SoFI~. The experiments demonstrate that  ~is highly scalable.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.321,
      "datasets_score": 0.314,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669243",
      "updated_at": "2025-08-11T23:43:05.607129",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16204",
      "title": "CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced\n  Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted\n  Space-Air-Ground Integrated Networks",
      "authors": [
        "Li-Hsiang Shen",
        "Jyun-Jhe Huang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "eess.SP (Signal Processing)"
      ],
      "abstract": "A space-air-ground integrated network (SAGIN) architecture is proposed,\nempowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)\ncapable of simultaneously reflecting, amplifying, and harvesting wireless\nenergy. The MF-RIS plays a pivotal role in addressing the energy shortages of\nlow-Earth orbit (LEO) satellites operating in shadowed regions, while\nexplicitly accounting for both communication and computing energy consumption\nacross the SAGIN nodes. To maximize the long-term energy efficiency (EE), we\nformulate a joint optimization problem over the MF-RIS parameters, including\nsignal amplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, user association, and computing capability.\nThe formulated problem is highly non-convex and non-linear and contains mixed\ndiscrete-continuous parameters. To tackle this, we conceive a compressed hybrid\nintelligence for twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture achieves superior EE performance due to its\ncomplementary coverage, offering notable advantages over either standalone\nsatellite, aerial, or ground-only deployments.",
      "published_date": "2025-07-22T03:40:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16204v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16204v1",
      "latex_url": "http://arxiv.org/src/2507.16204v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Background and Literature Review\nIn the revolutionary era of tele-traffic explosion, global communication technology is rapidly shifting from fifth-generation (5G) to sixth-generation (6G) paradigm, driving an ever-growing demands for high-coverage and high performance networks .\n\nCompared to conventional 5G, 6G aims to achieve significantly higher data rates, lower latency, and broader network coverage while supporting comprehensive connectivity requirements. Beyond focusing solely on improving communication performance, 6G further integrate advanced technologies such as artificial intelligence (AI), edge computing, and the Internet of Things. However, as the number of users grows explosively, terrestrial base stations (BSs) are gradually becoming overburdened and are unable to meet the future communication demands. To address this bottleneck, the space-air-ground integrated network (SAGIN) has been proposed ,\n\ncombining space, aerial, and terrestrial communication infrastructures into a multi-layered heterogeneous network. This architecture not only ensures global coverage via complementary coverages but also enhances network performance through efficient resource allocation, providing resilient support for the realization of 6G networks.\n\nThe low-Earth orbit (LEO) satellites have played an indispensable role in the SAGIN architecture .\n\nThanks to their notable features of global coverage, LEO satellites effectively compensate for coverage gaps in terrestrial BSs and aerial platforms, i.e., drones and high-altitude platform station (HAPS). In remote or disaster areas, where terrestrial communication infrastructure is inaccessible , LEOs or HAPS are capable of delivering stable and reliable communication support. Consequently, when conventional terrestrial communication systems approach overload, SAGIN establishes a crucial foundation for complementing coverage holes and maintaining broader and continuous service. Nevertheless, with aerospace advancements and the rapid expansion of ground users, LEO satellites face multiple challenges such as limited bandwidth and energy . Although LEO satellites realize global connectivity and high throughput, they still experience a non-negligible pathloss due to long transmission distances. To overcome these hurdles, LEO satellites in SAGIN must closely cooperate with both aerial and terrestrial BSs, leveraging dynamic network topology adjustments and resilient resource optimizations. In particular, HAPS as key nodes at aerial layer operates at stratospheric altitudes closer to Earth's surface, offering relatively stable connections and dynamic adaptations, thus complementing LEO satellites in lower-latency. Moreover, HAPS can perform as collaborative nodes, distributing satellite tasks with greater granularity to terrestrial stations in order to achieve better interference management and load balancing . This multi-layered heterogeneous integration not only further broadens overall coverage but also effectively enhances communication quality and efficiency.\n\nIn addressing the challenges of pathloss, frequent dynamic changes, and channel diversity in LEO satellite communications, reconfigurable intelligent surface (RIS) technology has emerged as a highly promising solution . By dynamically adjusting the configuration of RIS elements, one can establish a virtual line-of-sight (LoS) path between the transmitter and receiver, effectively bypassing obstacles and mitigating undesirable reflections as well as fading . Incorporating RIS into LEO introduces additional reflection paths between LEOs and ground users, thereby reducing pathloss and accommodating the rapid channel variations induced by orbital movement. RIS can refine signal quality through precise phase-shift control, mitigating interference under diverse channel conditions. Upon these advantages, paper in specifically emphasizes improving downlink transmission rates by leveraging RIS to reinforce LEO downlink signals via an augmented virtual LoS, enabling more stable transmissions toward target areas. By orchestrating precise signal allocation and configurations, RIS is capable of boosting communication performance and of providing a scalable scheme for LEO network deployment, thereby laying a solid foundation for the LEO-RIS development . Despite its great potential, RIS still faces several challenges, such as coverage limitations due to its half space reflection property and dependence on external power sources . To address these issues, new frameworks have been developed such as simultaneously transmitting and reflecting RIS (STAR-RIS) . STAR-RIS is capable of both transmitting and reflecting electromagnetic waves, allowing it to control signals across the entire space. This capability enhances coverage and increases network flexibility. Beyond STAR-RIS, the concept of MF-RIS has also been introduced . Unlike traditional RIS, which relies on external grid power sources for manipulating signals, MF-RIS can additionally harvest energy from radio frequency (RF) signals, enabling self-sustainable operation while reducing the need of batteries or grid power. This energy harvesting (EH) capability improves the energy efficiency (EE) and self-sustainability of MF-RIS. Additionally, the MF-RIS incorporates active RIS functionality to support signal amplification, enabling dynamic enhancement of signal strength when needed. Leveraging these capabilities, the MF-RIS plays a pivotal role in the SAGIN network by compensating for limited solar energy in LEO satellites, boosting signal strength across both the LEO and HAPS layers, and alternating non-line-of-sight (NLoS) connectivity within the SAGIN architecture.\n\nWith the rapid advancement of AI and computing capabilities, the application of AI in wireless communication has become a main research focus , particularly in complex and highly dynamic multi layered systems such as SAGIN. Given the highly dynamic topology and rapidly changing channel conditions in SAGIN, traditional static or analytical optimization methods often struggle to adapt effectively. To overcome these challenges, increasing attention has been directed toward advanced AI-based optimization techniques of deep reinforcement learning (DRL) schemes .\n\nBy continuously interacting with the varying environment, DRL can progressively learn the optimal strategies and network configurations, offering greater flexibility and adaptability in scenarios constrained by limited bandwidth, energy, computing capability, and multi-user interference. Recent studies in paper have demonstrated that using DRL to replace relay nodes with RIS can significantly enhance EE performance. A doubled dueling deep-Q network (DQN)-based DRL framework was proposed to jointly optimize bandwidth allocation and the three dimensional positioning of aerial RIS. The work in integrates aerial RIS with satellite-based mobile edge computing, and leverages temporal-enhanced deep deterministic policy gradient (DDPG) and twin-delay-based algorithms to accelerate convergence and reduce system costs. Within this framework, DRL enables real-time analysis of dynamic environments and continuously optimizes network parameters and topology.\n\nChallenges and Contributions\n In this work, we focus on enhancing the EE in SAGIN architecture by incorporating MF-RIS capabilities. However, the system still faces several challenges, including the rapid variation of complex channel conditions caused by the orbital movement of LEOs, the dynamic deployment of HAPS, and distinct constraints and high-dimensional arguments to be determined, as well as complex coordination across the different layers in SAGIN network. Furthermore, the non-stationary environment and partial observation demand adaptive learning strategies capable of generalizing across time-varying topologies and uncertain environments. To address the issues, we propose a compressed hybrid intelligence framework assisted by twin-models and multi-agent DRL systems. The competitive mechanism improves both learning stability and convergence in complex SAGIN-MF-RIS network. Moreover, parameters might include both continuous and discrete properties which cannot be solved by a standalone model. A shared mechanism is introduced between the two models to enable the exchange of partial state representations and action evaluations, enhancing hidden knowledge transfer between different decision making modules. In addition, the high dimensionality of state and action spaces in DRL can lead to significant low training efficiency, slow convergence and high memory requirement. To mitigate this, semantic compression techniques should be designed to reduce dimensionality while preserving essential features. The main contributions of this paper are elaborated as follows:\n {itemize}\n   We have proposed a novel SAGIN-MF-RIS framework, in which the SAGIN architecture offers complementary global and local coverage through a three-layer network consisting of LEO satellites, HAPS, and ground BSs. The integration of MF-RIS further extends the signal transmission range and enhances system self-sustainability by leveraging EH capabilities, thereby enabling high EE performance.\n\n   We have formulated a long-term EE maximization problem, considering both communication and computing capabilities. The optimization variables include the MF-RIS parameters of amplitudes, phase-shifts, EH ratios, and element activation states, transmit antenna beamforming, computing cycles, HAPS deployment as well as user association strategies. The proposed problem is constrained by the limited power consumption, battery capacity, required minimum rate per user, total latency of communication-computing, computing capability, and deployment boundaries.\n\n   We propose a compressed hybrid intelligence for twin-model enhanced multi-agent deep reinforcement learning (CHIMERA) framework to tackle the high-dimensional parameters and mixed discrete-continuous action space: (1) Hybrid DRL framework contains DQN dedicated to discrete decisions (MF-RIS element selection, user association, computing cycles, and HAPS grid-based deployment) and DDPG taking care of continuous variables (transmit beamforming as well as MF-RIS amplitude, phase-shifts, and EH ratio); (2) Twin-models provide a parallel hybrid DRL, preventing policy overfitting and allowing to compete to provide a better action set;\n(3) Parametrized sharing mechanism is designed to provide the determined continuous/discrete action outputs as DQN/DDPG's inputs; (4) Variational autoencoder (VAE)-based semantic compression is employed to pre-train three compression models tailored for continuous actions, discrete actions, and state representations. The encoder compresses the state-action inputs fed into the hybrid DRL networks, while the decoder reconstructs the original parameters for accurate policy execution.\n\n  Simulation results have demonstrated that the proposed SAGIN-MF-RIS architecture outperforms the standalone deployments of LEO satellites, HAPS, or ground BSs, as well as the scenarios of fixed-EH configurations, conventional RIS, and no-RIS scenarios under various system settings. Furthermore, the proposed CHIMERA scheme achieves the highest EE compared to centralized learning approaches such as DQN and DDPG, multi-agent systems, conventional optimization techniques, and heuristic methods. Notably, the VAE-based semantic compression accelerates learning process with moderate performance degradation, enhancing overall system scalability.\n\n {itemize}\n\nThis paper is organized as follows. Section introduces the system model of SAGIN-MF-RIS and problem formulation. Section elaborates the proposed CHIMERA framework. Simulation results are provided in Section , whereas the conclusion is drawn in Section .",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.418,
      "datasets_score": 0.316,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on deep reinforcement learning (DRL) techniques like DQN and DDPG for optimizing SAGIN networks, without any mention of human feedback, preference data, or reward models trained on human rankings. It relies solely on environmental interactions for learning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper involves multi-agent DRL with twin-models and parametrized sharing, which implies some coordination across agents, but it does not emphasize distributed training methods like data partitioning or parallel computing across nodes. It primarily addresses optimization in a multi-agent context rather than accelerating training via distributed systems.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668468",
      "updated_at": "2025-08-11T23:43:05.607012",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16206",
      "title": "METER: Multi-modal Evidence-based Thinking and Explainable Reasoning --\n  Algorithm and Benchmark",
      "authors": [
        "Xu Yang",
        "Qi Zhang",
        "Shuming Jiang",
        "Yaowen Xu",
        "Zhaofan Zou",
        "Hao Sun",
        "Xuelong Li"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the rapid advancement of generative AI, synthetic content across images,\nvideos, and audio has become increasingly realistic, amplifying the risk of\nmisinformation. Existing detection approaches predominantly focus on binary\nclassification while lacking detailed and interpretable explanations of\nforgeries, which limits their applicability in safety-critical scenarios.\nMoreover, current methods often treat each modality separately, without a\nunified benchmark for cross-modal forgery detection and interpretation. To\naddress these challenges, we introduce METER, a unified, multi-modal benchmark\nfor interpretable forgery detection spanning images, videos, audio, and\naudio-visual content. Our dataset comprises four tracks, each requiring not\nonly real-vs-fake classification but also evidence-chain-based explanations,\nincluding spatio-temporal localization, textual rationales, and forgery type\ntracing. Compared to prior benchmarks, METER offers broader modality coverage\nand richer interpretability metrics such as spatial/temporal IoU, multi-class\ntracing, and evidence consistency. We further propose a human-aligned,\nthree-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a\nnovel GRPO stage that integrates a human-aligned evaluator with CoT reasoning.\nWe hope METER will serve as a standardized foundation for advancing\ngeneralizable and interpretable forgery detection in the era of generative\nmedia.",
      "published_date": "2025-07-22T03:42:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16206v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16206v1",
      "latex_url": "http://arxiv.org/src/2507.16206v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure*}[t]\n {center}\n [width= ]{overview.pdf}\n {center}\n {Overview of the METER Framework. Our framework takes a multi-modal input (e.g., an audio-visual clip) and performs a comprehensive, explainable forensic analysis. The central Omni-modal Analysis block shows how our model dissects the media across four tracks: audio-only, image-only (per-frame), video-only, and joint audio-visual. For each track, it generates a Chain-of-Thought evidence trail, identifying specific forgery clues with precise spatiotemporal localization (red boxes) and a corresponding textual rationale (e.g., anatomically incorrect paw structures, unrealistic object interactions, audio-visual desynchronization). These fine-grained clues are then synthesized into a final Summary that provides the high-level source attribution (e.g., T2V with post-produced audio) and a consolidated list of findings. The bottom Evaluation Benchmark block illustrates our comprehensive evaluation protocol, where the model's outputs are rigorously assessed on: (1) Temporal and Spatial IoU, (2) Source Attribution accuracy, and (3) Evidence Quality, using our specialized evaluation model to score the rationality of each generated clue.}\n\n {figure*}\nIn recent years, generative AI has undergone a paradigm shift, driven by breakthroughs in visual synthesis models like Stable Diffusion and Sora , and complemented by remarkable progress in audio synthesis, including high-fidelity voice cloning . The result is a new era where synthetic media achieves unprecedented levels of perceptual realism.\n\nWhile this technological leap unlocks immense creative potential, its widespread availability presents a severe and escalating challenge to information integrity. A deluge of hyper-realistic forged images, deepfake videos, and cloned audio is being weaponized for political propaganda, financial scams, and the viral spread of disinformation across social platforms, posing a direct and tangible threat to societal stability and personal security. The development of efficient, reliable, and trustworthy forgery detection technologies is therefore no longer just an academic pursuit but a societal imperative.\n\nHowever, the vast majority of current forgery detection methods are limited to a binary classification task: real or fake. While these models can achieve high accuracy in controlled environments, their \"black-box\" nature is a fundamental limitation. They provide a probability score but offer no insight into *why* a piece of media was flagged. In high-stakes domains such as journalism, law enforcement, and finance, an unexplainable verdict is often unusable. It fails to build user trust and is inadmissible as actionable evidence. This has propelled explainability to the forefront of the media forensics research agenda. Furthermore, detection techniques for different modalities are typically developed in isolation, lacking a unified standard for assessment and deployment.\n\nPioneering work has begun to leverage Large Multimodal Models (LMMs) for explainable detection , some using Chain-of-Thought (CoT) to generate reasoning steps. While promising, these efforts are hampered by critical limitations:\n {itemize}\n   Inadequate Data Coverage: Nearly all existing explainable forgery datasets, such as Ivy-Fake , focus exclusively on the visual modality, neglecting audio or complex audio-visual deepfakes.\n   Oversimplified Task Definition: Current tasks often lack structured evidence components like precise spatio-temporal localization, systematic forgery type traceability, and a rigorous assessment of the explanation's rationality.\n   Weak Evaluation Standards: Prevailing evaluation protocols rely on methods like GPT-assisted scoring , which may not align with human perception or provide a robust, quantitative assessment.\n {itemize}\n\nWe argue that a truly practical and trustworthy forensic system must provide full-modal, human-aligned, and precise explainability. To this end, we structure our work around three fundamental research questions that define a complete evidence chain: 1) Localization: Where is the forgery? 2) Explanation: Why is it a forgery? 3) Traceability: How was it forged?\n\nTo address these challenges, we introduce METER, a holistic framework featuring a new benchmark and a novel training methodology. The complete overview of our proposed method is depicted in Figure . Our main contributions are:\n {itemize}\n   A Full-Modal Forgery Evidence Chain Dataset: We construct the first dataset that unifies image, video, audio, and audio-visual modalities under a single, explainable framework for forgery detection. Additionally, our approach is the first to comprehensively cover both physical and digital attacks across all these modalities.\n   Comprehensive and Principled Evaluation Metrics: We design a unified evaluation protocol featuring spatio-temporal IoU, multi-class traceability accuracy, and a novel evidence rationality score.\n   An Innovative Human-Aligned Training Methodology: We propose a novel three-stage training strategy (SFT, DPO, GRPO) that systematically cultivates trustworthy, explainable reasoning.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.369,
      "weak_supervision_score": 0.347,
      "diffusion_reasoning_score": 0.477,
      "distributed_training_score": 0.361,
      "datasets_score": 0.411,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a multi-modal benchmark for forgery detection and a Chain-of-Thought training strategy, but it does not adapt the iterative refinement process of diffusion models for logical reasoning tasks. While it mentions generative AI like Stable Diffusion in the context of the problem, there is no component that treats a Chain-of-Thought as a single entity for holistic correction via diffusion-based methods.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's primary contribution is the introduction and benchmarking of a new multi-modal dataset for forgery detection, covering images, videos, audio, and audio-visual content. It includes dataset curation methodologies, such as evidence-chain-based explanations, and comprehensive evaluation metrics, directly aligning with research on creating, analyzing, and evaluating datasets for AI applications.",
      "summary": "The paper introduces METER, a unified benchmark for multi-modal forgery detection across images, videos, audio, and audio-visual content, addressing the limitations of existing methods by emphasizing interpretable explanations through evidence chains that include spatio-temporal localization, textual rationales, and forgery type tracing. It proposes a novel three-stage Chain-of-Thought training strategy—SFT, DPO, and a new GRPO stage—to develop human-aligned models, while providing comprehensive evaluation metrics like spatial/temporal IoU and evidence rationality, aiming to advance generalizable and trustworthy forgery detection in the era of generative AI.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new multi-modal benchmark with evidence-chain-based explanations and a novel training strategy, significantly advancing the state-of-the-art by unifying modalities and enhancing interpretability in forgery detection.",
      "impact_score": "High",
      "impact_justification": "This work could influence a wide range of future research in AI safety and commercial applications for misinformation detection, given its timely address of synthetic media risks and provision of standardized tools.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to interpretable forgery detection, making it essential for researchers in AI and media forensics to be aware of its innovative benchmark and methodology.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/e9dc981f5c7d3a0154453b3b8b5c62f150e07070",
      "h_index_fetch_method": "full_id",
      "total_authors": 7,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 0.42857142857142855,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Xu Yang",
          "profile_url": "https://www.semanticscholar.org/author/2372370416",
          "h_index": 0
        },
        {
          "name": "Qi Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2374299078",
          "h_index": 0
        },
        {
          "name": "Shuming Jiang",
          "profile_url": "https://www.semanticscholar.org/author/2374178143",
          "h_index": 0
        },
        {
          "name": "Yaowen Xu",
          "profile_url": "https://www.semanticscholar.org/author/2323097068",
          "h_index": 0
        },
        {
          "name": "Zhaofan Zou",
          "profile_url": "https://www.semanticscholar.org/author/17304119",
          "h_index": 3
        },
        {
          "name": "Hao Sun",
          "profile_url": "https://www.semanticscholar.org/author/2373373962",
          "h_index": 0
        },
        {
          "name": "Xuelong Li",
          "profile_url": "https://www.semanticscholar.org/author/2372356770",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668478",
      "updated_at": "2025-08-11T23:45:24.983035",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16207",
      "title": "A Human-Centered Approach to Identifying Promises, Risks, & Challenges\n  of Text-to-Image Generative AI in Radiology",
      "authors": [
        "Katelyn Morrison",
        "Arpit Mathur",
        "Aidan Bradshaw",
        "Tom Wartmann",
        "Steven Lundi",
        "Afrooz Zandifar",
        "Weichang Dai",
        "Kayhan Batmanghelich",
        "Motahhare Eslami",
        "Adam Perer"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "As text-to-image generative models rapidly improve, AI researchers are making\nsignificant advances in developing domain-specific models capable of generating\ncomplex medical imagery from text prompts. Despite this, these technical\nadvancements have overlooked whether and how medical professionals would\nbenefit from and use text-to-image generative AI (GenAI) in practice. By\ndeveloping domain-specific GenAI without involving stakeholders, we risk the\npotential of building models that are either not useful or even more harmful\nthan helpful. In this paper, we adopt a human-centered approach to responsible\nmodel development by involving stakeholders in evaluating and reflecting on the\npromises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through\nexploratory model prompting activities, we uncover the perspectives of medical\nstudents, radiology trainees, and radiologists on the role that text-to-CT Scan\nGenAI can play across medical education, training, and practice. This\nhuman-centered approach additionally enabled us to surface technical challenges\nand domain-specific risks of generating synthetic medical images. We conclude\nby reflecting on the implications of medical text-to-image GenAI.",
      "published_date": "2025-07-22T03:53:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16207v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16207v1",
      "latex_url": "http://arxiv.org/src/2507.16207v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The rapid integration of Generative Artificial Intelligence (GenAI) into various workplace and educational settings, including healthcare, has simultaneously\nsparked excitement and concern.\nGenAI holds great promise for augmenting workflows, improving productivity, and enhancing training across a wide range of industries~.\nHowever, the rapid development of domain-specific GenAI without understanding the needs of and challenges faced by domain stakeholders raises the risk of GenAI being misused.\nWithin healthcare, there is a growing amount of literature on new architectures for text-to-image (T2I) GenAI capable of generating complex medical imagery from text prompts~. However, much of the research has focused on technical advancements, such as novel architectures, instead of being rooted in the needs and challenges of medical stakeholders.\n\nTraditionally, synthetic image generation has been used to address pivotal technical medical imaging tasks~, such as image reconstruction or correction~.\n\nNow, with foundation T2I models becoming widely accessible, the allure of simulating rare conditions, creating patient vignettes, or augmenting training datasets is attracting significant interest~. However, much of this enthusiasm is driven by technological evaluations~ rather than human-centered ones, leading to a disconnect from key stakeholders' real needs and challenges. As recently emphasized by human-centered AI researchers, evaluations need to extend beyond algorithmic metrics by understanding how the AI can address stakeholders' real needs and what challenges arise by doing so~. Although recent research captures medical stakeholders' perspectives on GenAI's potential applications when used in practice~, few works capture medical stakeholders' perspectives on the technical challenges and domain-specific risks of generating synthetic medical imagery.\n\nSeveral studies have identified and evaluated the challenges and risks of domain-agnostic T2I models~. However, it is unclear how these challenges and risks translate to domain-specific T2I models. To address these gaps in literature, we follow up a quantitative evaluation of a novel medical T2I model~ with a human-centered qualitative evaluation. As an interdisciplinary team with expertise in human-computer interaction, machine learning, and radiology, we conducted a human-centered evaluation of the promises, technical challenges, and domain-specific risks of a text-guided model that generates Computed Tomography (CT) images of the lung (text-to-CT Scan GenAI)~. CT scans provide radiologists with cross-sectional views of the internal structures of the human body to aid in diagnosis~.\n\nOur human-centered approach in this work consists of two phases (formative discussions and model exploration) to facilitate discussions about: (RQ1) How do key stakeholders imagine using text-to-CT Scan GenAI for medical education, training, and practice? and (RQ2) What technical and domain challenges emerge from stakeholders' needs and workflows for medical text-to-CT Scan GenAI? To address these questions, we engaged with a total of eight radiologists, four trainees, and two medical students. We will refer to this group as key stakeholders throughout the paper.\n\nOur formative discussions included six radiologists and one senior trainee, and they took place at the end of the quantitative evaluation of the model in the user study conducted in~. These discussions focused on capturing stakeholders' ideas on how the model might be applied in practice (RQ1).\n\nThis informed the focus of tasks we gave participants during the second phase (model exploration), broadened our network for recruiting additional participants, and informed the selection of participant demographics.\nFor the second phase, we recruited two medical students, three radiology trainees, and four radiologists (including two from the formative discussions) to prompt and evaluate the model outputs in a semi-structured interview format. Participants interacted with the model through an open-source medical imaging interface (created by~ {ohif_viewer}) that we integrated the text-to-CT Scan model into for easy prompting and output review. By interacting with the model, participants imagined scenarios of how it can address real needs and challenges faced throughout radiology workflows (RQ1). The exploratory prompting activity also shed light on the technical challenges and potential domain-specific risks that researchers should consider when developing medical T2I models (RQ2).\n\nOur work makes three contributions to the AIES and broader Responsible AI (RAI) community. First, we present a text-to-CT Scan GenAI plugin for a popular open-source medical imaging viewer (created by  {ohif_viewer}), which researchers can extend to explore additional approaches for human-centered evaluations of the ethical and safety challenges of domain-specific T2I models.\nSecond, we are the first work to leverage a human-centered approach to explore a medical T2I model with medical stakeholders. As a result of this, our paper expands upon~ {yildirim2024multimodal}'s human-centered GenAI work for applications of multimodal AI in radiology by mapping out applications of domain-specific T2I across medical education, radiology training, and practice.\nThird, we extend existing RAI work by presenting technical challenges and domain-specific risks that emerged from participants interacting with the model, extending~ {munuera2023generative}'s position on the implications of using medical T2I GenAI. These challenges and risks range from topics such as confirmation bias, misrepresentations of image findings, and output image resolution. We discuss the implications of developing medical T2I and suggest future research directions to consider exploring.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "aaai25.tex",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.434,
      "distributed_training_score": 0.32,
      "datasets_score": 0.373,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on human-centered evaluation of a text-to-image generative AI model in radiology, involving stakeholder feedback to assess promises, risks, and challenges. However, it does not involve training or fine-tuning a model using human-ranked data and reinforcement learning, which is the core of RLHF. The feedback is used for qualitative assessment, not for aligning the model with human preferences through a reward model.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper evaluates a text-to-image generative AI model for generating CT scans and explores its applications in medical contexts, but it does not describe any adaptation of diffusion processes for multi-step logical reasoning or treating a chain-of-thought as an entity for refinement. The work centers on image generation and human evaluation, without components for diffusion-based reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669254",
      "updated_at": "2025-08-11T23:43:05.607131",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16208",
      "title": "LOCOFY Large Design Models -- Design to code conversion solution",
      "authors": [
        "Sohaib Muhammad",
        "Ashwati Vipin",
        "Karan Shetti",
        "Honey Mittal"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite rapid advances in Large Language Models and Multimodal Large Language\nModels (LLMs), numerous challenges related to interpretability, scalability,\nresource requirements and repeatability remain, related to their application in\nthe design-to-code space. To address this, we introduce the Large Design Models\n(LDMs) paradigm specifically trained on designs and webpages to enable seamless\nconversion from design-to-code. We have developed a training and inference\npipeline by incorporating data engineering and appropriate model architecture\nmodification. The training pipeline consists of the following: 1)Design\nOptimiser: developed using a proprietary ground truth dataset and addresses\nsub-optimal designs; 2)Tagging and feature detection: using pre-trained and\nfine-tuned models, this enables the accurate detection and classification of UI\nelements; and 3)Auto Components: extracts repeated UI structures into reusable\ncomponents to enable creation of modular code, thus reducing redundancy while\nenhancing code reusability. In this manner, each model addresses distinct but\nkey issues for design-to-code conversion. Separately, our inference pipeline\nprocesses real-world designs to produce precise and interpretable instructions\nfor code generation and ensures reliability. Additionally, our models\nillustrated exceptional end-to-end design-to-code conversion accuracy using a\nnovel preview match score metric. Comparative experiments indicated superior\nperformance of LDMs against LLMs on accuracy of node positioning,\nresponsiveness and reproducibility. Moreover, our custom-trained tagging and\nfeature detection model demonstrated high precision and consistency in\nidentifying UI elements across a wide sample of test designs. Thus, our\nproposed LDMs are a reliable and superior solution to understanding designs\nthat subsequently enable the generation of efficient and reliable\nproduction-ready code.",
      "published_date": "2025-07-22T03:54:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16208v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16208v1",
      "latex_url": "http://arxiv.org/src/2507.16208v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.476,
      "distributed_training_score": 0.397,
      "datasets_score": 0.394,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on training Large Design Models (LDMs) for design-to-code conversion using datasets and fine-tuning, but it does not mention human feedback, reward models, or reinforcement learning techniques. There is no indication of aligning models with human preferences through RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes LDMs for design-to-code tasks with pipelines for optimization and feature detection, but it does not involve diffusion models, iterative refinement for logical reasoning, or treating Chain-of-Thought as a holistic entity for multi-step corrections.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668550",
      "updated_at": "2025-08-11T23:43:05.607015",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16213",
      "title": "Advancing Visual Large Language Model for Multi-granular Versatile\n  Perception",
      "authors": [
        "Wentao Xiang",
        "Haoxian Tan",
        "Cong Wei",
        "Yujie Zhong",
        "Dengjie Li",
        "Yujiu Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Perception is a fundamental task in the field of computer vision,\nencompassing a diverse set of subtasks that can be systematically categorized\ninto four distinct groups based on two dimensions: prediction type and\ninstruction type. Notably, existing researches often focus solely on a limited\nsubset of these potential combinations, which constrains their applicability\nand versatility across various contexts. In response to this challenge, we\npresent MVP-LM, a Multi-granular and Versatile Perception framework\nincorporating Visual Large Language Model. Our framework is designed to\nintegrate both word-based and sentence-based perception tasks alongside box and\nmask predictions within a single architecture. MVP-LM features an innovative\nmulti-granularity decoder in conjunction with a CoT-inspired dataset\nunification strategy, enabling seamless supervised fine-tuning across a wide\nspectrum of tasks, including but not limited to panoptic segmentation,\ndetection, grounding, and referring expression segmentation. Furthermore, we\nintroduce a query enhancement strategy aimed at harnessing the decoding and\ngenerative capabilities inherent in VLLMs. Extensive experiments conducted\nacross a range of benchmarks in both word-based and sentence-based perception\ntasks substantiate the efficacy of our framework. The code will be available at\nhttps://github.com/xiangwentao666/MVP-LM.",
      "published_date": "2025-07-22T04:09:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16213v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16213v1",
      "latex_url": "http://arxiv.org/src/2507.16213v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Perception constitutes a fundamental task within the field of computer vision, necessitating that models accurately identify and locate objects within images or videos in accordance with specified instructions. These perception tasks can be categorized along two dimensions: prediction type (box vs. mask) and instruction type (word-based vs. sentence-based). This classification yields four different groups.\n\n {table}[t]\n  \n  { }{-5pt}\n  { }{5pt}\n  {The comparison of capabilities of different methods. Current works can only address a subset of the combinations, while our  ~ can cover all tasks.}\n  {0.65}{\n  {tabular}{p{7cm}cccc}\n  [1.1pt]\n  {2}{*}{Method} &  {2}{c}{Output Type} &  {2}{c}{Instruction Type}\n\n & Box & Mask & Word & Sentence\n\n  [0.9pt]\n\n Specialists\n\n  [0.5pt]\n\n { OWL, PromptDet, OV-DINO} &   & &   &\n\n X-Decoder, OpenSeeD, Mask DINO &   &   &   &  {2}{*}{}\n\n MDETR, GLIP, Grounding DINO &   & &   &  \n\n VLT, LAVT, PolyFormer & &   & &  \n\n  [0.9pt]\n\n VLLM-based Generalists\n\n  [0.5pt]\n QwenVL, InternVL, DeepseekVL &   & & &  \n\n LISA, PixelLM, Glamm & &   & &  \n\n OMG-LLaVA, PSALM, HyperSeg & &   &   &  \n\n  ~(Ours) &   &   &   &  \n\n  [1.1pt]\n  {tabular}\n }\n  {-5mm}\n\n {table}\n\nThe distinction between coordinate-level prediction and pixel-level prediction has been extensively explored in the literature. Given that mask annotations can be converted to box annotations without loss of information, the concurrent optimization of box predictions and mask predictions within the same dataset has emerged as a prominent training paradigm. This approach has demonstrated mutual benefits for both prediction types~. In contrast to the costly annotations required for segmentation tasks, box annotations are significantly easier to obtain, resulting in a considerably larger dataset. Consequently, another line of research focuses on enhancing segmentation performance by leveraging additional box-annotated data. Numerous methods~ have been developed to achieve this, ranging from the utilization of pre-trained models to natively annotate box data, to the design of specialized segmentation loss functions tailored for box annotations.\n\nWord-based perception employs individual words to denote the categories of targets, which often results in a degree of vagueness and ambiguity. A category name can correspond to none or multiple objects within an image.\nIn contrast, sentence-based perception utilizes complete sentences to describe the objects of interest, resulting in greater precision. In most cases, only one object within the image satisfies the specified conditions. A common approach in the literature~ involves the deployment of a two-stream framework, wherein visual queries extracted by any vision backbone are utilized to decode object locations and compute similarity with the text features generated by a language encoder to identify the objects. However, many of these methods overlook the underlying semantic concepts inherent in the descriptions, resulting in suboptimal outcomes, particularly for sentence-based perception.\n\nRecently, Large Language Models (LLMs) have demonstrated a remarkable ability to comprehend user queries and generate appropriate responses, positioning them as a viable option for sentence-based perception~. Additionally, LLMs possess the capability to extract language features through forwarding, which aligns well with word-based perception. Therefore, LLMs reveal substantial potential for unifying both perceptions.\n\nThe representative of sentence-based location tasks, Referring Expression Comprehension (REC), has emerged as the standard benchmark for Visual Large Language Models (VLLMs), as box coordinates can be expressed as a sequence of numbers in text form, rendering them compatible with traditional VLLMs. However, many existing models~ encounter difficulties in achieving pixel-level predictions. Recent advances~ have successfully output masks by incorporating an additional mask decoder. Nevertheless, these approaches have not adequately addressed the challenges associated with word-based perception.\n\nAlternatively, other studies~ have demonstrated comparable performance on standard word-based perception benchmarks by employing the VLLM as a versatile decoder. Nonetheless, these models focus solely on pixel-level prediction and overlook box prediction.\n\nAs illustrated in  {tab:task-compare}, although joint training has been examined for certain combinations of tasks, the collaborative effects of joint training across all four groups have not been thoroughly explored in the existing literature.\n\nTo this end, we introduce a  {M}ulti-granular and  {V}ersatile  {P}erception framework incorporating VL {LM}, termed  , which is capable of detecting and segmenting targeted objects in images according to various types of user instructions within a single model. By employing a Chain-of-Thought (CoT)-inspired data curation~, we transform multiple box- and mask-annotated datasets into a unified supervised fine-tuning (SFT) dataset format, thereby accommodating a diverse set of tasks, including panoptic segmentation, object detection, visual grounding, and referring expression segmentation. Furthermore, to leverage the decoding and generative capabilities of LLMs, we enhance the queries for object identification and localization by incorporating features derived from LLM-generated sequences.\n\nIn summary, our contributions are outlined as follows:\n\n {itemize}\n   We introduce  , a framework that harnesses both the decoding and generative capabilities of VLLM to perform both word-based and sentence-based perception tasks across varying granularities.\n   We have developed a CoT-inspired data curation that unifies datasets from diverse tasks into a single SFT dataset, thereby encouraging the model to adopt a \"thinking-then-perceiving\" paradigm.\n    ~ demonstrates competitive performance across various benchmarks in both word-based and sentence-based perception, thereby validating the potential of our framework. Notably, our  ~achieves remarkable results on both closed-set and open-set tasks compared with other specialists and VLLMs.\n {itemize}\n\n {figure*}[t]\n  \n  [width=0.9 , trim=0in 2.1in 0in 2.2in, clip]{fig/arch.pdf}\n  {Overview of  .  ~ implements perception by integrating a multi-granularity decoder into the existing VLLM framework. We utilize a unified prompt template to construct the input sequence for the LLM across different tasks. The base query is derived from the summary token of the generated response. Concurrently, we extract the instruction embeddings from the input sequence (denoted by  {blue}{the same color}) and select the corresponding residuals from the multi-scale visual features based on these embeddings. After aggregating the base query vector with the residuals, we decode the bounding box and segmentation mask by inputting the resulting queries into the multi-granularity decoder.}\n\n  {-5mm}\n {figure*}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_introduction.tex",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.398,
      "datasets_score": 0.331,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MVP-LM, a framework for visual perception tasks using Visual Large Language Models (VLLMs), with a Chain-of-Thought (CoT)-inspired data curation for unifying datasets. While it mentions CoT for encouraging a \"thinking-then-perceiving\" paradigm, this is a prompting technique for reasoning in LLMs, not an adaptation of diffusion models' iterative refinement process for logical tasks. The paper focuses on perception tasks like detection and segmentation, with no components involving diffusion-based mechanisms, multi-step refinement of reasoning paths, or holistic correction as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667483",
      "updated_at": "2025-08-11T23:43:05.606785",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16214",
      "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for\n  Safe Approaching Maneuvers",
      "authors": [
        "Batu Candan",
        "Simone Servadio"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.",
      "published_date": "2025-07-22T04:13:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16214v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16214v2",
      "latex_url": "http://arxiv.org/src/2507.16214v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The capability to estimate the relative pose of uncooperative targets, such as derelict satellites, is critical for enabling future ADR, on-orbit servicing, and space situational awareness missions. These scenarios are characterized by severe visibility challenges, unpredictable dynamics, and uncertain sensor performance, all of which make traditional, rigid estimation pipelines inadequate. In particular, ESA’s ENVISAT has become a key benchmark target for ADR due to its large size, complex structure, and non-cooperative nature [ {ESTABLE202052, envisatdl, cakal}]. Moreover, it represents a risk for the sustainability of the LEO space environment [ {servadio2024risk,servadio2024threat}]. Recent advances in vision-based navigation and machine learning have dramatically improved object detection in space imagery [ {lentaris, Rizzuto2025, furano, RENAUT2025231}]. CNNs and deep learning models enable the detection of structural markers on a spacecraft from camera images, even under challenging lighting and motion conditions [ {zhou, BECHINI202420, DIXON2025110055}]. However, these visual outputs still face significant uncertainty due to sensor limitations, jitter, and partial or complete occlusions [ {Gao2023}]. Therefore, fusing these noisy, intermittent detections into a reliable navigation solution remains a central challenge. To address this, Kalman filtering and its nonlinear variants, particularly the UKF, have emerged as robust solutions for visual navigation [ {WEI2024108832, oproukf, servadiojsr, aukff, servadio2020recursive, servadio2020nonlinear}]. Yet, traditional filters assume fixed noise models, which often do not reflect the real-world variability encountered during proximity operations [ {Driedger2020,kaidan,sampath}]. This results in either over-conservative or overly confident estimations depending on operational conditions. While prior works have attempted to tune the process noise covariance matrix $ {Q}$ adaptively, many approaches suffer from either computational complexity or limited scalability. Mamich et al. [ {mamich}] propose a variational Bayesian approach to estimate $ {Q}$ using a probabilistic inference framework, but this requires iterative updates of the evidence lower bound, posing a significant challenge for real-time systems. Similarly, Moghe et al. [ {moghe}] explore dynamic $ {Q}$ estimation in reinforcement learning settings, relying on sampling-based uncertainty propagation that increases computational burden with state dimension. Zanetti and D’Souza [ {zanetti}] analyze filter robustness under observability constraints and propose cautious tuning heuristics for $ {Q}$, but lack an adaptive mechanism that reacts online to system dynamics.\n\nIn contrast to existing approaches, this paper introduces a fully adaptive, dual-noise tuning framework for vision-based relative navigation, with particular emphasis on spacecraft proximity operations under uncertainty. Our main novelty lies in the joint online adaptation of both the process noise covariance matrix $ {Q}$ and the measurement noise covariance matrix $ {R}$ using real-time filter statistics, without requiring prior eclipse scheduling, tuning heuristics, or batch post-processing. The $ {Q}$ adaptation leverages a forward–backward residual matrix, inspired by the Rauch–Tung–Striebel (RTS) smoother [ {Sarkka2013}], to capture the mismatch between prior and propagated sigma points and to adjust process noise during unobservable or eclipse phases. Simultaneously, the $ {R}$ tuning strategy employs a residual consistency filter that tracks innovation growth in real-time and injects per-marker correction factors through the Multiple Tuning Factor (MTF) diagonal matrix inspired by the author's previous works and others [ {batumtf, sokencandan, sokenukf}]. This dual-adaptation architecture allows the filter to remain responsive to both system-driven and measurement-driven uncertainty, achieving consistent and bounded covariance behavior. To the best of our knowledge, this is the first integrated application of RTS-inspired $ {Q}$ adaptation with innovation-based $ {R}$ tuning for monocular vision-based relative navigation in space. The framework's efficacy is demonstrated on high-fidelity simulations of ESA's ENVISAT, where our approach outperforms previous work and variational Bayesian methods in both accuracy and computational efficiency, particularly under full measurement outages. The results from this effort advance the need for ADR [ {simha2025optimal}] for the safety of the space environment and avoiding the predicted Kessler's syndrome [ {jang2025new}].",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.357,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.323,
      "datasets_score": 0.292,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668560",
      "updated_at": "2025-08-11T23:43:05.607017",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16217",
      "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
      "authors": [
        "Shahriar Golchin",
        "Yanfei Chen",
        "Rujun Han",
        "Manan Gandhi",
        "Tianli Yu",
        "Swaroop Mishra",
        "Mihai Surdeanu",
        "Rishabh Agarwal",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
      "published_date": "2025-07-22T04:21:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16217v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16217v1",
      "latex_url": "http://arxiv.org/src/2507.16217v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In-context learning (ICL) is a popular technique for adapting large language models (LLMs) to downstream tasks  {brown2020language}.\nWith long-context LLMs  {team2023gemini,team2024gemini,fu2024data,ding2024longrope} and the ability to cache and reuse computations  {pope2023efficiently}, it has become practical to extremely increase the number of demonstrations in ICL, shifting from few-shot to many-shot settings to further enhance performance  [inter alia]{agarwal2024many,bertsch2024context,jiang2024many}.\nIn many-shot scenarios, random selection of demonstrations is often preferred  {agarwal2024many,bertsch2024context}, as it uses a fixed set of demonstrations without any selection criteria. This allows caching computations to control inference cost while achieving satisfactory performance.\nOn the other hand, selection strategies based on specific criteria, such as similarity  {liu2021makes}, often outperform random selection. However, such methods are impractical in many-shot scenarios, as they dynamically update the input prompt for each downstream test sample, which prevents caching and leads to substantial inference cost.\n\nWe propose two novel strategies for demonstration selection in many-shot ICL. These strategies enable the dynamic customization of many-shot input prompt for each test sample while still remaining largely cacheable. Specifically, prompt customization is achieved by selecting a small number of demonstrations that are similar to each test sample, while the remaining demonstrations are chosen randomly and cached. This approach allows the cached random demonstrations to remain fixed across all test samples, significantly reducing computational overhead by requiring new computations only for the small number of similar demonstrations for each test sample.\nFor example, in our selection strategy under a 100-shot setting, we select only 20 most similar demonstrations for each test sample and the other 80 random demonstrations remain cached. Figure depicts this many-shot prompt format.\nThis idea is motivated by an observation derived from analyzing results reported in previous studies  {agarwal2024many,bertsch2024context}: in many-shot settings, the influence of selection criteria (e.g., similarity) on performance diminishes as the number of demonstrations increases substantially, and beyond a certain point, their impact becomes nearly equivalent to that of random demonstrations. We further confirm this through our own experiments, presented in Subsection .\n\n {wrapfigure}{r}{0.4 }\n  \n  {- }\n  [width=0.4 ]{images/prompt_format_colorful.png}\n  {Our proposed many-shot ICL prompt format. Hatched blocks represent cached content, while solid blocks indicate non-cached content. When LLM is prompted, new computations are performed only for a small set of demonstrations selected based on similarity to the downstream test sample, while previous computations for a larger set of random or $k$-means-selected demonstrations are reused. Block sizes in the figure aim to reflect their actual proportions in the input prompt.}\n\n {wrapfigure}\n\n {figure}[!t]\n  \n  {minipage}{0.24 }\n  \n  [height=2.1cm, keepaspectratio]{images/pareto_plot_anli_proportional_cost.png}\n\n  [height=2.1cm, keepaspectratio]{images/pareto_plot_anli_proportional_cost_flash.png}\n  {minipage}\n  {minipage}{0.24 }\n  \n  [height=2.1cm, keepaspectratio]{images/pareto_plot_trec_proportional_cost.png}\n\n  [height=2.1cm, keepaspectratio]{images/pareto_plot_trec_proportional_cost_flash.png}\n  {minipage}\n  {minipage}{0.24 }\n  \n  [height=2.1cm, keepaspectratio]{images/pareto_plot_gsm_plus_proportional_cost.png}\n\n  [height=2.1cm, keepaspectratio]{images/pareto_plot_gsm_plus_proportional_cost_flash.png}\n  {minipage}\n  {minipage}{0.24 }\n  \n  [height=2.1cm, keepaspectratio]{images/pareto_plot_metatool_proportional_cost.png}\n\n  [height=2.1cm, keepaspectratio]{images/pareto_plot_metatool_proportional_cost_flash.png}\n  {minipage}\n\n  {Pareto analysis of performance vs. inference cost across various datasets, comparing our hybrid selection strategies, i.e., similarity-random and similarity-$k$-means, with other methods. Our strategies balance the low inference cost of random or $k$-means-based selection with the performance gains of dynamic prompt-updating strategies, e.g., similarity-based selection, achieving results comparable to or better than the strongest selection approach.}\n\n {figure}\n\n {comment}\n\n {figure}[ht]\n  \n  {minipage}{0.35 }\n  \n  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}\n\n  {minipage}\n  {minipage}{0.35 }\n  \n  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}\n\n  {minipage}\n\n  {-0.45cm}\n\n  {minipage}{0.35 }\n  \n  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}\n\n  {minipage}\n  {minipage}{0.4 }\n  \n  [width= ]{latex/images/pareto_plot_anli_proportional_cost.png}\n\n  {minipage}\n\n  {Pareto analysis of performance versus inference cost across various datasets and tasks for our proposed selection strategies (hybrid methods) compared to other baseline methods. The base model used is Gemini 1.5 Pro.}\n\n {figure}\n\n {comment}\n\nBuilding on the first strategy, our second strategy replaces the cached random demonstrations with a fixed, diverse set of demonstrations selected using $k$-means clustering. In particular, we compute centroids based on test sample representations, map these centroids to the representations of the available demonstrations, and select the most similar ones to cache for use during inference. Figure illustrates this selection strategy.\n\nThe key contributions are as follows:\n\n(1) We propose two novel strategies for demonstration selection in many-shot ICL that outperform or perform on par with the strongest selection approach while significantly reducing inference cost by making them largely cacheable.\n\n(2) We show that LLMs benefit more from ICL when demonstrations are selected using multiple criteria, rather than relying on a single criterion such as similarity or diversity, and the proportion of demonstrations selected based on multiple criteria can balance performance and inference cost, as shown in Figure .\n\n {comment}\n\n {figure}[!t]\n  \n  [width=0.4 ]{images/prompt_format_final.png}\n  {Our proposed hybrid prompt format for many-shot ICL. Blue hatched blocks represent cached content, while solid red blocks indicate non-cached content. When the LLM is prompted, new computations are performed only for a small set of demonstrations selected based on similarity to the downstream test sample, while previous computations for a larger set of randomly or $k$-means-selected demonstrations are reused. The block sizes in the figure approximately reflect their proportions in the actual input prompt.}\n\n {figure}\n\n {comment}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "colm2025_conference.tex",
      "rlhf_score": 0.433,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.411,
      "distributed_training_score": 0.408,
      "datasets_score": 0.331,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on in-context learning strategies for large language models, specifically demonstration selection and caching to optimize inference costs. It does not involve human feedback, reward models, or reinforcement learning techniques for aligning models with preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses in-context learning with demonstration selection and computational efficiency, but it does not incorporate diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "The paper discusses caching and reusing computations for inference in in-context learning, which relates to computational efficiency, but it does not cover distributed training, parallel computing across nodes, or strategies for accelerating model training.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669265",
      "updated_at": "2025-08-11T23:43:05.607132",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16219",
      "title": "Bayesian Deep Learning for Convective Initiation Nowcasting Uncertainty\n  Estimation",
      "authors": [
        "Da Fan",
        "David John Gagne II",
        "Steven J. Greybush",
        "Eugene E. Clothiaux",
        "John S. Schreck",
        "Chaopeng Shen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study evaluated the probability and uncertainty forecasts of five\nrecently proposed Bayesian deep learning methods relative to a deterministic\nresidual neural network (ResNet) baseline for 0-1 h convective initiation (CI)\nnowcasting using GOES-16 satellite infrared observations. Uncertainty was\nassessed by how well probabilistic forecasts were calibrated and how well\nuncertainty separated forecasts with large and small errors. Most of the\nBayesian deep learning methods produced probabilistic forecasts that\noutperformed the deterministic ResNet, with one, the initial-weights ensemble +\nMonte Carlo (MC) dropout, an ensemble of deterministic ResNets with different\ninitial weights to start training and dropout activated during inference,\nproducing the most skillful and well-calibrated forecasts. The initial-weights\nensemble + MC dropout benefited from generating multiple solutions that more\nthoroughly sampled the hypothesis space. The Bayesian ResNet ensemble was the\nonly one that performed worse than the deterministic ResNet at longer lead\ntimes, likely due to the challenge of optimizing a larger number of parameters.\nTo address this issue, the Bayesian-MOPED (MOdel Priors with Empirical Bayes\nusing Deep neural network) ResNet ensemble was adopted, and it enhanced\nforecast skill by constraining the hypothesis search near the deterministic\nResNet hypothesis. All Bayesian methods demonstrated well-calibrated\nuncertainty and effectively separated cases with large and small errors. In\ncase studies, the initial-weights ensemble + MC dropout demonstrated better\nforecast skill than the Bayesian-MOPED ensemble and the deterministic ResNet on\nselected CI events in clear-sky regions. However, the initial-weights ensemble\n+ MC dropout exhibited poorer generalization in clear-sky and anvil cloud\nregions without CI occurrence compared to the deterministic ResNet and\nBayesian-MOPED ensemble.",
      "published_date": "2025-07-22T04:29:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16219v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16219v1",
      "latex_url": "http://arxiv.org/src/2507.16219v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Accurate prediction of convective initiation (CI) remains difficult for both empirical and numerical weather prediction (NWP) models  [e.g.,][]{Mecikalski2015, Lawson2018, Cintineo2020a} due to its sensitivity to sub-kilometer processes. Our inability to provide timely and precise CI forecasts often results in delayed responses to convective hazards such as hailstorms and tornadoes  [][]{Brooks2008, Dixon2011}. Recently, several novel machine learning (ML) models  [][]{Lee2017, Sun2023, Fan2024} have been developed to enhance short-term CI forecast skill. developed a random forest model to predict CI for tracked cloud objects using cloud characteristics from satellite observations. Using similar features, applied a convolutional neural network that produced skillful CI forecasts at lead times up to 1 hour. developed a convolutional recurrent neural network that leverages spatiotemporal features from both satellite and radar data, demonstrating good performance in predicting several CI cases at lead times up to 30 min. However, due to the lack of very fine-resolution observations, these models are unable to resolve characteristics on scales of O(1 km), such as differential topography and fast-growing cumulus clouds, and still generate incorrect CI forecasts in some scenarios. An assessment of uncertainty is thus needed to arrive at a reasonable confidence level in any prediction and to provide a better understanding of model behavior.\n\nRobust estimates of predictive uncertainty and well-calibrated forecasts strengthen the reliability of forecasts and aid in the decision-making process  [e.g.,][]{Nadav-Greenberg2009, Kendall2017}. Uncertainty estimates support identification of inherently challenging cases and indicate when the model is operating beyond the scope of the training data  {Kendall2017, Karpatne2017, Fang2020}. Uncertainty decomposes into aleatoric, due to internal variability within the training data, and epistemic, arising from limitations in model architecture and the constrained realm of training data, uncertainties  {Kendall2017}. Recently, uncertainty quantification of ML models for weather and climate applications has received much attention  {McGovern2017,Haynes2023,Schreck2023}. Popular approaches include parametric probabilistic models  {Ghazvinian2021, Rasp2018, Chapman2022, Guillaumin2021, Barnes2021, Foster2021, Delaunay2022, Gordon2022}, quantile-based distribution models  {Scheuerer2020, Bremnes2020, Yu2020, Schulz2022}, evidential models  {Sensoy2018, Amini2020, Ulmer2023, Schreck2023}, and Bayesian model averaging  [BMA;][]{Raftery2005}. BMA was originally introduced into the meteorological community  {Raftery2005} to calibrate forecast ensembles by combining forecasts from multiple models and analyses. More recently, it has been applied in the ML community  {Wilson2020} to represent predictive distributions generated by aggregating forecasts sampled from Bayesian models.\n\nParametric probabilistic models predict the parameters of a probability distribution, such as a Gaussian distribution, but they only account for aleatoric uncertainty  {Nix1994}. Evidential models modify the output layer of a single deterministic model to estimate the parameters of a higher-order evidential distribution to capture both aleatoric and epistemic uncertainty  {Gelman2003}, but might underrepresent epistemic uncertainty. BMA generates the predictive distribution by averaging the predictions of different models, weighted by their posterior probabilities, and estimates uncertainty from the predictive distribution. BMA is able to accurately estimate both aleatoric and epistemic uncertainty. As one BMA approach, Bayesian neural networks  [BNNs;][]{neal2012bayesian,Ortiz2023} learn the distribution of each parameter and provide a robust estimate of the predictive distribution by approximating the posterior distribution. and applied BNNs to classify precipitation type and generate synthetic microwave images from satellite infrared observations, respectively, achieving performances comparable to a deterministic model while providing robust uncertainty estimates. However, BNNs require more weights than deterministic models with the same architecture and often struggle to converge to a solution that performs comparably to deterministic models in complex applications  {Krishnan2020}. To address the convergence issue, the MOdel Priors with Empirical Bayes using Deep neural network (MOPED) method was introduced by . This method initializes the weight priors in BNNs using pretrained deterministic models, accelerating training convergence and enhancing performance for different tasks  {Krishnan2020, Zhang2022,Milanes-Hermosilla2023}. The informed priors in the MOPED method were also argued to improve generalization by BNNs  {Zhang2022}. Additionally, applying Monte Carlo dropouts to randomly deactivate neurons of deterministic models can be interpreted as approximating the posterior with a set of sampled points  {Gal2016}.\n\nThe initial-weights ensemble approach, often referred to as the \"deep ensemble\"  {Lakshminarayanan2017}, provides accurate and well-calibrated predictive distributions. An initial-weights ensemble comprises an ensemble of deterministic models, each trained with a different set of random initial weights. By searching for various solutions in the hypothesis space, the initial-weights ensemble offers a better approximation of the true predictive distribution than BNNs  {Ovadia2019,Wilson2020}. demonstrated that initial-weights ensemble methods generate better forecasts than deterministic models and provide accurate uncertainty estimates. suggested that functional diversity is important for a good approximation of the predictive distribution.\n\nIn this study, we extend the work of to explore CI nowcasting skill and uncertainty estimates produced by Bayesian deep learning methods, including Bayesian and initial-weights neural network ensembles. The objectives of our study include: (1) systematically comparing forecast skill of Bayesian deep learning methods for CI nowcasting; (2) evaluating the relationship between performance and predictive uncertainty; and (3) investigating convergence issues in a Bayesian neural network and the impact of the MOPED method when incorporated within it. The rest of this paper is organized as follows. Section 2 describes CI identification and data preprocessing. Section 3 lays out the model architectures and evaluation methods used in the study. Section 4 evaluates the performances of the Bayesian deep learning methods and their uncertainties. Section 5 discusses the convergence issues of Bayesian neural networks. Finally, Section 6 presents the main findings and limitations of the study, and includes concluding remarks.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "Manuscript.tex",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.345,
      "datasets_score": 0.325,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668570",
      "updated_at": "2025-08-11T23:43:05.607019",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16224",
      "title": "LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D\n  object detection",
      "authors": [
        "Jijun Wang",
        "Yan Wu",
        "Yujian Mo",
        "Junqiao Zhao",
        "Jun Yan",
        "Yinghao Hu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing LiDAR-Camera fusion methods have achieved strong results in 3D\nobject detection. To address the sparsity of point clouds, previous approaches\ntypically construct spatial pseudo point clouds via depth completion as\nauxiliary input and adopts a proposal-refinement framework to generate\ndetection results. However, introducing pseudo points inevitably brings noise,\npotentially resulting in inaccurate predictions. Considering the differing\nroles and reliability levels of each modality, we propose LDRFusion, a novel\nLidar-dominant two-stage refinement framework for multi-sensor fusion. The\nfirst stage soley relies on LiDAR to produce accurately localized proposals,\nfollowed by a second stage where pseudo point clouds are incorporated to detect\nchallenging instances. The instance-level results from both stages are\nsubsequently merged. To further enhance the representation of local structures\nin pseudo point clouds, we present a hierarchical pseudo point residual\nencoding module, which encodes neighborhood sets using both feature and\npositional residuals. Experiments on the KITTI dataset demonstrate that our\nframework consistently achieves strong performance across multiple categories\nand difficulty levels.",
      "published_date": "2025-07-22T04:35:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16224v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16224v1",
      "latex_url": "http://arxiv.org/src/2507.16224v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In the field of autonomous driving and advanced driving assistance systems (ADAS)~, 3D object detection provides critical spatial information, such as the size, orientation, and precise location of objects in the real world. This capability is essential for downstream tasks like trajectory prediction, collision avoidance, and decision-making in complex driving scenarios.\n\nVarious sensors can serve as inputs for the 3D detection task, including LiDAR, RGB cameras, millimeter-wave radar etc. Among them, LiDAR stands out for its ability to provide precise positional coordinates. Therefore, many methods~ been developed to process point cloud data and achieve effective object recognition. Considering the spatial structural characteristics of point clouds, existing methods often employ voxel-based data processing methods and utilize sparse convolution for feature extraction. To further enhance detection capabilities, some methods adopt a two-stage detection framework, leveraging RoI pooling~ or graph neural networks~ to refine proposals generated by the Region Proposal Network (RPN).\n\nHowever, LiDAR-based methods are hindered by the sparsity of point clouds, resulting in weaker detection performance for distant or structurally ambiguous objects, as illustrated by blue boxes in Figure~ (a) and (b). To address this limitation, multi-sensor fusion approaches~ have attempted to incorporate RGB cameras into 3D perception pipelines. Unlike point clouds, images can provide rich semantic information and are densely distributed on a 2D pixel plane. Given the heterogeneity between images and point clouds, some methods~ perform depth estimation on images and project them into 3D space to generate pseudo point clouds. As illustrated in Figure~ (a), pseudo point clouds are treated as another type of point cloud and fused equally with real point clouds to generate bounding boxes. Nevertheless, since projecting images into 3D space is an ill-posed problem, the genrated \"fake\" point clouds inevitably contain noise. In Figure~ (b), pseudo points with uncertain depth and semantics within the purple box cause the detector to misclassify background regions as vehicles. In contrast, LiDAR alleviates false detections through its precise short-range localization capability.\n\n {figure*}[htpb]\n \n [scale=0.45]{figs/vis_moti.png}\n {Visualization of detection results from different models: (a) using real point clouds, (b) using real and pseudo point clouds, and (c) using the proposed refinement strategy (ours). Ground truths are shown in red boxes, predictions in green. Black points inside predictions represent pseudo point clouds, while red points indicate real ones.}\n\n {figure*}\n\n {figure}[htpb]\n \n [scale=0.4]{figs/teaser.png}\n {Comparison of different fusion paradigms. Previous approaches rely on a symmetric fusion strategy. In contrast, we adopt a LiDAR-dominant two-stage refinement scheme, which integrates instance-level outputs from multiple stages.}\n\n {figure}\n\nConsidering both the advantages and limitations of each sensor, along with the requirement for distance prediction in 3D detection, LiDAR is well-suited to be the primary sensor, while the camera should serve as an auxiliary modality. In light of the above observations and analysis, we present a simple and effective LiDAR-Camera fusion framework, termed LDRFusion, which enables pseudo point clouds and real point clouds to complement each other more effectively. As shown in Figure~ (b), we adopt a two-stage asymmetric cascade optimization strategy. The first stage of the framework exclusively processes real point clouds to generate initial bounding boxes. In the subsequent stage, both types of point clouds are fed into the network to mitigate the sparsity issue. During inference, the detection results from different stages are integrated through a weighted fusion strategy. This progressive refinement paradigm fully leverages the inherent characteristics of both types of point clouds to improve both recall and precision of the model.\n\nMoreover, the capacity to extract local features from each type of point clouds also influences recognition performance. Due to the modality gap between LiDAR and cameras, designing an appropriate feature encoder for pseudo points remains a problem. Given that existing methods neglect the fine-grained variations among points, we devise a hierarchical pseudo point residual encoding (HPR) module that jointly models positional and feature residuals of pseudo point clouds. This design captures local structural and contextual relationships while maintaining computational efficiency.\n\nOur contributions can be summarized as follows:\n {itemize}\n\n  We propose a straightforward LiDAR-dominant refinement method to fully exploit the complementary feature information between real point clouds and pseudo point clouds.\n  We introduce an efficient feature extractor based on residuals, which enhances the contextual representation ability of each pseudo point cloud.\n  Extensive experiments on the KITTI dataset demonstrate that our method effectively enhances the detection performance of the model without introducing significant computational overhead.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.378,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.415,
      "distributed_training_score": 0.357,
      "datasets_score": 0.345,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper presents a LiDAR-dominant framework for 3D object detection, focusing on sensor fusion and refinement of proposals using real and pseudo point clouds. It does not involve diffusion models, iterative denoising processes, or any adaptation for multi-step logical reasoning tasks. The refinement described is specific to computer vision for object detection, not holistic correction of reasoning paths as defined in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667984",
      "updated_at": "2025-08-11T23:43:05.606904",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16226",
      "title": "Distilled Large Language Model in Confidential Computing Environment for\n  System-on-Chip Design",
      "authors": [
        "Dong Ben",
        "Hui Feng",
        "Qian Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications.",
      "published_date": "2025-07-22T04:41:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16226v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16226v1",
      "latex_url": "http://arxiv.org/src/2507.16226v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large language models (LLMs) show significant capabilities in processing multimodal information, performing complex reasoning, and generating code, making them valuable tools for Computer-aided Design (CAD) tasks in System-on-Chip (SoC) designs. However, the use of LLMs in circuit design inevitably involves providing sensitive information, such as RTL designs or circuit specifications. This raises concerns about potential data breaches and the exposure of proprietary specifications through reverse data inference attacks. Even when smaller LLMs such as DeepSeek-R1 (1.5 billion parameters) or Llama 3.2 (1 billion parameters) are deployed locally to preserve data privacy, sensitive information may still be vulnerable to memory-snooping or side-channel attacks. Confidential computing through Trusted Execution Environments (TEEs) provides a promising solution to secure both data and model parameters.\n\nTEE provides a secure enclave for computation and safeguards both data and models in cloud computing environments. Prior research has leveraged TEEs, such as Intel’s Software Guard Extensions (SGX) , to secure ML workloads by isolating sensitive computations within secure enclaves . While SGX offers strong protection, its limited memory capacity (approximately 1 GB) and complex interface pose challenges for large-scale ML applications . To overcome these limitations, we use Intel’s Trust Domain Extensions (TDX) , which introduces secure Virtual Machines (VMs) that can process larger ML models within the secure enclaves.\n\nEven though TDX significantly expands memory capacity compared to its predecessors, deploying advanced LLMs with large parameter counts such as GPT and Gemini in a confidential computing environment presents additional challenges.\nTo begin with, these models are relatively large, typically starting from at least 1 billion parameters and reaching over 100 billion or more. Even hosting LLMs locally faces significant difficulties due to memory constraints . In the SoC design domain, LLMs are increasingly used to process sensitive data such as netlists, design constraints, and proprietary specifications, further elevating the need for secure execution. In addition, LLMs used in hardware design are often fine-tuned with proprietary datasets and specifications , which must be evaluated within confidential environments to ensure data security. Furthermore, both training and inference require substantial computational resources, and ensuring secure, optimized deployment in TEE-based settings like TDX involves resolving key constraints.\n\nAmong these LLMs, DeepSeek stands out as an advanced model optimized for efficient resource utilization . Its key distinction lies in its ability to conserve computational resources while maintaining strong reasoning capabilities. For example, the distilled version of DeepSeek significantly reduces model size and memory footprint, enabling effective performance. This efficiency is particularly valuable in secure settings such as TDX, where both performance and data security are critical .\n\nThis study explores the adaptability of LLMs in secure computing environments, with a particular emphasis on their use in SoC designs. We compare performance across TEE (confidential computing), CPU-only, and hybrid CPU-GPU implementations to assess their computational efficiency and suitability for confidential computing. This work aims to identify optimal models and deployment strategies for LLMs on resource-constrained devices, with a focus on maintaining robust data security.\n\nThe main contributions of this paper are summarized as follows:\n {itemize}\n   We compare the performance across TEE-based, CPU-only, and CPU-GPU implementations, identifying key performance bottlenecks and trade-offs between security and computational efficiency. These findings offer practical insights for confidential computing vendors aiming to address scalability challenges in secure AI workloads.\n   To the best of our knowledge, this is the first evaluation of a distilled LLM within a TEE. We provide a comprehensive analysis of its performance and behavior in a confidential computing environment.\n   We evaluate lightweight LLMs with fewer than 8 billion parameters running in TEEs for use in SoC design. The results show that their performance in TEEs exceeds that of traditional CPU-only execution.\n\n   We conduct a detailed benchmarking analysis of different quantization levels of LLMs and assess their efficiency for deployment in confidential computing settings.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.417,
      "weak_supervision_score": 0.415,
      "diffusion_reasoning_score": 0.453,
      "distributed_training_score": 0.507,
      "datasets_score": 0.368,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on evaluating and deploying distilled LLMs in confidential computing environments for SoC design, with no mention of human feedback, reward models, or reinforcement learning techniques for model alignment.",
      "weak_supervision_justification": "The paper discusses the deployment and performance of pre-trained LLMs in secure settings, but does not address training methods, programmatically generated labels, or any form of weak supervision for model training.",
      "diffusion_reasoning_justification": "The paper evaluates LLMs like DeepSeek for performance in TEEs and SoC tasks, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion techniques.",
      "distributed_training_justification": "The paper compares CPU-only, CPU-GPU hybrid, and TEE-based implementations for LLM inference, which involves parallel computing elements, but it does not focus on distributed training algorithms, data partitioning across nodes, or strategies for accelerating model training; instead, it emphasizes secure deployment and performance evaluation.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668579",
      "updated_at": "2025-08-11T23:43:05.607021",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16227",
      "title": "Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion\n  Experiments via Artificial Intelligence",
      "authors": [
        "Zixu Wang",
        "Yuhan Wang",
        "Junfei Ma",
        "Fuyuan Wu",
        "Junchi Yan",
        "Xiaohui Yuan",
        "Zhe Zhang",
        "Jie Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This work presents predictive hydrodynamic simulations empowered by\nartificial intelligence (AI) for laser driven implosion experiments, taking the\ndouble-cone ignition (DCI) scheme as an example. A Transformer-based deep\nlearning model MULTI-Net is established to predict implosion features according\nto laser waveforms and target radius. A Physics-Informed Decoder (PID) is\nproposed for high-dimensional sampling, significantly reducing the prediction\nerrors compared to Latin hypercube sampling. Applied to DCI experiments\nconducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict\nthe implosion dynamics measured by the x-ray streak camera. It is found that an\neffective laser absorption factor about 65\\% is suitable for the\none-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean\nimplosion velocity and collided plasma density reached 195 km/s and 117 g/cc,\nrespectively. This study demonstrates a data-driven AI framework that enhances\nthe prediction ability of simulations for complicated laser fusion experiments.",
      "published_date": "2025-07-22T04:57:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16227v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16227v1",
      "latex_url": "http://arxiv.org/src/2507.16227v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "There has been many progress toward the realization of laser driven fusion energy in the past decades~. In 2022, the National Ignition Facility (NIF) realized indirect-drive fusion ignition with an energy gain larger than one for the first time~. In 2024, The OMEGA facility achieved the direct-drive hot-spot ignition~. Recently, the study of the double-cone ignition (DCI) scheme showed promising prospects for the laser fusion energy~. However, the development of laser fusion still faces a major challenge: the traditional hydrodynamic simulations have poor prediction ability for complicated experiments.\n\nIn recent years, artificial intelligence has provided new ideas for fusion research ~. By constructing a surrogate model to replace traditional simulations, AI model can significantly improve computational efficiency~. Moreover, advanced artificial intelligence techniques such as transfer learning and Bayesian inference provide a possibility to bridge the gap between simulations and experiments~. However, the existing studies still have some limitations. Firstly, the traditional multilayer perceptron (MLP) architecture has a low efficiency for long sequences of laser waveforms. Secondly, the Latin Hypercube Sampling (LHS) method cannot satisfy the sampling quality requirements in the high-dimensional space.\n\nIn this work, we propose a novel artificial intelligence method to improve the prediction ability of hydrodynamic simulations for laser fusion experiments. The method has three characteristics. (1) A deep learning model MULTI-Net is established based on the Transformer architecture to match the time sequence of laser waveform, (2) A high-quality dataset is constructed to increase the prediction ability of the surrogate model by using a physics-informed decoder. (3) The deep learning model is employed to predict laser driven implosion experiments on the SG-II facility.\n\nThis paper is organized as follows. Sec.~ describes the workflow of the AI-empowered prediction method. Sec.~ and Sec.~ present the training and improvement of the MULTI-Net model with the physics-informed decoder sampling method. Sec.~ applies the MULTI-Net model to predict DCI implosion experiments. Finally, a summary and discussions are given in Sec.~.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "hpl-sample.tex",
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.341,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.428,
      "datasets_score": 0.334,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a Transformer-based deep learning model (MULTI-Net) for predicting hydrodynamic simulations in laser fusion experiments. It does not involve diffusion models, iterative refinement processes for logical tasks, or treating a Chain-of-Thought as a single entity for holistic correction. There is no component for multi-step logical reasoning using diffusion techniques.",
      "distributed_training_justification": "The paper describes the development and application of a deep learning model for simulations but does not address distributed training, parallel computing, or multi-node machine learning. There is no mention of partitioning data, model architecture, or computation across multiple processors or nodes to accelerate training.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668589",
      "updated_at": "2025-08-11T23:43:05.607023",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16228",
      "title": "MONITRS: Multimodal Observations of Natural Incidents Through Remote\n  Sensing",
      "authors": [
        "Shreelekha Revankar",
        "Utkarsh Mall",
        "Cheng Perng Phoo",
        "Kavita Bala",
        "Bharath Hariharan"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Natural disasters cause devastating damage to communities and infrastructure\nevery year. Effective disaster response is hampered by the difficulty of\naccessing affected areas during and after events. Remote sensing has allowed us\nto monitor natural disasters in a remote way. More recently there have been\nadvances in computer vision and deep learning that help automate satellite\nimagery analysis, However, they remain limited by their narrow focus on\nspecific disaster types, reliance on manual expert interpretation, and lack of\ndatasets with sufficient temporal granularity or natural language annotations\nfor tracking disaster progression. We present MONITRS, a novel multimodal\ndataset of more than 10,000 FEMA disaster events with temporal satellite\nimagery and natural language annotations from news articles, accompanied by\ngeotagged locations, and question-answer pairs. We demonstrate that fine-tuning\nexisting MLLMs on our dataset yields significant performance improvements for\ndisaster monitoring tasks, establishing a new benchmark for machine\nlearning-assisted disaster response systems. Code can be found at:\nhttps://github.com/ShreelekhaR/MONITRS",
      "published_date": "2025-07-22T04:59:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16228v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16228v1",
      "latex_url": "http://arxiv.org/src/2507.16228v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure}[ht]\n  \n  [width=0.9 ]{images/title_fig_v2.pdf}\n  {Using news articles, we extract exact locations of disaster events and corresponding captions for event timelines. Our MONITRS dataset enables precise disaster monitoring, as shown in this Minnesota severe storm sequence. The May 27th image shows evidence of flooding with increased vegetation and darker water-saturated regions. Models finetuned with MONITRS correctly identify the temporal onset of the storm while baseline models fail to detect the initial evidence.}\n\n {figure}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "figures/intro_fig.tex",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.32,
      "distributed_training_score": 0.347,
      "datasets_score": 0.419,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of MONITRS, a novel multimodal dataset for disaster monitoring, which directly aligns with research on creating datasets for machine learning and AI applications. It details dataset curation methodologies, such as extracting data from FEMA events, news articles, satellite imagery, and annotations, and evaluates its utility through fine-tuning models and establishing benchmarks. This fits the topic's focus on dataset creation, analysis, and benchmarking.",
      "summary": "The paper introduces MONITRS, a novel multimodal dataset designed to enhance disaster monitoring through remote sensing, comprising over 10,000 FEMA disaster events with temporal satellite imagery, natural language annotations extracted from news articles, geotagged locations, and question-answer pairs. The authors demonstrate that fine-tuning existing multimodal large language models on this dataset significantly improves performance in identifying and tracking disaster progression, addressing limitations in current methods and establishing a new benchmark for machine learning-assisted disaster response systems.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new dataset that integrates multimodal data for comprehensive disaster monitoring, significantly advancing the state-of-the-art by addressing gaps in temporal granularity and natural language annotations.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future research in computer vision and emergency management, potentially leading to real-world applications in disaster response systems that save lives and resources.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers a valuable and practical contribution with its innovative dataset and demonstrated improvements, making it essential for researchers in computer vision and disaster monitoring but not universally critical for all audiences.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f243f54b20e12d7cec32ad0efadb9bfee0993713",
      "h_index_fetch_method": "full_id",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 21,
      "average_h_index": 6.4,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Shreelekha Revankar",
          "profile_url": "https://www.semanticscholar.org/author/2211110100",
          "h_index": 1
        },
        {
          "name": "Utkarsh Mall",
          "profile_url": "https://www.semanticscholar.org/author/107979543",
          "h_index": 7
        },
        {
          "name": "Cheng Perng Phoo",
          "profile_url": "https://www.semanticscholar.org/author/2264361525",
          "h_index": 2
        },
        {
          "name": "Kavita Bala",
          "profile_url": "https://www.semanticscholar.org/author/2273644672",
          "h_index": 1
        },
        {
          "name": "B. Hariharan",
          "profile_url": "https://www.semanticscholar.org/author/73710317",
          "h_index": 21
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667992",
      "updated_at": "2025-08-11T23:44:50.243491",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16229",
      "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health\n  Delivery",
      "authors": [
        "Bo Wen",
        "Chen Wang",
        "Qiwei Han",
        "Raquel Norel",
        "Julia Liu",
        "Thaddeus Stappenbeck",
        "Jeffrey L. Rogers"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)",
        "cs.HC (Human-Computer Interaction)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.",
      "published_date": "2025-07-22T05:01:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16229v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16229v1",
      "latex_url": "http://arxiv.org/src/2507.16229v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Healthcare systems worldwide face growing challenges in allocating limited medical resources to meet increasing demand~. Traditional healthcare delivery models, centered on episodic patient-provider interactions, often result in significant gaps in continuous care, particularly in preventive health monitoring and chronic disease management~. These shortcomings disproportionately affect vulnerable populations, including those with limited access to healthcare facilities~, lower technological literacy~, or socio-economic constraints~.\n\nThe advent of Large Language Models (LLMs) and multimodal AI has opened new avenues for digital health applications~, notably in voice-based patient engagement~. Unlike earlier rule-based conversational agents, modern AI-driven voice assistants can facilitate context-aware, adaptive, and natural conversations that dynamically adjust to user preferences, health literacy levels, and immediate needs~. Voice, as humanity's most intuitive mode of communication, reduces engagement barriers and broadens access to healthcare, especially for underserved communities~.\n\nFor healthcare providers, these technologies promise to extend clinical capabilities beyond facility walls while optimizing resource allocation. A primary care physician managing thousands of patients can realistically engage with only dozens per day, leaving a vast monitoring gap that technology must fill. Meanwhile, technologists seek platforms where AI advances can deliver measurable health outcomes and sustainable business models. At this intersection lies the potential for voice-based AI agents to transform healthcare delivery economics.\n\nThis paper presents our viewpoint on how voice-based AI agents can help bridge significant economic gaps in healthcare delivery. We argue that voice interaction, combined with modern AI capabilities, provides an effective ``entry point\" for healthcare services, enabling scalable, cost-effective, and equitable solutions. Based on our clinical trial experience with Agent PULSE (Patient Understanding and Liaison Support Engine)—developed by IBM Research and validated in collaboration with the Cleveland Clinic Foundation and Morehouse School of Medicine—we outline an economic model that demonstrates where AI agents can enhance patient monitoring in scenarios where human medical expertise is either unavailable or economically unjustifiable.\n\nWe also highlight the technical challenges, scaling opportunities, and policy implications associated with deploying AI-powered voice agents in digital health ecosystems. For clinicians and healthcare administrators, we provide actionable insights on implementation pathways and economic models. For technologists, we outline technical optimization priorities that directly impact healthcare delivery quality. For policymakers, we identify regulatory considerations that balance innovation with patient safety. This perspective aims to inform research, guide development, and shape policies to create accessible, cost-efficient, and patient-centered AI health solutions.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.409,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.331,
      "datasets_score": 0.345,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper primarily discusses the application of voice-based AI agents, such as LLM-powered assistants, in healthcare for improving accessibility and economic efficiency. It covers topics like pilot studies, patient feedback, technical challenges, and economic models, but does not mention, describe, or utilize Reinforcement Learning from Human Feedback (RLHF). There is no reference to training a reward model, using human-ranked data for fine-tuning, or applying reinforcement learning techniques. Therefore, the paper's main contribution is unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669754",
      "updated_at": "2025-08-11T23:43:05.607186",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16238",
      "title": "Positive Style Accumulation: A Style Screening and Continuous\n  Utilization Framework for Federated DG-ReID",
      "authors": [
        "Xin Xu",
        "Chaoyue Ren",
        "Wei Liu",
        "Wenke Huang",
        "Bin Yang",
        "Zhixi Yu",
        "Kui Jiang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The Federated Domain Generalization for Person re-identification (FedDG-ReID)\naims to learn a global server model that can be effectively generalized to\nsource and target domains through distributed source domain data. Existing\nmethods mainly improve the diversity of samples through style transformation,\nwhich to some extent enhances the generalization performance of the model.\nHowever, we discover that not all styles contribute to the generalization\nperformance. Therefore, we define styles that are beneficial or harmful to the\nmodel's generalization performance as positive or negative styles. Based on\nthis, new issues arise: How to effectively screen and continuously utilize the\npositive styles. To solve these problems, we propose a Style Screening and\nContinuous Utilization (SSCU) framework. Firstly, we design a Generalization\nGain-guided Dynamic Style Memory (GGDSM) for each client model to screen and\naccumulate generated positive styles. Meanwhile, we propose a style memory\nrecognition loss to fully leverage the positive styles memorized by Memory.\nFurthermore, we propose a Collaborative Style Training (CST) strategy to make\nfull use of positive styles. Unlike traditional learning strategies, our\napproach leverages both newly generated styles and the accumulated positive\nstyles stored in memory to train client models on two distinct branches. This\ntraining strategy is designed to effectively promote the rapid acquisition of\nnew styles by the client models, and guarantees the continuous and thorough\nutilization of positive styles, which is highly beneficial for the model's\ngeneralization performance. Extensive experimental results demonstrate that our\nmethod outperforms existing methods in both the source domain and the target\ndomain.",
      "published_date": "2025-07-22T05:21:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16238v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16238v1",
      "latex_url": "http://arxiv.org/src/2507.16238v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "failed",
      "introduction_text": "In recent years, Person re-identification (ReID) has garnered significant research attention, aiming to achieve accurate cross-camera recognition of the same individual. With the success of deep learning, numerous high-performance ReID methods have been proposed . However, constrained by limitations in data collection and the complexity of real-world scenarios, these methods often underperform when deployed to unseen domains. To address this, recent efforts have focused on Domain Generalization for Person re-identification (DG-ReID) . DG-ReID aims to train models on multiple source domains and test on unseen target domains, thereby enhancing robust cross-domain generalization. However, existing approaches rely on large-scale centralized labeled datasets, which often raise critical data privacy concerns in practical applications .\n\nTo solve this issue, federated learning has been introduced into DG-ReID , termed Federated Domain Generalization for Person re-identification (FedDG-ReID). Federated learning is a distributed machine learning framework that facilitates knowledge sharing through a cross-device/cross-institution collaborative training paradigm while rigorously safeguarding data privacy. The technology embodies the core principle of \"moving models, not data,\" where participants solely exchange updates to model parameters while retaining raw data locally, thereby effectively addressing the dual challenges of data silos and privacy leakage. However, due to the limited amount of data that each client can access, and the significant heterogeneity between the data of different clients, traditional centralized generalization strategies cannot be directly applied. Existing methods predominantly focus on generating synthetic data via style transfer to simulate unseen domains . However, as shown in Fig. ,these methods overlook that not all styles contribute to the model's generalization performance, thereby lacking the capability to screen and utilize positive styles, which is crucial for the model’s generalization performance in both the source and target domains.\n\nIn this paper, we propose a Style Screening and Continuous Utilization (SSCU) framework to address the previously outlined issues of positive style selection, memory, and continuous utilization, achieving robust cross-domain generalization while ensuring privacy preservation. Specifically, we design a  {G}eneralization  {G}ain-guided  {D}ynamic  {S}tyle  {M}emory (GGDSM) for each client to enable selection and cross-round accumulation of positive styles. (1) Initialization: To build a robust identity-discriminative feature representation for each person identity, we perform clustering and averaging of all training data based on identity before the official training starts on each client. category prototypes emphasize consistency within classes and differences between classes, and can be used to guide the model to learn more discriminative feature representations. (2) Positive style selection: At the end of each training round on the client, we evaluate the optimization effect of the generated styles on the global model. Based on the evaluation results, we determine whether these generated styles are positive for the model update, and if they are, we update them to the memory for continuous utilization, otherwise, we consider them to be negative styles and discard them directly. (3) Update strategy: Category prototypes are updated via momentum-based integration, ensuring stable incorporation of new styles while preserving previously memorized positive patterns. This helps the model progressively learn domain-invariant feature extraction capabilities. Furthermore, in order to realize the full use of the style in the memory, we propose a  {C}ollaborative  {S}tyle  {T}raining (CST) training strategy comprising two parallel training branches: (1) New style adaptation branch: In each iteration, new stylized data is randomly generated, and features are extracted using the client-global model for loss calculation. The client-global model downloaded from the server possesses better generalization knowledge, making it more suitable for rapidly learning new style changes within a short period of time. (2) Positive style continuous utilization branch: In this branch, the client-local model and the client-global model are trained using the original images, and then optimized using a loss function based on the dynamic style memory. Since the category prototypes stored in the memory remember all the positive styles from previous rounds, this branch allows the model to continuously make use of them.\n\nOur main contributions can be summarized as follows:\n\n {itemize}\n  Empirical Contribution. We discover that not all styles generated through style transformation methods contribute to the improvement of model generalization performance. Some styles may introduce invalid data, which is instead detrimental to model optimization.\n\n  Framework Contribution. We propose a style screening and continuous utilization framework that effectively screens, memorizes, and continuously utilizes generative styles beneficial to model generalization performance with minimal additional overhead.\n\n  Technical Contribution. We propose GGDSM and CST. GGDSM screens and memorizes styles that are positive for model generalization, while CST leverages both newly generated styles and the accumulated positive styles stored in memory to train client models. This enables the models to quickly adapt to new styles and continuously utilize positive styles, resulting in significant improvements in generalization performance in both the source and target domains.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sample-sigconf.tex",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.358,
      "distributed_training_score": 0.419,
      "datasets_score": 0.353,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution involves federated learning for Domain Generalization in Person re-identification (FedDG-ReID), which is a form of distributed training. It describes a framework where multiple clients train models locally on their data and share model updates with a global server, aligning directly with distributed training principles such as partitioning data across nodes and aggregating computations. Techniques like GGDSM and CST are integrated into this distributed setup to enhance training efficiency and generalization, making the paper's contributions central to distributed training algorithms and systems.",
      "datasets_justification": "below_threshold",
      "summary": "The paper addresses limitations in Federated Domain Generalization for Person Re-identification (FedDG-ReID) by proposing a Style Screening and Continuous Utilization (SSCU) framework that identifies and accumulates positive styles through a Generalization Gain-guided Dynamic Style Memory (GGDSM) while discarding negative ones. It introduces Collaborative Style Training (CST) to train client models using both newly generated styles and stored positive styles, enhancing model generalization across source and target domains, with experimental results showing superior performance compared to existing methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing style screening and continuous utilization mechanisms in federated learning, cleverly combining existing style transformation techniques to address overlooked issues in generalization performance.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of federated learning and domain generalization for computer vision, as it effectively tackles privacy and style utilization challenges in real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution to FedDG-ReID by enhancing model generalization through innovative style management, making it essential for researchers focused on privacy-preserving computer vision techniques.",
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "H-index fetching failed: not found in Semantic Scholar"
      ],
      "created_at": "2025-08-11T23:15:40.668002",
      "updated_at": "2025-08-11T23:44:52.799058",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16240",
      "title": "Scale Your Instructions: Enhance the Instruction-Following Fidelity of\n  Unified Image Generation Model by Self-Adaptive Attention Scaling",
      "authors": [
        "Chao Zhou",
        "Tianyi Wei",
        "Nenghai Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advancements in unified image generation models, such as OmniGen, have\nenabled the handling of diverse image generation and editing tasks within a\nsingle framework, accepting multimodal, interleaved texts and images in free\nform. This unified architecture eliminates the need for text encoders, greatly\nreducing model complexity and standardizing various image generation and\nediting tasks, making it more user-friendly. However, we found that it suffers\nfrom text instruction neglect, especially when the text instruction contains\nmultiple sub-instructions. To explore this issue, we performed a perturbation\nanalysis on the input to identify critical steps and layers. By examining the\ncross-attention maps of these key steps, we observed significant conflicts\nbetween neglected sub-instructions and the activations of the input image. In\nresponse, we propose Self-Adaptive Attention Scaling (SaaS), a method that\nleverages the consistency of cross-attention between adjacent timesteps to\ndynamically scale the attention activation for each sub-instruction. Our SaaS\nenhances instruction-following fidelity without requiring additional training\nor test-time optimization. Experimental results on instruction-based image\nediting and visual conditional image generation validate the effectiveness of\nour SaaS, showing superior instruction-following fidelity over existing\nmethods. The code is available https://github.com/zhouchao-ops/SaaS.",
      "published_date": "2025-07-22T05:25:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16240v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16240v1",
      "latex_url": "http://arxiv.org/src/2507.16240v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure*}[h]\n  \n  [width= ]{crossattn.pdf}\n  {Cross-attention maps for the input image and different sub-instructions. We can get three key observations: (a) we can pre-identify the regions where each sub-instruction will appear according to the corresponding cross-attention map; (b) the regions of activation for the neglected sub-instruction are highly conflicting with those for the input image, where the input image dominates (red box); (c) the cross-attention maps remain highly consistent across adjacent timesteps.}\n\n  {-1em}\n {figure*}\n\nIn recent years, image generation models have advanced rapidly. Using the Latent Diffusion Model (LDM) series as a benchmark, researchers have continuously improved the generated image quality. However, this progress has come at the cost of increasing model size and a growing reliance on larger, more complex text encoders to process text instructions. Moreover, for complex downstream tasks such as image editing and visual conditional image generation, these models often require additional structures or specialized methods , making them less accessible and user-friendly.\n\nUnlike the LDM series, unified image generation models such as OmniGen are trained on large unified output datasets, enabling them to handle diverse and complex downstream tasks within a single diffusion framework. Notably, OmniGen achieves this with remarkable efficiency, featuring a minimalistic yet powerful architecture composed of only two core components: a VAE and a transformer model, without relying on additional text encoders. This streamlined architecture allows OmniGen to accept interwoven text prompts and image inputs as conditions for guiding image generation. Achieving comparable generation quality, OmniGen balances a lightweight design with enhanced user-friendliness.\n\nAs an all-in-one editing model, OmniGen demonstrates strong instruction-based image editing capabilities. However, as shown in Fig. , it frequently overlooks specific text instructions, particularly when handling multiple sub-instructions within a single prompt. To uncover the root causes of this issue, we conducted input perturbation experiments to pinpoint critical steps and layers in the denoising process. By further analyzing cross-attention maps at these key stages, we examined how generated pixels correlate with different input tokens, shedding light on the underlying mechanisms behind instruction adherence and omission.\n\nInterestingly, our investigation revealed that the tendency to overlook instructions arises from significant conflicts between the activated regions on the cross-attention maps for the neglected sub-instructions and the input image. As illustrated in Fig. , the brightness of the maps reflects the magnitude of the activation values, with brighter regions indicating higher activations. In the red-boxed area (the bike region of the generated image), the input image exhibits much stronger activations than the neglected sub-instruction, effectively suppressing its influence. Additionally, we made two key observations: first, the regions with high activation values correspond roughly to areas where the sub-instructions influence the generated image; second, there is notable consistency in cross-attention between adjacent timesteps.\n\nTo address the issue of neglected sub-instructions, we propose Self-Adaptive Attention Scaling (SaaS), a method that enhances the instruction-following fidelity of unified image generation models like OmniGen without requiring additional training or test-time optimization.\nBuilding on the previously observed conflicts between the activation regions of text instructions and input images in the cross-attention maps, we adaptively scale the cross-attention values corresponding to the instructions during the denoising process.\nThis approach is essentially a free lunch for inference-time scaling, as it leverages the consistency of the cross-attention maps between adjacent denoising timesteps. At timestep $t$, we extract the mask for each sub-instruction and calculate the scaling factor. At timestep $t-1$, we apply the scaling factor to the activation values within the masked region of the corresponding sub-instruction. Masks scaling factors are iteratively updated throughout the denoising process.\n\nExperimental results demonstrate that SaaS significantly enhances instruction-following fidelity across both image editing and visual conditional image generation tasks, ensuring more precise and consistent outputs.\n\nOur contributions can be summarized as follows.\n {itemize}\n   We identified for the first time that unified image generation models like OmniGen tend to overlook text instructions and confirmed the vital steps and layers in the denoising process through input perturbation analysis.\n   We attributed the tendency to overlook instructions to conflicts between the activated regions of the neglected sub-instructions and the input image in the cross-attention maps, as revealed by analyzing the cross-attention maps of vital steps and layers.\n   We propose SaaS, a novel self-adaptive attention scaling method to enhance instruction-following fidelity without any additional training or test-time optimization.\n   Qualitative and quantitative results demonstrate the effectiveness of the proposed SaaS.\n\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.377,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.463,
      "distributed_training_score": 0.377,
      "datasets_score": 0.322,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on enhancing instruction-following in image generation models through attention scaling during inference, without any discussion of training models using programmatically generated labels or weak supervision sources. It does not involve generating or using noisy labels for training, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper applies diffusion models to image generation and editing tasks, using iterative refinement for attention adjustments, but it does not adapt this process for complex logical tasks or multi-step reasoning like Chain-of-Thought. It lacks any component for holistic correction of reasoning paths, focusing solely on visual outputs.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668010",
      "updated_at": "2025-08-11T23:43:05.606909",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16241",
      "title": "eX-NIDS: A Framework for Explainable Network Intrusion Detection\n  Leveraging Large Language Models",
      "authors": [
        "Paul R. B. Houssel",
        "Siamak Layeghy",
        "Priyanka Singh",
        "Marius Portmann"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces eX-NIDS, a framework designed to enhance\ninterpretability in flow-based Network Intrusion Detection Systems (NIDS) by\nleveraging Large Language Models (LLMs). In our proposed framework, flows\nlabelled as malicious by NIDS are initially processed through a module called\nthe Prompt Augmenter. This module extracts contextual information and Cyber\nThreat Intelligence (CTI)-related knowledge from these flows. This enriched,\ncontext-specific data is then integrated with an input prompt for an LLM,\nenabling it to generate detailed explanations and interpretations of why the\nflow was identified as malicious by NIDS. We compare the generated\ninterpretations against a Basic-Prompt Explainer baseline, which does not\nincorporate any contextual information into the LLM's input prompt. Our\nframework is quantitatively evaluated using the Llama 3 and GPT-4 models,\nemploying a novel evaluation method tailored for natural language explanations,\nfocusing on their correctness and consistency. The results demonstrate that\naugmented LLMs can produce accurate and consistent explanations, serving as\nvaluable complementary tools in NIDS to explain the classification of malicious\nflows. The use of augmented prompts enhances performance by over 20% compared\nto the Basic-Prompt Explainer.",
      "published_date": "2025-07-22T05:26:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16241v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16241v1",
      "latex_url": "http://arxiv.org/src/2507.16241v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large Language Models (LLMs) have revolutionised Natural Language Processing (NLP), excelling in tasks involving unstructured data such as text generation, contextual understanding, and language translation. Their great performance has led to widespread adoption in conversational AI, code generation, and beyond. However, applying LLMs in cybersecurity, specifically for Network Intrusion Detection Systems (NIDS), remains under-explored.\n\nNIDS are essential for monitoring and analysing network traffic to identify malicious activities and potential security breaches. Current systems rely on a mix of signature-based methods, which detect known attack patterns, and anomaly-based techniques, which identify deviations from typical network behaviour. While deep learning-based NIDS have shown near-perfect performance on benchmark datasets~, they often suffer from a lack of explainability~. This makes it difficult for security analysts to interpret, trust, and act on the detected threats, highlighting this need. Traditional explainable AI techniques which compute an importance score for each feature of the feature space, like SHapley Additive exPlanations (SHAP)~ have limitations. They require a strong understanding of machine learning, focus only on statistical anomalies, and lack contextual insights or external knowledge to explain feature importance. A limitation that potentially could be handled by LLMs. Although a few studies have explored using transformers and LLMs for threat detection~, most adapt these models by replacing their sequence-to-sequence output with classification heads. While suitable for classification tasks, this modification eliminates the models' ability to provide explanations alongside their predictions, a key advantage of LLMs. As such, the potential of LLMs to improve explainability in NIDS remains underutilised. Recent work by Houssel et al.~ has shown that while LLMs may not be optimal for real-time threat prediction due to performance and computational constraints, they offer promising opportunities to enhance the interpretability of NIDS alerts. The same work has shown that LLM's explanations correctly retrieve information from the NetFlow data while being consistent with the feature values. Nonetheless, these models failed to reason logically and be factually accurate.  {Overall, these LLMs can analyse a NetFlow sample as a whole and correctly identify traffic types (e.g., DNS queries or HTTP traffic). While that is useful for network operators, a simpler deterministic algorithm would achieve the same result.}\nFurthermore, it struggles to correlate multiple features together to identify the nature of the attack correctly, instead, it treats individual features which appear suspicious to the model as independent explainability arguments for malicious activity. One significant problem is their tendency to hallucinate, by generating nonsensical or unfaithful content~. These models lack comprehension of facts and logical reasoning, as it has been shown specifically for the network domain by Donadel et al.~.\n\n {This paper proposes a hybrid framework called eX-NIDS, which is designed to complement existing NIDS. By augmenting LLM prompts with Cyber Threat Intelligence (CTI) and context‐specific knowledge, eX-NIDS makes NIDS alerts explainable.} We evaluate this framework using the pre-trained models of Meta's LLama3~ and OpenAI's GPT-4~ on a standardised NetFlow dataset, using a quantitative assessment of its potential to improve the explainability of NIDS. Our assessment framework evaluates the provided explanations for correctness, factual consistency and feature consistency.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "body.tex",
      "rlhf_score": 0.414,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.332,
      "datasets_score": 0.331,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using pre-trained LLMs for generating explanations in NIDS through prompt augmentation, with no mention of training models using human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes LLMs for explanation generation and does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668597",
      "updated_at": "2025-08-11T23:43:05.607025",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16247",
      "title": "PRAC3 (Privacy, Reputation, Accountability, Consent, Credit,\n  Compensation): Long Tailed Risks of Voice Actors in AI Data-Economy",
      "authors": [
        "Tanusree Sharma",
        "Yihao Zhou",
        "Visar Berisha"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Early large-scale audio datasets, such as LibriSpeech, were built with\nhundreds of individual contributors whose voices were instrumental in the\ndevelopment of speech technologies, including audiobooks and voice assistants.\nYet, a decade later, these same contributions have exposed voice actors to a\nrange of risks. While existing ethical frameworks emphasize Consent, Credit,\nand Compensation (C3), they do not adequately address the emergent risks\ninvolving vocal identities that are increasingly decoupled from context,\nauthorship, and control. Drawing on qualitative interviews with 20 professional\nvoice actors, this paper reveals how the synthetic replication of voice without\nenforceable constraints exposes individuals to a range of threats. Beyond\nreputational harm, such as re-purposing voice data in erotic content, offensive\npolitical messaging, and meme culture, we document concerns about\naccountability breakdowns when their voice is leveraged to clone voices that\nare deployed in high-stakes scenarios such as financial fraud, misinformation\ncampaigns, or impersonation scams. In such cases, actors face social and legal\nfallout without recourse, while very few of them have a legal representative or\nunion protection. To make sense of these shifting dynamics, we introduce the\nPRAC3 framework, an expansion of C3 that foregrounds Privacy, Reputation,\nAccountability, Consent, Credit, and Compensation as interdependent pillars of\ndata used in the synthetic voice economy. This framework captures how privacy\nrisks are amplified through non-consensual training, how reputational harm\narises from decontextualized deployment, and how accountability can be\nreimagined AI Data ecosystems. We argue that voice, as both a biometric\nidentifier and creative labor, demands governance models that restore creator\nagency, ensure traceability, and establish enforceable boundaries for ethical\nreuse.",
      "published_date": "2025-07-22T05:39:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16247v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16247v1",
      "latex_url": "http://arxiv.org/src/2507.16247v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Data sharing has long been a contested domain between individual contributors, professionals, and data controllers. Individuals or groups contribute data either deliberatively, whether in pursuit of social value, to receive financial compensation, or as part of their primary profession~. These contributions are influenced by a variety of motivations, such as financial incentives from industry~ and academia~, creative expression and social interaction on online platforms~, participation in public-interest initiatives like Mozilla Common Voice~, LibriSpeech~ and AESDD~. The European Commission estimates that data sharing has the potential to save billions of euros~. Shared data are typically governed by various licensing frameworks, including Creative Commons~, Open Data Commons (ODC)~, GNU General Public License (GPL)~.\n\nDespite these practices, recent legal developments indicate increasing friction between AI companies and creative workers~. In 2024, a YouTube creator initiated a lawsuit against OpenAI, arguing the company transcribed millions of hours of video content to train models like ChatGPT without consent~. Likewise, voice actor Bev Standing filed legal action against TikTok regarding the unauthorized use of her voice in its text-to-speech feature~. Although companies like OpenAI reported that their training datasets consist of publicly available resources~, public access does not automatically grant legal or ethical approval for such usage~. Therefore, creative workers contend that their materials were gathered without authorization, credit, or financial remuneration, resulting in several lawsuits, purported violations of terms of service, and coordinated protests~. These cases reflect broader concerns among creative professionals about the unconsented use of their work for AI training, particularly when such work is copyrighted or personally attributable.\n\nWhile prior work reasonably explored the risk of creative workers, particularly visual artists and writers, research on voice professionals remains limited. Much of the current discourse focuses on fairness in compensation, credit, and consent~ and frames Gen AI as a collaboration tool for creativity~. In contrast, voice actors and narrators carry a unique intersection of creative labor and biometric vulnerability. Unlike textual or visual data, voice is not only expressive but also biometric, and it is uniquely identifiable to a person~. Thus, voice contributors are prone to a wide range of harms, including unauthorized cloning, impersonation, reputational damage, and identity theft~; however these risks have received little systematic attention.\n\nMoreover, voice actors and contributors played a foundational role in the development of speech technologies~. A notable innovation was the early large-scale audio datasets, LibriSpeech, derived from thousands of contributions to LibriVox and other public domain audiobook platforms, underpinned early breakthroughs in automatic speech recognition and the voice assistants we use today~. These contributions, originally made in the spirit of open knowledge and accessibility, have since been repurposed into commercial AI pipelines often without consent, attribution, or safeguards~. A decade later, these same contributions have exposed voice actors to a range of harms and may automate, devalue, or displace the very actors who created it.\nYet, despite their centrality to the voice technology landscape, voice actors remain underrepresented in discussions of data labor and AI ethics and risk pertaining to the voiceprint (both a personal and professional tool for voice actors). To address this urgent gap, this study investigates how professional voice actors perceive, negotiate, and respond to risk in the generative AI landscape.\n\n {enumerate}\n   RQ1: In what ways do voice actors recognize, interpret, and negotiate risk when engaging with digital platforms, clients, and publishers, given the rise of generative AI?\n   RQ2: How do voice actors perceive the long-term risks associated with voice data?\n   RQ3: How do voice actors’ perceptions and lived experiences of risks contribute to forming threat models in assessing risk over time?\n  {enumerate}\n\nTo answer these questions, we interviewed a total of 20 voice actors at different stages of their careers and in different work modes (e.g., freelancers, contract workers, and studio owners). We found that voice actors face unique challenges that are different from other creative workers, including: 1) Biometric Identity Risks: Voice data combines creative work with biometric identity, thus exposing voice actors to unique risks of unauthorized cloning, identity theft, and reputational harm when their recordings are misused in unauthorized illegal contexts.\n2) Long-Tailed Risks: Voice actors face ongoing and evolving risks as their recordings can be continually reused, repurposed, redistributed, and integrated into new AI models long after initial consent, often without their knowledge or further compensation. 3) Difficulties in Data Traceability and Control: Voice actors experience a significant loss of control over their voice data post-delivery, with an absence of effective mechanisms to track how their voice files are used, shared, or altered, particularly for AI training or cloning. Based on this context-dependent risk, we propose PRAC³ framework, which expands the existing C³ (Consent, Credit,\nCompensation) to adapt emerging risks related to voice in Privacy, Reputation, and Accountability.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1-intro.tex",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.323,
      "datasets_score": 0.368,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the introduction of the PRAC3 framework to address ethical risks for voice actors in AI data economies, based on qualitative interviews and focusing on privacy, reputation, and accountability. It does not involve reinforcement learning, human feedback mechanisms, or any technical aspects of training AI models with human-ranked data, making it entirely unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669274",
      "updated_at": "2025-08-11T23:43:05.607133",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16251",
      "title": "HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size\n  Remote Sensing Imagery",
      "authors": [
        "Yu Wang",
        "Bo Dang",
        "Wanchun Li",
        "Wei Chen",
        "Yansheng Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "With the increasing resolution of remote sensing imagery (RSI), large-size\nRSI has emerged as a vital data source for high-precision vector mapping of\ngeographic objects. Existing methods are typically constrained to processing\nsmall image patches, which often leads to the loss of contextual information\nand produces fragmented vector outputs. To address these, this paper introduces\nHoliTracer, the first framework designed to holistically extract vectorized\ngeographic objects from large-size RSI. In HoliTracer, we enhance segmentation\nof large-size RSI using the Context Attention Net (CAN), which employs a\nlocal-to-global attention mechanism to capture contextual dependencies.\nFurthermore, we achieve holistic vectorization through a robust pipeline that\nleverages the Mask Contour Reformer (MCR) to reconstruct polygons and the\nPolygon Sequence Tracer (PST) to trace vertices. Extensive experiments on\nlarge-size RSI datasets, including buildings, water bodies, and roads,\ndemonstrate that HoliTracer outperforms state-of-the-art methods. Our code and\ndata are available in https://github.com/vvangfaye/HoliTracer.",
      "published_date": "2025-07-22T05:55:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16251v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16251v1",
      "latex_url": "http://arxiv.org/src/2507.16251v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Vector maps provide precise representations of the Earth’s surface and are essential for various downstream tasks, such as navigation and urban planning~.\nAutomatic extraction of accurate vector maps from remote sensing imagery (RSI) has emerged as a cost-effective approach, achieving significant progress in recent years~.\n\nWith advancements in remote sensing technology, the resolution of RSI continues to improve, resulting in increasingly large images that require interpretation~.\nHowever, existing vector mapping methods~ are typically designed for small image patches and struggle to process such large-size imagery, as illustrated in Fig.~.\nSpecifically, due to computational constraints, most vectorization algorithms are limited to processing image inputs of $512   512$ pixels~.\nWhen applied to large-size imagery exceeding $10,000   10,000$ pixels, these methods resort to a simplistic patch-based strategy—cropping the image, processing patches independently, and merging the results~.\nThis strategy, however, introduces significant challenges, referred to here as the ``large-size challenge,\" as depicted in Fig.~b.\nOn one hand, the patch-based strategy discards critical contextual information in large-size RSI, hindering the model’s ability to accurately distinguish objects requiring broader context. For instance, buildings may be confused with parking lots without sufficient surrounding information, as shown in Fig.~b.\nOn the other hand, vector outputs derived from individual patches often exhibit fragmentation at patch boundaries, compromising the geometric integrity of the results, as illustrated in Fig.~b.\n\nIn addition to the ``large-size challenge\" posed by patch-based methods, geographic objects in large-size RSI exhibit significant scale variations across different categories.\nFor example, continuous water bodies typically require more vector points for representation than scattered buildings.\nThis scale variation exacerbates the fragmentation issue and poses challenges for achieving a unified representation across diverse object categories.\nWhile existing research has primarily focused on single-category extraction, such as buildings~ or roads~, a few frameworks have been proposed for unified geographic object extraction~.\nNevertheless, these methods remain constrained by their reliance on patch-based processing, failing to address the large-size challenge effectively.\nConsequently, there is a pressing need to develop a unified vectorization approach capable of directly handling large-size RSI.\n\nTo address these challenges, this paper proposes a framework called HoliTracer for holistic vector extraction directly from large-size RSI.\nTo enable holistic vectorization under limited computational resources, HoliTracer draws inspiration from segmentation-based methods~, which adopt a two-stage process of segmentation followed by vectorization.\nHoliTracer first performs segmentation on large-size RSI, leveraging the fact that pixel-level extraction preserves feature completeness without requiring post-processing while effectively capturing contextual information.\nSubsequently, based on these complete segmentations, HoliTracer traces the contours of each segmentation polygon to generate the final vector results.\nMore specifically, HoliTracer employs a Context Attention Network (CAN) to capture information using a local-to-global attention mechanism within large-size RSI. By adaptively integrating this information, CAN achieves more complete segmentation compared to patch-based methods.\nThereafter, to derive vector results from the segmentation mask, we introduce a robust vectorization pipeline leveraging the Mask Contour Reformer (MCR) and the Polygon Sequence Tracer (PST).\nMCR reconstructs irregular polygon contours while ensuring alignment with ground truth polygons through a bidirectional matching mechanism.\nThe reconstructed polygons are then processed by PST for polygon refinement and vertex identification, yielding precise vector representations.\nThanks to the robustness of the MCR algorithm and PST's sequence tracing strategy, HoliTracer effectively handles objects across diverse categories and scales.\nWe conducted comparative experiments on multiple large-size datasets featuring various geographic objects, including buildings, water bodies, and roads.\nExtensive experiments demonstrate that our method significantly outperforms existing patch-based approaches in vectorizing large-size RSI.\nIn summary, our contributions are as follows:\n {itemize}\n   To the best of our knowledge, HoliTracer is the first method designed for large-size RSI vectorization, holistically extracting diverse geographic objects.\n   We propose CAN, using a local-to-global attention mechanism to enhance segmentation and address context loss in patch-based methods.\n   We design a pipeline with MCR and PST for precise polygon reconstruction and vertex tracing across varied objects in large-size RSI.\n   Experiments on large-size RSI datasets of buildings, water bodies, and roads show HoliTracer outperforms existing state-of-the-art methods.\n {itemize}\n\n {figure*}\n  {center}\n  [width=1 ]{figs/framework.pdf}\n  {center}\n  {-12pt}\n  {\n The overall framework of HoliTracer. HoliTracer includes Context Attention Net (CAN) for context understanding from large-size RSI, Mask Contour Reformer (MCR) for polygon contours reconstruction, and Polygon Sequence Tracer (PST) for polygon refinement and vertex identification. With the proposed pipeline, HoliTracer can directly extract diverse geographic objects from large-size RSI.\n }\n\n {figure*}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.305,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.327,
      "distributed_training_score": 0.367,
      "datasets_score": 0.324,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667493",
      "updated_at": "2025-08-11T23:43:05.606788",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16252",
      "title": "Efficient RL for optimizing conversation level outcomes with an\n  LLM-based tutor",
      "authors": [
        "Hyunji Nam",
        "Omer Gottesman",
        "Amy Zhang",
        "Dean Foster",
        "Emma Brunskill",
        "Lyle Ungar"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) built on existing reinforcement learning with\nhuman feedback (RLHF) frameworks typically optimize responses based on\nimmediate turn-level human preferences. However, this approach falls short in\nmulti-turn dialogue settings, such as online math tutoring. We propose a method\nto enhance LLM-based tutors by representing the dialogue history with a\nlower-dimensional latent state representation of a student and optimizing a\nlong-term policy to determine high-level actions based on the latent state. The\ngoal is to better align the tutor's behavior with the long-term objective of\nguiding the student towards solving a target math problem on their own. Our\nmodel is lightweight, requiring less computational resources than prior work of\ntraining the tutor policy end-to-end to directly output the tutor's next\nutterance. Our experiment results demonstrate that these modifications lead to\nimproved long-term outcomes compared to prompting in LLM-simulated tutoring\ntasks.",
      "published_date": "2025-07-22T05:56:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16252v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16252v1",
      "latex_url": "http://arxiv.org/src/2507.16252v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "failed",
      "introduction_text": "Large language models (LLMs) have achieved remarkable success in complex tasks, such as solving math problems , summarization , and code generation . These models can interact with humans through open-ended text outputs and have been explored across a wide range of domains, including education and healthcare . This widespread application is largely due to their easily leveraged capabilities, including in-context learning from user-provided demonstrations , instruction-tuning , as well as reasoning . A major area of research focuses on aligning the behavior of language models with human preferences, a process referred to as reinforcement learning with human feedback (RLHF) .\n\n {figure}[h]\n \n [width=0.5 ]{main.png}\n {The default tutor trained with existing RLHF algorithms responds to the student's response with an optimal turn-level response, as the preference label is provided per turn. However, a better tutor should respond with the conversation-level outcome in mind, which may include asking a follow-up question to assess the student's background knowledge about the problem. We propose a three-step approach to optimizing conversation level outcomes with LLM-based tutor.}\n\n {figure}\n\nHowever, one main limitation of the existing RLHF framework  {ouyang2022, rafailov2024directpreferenceoptimizationlanguage} is that LLMs are optimized only to generate the most preferred single-turn responses, rather than optimizing for conversation-level outcomes. This is surprising given that many common use cases of LLMs involve multi-turn interactions, as discussed in prior work  {hong2023zeroshotgoaldirecteddialoguerl, zhou2024archertraininglanguagemodel, shani2024multiturnreinforcementlearningpreference, chen2025broaden}. In particular,  {hong2023zeroshotgoaldirecteddialoguerl} highlights that the default LLM's response tends to be generic and verbose, which is sub-optimal in many goal-directed conversations, such as teaching a new concept, or personalizing a travel itinerary to specific user's interests.\n\nIn this work, we focus on online math tutoring as an example of complex, goal-directed dialogue, where short-term and long-term optimal behaviors differ substantially. For example, imagine a scenario in which a sixth-grade student, struggling with a math problem, asks an online tutor for help. In a multi-turn conversation, the tutor can ask follow-up questions to assess the student's background knowledge, provide scaffolds to help the student solve the problem independently, and even encourage the student to try again. However, if the tutor expects no further interaction with the student, the tutor may instead give the solution directly, thereby, hurting the student's chance of solving the problem on their own. In order to help the student solve the problem independently, the tutor needs to leverage long-term optimal strategies based on the student's anticipated response and future dialogue turns.\n\nCustomized prompts may help mitigate this issue, but as pointed out by  {wang-etal-2024-bridging}, prompt engineering often fails to produce pedagogically meaningful behaviors from LLM tutors. Other works train language models with reinforcement learning (RL) objectives using long-term outcomes, rather than turn-level preferences, as the reward signal  {hong2023zeroshotgoaldirecteddialoguerl,snell2023offlinerlnaturallanguage,\nhong2024interactivedialogueagentsreinforcement, zhou2024archertraininglanguagemodel, shani2024multiturnreinforcementlearningpreference}. However, these policies are trained at the token level, which lacks interpretability of the generated response and requires substantial computational resources and training data.\n\nTo improve on the existing methods, we propose a novel decomposition of this problem into four parts:  {enumerate}\n  Inferring the student's internal state based on dialogue history using an LLM,\n  Choosing an optimal high-level action based on the inferred state and the long-term goal,\n  Few-shot instruction-tuning of an LLM to generate the tutor's response conditioned on the selected high-level action,\n  Collecting exploratory data to improve the quality of the policy learned in (2).\n {enumerate}\n\nOur method draws on ideas from Reinforcement Learning, an area of research focused on planning optimal actions for long-term rewards ~. Specifically, we define long-term rewards based on whether the student solves the target math problem correctly within the maximum number of dialogue turns. Unlike prompt engineering, we provide a principled framework grounded in RL for optimizing future outcomes. In contrast to prior work using RL, we reduce the computational burden of learning a tutor policy by defining the policy over substantially smaller state and action spaces. By inferring a low-dimensional student state from a longer conversation history, we keep the state space small and fixed-sized, even as the conversation length increases. The policy selects an optimal high-level action, which is interpretable, and the tutor's intent is clear to the system designer.\n\n {Contributions}Tutoring middle-school students on math problems requires planning for long horizons. Strategies like probing the student's math level and encouraging them to make another attempt are important, but they do not naturally emerge in chat-bots optimized for single-turn responses. In order to optimize for conversation-level outcomes, we focus on the following key aspects:  {itemize}\n  Extracting a compact representation of the student's states from long conversation history,\n  Learning a long-term optimal RL policy that maps student's state representation to a high-level action,\n  Introducing a new exploratory data collection strategy to simulate diverse tutoring scenarios, which are ultimately used for policy optimization.\n {itemize}\n\nOur experiment results with the simulated student based on Claude 3 Sonnet ~ {TheC3} show that our proposed method substantially improves the student's problem-solving success rate compared to prompt engineering.\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "acl2023.tex",
      "rlhf_score": 0.509,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.35,
      "datasets_score": 0.276,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper builds on existing RLHF frameworks by addressing their limitations in multi-turn dialogues, such as optimizing for immediate turn-level preferences. It proposes a method using RL with long-term rewards based on student outcomes, which aligns with RLHF principles but shifts focus to conversation-level optimization rather than direct human feedback for rewards. This makes it relevant but not a core RLHF application.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for optimizing LLM-based tutors in dialogues, with no mention of diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. It does not involve any components related to diffusion-based techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper addresses the limitations of traditional reinforcement learning with human feedback (RLHF) for large language models (LLMs) in multi-turn dialogues, particularly in online math tutoring, by proposing an efficient method that infers a lower-dimensional latent state from dialogue history to represent the student's knowledge and optimizes a long-term policy for high-level actions. The methodology involves four key steps—inferring the student state using an LLM, selecting optimal high-level actions via RL, generating tutor responses through few-shot instruction-tuning, and collecting exploratory data—resulting in improved student problem-solving success rates in simulated experiments compared to standard prompting techniques.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining RL with a lower-dimensional state representation for LLMs in multi-turn dialogues, offering a clever way to optimize long-term outcomes without end-to-end training, though it builds on existing RLHF concepts rather than introducing a entirely new problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work could influence research and applications in AI-driven education by providing an efficient framework for conversational tutors, making it likely to be cited and built upon in subfields like RL for LLMs, though its impact may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, practical contribution to enhancing LLM-based tutoring through efficient RL techniques, making it valuable for researchers in AI and education who are interested in multi-turn dialogue optimization.",
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "H-index fetching failed: not found in Semantic Scholar"
      ],
      "created_at": "2025-08-11T23:15:40.668608",
      "updated_at": "2025-08-11T23:45:27.562956",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16254",
      "title": "Edge-case Synthesis for Fisheye Object Detection: A Data-centric\n  Perspective",
      "authors": [
        "Seunghyeon Kim",
        "Kyeongryeol Go"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Fisheye cameras introduce significant distortion and pose unique challenges\nto object detection models trained on conventional datasets. In this work, we\npropose a data-centric pipeline that systematically improves detection\nperformance by focusing on the key question of identifying the blind spots of\nthe model. Through detailed error analysis, we identify critical edge-cases\nsuch as confusing class pairs, peripheral distortions, and underrepresented\ncontexts. Then we directly address them through edge-case synthesis. We\nfine-tuned an image generative model and guided it with carefully crafted\nprompts to produce images that replicate real-world failure modes. These\nsynthetic images are pseudo-labeled using a high-quality detector and\nintegrated into training. Our approach results in consistent performance gains,\nhighlighting how deeply understanding data and selectively fixing its\nweaknesses can be impactful in specialized domains like fisheye object\ndetection.",
      "published_date": "2025-07-22T06:07:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16254v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16254v1",
      "latex_url": "http://arxiv.org/src/2507.16254v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The increasing deployment of edge AI in traffic monitoring and smart mobility applications has brought renewed attention to fisheye cameras due to their wide-angle field of view (FoV). Compared to traditional perspective cameras, fisheye cameras can cover a significantly larger area, thereby reducing the number of cameras required in real-world deployments such as urban intersections and highways. This cost-effective and infrastructure-friendly characteristic makes them highly attractive for scalable traffic surveillance systems.\n\nHowever, the use of fisheye cameras introduces new challenges. Their images suffer from strong radial distortions, making conventional image processing pipelines less effective. Correcting these distortions either requires computationally intensive undistortion techniques or dedicated model architectures that can directly process the distorted inputs.\n\nTo address this, the 9th AI City Challenge was organized to foster research in real-time object detection using fisheye cameras under diverse traffic conditions. The challenge adopts an evaluation metric that is the harmonic mean of F1-score and inference speed (FPS), encouraging participants to strike a balance between accuracy and computational efficiency. A minimum performance requirement of 10 FPS on a Jetson AGX Orin edge device is enforced to ensure practical deployability of the models.\n\nThe dataset provided in this challenge, FishEye8K and FishEye1Keval, comprises fisheye images annotated for five traffic object classes: Bus, Bike, Car, Pedestrian, and Truck. The data reflects a wide range of traffic scenarios, including varying congestion levels, different road geometries such as intersections, and diverse lighting conditions across different times of day and viewing angles. The dataset is split into 5,288 training images, 2,712 validation images, and 1,000 test images.\n\nAn analysis of the dataset reveals significant imbalances across time of day. Notably, the Afternoon class dominates the training split, whereas Evening samples are absent. Night and Morning data are present but are limited to only one camera each, indicating limited scene diversity. Furthermore, the scale distribution is skewed such that most classes (especially Pedestrian and Bike) are heavily biased toward smaller scales. In addition, many object instances appear near the image boundaries where fisheye distortion is most prominent, further complicating detection.\n\nThis paper presents a comprehensive pipeline designed to enhance object detection performance for fisheye camera imagery, structured around a data-centric methodology. In Section~, we briefly review the overall research trends in real-time object detection, summarize the winning solutions from last year’s challenge, and discuss recent advances in synthetic data generation. Section~ details each step of our pipeline: data collection, edge-case analysis, synthetic data generation, and data augmentation. Section~ presents implementation details, shows the incremental performance gains from each stage of data enhancement, conducts ablation studies on various training options, and reports the final results submitted to the challenge. Finally, Section~ summarizes key insights and the effectiveness of our edge-case-focused, data-centric approach.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro-spb.tex",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.379,
      "datasets_score": 0.406,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves a detailed analysis of the FishEye8K and FishEye1Keval datasets, identifying imbalances in factors like time of day, object scales, and distortions. It also includes methodologies for dataset enhancement through synthetic data generation and integration, which directly aligns with creating, analyzing, and evaluating datasets for AI applications in object detection. This focus on dataset curation and error analysis makes it highly pertinent to the topic.",
      "summary": "This paper presents a data-centric pipeline to improve object detection in fisheye camera images by systematically identifying model blind spots through error analysis, focusing on issues like confusing class pairs, peripheral distortions, and underrepresented contexts. The methodology involves fine-tuning an image generative model with crafted prompts to synthesize edge-case images, pseudo-labeling them using a high-quality detector, and integrating them into training, resulting in consistent performance gains and highlighting the effectiveness of targeted data enhancement in specialized domains like fisheye object detection.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers a notable improvement by combining error analysis and synthetic data generation to address specific challenges in fisheye object detection, presenting a clever adaptation of existing techniques rather than a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in subfields like computer vision for edge AI applications, such as traffic monitoring, due to its practical approach to enhancing detection performance in distorted images.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, valuable contribution with practical insights for improving object detection in specialized domains, making it essential for researchers in computer vision and AI, though not groundbreaking enough for a broader audience.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/26071a10eb0fc344b96e2b37c8e7c785bce2b1e5",
      "h_index_fetch_method": "full_id",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Seunghyeon Kim",
          "profile_url": "https://www.semanticscholar.org/author/2373994665",
          "h_index": 0
        },
        {
          "name": "Kyeongryeol Go",
          "profile_url": "https://www.semanticscholar.org/author/2372764558",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667622",
      "updated_at": "2025-08-11T23:44:35.582101",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16257",
      "title": "Quality Text, Robust Vision: The Role of Language in Enhancing Visual\n  Robustness of Vision-Language Models",
      "authors": [
        "Futa Waseda",
        "Saku Sugawara",
        "Isao Echizen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Defending pre-trained vision-language models (VLMs), such as CLIP, against\nadversarial attacks is crucial, as these models are widely used in diverse\nzero-shot tasks, including image classification. However, existing adversarial\ntraining (AT) methods for robust fine-tuning largely overlook the role of\nlanguage in enhancing visual robustness. Specifically, (1) supervised AT\nmethods rely on short texts (e.g., class labels) to generate adversarial\nperturbations, leading to overfitting to object classes in the training data,\nand (2) unsupervised AT avoids this overfitting but remains suboptimal against\npractical text-guided adversarial attacks due to its lack of semantic guidance.\nTo address these limitations, we propose Quality Text-guided Adversarial\nFine-Tuning (QT-AFT), which leverages high-quality captions during training to\nguide adversarial examples away from diverse semantics present in images. This\nenables the visual encoder to robustly recognize a broader range of image\nfeatures even under adversarial noise, thereby enhancing robustness across\ndiverse downstream tasks. QT-AFT overcomes the key weaknesses of prior methods\n-- overfitting in supervised AT and lack of semantic awareness in unsupervised\nAT -- achieving state-of-the-art zero-shot adversarial robustness and clean\naccuracy, evaluated across 16 zero-shot datasets. Furthermore, our\ncomprehensive study uncovers several key insights into the role of language in\nenhancing vision robustness; for example, describing object properties in\naddition to object names further enhances zero-shot robustness. Our findings\npoint to an urgent direction for future work -- centering high-quality\nlinguistic supervision in robust visual representation learning.",
      "published_date": "2025-07-22T06:13:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16257v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16257v1",
      "latex_url": "http://arxiv.org/src/2507.16257v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Pre-trained vision-language (VL) models, such as CLIP~, are trained on large-scale image-text pairs via contrastive learning, enabling the models to obtain joint image-text representations.\nThis approach allows them to perform a variety of zero-shot tasks, such as zero-shot image classification, where images are matched with arbitrary class labels by comparing image embeddings with the text embeddings of those labels (e.g., ``a photo of \\{class\\}'').\nHowever, recent studies reveal that CLIP is vulnerable to adversarial examples (AEs)~ {mao2022understanding, schlarmann2024robust}, which introduce imperceptible perturbations on input images, leading to incorrect model predictions.\nThis vulnerability poses significant risks in real-world applications.\nGiven the widespread adoption of VL models like CLIP, ensuring zero-shot robustness is a critical challenge in building reliable AI systems.\n\nTo address adversarial vulnerability, recent studies~ have proposed robust fine-tuning methods for CLIP's vision encoder based on adversarial training (AT)~.\nThese approaches achieve robustness by fine-tuning for only a few epochs rather than performing AT from scratch, making them more practical.\nAdditionally, they focus on enhancing zero-shot robustness by assuming that downstream tasks are unknown during fine-tuning and aiming to generalize robustness across diverse zero-shot datasets.\n\nHowever, we point out that existing defense methods largely overlook the role of language in enhancing vision robustness, making them suboptimal for achieving zero-shot robustness (Fig.~).\nFor example, supervised (text-guided) AT methods, such as TeCoA~, PMG-AFT~, and TGA-ZSR~ rely solely on class labels to guide adversarial perturbations during training (Fig.~a).\nBy depending on class labels, these methods are highly prone to overfitting on the trained dataset, limiting generalization to unseen downstream tasks.\nIn contrast, FARE~ employs an unsupervised AT approach that avoids text guidance, mitigating overfitting (Fig.~b). However, due to the absence of semantic guidance from texts, it may fail to capture the diverse semantics present in images during training, limiting its robustness in a wide range of downstream tasks that involve diverse objects or image properties.\n\nTo address these challenges, this work introduces a novel perspective on the importance of leveraging language for robust vision in VL models.\nSpecifically, we propose a simple yet highly effective approach---Quality Text-guided Adversarial Fine-Tuning (QT-AFT)---which leverages detailed image captions instead of simple class labels to enhance the zero-shot robustness of CLIP (Fig.~c).\n\nBy incorporating detailed descriptions, the visual encoder learns to robustly recognize a broader range of image features even under adversarial noise, thereby improving performance on diverse downstream tasks.\nThis approach contrasts with existing text-guided AT methods, which use simple text embeddings of ``a photo of \\{class\\}'' for image classification.\n\nWe conduct extensive experiments by training CLIP on ImageNet and evaluating it across 16 zero-shot datasets. The results show that our method significantly enhances robustness, achieving state-of-the-art zero-shot robustness on 12 out of the 16 datasets and the best average performance.\nMoreover, unlike existing supervised AT methods, our approach does not sacrifice accuracy on clean images; instead, it maintains state-of-the-art accuracy.\nThese findings highlight that our approach effectively addresses the overfitting issues in supervised AT and the lack of semantic awareness in unsupervised AT.\n\nFurthermore, our comprehensive study uncovers several key insights into the role of language in enhancing vision robustness.\nFor example, we demonstrate that describing object properties using adjectives and adverbs---not just mentioning objects---further enhances zero-shot robustness.\nAdditionally, for texture classification tasks where class labels describe textures using adjectives, removing nouns from captions can further improve robustness, showing that the effectiveness of language guidance is task-specific.\n\nBy highlighting the critical role of language in enhancing visual robustness, our work points to an urgent direction for future work---centering high quality linguistic supervision in robust visual representation learning.\nThis direction is unique to multimodal models and distinguishes itself from a wide range of studies focused on unimodal AT methods for traditional image classification tasks.\n\nOur contributions are summarized as follows:\n {itemize}\n\n   We highlight that existing adversarial fine-tuning methods for CLIP overlook the critical role of language in enhancing the visual robustness of VL models.\n   We propose Quality Text-guided Adversarial Fine-Tuning (QT-AFT), which leverages detailed image captions to guide adversarial training. QT-AFT enables the visual encoder to recognize diverse features under adversarial noise, achieving state-of-the-art robustness while maintaining high clean accuracy across downstream tasks.\n   Our analysis provides key insights into the role of language in enhancing vision robustness, showing that linguistic cues---such as describing object properties in addition to object names---further enhances zero-shot robustness.\n\n {itemize}\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.346,
      "datasets_score": 0.337,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for enhancing adversarial robustness in vision-language models like CLIP through quality text-guided adversarial fine-tuning, focusing on image-text pairs and zero-shot tasks. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning as defined in the topic. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668018",
      "updated_at": "2025-08-11T23:43:05.606911",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16260",
      "title": "ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer\n  Inference",
      "authors": [
        "Haoyue Zhang",
        "Jie Zhang",
        "Song Guo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Although vision transformers (ViT) have shown remarkable success in various\nvision tasks, their computationally expensive self-attention hinder their\ndeployment on resource-constrained devices. Token reduction, which discards\nless important tokens during forward propagation, has been proposed to enhance\nthe efficiency of transformer models. However, existing methods handle\nunimportant tokens irreversibly, preventing their reuse in subsequent blocks.\nConsidering that transformers focus on different information among blocks,\ntokens reduced in early blocks might be useful later. Furthermore, to adapt\ntransformer models for resource-constrained devices, it is crucial to strike a\nbalance between model performance and computational overhead. To address these\nchallenges, in this paper, we introduce a novel Token Freezing and Reusing\n(ToFe) framework, where we identify important tokens at each stage and\ntemporarily freeze the unimportant ones, allowing their lagged reusing at a\nlater stage. Specifically, we design a prediction module for token\nidentification and an approximate module for recovery of the frozen tokens. By\njointly optimizing with the backbone through computation budget-aware\nend-to-end training, ToFe can adaptively process the necessary tokens at each\nblock, thereby reducing computational cost while maintaining performance.\nExtensive experiments demonstrate that ToFe reduces the computational cost of\nLV-ViT model by 50% with less than 2% drop in Top-1 accuracy, achieving a\nbetter trade-off between performance and complexity compared to\nstate-of-the-art methods.",
      "published_date": "2025-07-22T06:17:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16260v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16260v1",
      "latex_url": "http://arxiv.org/src/2507.16260v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[t]\n \n [width=0.48 ]{Figs/TokenRed.pdf}\n {Comparison of (a) Token pruning~, (b) Token Re-organization~, and (c) Our token freezing and reusing. Compared with (a) and (b), our ToFe temporarily freezes ``unimportant'' tokens instead of handling the tokens directly. Some frozen tokens will be reused in later blocks if necessary, avoiding mistakenly discarding important tokens.}\n\n {figure}\n\nLarge-scale pre-trained vision transformer (ViT) models~ have achieved remarkable progress in the field of vision tasks.\n\nHowever, the explosion of diverse ViT applications highlights a critical challenge: the extremely high computational cost caused by the quadratic computational complexity in the self-attention module of transformer models~, which poses significant barriers to practical deployment, especially under resource-constrained circumstances.\n\nTo improve the efficiency of ViTs, numerous studies apply traditional model compression techniques like model distillation~,\n\nparameter pruning~ and quantization~, etc., to pursue smaller sized models.\nHowever, traditional model compression techniques tend to prune a large portion of the model to meet tight deployment constraints, which may not guarantee optimal accuracy, especially for smaller models with long input tokens.\n\nUnlike the above approaches that focus on building efficient transformer models, token reduction, leveraging the unique characteristics of transformer architectures and the inherent sparsity of the attention mechanism~ to prune unimportant input tokens, has emerged as a promising approach~. It is based on the intuition that not all tokens in the input sequences are critical for making a final prediction. Pruning these uninformative tokens within each block can increase the model's inference speed without sacrificing performance accuracy. Moreover, the removal of these informative tokens also reduces the computation and memory requirements for its subsequent blocks, leading to a linear or even quadratic reduction and bringing greater acceleration benefits.\n\nGenerally, there are three key problems in implementing token reduction:\n\n {enumerate}[]\n   How to identify important tokens that should be processed in each transformer block, while the less important ones can be reduced?\n   How to handle the less important tokens (directly discard, merge, or re-organize them, etc.)?\n   How to find the optimal number of the reserved tokens or the reduction ratio in each block?\n {enumerate}\n\nFor the first problem, since the final output of the transformer model primarily depends on the [CLS] token (i.e., Fig.~), the task-relevant information is concentrated in the [CLS] token. Thus, the attention values of the [CLS] token to other tokens have been commonly used as a metric for evaluating token importance~.\n\nHowever, to ensure accurate predictions during inference, the [CLS] token will be forced to pay more attention to the most task-relevant tokens as the block gets deeper, meaning that using [CLS] attention values in deeper blocks is more feasible than in shallower blocks, where this characteristic may not be shown in shallower blocks.\n\nFor the second problem, mainstream token reduction mechanisms manipulate the less important tokens in two ways: token pruning, i.e., directly discard~ and token re-organization, i.e., merge~, fusion~, squeeze~. All these methods treat the unimportant/inattentive tokens as totally useless ones and handle them irreversibly, where the reduced tokens cannot be recovered and reused in deeper blocks.\n\nIn this case, the performance of the model would be significantly degraded due to the removal of some temporary inattentive tokens in shallower blocks.\n\nFor the last problem, most of the existing methods either require manual selection of the keeping ratio for different reduction stage~, or consider the keeping ratio as a learnable parameter~ or optimizable variables~. However, due to the inaccuracy of using [CLS] token's attention values as importance proxy, both manually and automatically determined keeping ratios are usually higher in shallow blocks to avoid mistakenly pruning on useful tokens, thus hindering further acceleration of transformer models.\n\n {figure}[t]\n \n [width=0.48 ]{Figs/transformer.pdf}\n {An illustration of a typical transformer model with $L$ blocks. The input image is first split into patches, linearly projected and embedded. Then the tokens are forwarded by $L$ transformer blocks that contain layer normalization, multi-head self-attention, and multi-layer perception.}\n\n {figure}\n\nIn fact, the attention of [CLS] token is relatively scattered at shallow blocks. For instance, we use DeiT-S model and three image samples from ImageNet-1K dataset, and simply keep the top-50% of tokens based on the [CLS] attention values in the 4-th, 7-th, and 10-th blocks of each image, respectively. As illustrated in Fig.~, most of the informative tokens are preserved in the 10-th block (e.g., the dog's head and the entire body of the fish), but a large part of these tokens are removed in the 4-th block. Since the removed tokens cannot be used in subsequent blocks, the performance of token reduction deteriorates dramatically when we decrease the keeping ratio.\n\nIn summary, the primary limitation of existing token reduction methods is that [CLS] token's attention values cannot accurately represent the importance of tokens in shallower blocks, and the tokens reduced in shallower blocks will not be used again in deeper blocks. To tackle this problem and further solve the trade-off between model performance and model complexity, we propose lagged\n\n {To}ken  {F}reezing and R {e}using (ToFe), a simple yet highly effective and efficient token reduction framework. Specifically, ToFe utilizes a lightweight prediction module to adaptively decide which token is important at each stage in a global computation budget-aware view, temporarily processing them only for the current stage {Similar to the previous token reduction works~, ToFe implements multi-stage token reduction. Denote that the $s$-th ($s   {1, ,S}$) token reduction operation is adopted at the $l_s$-th block, where $l_s   \\{l_1, ,l_S\\}$. For example, for a 16-block ViT model like LV-ViT-S~, a three-stage token reduction is deployed at the 5-th, 9-th, and 13-th blocks.}.\nBy temporarily freezing ``unimportant'' tokens instead of handling them directly, some frozen tokens will be reused in later blocks if necessary.\n\nTo compensate for the potential inaccuracy of the frozen tokens when skipping multiple blocks, we further introduce a lightweight approximation module to recover the error of the frozen tokens.\nWith the freeze-then-reuse framework, transformer models can choose the tokens exactly needed in each block, minimizing the number of tokens calculated by transformer blocks while maintaining better performance.\n\nOverall, our main contributions are summarized as follows.\n\n {figure}[t]\n \n [width=0.45 ]{Figs/visattn.pdf}\n {Visualization of [CLS] Token Attention. We use DeiT-S model and three image samples from ImageNet-1K dataset. We sort the tokens based on the [CLS] attention values in the 4-th, 7-th, and 10-th blocks. In each block, tokens with top-50% [CLS] attention value are kept, while others are removed and marked with black squares.}\n\n {figure}\n\n {itemize}\n\n  We present ToFe, a novel Token Freezing and Reusing framework for efficient transformer inference. At each stage, all tokens are fed into a prediction module to decide which tokens to use (or reuse) in the following stage. Only the selected tokens are input to transformer blocks, while others are frozen and approximated for later blocks, thereby reducing the computational cost.\n\n  We introduce a computation budget-aware training framework to jointly learn the prediction module and approximation module, enabling a globally optimized token selection phase and a flexible token recovery process.\n\n  We conduct extensive experiments on widely used vision transformer backbones. The experimental results show that our method reduces the computational cost of LV-ViT by 50% and brings less than 2% drop in Top-1 accuracy, achieving a better trade-off between model performance and model complexity than previous methods.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.411,
      "datasets_score": 0.271,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a framework for efficient inference in Vision Transformers by freezing and reusing tokens to reduce computational costs during forward propagation. It addresses inference optimization on resource-constrained devices, with no discussion of distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation to accelerate model training. Therefore, it does not relate to distributed training topics.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669038",
      "updated_at": "2025-08-11T23:43:05.607107",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16267",
      "title": "SFNet: A Spatial-Frequency Domain Deep Learning Network for Efficient\n  Alzheimer's Disease Diagnosis",
      "authors": [
        "Xinyue Yang",
        "Meiliang Liu",
        "Yunfang Xu",
        "Xiaoxiao Yang",
        "Zhengye Si",
        "Zijin Li",
        "Zhiwen Zhao"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\npredominantly affects the elderly population and currently has no cure.\nMagnetic Resonance Imaging (MRI), as a non-invasive imaging technique, is\nessential for the early diagnosis of AD. MRI inherently contains both spatial\nand frequency information, as raw signals are acquired in the frequency domain\nand reconstructed into spatial images via the Fourier transform. However, most\nexisting AD diagnostic models extract features from a single domain, limiting\ntheir capacity to fully capture the complex neuroimaging characteristics of the\ndisease. While some studies have combined spatial and frequency information,\nthey are mostly confined to 2D MRI, leaving the potential of dual-domain\nanalysis in 3D MRI unexplored. To overcome this limitation, we propose\nSpatio-Frequency Network (SFNet), the first end-to-end deep learning framework\nthat simultaneously leverages spatial and frequency domain information to\nenhance 3D MRI-based AD diagnosis. SFNet integrates an enhanced dense\nconvolutional network to extract local spatial features and a global frequency\nmodule to capture global frequency-domain representations. Additionally, a\nnovel multi-scale attention module is proposed to further refine spatial\nfeature extraction. Experiments on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset demonstrate that SFNet outperforms existing baselines\nand reduces computational overhead in classifying cognitively normal (CN) and\nAD, achieving an accuracy of 95.1%.",
      "published_date": "2025-07-22T06:33:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16267v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16267v2",
      "latex_url": "http://arxiv.org/src/2507.16267v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Alzheimer’s disease (AD) is an irreversible neurodegenerative disorder characterized by the progressive deterioration of memory and cognitive functions . Due to the lack of effective pharmacological treatments, early diagnosis and intervention are essential for patients . Magnetic resonance imaging (MRI), as a non-invasive neuroimaging technique with high spatial resolution, is valuable for identifying structural brain alterations associated with AD . However, manual analysis of MRI in clinical practice is often time-consuming and subject to inter-observer variability. Consequently, deep learning-based methods have been increasingly explored for the automatic detection of AD from MRI data. These approaches offer the potential to enhance diagnostic accuracy and efficiency, thereby facilitating more reliable clinical decision-making.\n\nAmong various deep learning architectures, convolutional neural networks (CNNs) have shown considerable effectiveness in AD classification tasks. For instance, Korolev et al. proposed a 3D neural network that combined VGGNet and ResNet for AD diagnosis. Fan et al. applied the U-Net to AD classification, enhancing the diagnostic accuracy. Wang et al. proposed an ensemble of 3D densely connected convolutional networks with a probability-based fusion method to boost the performance of AD diagnosis.\n\nTo further extract spatial features from MRI, spatial attention and channel attention have been widely proposed . Gao et al. proposed a multi-scale attention convolution to learn feature maps with multi-scale kernels. Dutta et al. combined spatial attention and self-attention blocks in parallel to comprehensively capture the feature dependencies along spatial dimensions. Liu et al. proposed a multi-plane and multi-scale feature-level fusion attention model, which used a multi-scale feature extractor with hybrid attention layers to simultaneously capture and fuse multiple pathological features in the sagittal, coronal, and axial planes. Zhang et al. integrated two squeeze-and-excitation (SE) blocks to embed channel attention into a multi-scale fusion (MSF) feature extraction module. Tang et al. introduced a spatial channel attention module (ECSA), which can focus on important AD-related features in images more efficiently.\n\nIn addition, the self-attention mechanism introduced by the Transformer has exhibited remarkable performance across various computer vision applications . However, the direct application of the Vision Transformer (ViT) to MRI entails two significant challenges. First, the self-attention mechanism is computationally intensive, with a complexity of $O(n^2)$ . Second, MRI datasets are generally much smaller than those commonly used in computer vision, limiting the effective training of ViT. Therefore, numerous studies have explored the integration of Transformer with CNN to improve AD classification while reducing computational costs . Li et al. introduced Trans-ResNet, which combined ResNet-18 for spatial feature extraction with a Transformer to capture long-range dependencies. Jang et al. proposed a hybrid model that integrated 3D CNN, 2D CNN, and Transformer for AD diagnosis, achieving an accuracy of 93.21%. Hu et al. proposed Conv-Swinformer, which utilized VGG-16 for low-level spatial feature extraction and employed a sliding window strategy to fuse adjacent features. Khatri et al. incorporated convolution-attention mechanisms within Transformer-based classifiers to enhance performance while maintaining computational efficiency. Miao et al. proposed a Multi-modal Multi-scale Transformer Fusion Network (MMTFN) for AD diagnosis. The multi-scale features were extracted from each modality using 3D multi-scale residual blocks and fused via the Transformer.\n\nWhile the above models reduced computational cost to some extent by incorporating CNN and Transformer, they did not fundamentally address the $O(n^2)$ complexity brought by the self-attention mechanism. To tackle this issue, Rao et al. proposed the Global Filter Network (GFNet), which substituted the self-attention with the fast Fourier transform. This approaches achieved a computational complexity of $O(n   n)$ while effectively capturing long-term spatial dependencies in the frequency domain. Building on this idea, Zhang et al. extended the model to a 3D Global Fourier Network and applied it to extract long-range dependencies in MRI of AD patients. Furthermore, Kushol et al. proposed ADDformer, which leveraged transfer learning to train ViT and GFNet with a majority voting strategy for classification, resulting in an accuracy of 88.2%.\n\nDespite significant progress achieved by frequency domain models, several limitations remain. First, certain models, such as GFNet, exclusively extracted features from the frequency domain while neglecting the spatial structural information inherent in MRI data. Second, although some models integrated spatial and frequency domain features, they often rely on 2D slices, resulting in a lack of spatial continuity across the three-dimensional volume. To overcome these limitations, we propose a 3D deep learning model that integrates spatial and frequency domain features to fully exploit the complementary information embedded in MRI data. In the spatial domain, an improved DenseNet architecture is employed to extract local structural features, enhanced by a multi-scale spatial attention module to improve the perception of brain regions at different spatial resolutions and scales. In the frequency domain, a global frequency module applies the fast Fourier transform to feature maps, capturing long-range dependencies across regions and enabling effective global context modeling. By fusing spatial-frequency domain features, the model enhances inter-regional interactions and significantly improves its discriminative performance in predicting early-stage Alzheimer's disease. The main contributions of this paper are as follows:\n\n {itemize}\n   We propose a novel model for AD diagnosis, termed SFNet, which is the first to integrate spatial local features and global frequency-domain dependencies based on 3D MRI data, effectively enhancing the accuracy of AD classification.\n   A multi-scale attention module is proposed in the spatial domain to expand the receptive field and capture multi-scale local spatial features.\n   We employ a low-rank MLP layer in the frequency domain, allowing the model to reduce model parameters and computation. Furthermore, learnable global filters are visualized to improve model interpretability by revealing spectral responses.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "manuscript.tex",
      "rlhf_score": 0.314,
      "weak_supervision_score": 0.299,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.371,
      "datasets_score": 0.338,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667631",
      "updated_at": "2025-08-11T23:43:05.606822",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16274",
      "title": "Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for\n  Efficient Large-Scale Model Training",
      "authors": [
        "Zixiao Huang",
        "Junhao Hu",
        "Hao Lin",
        "Chunyang Zhu",
        "Yueran Tang",
        "Quanlu Zhang",
        "Zhen Guo",
        "Zhenhua Li",
        "Shengen Yan",
        "Zhenhua Zhu",
        "Guohao Dai",
        "Yu Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)",
        "cs.PF (Performance)"
      ],
      "abstract": "The rapid scaling of large language models (LLMs) has significantly increased\nGPU memory pressure, which is further aggravated by training optimization\ntechniques such as virtual pipeline and recomputation that disrupt tensor\nlifespans and introduce considerable memory fragmentation. Default GPU memory\nallocators of popular deep learning frameworks like PyTorch use online\nstrategies without knowledge of tensor lifespans, which can waste up to 43\\% of\nmemory and cause out-of-memory errors, rendering optimization techniques\nineffective or even unusable.\n  To address this, we introduce STWeaver, a GPU memory allocator for deep\nlearning frameworks that reduces fragmentation by exploiting the spatial and\ntemporal regularity in memory allocation behaviors of training workloads.\nSTWeaver introduces a novel paradigm that combines offline planning with online\nallocation. The offline planning leverages spatio-temporal regularities to\ngenerate a near-optimal allocation plan, while the online allocation handles\ncomplex and dynamic models such as Mixture-of-Experts (MoE). Built as a\npluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by\n79.2\\% (up to 100\\%) across both dense and sparse models, with negligible\noverhead. This enables more efficient, high-throughput training configurations\nand improves performance by up to 32.5\\%.",
      "published_date": "2025-07-22T06:39:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16274v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16274v1",
      "latex_url": "http://arxiv.org/src/2507.16274v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "In recent years, large-scale models, particularly large language models (LLMs)~,\n have demonstrated extraordinary performance in language comprehension, problem reasoning, code generation, etc.\nThe scaling law~ dictates that such powerful capabilities stem from the models' massive parameters and training data.\nAs a result, nowadays even a medium-sized model such as Llama-3~ with 70 billion parameters requires more than 1~TB GPU/accelerator memory for training, placing heavy demands on the scarce and expensive GPU memory resource.\n\n {figure}[t]\n  {subfigure}[b]{0.45 }\n  \n  [width= ]{figs/fragmentation.pdf}\n\n  {subfigure}\n  {subfigure}[b]{0.5 }\n  [width= ]{figs/memory_vs_throughput.pdf}\n\n  {subfigure}\n  {10pt}\n  {(a) Memory fragmentation in interleaved allocation. (b) Memory and training throughput of different training configurations for Llama2-7B on 8 NVIDIA A800 GPUs.}\n\n {figure}\n\nAdditionally, current large-scale model training often employs a combination of various optimization techniques to enhance overall training efficiency.\nSuch optimization techniques serve to either boost training throughput~ or reduce the theoretical GPU memory demand of the training~.\nFor instance, the Virtual Pipeline~ partitions a conventional pipeline parallel stage into several virtual stages, thereby minimizing idle periods (i.e., pipeline bubbles) inherent in pipeline parallelism.\nFurthermore, memory optimization techniques such as recomputation~, tensor offloading~, and ZeRO~ trade additional computation or transmission for reduced GPU memory usage.\n\nHowever, the application of these training optimization techniques alters GPU memory allocation patterns. First, the number of allocation requests increases significantly compared to the training configuration without these techniques (e.g., 30% increase). Second, the allocation pattern shifts from a regular sequence of allocations followed by deallocations (e.g., activation tensors reserved for backward computation) to a more complex, interleaved pattern with frequent alternation between the two.\n\nUnfortunately, the memory allocators in current deep learning frameworks, such as PyTorch~, struggle to efficiently handle such complex allocation patterns, leading to severe memory fragmentation (up to 43% in typical scenarios).\nConsequently, the actual memory consumption during training significantly exceeds the theoretical allocation requirements.\nThe root cause of fragmentation lies in the online best-fit allocation policy adopted by the allocator in popular deep learning frameworks (e.g., PyTorch).\nThis policy allocates a requested tensor of a certain size to the most suitable memory slot without considering the tensor's lifespan, which is unknown to the allocator.\nUnpredictable deallocations lead to a discontinuous memory space, making it difficult to fit new tensors, as illustrated in Figure~(a).\nOver time, this increases fragmentation as free space becomes scattered and less reusable for larger requests.\n\nMore critically, the increased GPU memory consumption caused by fragmentation can slow down model training. In large-scale training, configurations with higher throughput often require more GPU memory, as shown in Figure~(b), where each point represents a different setup, i.e., using different optimization techniques. Fragmentation reduces the amount of available GPU memory, limiting the feasibility of high-throughput configurations. When such configurations are used, fragmentation can cause actual memory usage to far exceed theoretical estimates, leading to out-of-memory (OOM) errors. As a result, model developers are forced to revert to less efficient configurations, thus reducing training efficiency (e.g., up to 24.5%).\n\nTo address these problems, we propose  , a novel GPU memory allocator for deep learning frameworks to reduce fragmentation.\nOur approach is based on the observation that GPU memory requests exhibit strong consistency across training iterations.\nTherefore, by pre-assigning memory addresses before training, we can reduce fragmentation caused by online allocation in current allocators.\n\nHowever, optimizing memory allocation requests ahead of training meets two challenges. First, offline allocation planning is NP-hard, known as Dynamic Storage Allocation problem~.\nIn large-scale model training, the number of memory requests can exceed $10^5$, making direct optimization intractable.\nTo obtain a near-optimal solution within an acceptable time, we extract spatio-temporal regularities from memory allocation during training and use them to guide a grouping-based optimization.\nThis grouping approach decomposes the time and space characteristics of memory requests, significantly reducing the complexity of the optimization problem.\n\nSecond, the recent emergence of sparse models of Mixture-of-Experts (MoE) models~ introduces dynamics in memory allocation patterns compared to dense models.\nMoE models replace MLP layers with expert layers,\n and decide which experts to use for each token at runtime, which results in the dynamic nature of allocation request sizes.\nConsequently, we cannot rely on planning of certain address for the allocation requests.\nTo address the challenge of dynamic request sizes, we propose a hybrid paradigm that combines offline planning with online allocation.\nBy identifying reusable regions for dynamic requests before training and performing online allocation at runtime,   supports the dynamicity of allocation requests while maintaining a low fragmentation rate.\n\nWe implement   as a pluggable memory allocator for PyTorch and evaluate it across over 48 training configurations on 3 different testbeds.\nThese configurations combine diverse dense and sparse models, model sizes, optimization techniques, microbatch sizes, and training frameworks.\n  reduces fragmentation memory by an average of 79.2% (up to 100%), saving up to 56.3GB GPU memory with negligible impact on end-to-end training throughput.\nBy reducing peak GPU memory usage, it enables efficient training configurations that would otherwise trigger Out-of-Memory errors, resulting in an up to 32.5% throughput improvement.\nWe will open source   to support more developers' efficient large-scale training.\n\nThis paper makes three main contributions:\n \n  We conduct an in-depth analysis of the memory allocation characteristics and fragmentation problem of large model training,\n identifying spatial and temporal regularity in the allocation pattern.\n\n  We propose a memory allocation paradigm for large-scale model training that combines offline planning with online allocation.\n  is capable of generating a near-optimal allocation plan based on spatio-temporal regularities, while effectively accommodating the dynamicity of allocation requests at runtime.\n  We comprehensively evaluated   using diverse training configurations on different testbeds, demonstrating its wide applicability and effectiveness. It also enables more efficient model training.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "introduction.tex",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.524,
      "datasets_score": 0.29,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on optimizing GPU memory allocation to reduce fragmentation during large-scale model training, which indirectly supports distributed training by enabling efficient use of multiple GPUs (e.g., as seen in configurations on 8 NVIDIA A800 GPUs). It references techniques like Virtual Pipeline and ZeRO, which are part of distributed training strategies, but the main contribution is a memory allocator rather than new algorithms for partitioning data, models, or computation across nodes. Thus, it enhances distributed setups without directly addressing core distributed training mechanisms.",
      "datasets_justification": "below_threshold",
      "summary": "This paper introduces STWeaver, a novel GPU memory allocator designed to reduce memory fragmentation during large-scale model training by exploiting spatial and temporal regularities in memory allocation patterns. It combines offline planning to generate near-optimal allocation plans for consistent workloads and online allocation to handle dynamic models like Mixture-of-Experts, resulting in an average 79.2% reduction in fragmentation, up to 56.3GB of memory savings, and performance improvements of up to 32.5% with negligible overhead.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining offline planning with online allocation to address memory fragmentation, offering a clever integration of existing ideas rather than introducing a completely new problem or technique. While it advances state-of-the-art memory management in deep learning, it builds on known issues like fragmentation in GPU allocators.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in large-scale AI training by enabling more efficient GPU memory usage and reducing out-of-memory errors. Its demonstrated improvements in performance and memory savings could lead to broader adoption in distributed computing and machine learning frameworks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to optimizing deep learning training efficiency, making it essential for researchers and practitioners dealing with GPU memory constraints. However, while insightful, it may not be groundbreaking for those outside specific subfields like AI hardware optimization.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0958f6ceca66b7b21439456dd984d9839db1465",
      "h_index_fetch_method": "full_id",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 24,
      "average_h_index": 3.5,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zixiao Huang",
          "profile_url": "https://www.semanticscholar.org/author/2278457016",
          "h_index": 2
        },
        {
          "name": "Junhao Hu",
          "profile_url": "https://www.semanticscholar.org/author/2374343189",
          "h_index": 0
        },
        {
          "name": "Hao Lin",
          "profile_url": "https://www.semanticscholar.org/author/2372664232",
          "h_index": 0
        },
        {
          "name": "Chunyang Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2374416301",
          "h_index": 0
        },
        {
          "name": "Yueran Tang",
          "profile_url": "https://www.semanticscholar.org/author/2373464833",
          "h_index": 0
        },
        {
          "name": "Quanlu Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2315889693",
          "h_index": 0
        },
        {
          "name": "Zhen Guo",
          "profile_url": "https://www.semanticscholar.org/author/2373325170",
          "h_index": 0
        },
        {
          "name": "Zhenhua Li",
          "profile_url": "https://www.semanticscholar.org/author/2373559922",
          "h_index": 0
        },
        {
          "name": "Shengen Yan",
          "profile_url": "https://www.semanticscholar.org/author/2283520504",
          "h_index": 8
        },
        {
          "name": "Zhenhua Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2135206587",
          "h_index": 8
        },
        {
          "name": "Guohao Dai",
          "profile_url": "https://www.semanticscholar.org/author/144290348",
          "h_index": 24
        },
        {
          "name": "Yu Wang",
          "profile_url": "https://www.semanticscholar.org/author/2374151736",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669285",
      "updated_at": "2025-08-11T23:45:59.229651",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16278",
      "title": "Understanding Generalization, Robustness, and Interpretability in\n  Low-Capacity Neural Networks",
      "authors": [
        "Yash Kumar"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although modern deep learning often relies on massive over-parameterized\nmodels, the fundamental interplay between capacity, sparsity, and robustness in\nlow-capacity networks remains a vital area of study. We introduce a controlled\nframework to investigate these properties by creating a suite of binary\nclassification tasks from the MNIST dataset with increasing visual difficulty\n(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,\nthe minimum model capacity required for successful generalization scales\ndirectly with task complexity. Second, these trained networks are robust to\nextreme magnitude pruning (up to 95% sparsity), revealing the existence of\nsparse, high-performing subnetworks. Third, we show that over-parameterization\nprovides a significant advantage in robustness against input corruption.\nInterpretability analysis via saliency maps further confirms that these\nidentified sparse subnetworks preserve the core reasoning process of the\noriginal dense models. This work provides a clear, empirical demonstration of\nthe foundational trade-offs governing simple neural networks.",
      "published_date": "2025-07-22T06:43:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16278v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16278v1",
      "latex_url": "http://arxiv.org/src/2507.16278v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The remarkable success of modern artificial intelligence is largely driven by the scaling of neural networks to massive sizes. Massive models like Transformers and deep convolutional networks have pushed the boundaries of performance across many challenging tasks. This progress often comes at the cost of heavy computational resources, consume substantial energy, and often operate as black boxes. Their lack of transparency makes them less suitable for scenarios where resources are limited or where understanding the model's reasoning is critical, especially given that their predictions can be vulnerable to imperceptible perturbations.\n\nHowever, smaller networks can serve as an effective and efficient alternative. They are essential for efficient deployment on edge devices and provide a tractable laboratory to develop a fundamental scientific understanding of how neural networks learn. While the trade-offs between a model's capacity, the sparsity of its learned connections, and its robustness to corruption are often studied in complex settings, a clear, controlled analysis of these foundational properties is necessary.\n\nIn this work, we introduce a controlled framework for systematically investigating these relationships. Our key methodological contribution is the creation of a suite of binary classification tasks from the MNIST dataset where task difficulty is precisely modulated by selecting digit pairs of increasing visual similarity (e.g., the simple  {0 and 1} task versus the challenging  {4 and 9} task). This allows us to isolate the effect of task complexity on model behavior.\n\nOur results reveal a clear and direct link between task difficulty and the minimum model capacity required for successful generalization. We demonstrate that these networks are extremely robust to magnitude pruning and show that over-parameterization, while not always necessary for clean data, provides a significant advantage in robustness against input noise and occlusion. We first describe our methodology, then present the results of our capacity, pruning, and robustness experiments, and conclude with a discussion of the findings.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "project/paper.tex",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.398,
      "datasets_score": 0.357,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves investigating generalization, robustness, and interpretability in low-capacity neural networks using standard MNIST datasets with precise labels for binary classification tasks. It does not address weak supervision, as there is no mention of programmatically generating labels from noisy or imprecise sources, nor does it rely on such methods for training.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667639",
      "updated_at": "2025-08-11T23:43:05.606825",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16279",
      "title": "MAN++: Scaling Momentum Auxiliary Network for Supervised Local Learning\n  in Vision Tasks",
      "authors": [
        "Junhao Su",
        "Feiyu Zhu",
        "Hengyu Shi",
        "Tianyang Han",
        "Yurui Qiu",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Jialin Gao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning typically relies on end-to-end backpropagation for training, a\nmethod that inherently suffers from issues such as update locking during\nparameter optimization, high GPU memory consumption, and a lack of biological\nplausibility. In contrast, supervised local learning seeks to mitigate these\nchallenges by partitioning the network into multiple local blocks and designing\nindependent auxiliary networks to update each block separately. However,\nbecause gradients are propagated solely within individual local blocks,\nperformance degradation occurs, preventing supervised local learning from\nsupplanting end-to-end backpropagation. To address these limitations and\nfacilitate inter-block information flow, we propose the Momentum Auxiliary\nNetwork++ (MAN++). MAN++ introduces a dynamic interaction mechanism by\nemploying the Exponential Moving Average (EMA) of parameters from adjacent\nblocks to enhance communication across the network. The auxiliary network,\nupdated via EMA, effectively bridges the information gap between blocks.\nNotably, we observed that directly applying EMA parameters can be suboptimal\ndue to feature discrepancies between local blocks. To resolve this issue, we\nintroduce a learnable scaling bias that balances feature differences, thereby\nfurther improving performance. We validate MAN++ through extensive experiments\non tasks that include image classification, object detection, and image\nsegmentation, utilizing multiple network architectures. The experimental\nresults demonstrate that MAN++ achieves performance comparable to end-to-end\ntraining while significantly reducing GPU memory usage. Consequently, MAN++\noffers a novel perspective for supervised local learning and presents a viable\nalternative to conventional training methods.",
      "published_date": "2025-07-22T06:50:19+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16279v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16279v1",
      "latex_url": "http://arxiv.org/src/2507.16279v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "In traditional deep learning, enhancing performance is typically achieved by increasing network depth, which in turn renders end-to-end backpropagation indispensable for model training .\n\nHowever, as the depth of the network increases, so does the computational cost associated with evaluating the loss function and performing continuous gradient descent through successive layers to optimize parameters .\n\nMoreover, parameter updates occur only after the complete forward and backward propagation passes, imposing a \"locking\" constraint on the updates .\n\nThis updating scheme, coupled with the locking issue, stands in stark contrast to the localized signal processing observed in biological synapses and exacerbates challenges such as reduced parallelism and increased GPU memory consumption , ultimately affecting training efficiency and scalability.\n {figure*}[!htbp]\n  \n  [width= ]{sec/ACC.pdf}\n  {Comparison of accuracy across different datasets and backbones for both MAN++ and E2E methods. Fig~.(a) shows the results of training from scratch on the ImageNet dataset for 90 epochs. Fig~.(b) presents the results of training on the COCO dataset for 100 epochs using pretrained weights from ImageNet. Fig~.(c) displays the results of training on the CityScapes dataset for 4,000 iterations, also using pretrained weights from ImageNet.}\n\n {figure*}\n\nTo address these challenges, an alternative training paradigm, namely local learning, has recently emerged .\n\nUnlike end-to-end training, local learning partitions the neural network into multiple gradient-isolated blocks, which independently perform backpropagation, thereby preventing gradient flow between them.\n\nThe parameters of each module are updated by its own auxiliary network, driven by distinct local objectives .\n\nThis approach mitigates the locking issue by enabling each gradient-isolated module to update its parameters immediately upon receiving local error signals, thereby avoiding the sequential update bottleneck of end-to-end training and significantly enhancing parallel training efficiency .\n\nFurthermore, local learning retains only the gradients of each module’s backbone and auxiliary networks during training and promptly releases them after local updates. It substantially reduces GPU memory requirements and obviates the need to store extensive global gradient information .\n\nNevertheless, while local learning alleviates the locking problem and conserves GPU memory, a significant performance gap relative to end-to-end training persists, precluding its complete replacement.\n\nExisting techniques in local learning primarily focus on refining the structure of the auxiliary network and narrowing the performance gap by enhancing the local loss function .\n\nHowever, these improvements do not resolve a fundamental challenge in local learning: the \"short-sighted\" problem. In the absence of a mechanism for inter-module communication, gradient-isolated modules exchange little information and focus solely on their respective local objectives at the expense of the global objective.\n\nThis isolation results in the loss of features that are beneficial for overall network performance, particularly in architectures divided into many modules.\n\nIn this paper, we propose a novel local learning network architecture—Momentum Auxiliary Network++ (MAN++).\n\nBy introducing a dedicated bridge for inter-module interaction, MAN++ enhances information flow between modules and mitigates the inherent short-sightedness of supervised local learning.\n\nSpecifically, MAN++ comprises two components: an EMA module and a Scale Learnable Bias module, both positioned between the main network and the auxiliary network of each gradient-isolated module.\n\nThe EMA module employs the Exponential Moving Average (EMA) technique to integrate parameters from the subsequent module into its update process. This innovative approach enables each local module to incorporate information beyond its immediate local objective, thereby promoting a closer alignment with the network’s global goal.\n\nHowever, directly applying EMA parameters for updates introduces challenges, such as constraints imposed by feature inconsistencies between gradient-isolated modules and a relatively slow update rate.\n\nTo overcome these issues, we introduce a Scale Learnable Bias that augments the EMA module’s updates, enhancing the effective sharing of information.\n\nNotably, the proposed MAN++ incurs only a minimal increase in GPU memory usage while delivering significant performance improvements.\n\nWe validate MAN++ through experiments on various CNN and ViT architectures across multiple datasets for image classification, object detection, and semantic segmentation.\n\nThe experimental results demonstrate that MAN++ overcomes the limitations of conventional supervised local learning, achieving performance comparable to end-to-end training and offering a promising alternative.\n\nThe contributions of this paper can be summarized as follows:\n {itemize}\n \n\nWe propose Momentum Auxiliary Network++ (MAN++), which leverages an Exponential Moving Average (EMA) mechanism and a novel Scale Learnable Bias to facilitate effective information exchange between modules. This design alleviates the myopic limitations of traditional supervised local learning, thereby improving overall network performance.\n\n  MAN++ is a plug-and-play method that can be seamlessly integrated into any supervised local learning framework, significantly broadening its applicability while imposing only minimal additional GPU memory overhead.\n\n  MAN++ demonstrates its effectiveness across different visual tasks using various network architectures, achieving state-of-the-art performance. It significantly reduces memory usage while matching the performance of end-to-end training methods, thus providing a potential alternative to end-to-end training for local learning.\n\n {itemize}\n {figure*}[t]\n  \n  [width= ]{sec/fig2_horiz.png}\n  {Comparison of (a) end-to-end backpropagation, (b) other supervised local learning methods, and (c) our proposed method. Unlike E2E, supervised local learning separates the network into K gradient-isolated local blocks. LB stands for the Learnable Bias.}\n\n {figure*}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1.intro.tex",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.459,
      "datasets_score": 0.319,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Moderately Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on enhancing supervised local learning in neural networks by introducing mechanisms like EMA and scaling bias for better inter-block communication, aiming to improve training efficiency in vision tasks. It does not involve training with noisy, imprecise, or programmatically generated labels, which is central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses partitioning neural networks into independent blocks for local learning, which enables parallel updates and reduces GPU memory usage, aligning with aspects of parallel computing in training. However, it primarily addresses intra-model parallelism rather than full distributed systems across multiple nodes or data partitioning.",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces MAN++, a novel enhancement to supervised local learning for vision tasks, addressing the limitations of end-to-end backpropagation by enabling efficient inter-block communication. It achieves this through an Exponential Moving Average (EMA) mechanism for sharing parameters between adjacent blocks and a learnable scaling bias to mitigate feature discrepancies, resulting in performance comparable to end-to-end training across image classification, object detection, and segmentation tasks while significantly reducing GPU memory usage.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique using EMA for inter-block information flow and a learnable scaling bias to handle feature differences, significantly advancing supervised local learning by resolving the short-sightedness problem. This represents a meaningful step beyond existing methods, potentially setting a new standard in efficient network training.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in efficient deep learning training and practical applications, especially in resource-constrained environments, by offering a viable alternative to end-to-end backpropagation. Its demonstrated reductions in memory usage and comparable performance could lead to broader adoption in computer vision tasks.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a strong and valuable contribution to supervised local learning, making it essential for researchers focused on efficient neural network training in vision tasks. While highly insightful, it is not groundbreaking enough to be considered a must-read for all in the field.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/390722ba890274c0bd66bd5bb12b6e9b9eb90120",
      "h_index_fetch_method": "full_id",
      "total_authors": 8,
      "authors_found": 7,
      "highest_h_index": 3,
      "average_h_index": 1.4285714285714286,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Junhao Su",
          "profile_url": "https://www.semanticscholar.org/author/2293668900",
          "h_index": 3
        },
        {
          "name": "Feiyu Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2302523495",
          "h_index": 3
        },
        {
          "name": "Hengyu Shi",
          "profile_url": "https://www.semanticscholar.org/author/2355564304",
          "h_index": 0
        },
        {
          "name": "Tianyang Han",
          "profile_url": "https://www.semanticscholar.org/author/2374092021",
          "h_index": 0
        },
        {
          "name": "Yurui Qiu",
          "profile_url": null,
          "h_index": null
        },
        {
          "name": "Junfeng Luo",
          "profile_url": "https://www.semanticscholar.org/author/2365051361",
          "h_index": 1
        },
        {
          "name": "Xiaoming Wei",
          "profile_url": "https://www.semanticscholar.org/author/2355688561",
          "h_index": 1
        },
        {
          "name": "Jialin Gao",
          "profile_url": "https://www.semanticscholar.org/author/2326256033",
          "h_index": 2
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668027",
      "updated_at": "2025-08-11T23:44:55.220439",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16280",
      "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of\n  Scientific Inquiry",
      "authors": [
        "Tianze Xu",
        "Pengrui Lu",
        "Lyumanshan Ye",
        "Xiangkun Hu",
        "Pengfei Liu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The emergence of deep research systems presents significant capabilities in\nproblem-solving, extending from basic queries to sophisticated research tasks.\nHowever, existing benchmarks primarily evaluate these systems as agents for web\nretrieval and report generation, overlooking their potential to discover novel\ninsights on the frontiers of scientific research. To address this gap, we\nintroduce ResearcherBench, the first benchmark focused on evaluating the\ncapabilities of these advanced, agentic systems - which we refer to as Deep AI\nResearch Systems (DARS) - on frontier AI scientific questions. We compiled a\ndataset of 65 research questions expertly selected from real-world scientific\nscenarios such as laboratory discussions and interviews, spanning 35 different\nAI subjects and categorized into three types: technical details, literature\nreview, and open consulting. Our dual evaluation framework combines rubric\nassessment, which uses expert-designed criteria to evaluate insight quality,\nwith factual assessment, which measures citation accuracy (faithfulness) and\ncoverage (groundedness). We evaluated several leading commercial DARS and\nbaseline systems. Results show that OpenAI Deep Research and Gemini Deep\nResearch significantly outperform other systems, with particular strength in\nopen-ended consulting questions. Such capabilities represent a meaningful step\ntoward AI self-improvement, aligning with the vision of ASI for AI. We\nopen-source ResearcherBench to provide a standardized platform for promoting\nthe development of next-generation AI research assistants, hoping to foster a\nnew perspective in AI research evaluation for a novel pattern of scientific\ncollaboration: https://github.com/GAIR-NLP/ResearcherBench.",
      "published_date": "2025-07-22T06:51:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16280v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16280v1",
      "latex_url": "http://arxiv.org/src/2507.16280v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The advent of artificial intelligence has fundamentally transformed how we approach complex problem-solving tasks , with deep research systems emerging as sophisticated AI agents capable of autonomously conducting intricate research workflows . These systems integrate advanced information retrieval, analysis, and synthesis capabilities, demonstrating remarkable proficiency in processing vast amounts of information and generating comprehensive reports across diverse domains .\n\nDeep research systems have also been increasingly deployed by researchers to assist with various aspects of scientific inquiry, such as investigating technical implementation details, conducting comprehensive literature reviews, and synthesizing existing knowledge . These applications have demonstrated clear value in streamlining traditional research workflows and enhancing productivity in well-established research domains. However, the scientific community might overlook a potentially transformative capability of these systems: their potential to assist researchers in exploring genuinely open-ended, frontier questions that exist at the cutting edge of scientific knowledge .\n\nThis transition from being systems that merely retrieve and summarize information to becoming genuine ``research partners'' capable of valuable collaboration on unexplored scientific territories represents one of the most significant challenges facing the development of AI research assistants today . We define this emerging category as Deep AI Research Systems (DARS), the sophisticated agentic systems capable of dynamic reasoning and autonomously conducting intricate research workflows with multi-iteration web retrieval and tool uses~. The capabilities of such systems points toward a profound objective: by systematically involving DARS in challenging frontier AI research, we can create a powerful feedback loop for recursive self-improvement, where AI is used to accelerate its own development, aligning with the broader vision of Artificial Superintelligence (ASI) for AI.\n\nHowever, this raises a critical question: can current DARS truly assist human researchers in tackling the most challenging, high-valued, and open-ended questions at the frontiers of science, where definitive answers do not yet exist and novel insights must be synthesized from fragmentary and cross-domain information?\n\nExisting benchmarks for evaluating deep research capabilities predominantly focus on assessing systems' abilities to retrieve and synthesize established knowledge rather than their capacity to engage with genuinely novel, frontier research questions. Current evaluation frameworks typically emphasize comprehensive report generation or agents' web interaction capabilities , focusing on breadth of information retrieval rather than conceptual understanding and insight generation. These frameworks fail to capture a crucial dimension of research assistance: the ability to understand, analyze, and provide meaningful insights on highly specialized, cutting-edge scientific problems. Such problems are characterized by inherent ambiguity, absent definitive answers, and the need for creative synthesis of disparate ideas .\n\nTo address this critical gap in evaluation methodology, we introduce ResearcherBench, the first benchmark specifically designed to evaluate DARS capabilities on frontier scientific questions, as shown in Figure . Unlike existing benchmarks that primarily assess information retrieval and general synthesis abilities, ResearcherBench focuses on evaluating whether AI systems can provide meaningful assistance to human researchers working on genuinely unsolved, cutting-edge problems in the field of artificial intelligence.\n\nOur benchmark represents a paradigm shift in evaluation philosophy—moving from assessing ``whether deep research systems can retrieve and summarize information'' to evaluating ``whether DARS can understand complex problems and provide meaningful insights as genuine research partners.'' This approach recognizes that true research assistance requires not just information gathering, but deep comprehension of nuanced concepts, the ability to deeply explore connections between different perspectives, and the capacity to generate novel insights that advance scientific understanding.\n\nResearcherBench makes several significant contributions to the field of AI research evaluation:\n\n {itemize}\n   A Novel Task Collection: We present a carefully curated dataset of 65 high-quality research questions sourced from authentic frontier scenarios, including laboratory discussions, interviews with leading AI researchers, and active scientific forums. These questions span 35 distinct AI research subjects and are categorized into three distinct types. This categorization enables nuanced evaluation of DARS capabilities across different types of research assistance scenarios.\n   A Unique Dual Evaluation Framework: Our assessment methodology combines rubric assessment with factual assessment. The rubric assessment employs domain expert-crafted evaluation criteria tailored to each specific question, ensuring alignment with human-anchored high-value insights. The factual assessment framework introduces two complementary metrics: faithfulness score and and groundedness score, to evaluate the overall factual accuracy and coverage of generated content.\n   Comprehensive Empirical Analysis: Our extensive evaluation of leading commercial DARS platforms provides a holistic, multi-faceted benchmark of their capabilities and fundamental limitations. The analysis reveals their primary strength in exploratory reasoning for open-ended tasks, rather than precise technical or literature synthesis. The evaluation also uncovers a paradoxical ``high faithfulness, low groundedness'' pattern, and shows that high citation coverage does not necessarily equate to superior insight quality. These findings from analysis validate DARS' potential as innovative research partners.\n   Open-Source Contribution: We are releasing ResearcherBench as a comprehensive evaluation platform, encompassing our curated dataset of frontier questions and the dual evaluation framework. This initiative provides the community with a standardized infrastructure to benchmark DARS capabilities of AI researching, to collaboratively advance the development of AI systems capable of valuable scientific research assistance.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.432,
      "weak_supervision_score": 0.4,
      "diffusion_reasoning_score": 0.428,
      "distributed_training_score": 0.426,
      "datasets_score": 0.533,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on benchmarking Deep AI Research Systems for frontier scientific questions, emphasizing evaluation frameworks and capabilities like insight generation. It does not discuss training AI models with human feedback or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper introduces a benchmark for evaluating AI research systems but does not address machine learning approaches involving programmatically generated labels or training with noisy data.",
      "diffusion_reasoning_justification": "The paper evaluates general capabilities of Deep AI Research Systems for research tasks, such as reasoning and insight generation, but does not mention or involve diffusion models for multi-step logical reasoning.",
      "distributed_training_justification": "The paper is centered on benchmarking AI systems for scientific inquiry, not on techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper's main contribution includes creating and evaluating a curated dataset of 65 research questions for benchmarking AI systems, spanning AI subjects and categorized for analysis, directly aligning with research on dataset creation, benchmarking, and evaluation for AI applications.",
      "summary": "This paper introduces ResearcherBench, a novel benchmark designed to evaluate Deep AI Research Systems (DARS) on frontier AI scientific questions, addressing the limitations of existing benchmarks that focus primarily on information retrieval and report generation. It compiles a dataset of 65 real-world questions from scenarios like laboratory discussions, categorizes them into technical details, literature reviews, and open consulting, and employs a dual evaluation framework combining rubric-based insight assessment with factual metrics for citation accuracy and coverage; key findings show that systems like OpenAI Deep Research and Gemini Deep Research outperform others, particularly in open-ended tasks, highlighting their potential for AI self-improvement and leading to the open-sourcing of the benchmark for community use.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark and evaluation framework specifically for assessing AI systems on frontier scientific questions, advancing the state-of-the-art by shifting focus from basic retrieval to insight generation and novel problem-solving.",
      "impact_score": "High",
      "impact_justification": "This work could broadly influence future AI research and development by providing an open-source platform for evaluating advanced systems, potentially accelerating innovations in AI self-improvement and scientific collaboration across academic and commercial domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and significant contribution to AI evaluation methodologies, making it valuable for researchers in the field to understand and build upon for advancing AI research assistants.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6c9288c709db52adf7df7a1e32df5683edbbdcc2",
      "h_index_fetch_method": "full_id",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 3,
      "average_h_index": 1.8,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Tianze Xu",
          "profile_url": "https://www.semanticscholar.org/author/2374431783",
          "h_index": 0
        },
        {
          "name": "Pengrui Lu",
          "profile_url": "https://www.semanticscholar.org/author/2353992730",
          "h_index": 1
        },
        {
          "name": "Lyumanshan Ye",
          "profile_url": "https://www.semanticscholar.org/author/2329324157",
          "h_index": 3
        },
        {
          "name": "Xiangkun Hu",
          "profile_url": "https://www.semanticscholar.org/author/2336880415",
          "h_index": 3
        },
        {
          "name": "Pengfei Liu",
          "profile_url": "https://www.semanticscholar.org/author/2327103110",
          "h_index": 2
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667780",
      "updated_at": "2025-08-11T23:44:40.373160",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16287",
      "title": "Beyond Label Semantics: Language-Guided Action Anatomy for Few-shot\n  Action Recognition",
      "authors": [
        "Zefeng Qian",
        "Xincheng Yao",
        "Yifei Huang",
        "Chongyang Zhang",
        "Jiangyong Ying",
        "Hong Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-shot action recognition (FSAR) aims to classify human actions in videos\nwith only a small number of labeled samples per category. The scarcity of\ntraining data has driven recent efforts to incorporate additional modalities,\nparticularly text. However, the subtle variations in human posture, motion\ndynamics, and the object interactions that occur during different phases, are\ncritical inherent knowledge of actions that cannot be fully exploited by action\nlabels alone. In this work, we propose Language-Guided Action Anatomy (LGA), a\nnovel framework that goes beyond label semantics by leveraging Large Language\nModels (LLMs) to dissect the essential representational characteristics hidden\nbeneath action labels. Guided by the prior knowledge encoded in LLM, LGA\neffectively captures rich spatiotemporal cues in few-shot scenarios.\nSpecifically, for text, we prompt an off-the-shelf LLM to anatomize labels into\nsequences of atomic action descriptions, focusing on the three core elements of\naction (subject, motion, object). For videos, a Visual Anatomy Module segments\nactions into atomic video phases to capture the sequential structure of\nactions. A fine-grained fusion strategy then integrates textual and visual\nfeatures at the atomic level, resulting in more generalizable prototypes.\nFinally, we introduce a Multimodal Matching mechanism, comprising both\nvideo-video and video-text matching, to ensure robust few-shot classification.\nExperimental results demonstrate that LGA achieves state-of-the-art performance\nacross multipe FSAR benchmarks.",
      "published_date": "2025-07-22T07:16:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16287v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16287v1",
      "latex_url": "http://arxiv.org/src/2507.16287v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Few-shot action recognition seeks to classify human actions in videos using only a limited number of labeled samples, addressing the challenge of data scarcity in real-world applications. Building on advances in few-shot image classification~, many existing approaches adopt a metric-based meta-learning paradigm, where videos are embedded into a shared feature space, and alignment metrics are leveraged to infer the labels of the queried video~. The core challenge lies in learning effective representations that can transfer to novel, unseen action categories, while overcoming the inherent complexity of video data and the limited supervision available~.\n\n {figure}[t]\n  \n  [width= ]{figs/figs_Motivation.pdf}\n  {Illustration of our motivation. By leveraging the powerful knowledge understanding capability of LLMs, we anatomize one action label into a three-stage atomic action description. Meanwhile, the video is divided into corresponding three phases.}\n\n  {-4mm}\n {figure}\n\nRecently, researchers have explored the integration of multimodal information to enhance the learned representations~.\nAmong these, text has emerged as a particularly effective modality~.\nFor example, CLIP-FSAR regards action labels as text and incorporates CLIP~ to boost the FSAR performance.\nShi et al.~ {shi2024commonsense} constructs an external action knowledge base and aligns video frames with textual sub-action proposals.\n\nHowever, human actions exhibit intricate variations and rich spatiotemporal details .\nSubtle differences in posture, motion dynamics, and object interactions across different action phases serve as crucial knowledge for action recognition, yet these cues cannot be fully exploited by relying on simple semantic cues such as action labels.\n\nTo address this limitation, we first take a deeper look beyond the label semantics.\nBeyond the textual label, key factors of actions include the subject, motion dynamics, and object interactions~.Meanwhile, temporally, the way an action initiates, progresses, and concludes plays a crucial role in determining its category~. Thus, achieving robust few-shot action recognition requires fine-grained alignment between query and support videos, ensuring that every critical aspect of the action is considered.\n\nWith this insight, in this work, we propose Language-Guided Action Anatomy (LGA): a novel framework that anatomizes both textual and visual modalities to fully exploit their inherent knowledge for fine-grained query-support matching.\nLGA first uses a Large Language Model (LLM) to decompose action labels into a sequence of semantic sub-steps, as illustrated in  {fig:motivation}.\nWe choose LLM due to its strong general world knowledge and instruction-following ability~ {mann2020language, raffel2020exploring}, enabling us to focus on the core elements of actions, i.e., subject, motion, and object.\nSimultaneously, a Visual Anatomy Module segments the video into distinct temporal phases, capturing the initiation, progression, and conclusion of the actions.\nA Fine-grained Multimodal Fusion Module then integrates the textual and visual features at the anatomized level, constructing modality-aligned action prototypes.\nFinally, to ensure robust few-shot matching, we propose a Multimodal Matching Module.\nThis module performs matching from two complementary perspectives: video-video matching compares query and support video features, while video-text matching aligns query video features with textual representations.\nBy fully decomposing and aligning the anatomized knowledge of actions, LGA enables fine-grained multimodal understanding and significantly improves few-shot action recognition performance.\n\nExperimental results on multiple benchmarks demonstrate that our method outperforms other FSAR methods. Also, we conduct extensive ablation studies and provide qualitative visualizations, both of which substantiate the effectiveness of our method. The contributions can be summarized as follows:\n {itemize}\n =0pt\n  We propose LGA, a novel method for FSAR that anatomizes textual and video modalities to fully exploit the inherent knowledge in actions.\n  We propose novel multimodal fusion and matching modules that effectively integrate atomic-level textual and visual features, enabling robust FSAR through both video-video and video-text matching.\n  Our method achieves state-of-the-art performance across five widely used benchmarks.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.322,
      "datasets_score": 0.351,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668035",
      "updated_at": "2025-08-11T23:43:05.606915",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16290",
      "title": "Dens3R: A Foundation Model for 3D Geometry Prediction",
      "authors": [
        "Xianze Fang",
        "Jingnan Gao",
        "Zhe Wang",
        "Zhuo Chen",
        "Xingyu Ren",
        "Jiangjing Lyu",
        "Qiaomu Ren",
        "Zhonglei Yang",
        "Xiaokang Yang",
        "Yichao Yan",
        "Chengfei Lyu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Recent advances in dense 3D reconstruction have led to significant progress,\nyet achieving accurate unified geometric prediction remains a major challenge.\nMost existing methods are limited to predicting a single geometry quantity from\ninput images. However, geometric quantities such as depth, surface normals, and\npoint maps are inherently correlated, and estimating them in isolation often\nfails to ensure consistency, thereby limiting both accuracy and practical\napplicability. This motivates us to explore a unified framework that explicitly\nmodels the structural coupling among different geometric properties to enable\njoint regression. In this paper, we present Dens3R, a 3D foundation model\ndesigned for joint geometric dense prediction and adaptable to a wide range of\ndownstream tasks. Dens3R adopts a two-stage training framework to progressively\nbuild a pointmap representation that is both generalizable and intrinsically\ninvariant. Specifically, we design a lightweight shared encoder-decoder\nbackbone and introduce position-interpolated rotary positional encoding to\nmaintain expressive power while enhancing robustness to high-resolution inputs.\nBy integrating image-pair matching features with intrinsic invariance modeling,\nDens3R accurately regresses multiple geometric quantities such as surface\nnormals and depth, achieving consistent geometry perception from single-view to\nmulti-view inputs. Additionally, we propose a post-processing pipeline that\nsupports geometrically consistent multi-view inference. Extensive experiments\ndemonstrate the superior performance of Dens3R across various dense 3D\nprediction tasks and highlight its potential for broader applications.",
      "published_date": "2025-07-22T07:22:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16290v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16290v1",
      "latex_url": "http://arxiv.org/src/2507.16290v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recovering 3D geometric structures from static images is a long-standing and fundamental problem in computer vision.\nClassical approaches, such as Structure-from-Motion (SfM) and Multi-View Stereo (MVS), demonstrate strong performance in controlled settings and have been widely adopted in a broad range of 3D reconstruction applications. However, in unconstrained scenarios—where camera intrinsics, extrinsics, or viewpoint information are unavailable—achieving accurate and dense geometric prediction remains highly challenging. These conditions demand more generalizable and robust solutions capable of handling diverse and unstructured visual inputs.\n\nExisting methods for dense geometric prediction primarily fall into two categories. The first category mostly adopts generative models, utilizing strong image priors from pre-trained diffusion models or large-scale training datasets for dense prediction. For example, GenPercept~ is used for depth prediction, and StableNormal~ for normal estimation. This raises a key issue: while image generation tasks typically benefit from their inherent ambiguity and multi-modal output characteristics, geometric prediction is fundamentally different. Geometric prediction is essentially a deterministic task that needs to closely reflect the structural information of the underlying scene. Moreover, the pixel continuity and spatial smoothness required by geometric representations are difficult to naturally obtain through standard diffusion sampling mechanisms without structural constraints. Therefore, the direct application of diffusion models in geometric regression tasks faces significant challenges, especially in such tasks where a strict one-to-one correspondence between input and output needs to be maintained. Based on this, we adopt a regression-oriented framework to construct geometric mapping models in a more efficient and interpretable way. Furthermore, the aforementioned methods mainly handle only one geometric quantity prediction and cannot generalize to output multiple geometric quantities in a single forward pass. The second category includes DUSt3R~ and its follow-up works~. These methods use regression models that can regress 3D point map representations with geometric properties, applied to dense prediction, including image pair matching and depth estimation. However, these methods typically focus on a single prediction task, and other geometric quantities suffer severe performance degradation due to representation influences.\n\nThis raises a natural question: can we build a unified model that simultaneously regresses multiple geometric quantities with high quality? We observe that existing methods like DUSt3R, when handling dense geometric regression tasks, overlook a crucial geometric information—surface normals. Traditionally, normals have been used to add high-frequency details to rough geometric structures to enhance rendering quality. However, our research finds that introducing normal information during geometric prediction can significantly improve the accuracy of point maps, resulting in more detailed and structurally consistent 3D representations. This is mainly because:\n1) From the perspective of normal prediction, the inherent image pair matching capability in dense vision backbone networks helps alleviate monocular ambiguity and improve the stability and accuracy of normal prediction;\n2) From the feature modeling perspective, normals possess good intrinsic invariance, which simplifies the mapping learning process and aids in model convergence and generalization.\nThis modeling approach enables the model to simultaneously predict multiple geometric quantities (such as depth, normals, and point maps) from a single view, effectively reducing dependence on multi-view supervision and simplifying the training process.\nHowever, training such a multi-task, multi-output 3D foundation model still faces significant challenges. Geometric quantities are tightly coupled, and how to coordinate these relationships to achieve optimal overall performance requires carefully designed training strategies and architectural support.\n\nIn this paper, we present Dens3R, a foundation model for high-quality geometric prediction. To this end, we design a two-stage training framework that gradually builds a versatile pointmap representation, which generalizes well to various downstream tasks.\nSpecifically, we first construct a dense vision backbone network with multi-task prediction capabilities. This network adopts a shared encoder-decoder architecture, which significantly reduces model parameters while maintaining expressive power. To accommodate high-resolution inputs, we introduce position-interpolated rotary positional encoding, which effectively mitigates prediction degradation caused by increased input resolution.\nFor the training strategy, we propose a novel two-staged approach. In the first stage, the model leverages image pair matching features to learn scale-invariant point maps, capturing consistent spatial geometric structures across viewpoints. Subsequently, to fully exploit the one-to-one mapping property in normal estimation, we extend the pointmap representation into an intrinsically invariant form. This allows the model to independently attend to each viewpoint, thereby improving the accuracy of normal prediction. The learned geometric structures also assist in estimating other geometric quantities, such as depth, thereby simplifying their training processes.\nFinally, we design a simple and efficient post-processing pipeline that supports multi-view inputs during inference, which enhances the geometric consistency of the model in real-world applications.\nIn summary, we make the following contributions:\n {itemize}\n   We introduce Dens3R, a dense 3D visual foundation model that demonstrates high-quality performance in various 3D tasks including pointmap reconstruction, depth estimation, normal prediction and image matching under several benchmark evaluations.\n   We design a novel training strategy with the intrinsic-invariant pointmap and shared Encoder-Decoder visual backbone to incorporate surface normals in unconstrained image-based dense 3D reconstruction, simplifying the training complexity of other 3D quantities and achieving better results without requiring excessive computation resources.\n   We employ a position-interpolated rotary positional encoding to preserve prediction accuracy at higher resolutions and support multi-resolution inputs.\n   Extensive experiments on various benchmarks showcase our high-quality predictions of 3D geometric quantities, which further enable a wide range of applications.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "final.tex",
      "rlhf_score": 0.336,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.455,
      "distributed_training_score": 0.392,
      "datasets_score": 0.352,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a regression-based model for 3D geometry prediction, specifically Dens3R, and critiques the use of diffusion models for such tasks due to their unsuitability for deterministic geometric regression. It does not adapt diffusion processes for multi-step logical reasoning or treat a 'Chain-of-Thought' as an entity for iterative refinement, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668055",
      "updated_at": "2025-08-11T23:43:05.606918",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16296",
      "title": "Cross-Modal Distillation For Widely Differing Modalities",
      "authors": [
        "Cairong Zhao",
        "Yufeng Jin",
        "Zifan Song",
        "Haonan Chen",
        "Duoqian Miao",
        "Guosheng Hu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Deep learning achieved great progress recently, however, it is not easy or\nefficient to further improve its performance by increasing the size of the\nmodel. Multi-modal learning can mitigate this challenge by introducing richer\nand more discriminative information as input. To solve the problem of limited\naccess to multi-modal data at the time of use, we conduct multi-modal learning\nby introducing a teacher model to transfer discriminative knowledge to a\nstudent model during training. However, this knowledge transfer via\ndistillation is not trivial because the big domain gap between the widely\ndiffering modalities can easily lead to overfitting. In this work, we introduce\na cross-modal distillation framework. Specifically, we find hard constrained\nloss, e.g. l2 loss forcing the student being exact the same as the teacher, can\neasily lead to overfitting in cross-modality distillation. To address this, we\npropose two soft constrained knowledge distillation strategies at the feature\nlevel and classifier level respectively. In addition, we propose a\nquality-based adaptive weights module to weigh input samples via quantified\ndata quality, leading to robust model training. We conducted experiments on\nspeaker recognition and image classification tasks, and the results show that\nour approach is able to effectively achieve knowledge transfer between the\ncommonly used and widely differing modalities of image, text, and speech.",
      "published_date": "2025-07-22T07:34:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16296v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16296v1",
      "latex_url": "http://arxiv.org/src/2507.16296v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "}\n\n {black}{The rapid advancement of deep learning has revolutionized numerous fields by enabling the development of increasingly complex and powerful models. However, as model sizes continue to grow, the marginal benefits of scaling up models diminish, prompting researchers to explore alternative strategies for improving performance. One such strategy is multi-modal learning, which leverages the complementary strengths of multiple data modalities—such as images, speech, and text—to enhance task performance. While multi-modal learning has shown promise in various applications, its practical adoption is often hindered by the high cost and complexity of acquiring and processing multi-modal data. This limitation raises a critical question: how can we effectively utilize multi-modal data during training when only uni-modal data is available during deployment? To address this challenge, we propose a novel framework for cross-modal knowledge distillation, which enables the transfer of knowledge from a strong modality (e.g., images) to a weak modality (e.g., speech) during training, even when only the weak modality is available during inference. This approach is particularly valuable in scenarios where multi-modal data is expensive or impractical to collect, such as in smart devices without cameras or in low-resource environments. By leveraging the correlation between modalities, our framework allows the weak modality to learn discriminative features that improve its performance without relying on the strong modality at test time. Our work builds on the observation that existing knowledge distillation methods, which primarily focus on transferring knowledge between similar modalities or within a single modality, often fail to address the unique challenges of cross-modal transfer. These challenges include significant differences in data representation, modality-specific features, and varying data quality across modalities. To overcome these issues, we introduce a series of innovations that enhance the efficiency and robustness of cross-modal knowledge distillation.}\n\nIn order to investigate the effect of different model sizes and different modalities on the performance, we employ the identity recognition task as a case study. Specifically, we conduct training on the VoxCeleb2 dataset , which is for speaker recognition with both audio and face images, using three different configurations: a quarter of convolutional channels, half channels, and the full ResNet34 model. The results are shown in Fig. . We can observe that the performance gains with only one modality, either audio or image, from doubling the number of parameters are progressively decreasing, indicating that it will be difficult to further improve the performance by simply increasing the model size. Nonetheless, we also find that altering the data modality (from audio to image) marginally enhances performance when the model is identical, despite this dataset not being designed for face recognition. Furthermore, different modalities can benefit from distinct training data. In the case of face images, being pre-trained on larger datasets leads to significantly increased effectiveness (IR-50 in this figure). Additionally, even when we simply calculate the average distance between the two modalities, the fusion of both is much more effective than relying on a single modality. This experiment shows that taking advantage of different modalities, multimodal learning, is an efficient path to improving the task performance.\n\n {figure}[ht]\n \n [width= ]{fig/fig1.jpg}\n {The effect of different model sizes and different modalities on the performance of identity recognition tasks. ResNet34 with different numbers (1/4, 1/2 and full) of convolutional channels are trained on the VoxCeleb2 dataset using audio and image as inputs, respectively.}\n\n {figure}\n\nHowever, the acquisition of multi-modal data poses significant challenges. In numerous practical scenarios, the collection of multi-modal data is hindered by constraints such as high costs. For example, for the aforementioned speaker recognition task, many smart speakers without cameras are unable to access multi-modal data from images and audio, even though the combination of the two modalities enables better performance. If only one modality is available at the time of use, it is difficult to efficiently improve the performance by multi-modal combination.\n\n {figure}[ht]\n \n [width=0.9 ]{fig/fig2.jpg}\n {Illustration of cross-modal knowledge transfer. The strong modality (e.g. image) transfers the discriminative knowledge to the weak modality (e.g. speech) during training. During test, the weak modality only is used.\n}\n\n {figure}\n\nThis knowledge transfer can be accomplished using knowledge distillation techniques . Knowledge distillation is a widely used technique that uses a stronger teacher model to supervise a student model training, allowing knowledge transfer from the teacher to the student. Although primarily utilized for model compression, this technique can also be applied in various contexts, such as our cross-modal knowledge transfer. Existing cross-modal knowledge distillation studies basically follow the uni-modal knowledge distillation methods and are primarily conducted between similar modalities (e.g. RGB images and depth maps).\n\nTo solve these problems, we propose our cross-modal knowledge distillation framework. Specifically, (1) In contrast to the fully fixed teacher model which is widely used by existing methods, we introduce a trainable projection head on teacher model to narrow down the gap between the teacher and the student. (2) We find the hard alignment, e.g. using\nl\nl2 loss to force the student to be the same as the teacher, is a major source of overfitting for cross-modality distillation due to the great discrepancies between two modalities.\n\nThus, we design two soft constraints, feature level and classifier level, for cross-modal distillation. As for the feature level, we introduce a relaxation parameter, margin, to the existing loss functions, which do not require perfect agreement between the two modalities, allowing the student to learn modality-shared features rather than learn both modality shared and specific features. For the classifier level, we map the two modalities to the same feature space which enables them to share the classifier, implicitly making two modalities closer in this feature space rather than forcing them perfectly the same.\nThese two soft alignment strategies can greatly reduce the overfitting for cross-modality distillation.\n\n(3) We propose a quality-aware adaptive weighting module that can adaptively adjust the training objectives according to the quality of the two modal input data, avoiding the interference of low-quality data on distillation training.\n\n{We conduct experiments in various challenging scenarios to validate our approach and successfully achieve knowledge transfer between the important and widely differing modalities of image, text, and speech. Specifically, we focus on two tasks: speaker recognition and image classification.\nIn speaker recognition, which usually utilizes speech to verify the identity of a speaker (during test), we aim to leverage the advancements made in face recognition, which has strong performance for identity recognition, to improve the performance of speech-based identification.\nTherefore, we use the face recognition recognition model as a teacher model to motivate the transfer of knowledge to the speaker recognition model during training.\nAs for image classification, this task holds great significance in the field of computer vision. Previous research indicates that text modal features can effectively guide image training. By constructing high-quality text data through prompts and labels, it becomes feasible to transfer knowledge from the text modality to the image modality during training.\nOverall, our approach achieves promising knowledge transfer results between all these widely differing modalities, proving its broad validity.}\n\nOur contributions can be summarized as:\n {itemize}\n\n  We find that the hard alignment is one important source of overfitting for cross-modality distillation. Therefore, we propose two soft alignment strategies, feature level and classifier level.\nThe former introduces a relaxation parameter, margin, in distillation loss functions, avoiding forcing the student to perfectly be the same as the teacher. The latter projects two modalities to the same feature space which share the classifier, making two modalities closer instead of forcing them to be the same. These two soft strategies can greatly reduce the overfitting.\n\n  We propose a quality-based adaptive weighting module that can adaptively weight training samples based on their quality, leading to improved performance and reduced overfitting during cross-modality distillation.\n\n  We conduct extensive experiments on speaker recognition and image classification tasks, and our results show that our approach can effectively transfer knowledge between the commonly used and widely varying modalities of image, text, and speech, and greatly improves the uni-modality performance.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.401,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.404,
      "datasets_score": 0.352,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on cross-modal knowledge distillation, where a teacher model transfers knowledge to a student model across different modalities. It does not involve programmatically generating noisy or imprecise labels for training, which is the core of weak supervision. Instead, it relies on a pre-trained teacher for supervision, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "The paper discusses knowledge distillation for multi-modal learning and does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Its contributions are centered on modality transfer and soft constraints, with no connection to diffusion-based methods.",
      "distributed_training_justification": "The paper's main contribution is a framework for cross-modal knowledge distillation, including soft constraints and adaptive weighting, but it does not address distributed training, parallel computing, or partitioning data/computation across multiple nodes or processors. There is no discussion of scaling training in a distributed environment.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667788",
      "updated_at": "2025-08-11T23:43:05.606857",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16302",
      "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against\n  Downstream Fine-tuning",
      "authors": [
        "Boheng Li",
        "Renjie Gu",
        "Junjie Wang",
        "Leyi Qi",
        "Yiming Li",
        "Run Wang",
        "Zhan Qin",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CR (Cryptography and Security)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Text-to-image (T2I) diffusion models have achieved impressive image\ngeneration quality and are increasingly fine-tuned for personalized\napplications. However, these models often inherit unsafe behaviors from toxic\npretraining data, raising growing safety concerns. While recent safety-driven\nunlearning methods have made promising progress in suppressing model toxicity,\nthey are identified to be fragile to downstream fine-tuning, where we reveal\nthat state-of-the-art methods largely fail to retain their effectiveness even\nwhen fine-tuned on entirely benign datasets. To mitigate this problem, in this\npaper, we propose ResAlign, a safety-driven unlearning framework with enhanced\nresilience against downstream fine-tuning. By modeling downstream fine-tuning\nas an implicit optimization problem with a Moreau Envelope-based reformulation,\nResAlign enables efficient gradient estimation to minimize the recovery of\nharmful behaviors. Additionally, a meta-learning strategy is proposed to\nsimulate a diverse distribution of fine-tuning scenarios to improve\ngeneralization. Extensive experiments across a wide range of datasets,\nfine-tuning methods, and configurations demonstrate that ResAlign consistently\noutperforms prior unlearning approaches in retaining safety after downstream\nfine-tuning while preserving benign generation capability well.",
      "published_date": "2025-07-22T07:40:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16302v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16302v1",
      "latex_url": "http://arxiv.org/src/2507.16302v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Text-to-image (T2I) diffusion models have emerged as a dominant class of generative AI due to their unprecedented ability to synthesize high-quality, diverse, and aesthetically compelling general images from natural language descriptions . Beyond synthesizing general images, there is also a growing interest in customizing pretrained models for personalized generation, e.g., generating images of specific facial identities or artistic styles that are underrepresented in the original training data. This is typically achieved by fine-tuning the pretrained base model on a small reference dataset for a few steps . The development of several advanced fine-tuning methods as well as the rapid proliferation of ``fine-tuning-as-a-service'' platforms has made personalization widely accessible, fueling a surge in applications such as stylized avatars, fan art, and thematic illustrations, which are becoming increasingly popular especially among younger users .\n\nYet alongside the rapid advancements of diffusion models, growing concerns have emerged regarding their potential to generate inappropriate or harmful content (e.g., sexually explicit imagery) . Due to the large-scale and web-crawled nature of their training datasets, modern T2I models inevitably ingest amounts of harmful materials during pretraining . As a result, these models can reproduce such content either when explicitly prompted or inadvertently triggered. For example, recent studies~ based on real-world user generations demonstrate that widely-deployed models like Stable Diffusion are particularly prone to producing unsafe content, even though many of the prompts that lead to unsafe outputs appear benign and may not be intended to generate harmful results. These vulnerabilities not only allow malicious exploitation through direct or adversarial prompting~, but also increase the risk of harmful unintended exposure for ordinary, benign users~, raising serious ethical concerns for real-world deployment. In response to these concerns, a variety of safety-driven unlearning methods~ have recently been proposed to modify the pretrained models’ parameters in order to suppress their capacity for unsafe generation.\n\nWhile existing methods have shown encouraging results in reducing model toxicity and the resulting unlearned models are promising to be used as ``safe'' base models for downstream fine-tuning in practical workflows, one natural yet largely unexplored question is whether the safety of unlearned models remains resilient after downstream fine-tuning. Unfortunately, recent studies~ have shown that many existing methods can be easily reversed, where fine-tuning on harmful samples for as few as 20 steps can largely recover a model’s unsafe capability. More strikingly, our empirical results reveal that even when fine-tuned on purely benign data, state-of-the-art unlearning methods can regress, with the model’s harmfulness approaching its pre-unlearning state. In other words, even entirely benign users without any malicious intent or harmful data may inadvertently trigger a recovery of unsafe behaviors, posing unforeseen safety risks in real-world use. These findings suggest that current methods may be significantly more brittle than previously assumed and are largely unprepared to serve as reliably safe base models for downstream fine-tuning, underscoring the urgent need for more resilient approaches that can withstand post-unlearning adaptation.\n\nTowards this end, in this paper, we propose a resilient safety-driven unlearning framework dubbed ResAlign to mitigate the aforementioned problem. The intuition behind our method is that unlearning should not only suppress harmfulness at the current model state, but also explicitly minimize the degree to which harmful behaviors can be regained after (simulated) downstream fine-tuning. While conceptually simple, it is particularly challenging to develop a principled and efficient optimization framework to realize this objective. This is because fine-tuning itself is a multi-step optimization process, making it non-trivial to predict which update direction on the original parameters helps minimize the regained harmfulness after downstream fine-tuning. To address this, we approximate fine-tuning as an implicit optimization problem with a {Moreau Envelope} formulation , which enables efficient gradient estimation via {implicit differentiation}. Besides, to ensure generalizability against the wide variability in real-world downstream fine-tuning procedures (e.g., different datasets, fine-tuning methods, and hyperparameters), we design a meta-learning approach that simulates a distribution of plausible fine-tuning configurations during training, allowing the model to generalize its resilience across a broad range of downstream adaptation scenarios. We also provide insights from the theoretical perspective to explain the empirical effectiveness of our method.\n\nIn conclusion, our main contributions are threefold. (1) We empirically reveal that existing safety-driven unlearning methods largely fail to retain their effectiveness after downstream fine-tuning, even when the data does not contain unsafe content. (2) We propose ResAlign, a resilient safety-driven unlearning framework for T2I diffusion models. By leveraging a Moreau Envelope-based approximation and a meta-learning strategy over diverse adaptation scenarios, ResAlign explicitly accounts for and minimizes post-unlearning degradation due to downstream fine-tuning efficiently with high generalizability. We further provide theoretical insights to help understand the empirical effectiveness of our method. (3) Through extensive experiments, we show that ResAlign consistently outperforms baselines in maintaining safety after fine-tuning, and generalizes well to a wide range of advanced fine-tuning methods, datasets, and hyperparameters. It also preserves both general and personalized generation quality well, and remains effective to multiple diffusion architectures, unsafe concepts, and even when the data is harmfully contaminated or adaptively attacked.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "arxiv.tex",
      "rlhf_score": 0.479,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.547,
      "distributed_training_score": 0.399,
      "datasets_score": 0.339,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on safety-driven unlearning for diffusion models, using optimization techniques like Moreau Envelope and meta-learning to enhance resilience against fine-tuning. It does not involve human feedback, reward models, or reinforcement learning for aligning models with preferences.",
      "weak_supervision_justification": "The paper addresses unlearning harmful behaviors in diffusion models and does not discuss programmatically generating labels from noisy or imprecise sources. It relies on standard pretraining and fine-tuning datasets without emphasizing weak supervision techniques.",
      "diffusion_reasoning_justification": "The paper applies diffusion models to text-to-image generation and safety enhancements, but it does not adapt them for multi-step logical reasoning, chain-of-thought processes, or iterative refinement of reasoning tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667648",
      "updated_at": "2025-08-11T23:43:05.606828",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16307",
      "title": "Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of\n  Precursor Additives and Experimental Design",
      "authors": [
        "Xin-De Wang",
        "Zhi-Rui Chen",
        "Peng-Jie Guo",
        "Ze-Feng Gao",
        "Cheng Mu",
        "Zhong-Yi Lu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in\nnext-generation photovoltaic technologies, owing to their exceptional power\nconversion efficiencies and advantageous material properties. Despite these\nadvances, challenges such as long-term stability, environmental sustainability,\nand scalable manufacturing continue to hinder their commercialization.\nPrecursor additive engineering has shown promise in addressing these issues by\nenhancing both the performance and durability of PSCs. However, the explosive\ngrowth of scientific literature and the complex interplay of materials,\nprocesses, and device architectures make it increasingly difficult for\nresearchers to efficiently access, organize, and utilize domain knowledge in\nthis rapidly evolving field. To address this gap, we introduce Perovskite-R1, a\nspecialized large language model (LLM) with advanced reasoning capabilities\ntailored for the discovery and design of PSC precursor additives. By\nsystematically mining and curating 1,232 high-quality scientific publications\nand integrating a comprehensive library of 33,269 candidate materials, we\nconstructed a domain-specific instruction-tuning dataset using automated\nquestion-answer generation and chain-of-thought reasoning. Fine-tuning the\nQwQ-32B model on this dataset resulted in Perovskite-R1, which can\nintelligently synthesize literature insights and generate innovative and\npractical solutions for defect passivation and the selection of precursor\nadditives. Experimental validation of several model-proposed strategies\nconfirms their effectiveness in improving material stability and performance.\nOur work demonstrates the potential of domain-adapted LLMs in accelerating\nmaterials discovery and provides a closed-loop framework for intelligent,\ndata-driven advancements in perovskite photovoltaic research.",
      "published_date": "2025-07-22T07:48:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16307v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16307v1",
      "latex_url": "http://arxiv.org/src/2507.16307v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In recent years, perovskite solar cells (PSCs) have garnered significant attention from both academia and industry due to their tremendous potential in sustainable energy applications. Since their introduction in 2009, the power conversion efficiency (PCE) of PSCs has rapidly increased from 3.8% to 26.95%~, highlighting their promising prospects as next-generation photovoltaic technology. The remarkable efficiency improvement of PSCs is mainly attributed to the unique advantages of perovskite materials, such as highly ordered crystal structures, excellent light absorption, tunable band gaps, and cost-effective solution-based fabrication~. These features not only enable high performance but also facilitate the scalability of PSCs for large-scale applications.\n\nHowever, the commercialization of PSCs still faces significant challenges, including long-term operational stability, environmental sustainability, and scalable manufacturing~. Issues such as sensitivity to moisture and heat, material degradation, and process compatibility remain key bottlenecks limiting practical deployment. To address these challenges, precursor additive engineering has emerged as an effective strategy to enhance both the stability and efficiency of PSCs. By introducing suitable additives into the precursor solution, researchers can optimize crystallization, passivate defects, and improve film quality, thereby significantly improving device performance and durability. This approach offers strong support for the further industrialization of PSC technology.\n\nTraditionally, progress in PSCs research has relied on researchers conducting comprehensive literature reviews and then applying intuition and experience-based judgment to analyze findings, followed by experimental validation. However, the rapid advancement of PSCs in recent years has led to an exponential increase in the number of related publications, making it increasingly challenging for researchers to efficiently access and utilize the ever-growing body of knowledge in this field. This challenge is particularly acute given the complex interplay among material composition, fabrication processes, and device architecture that characterize PSCs research. On the other hand, existing artificial intelligence systems~ in materials science typically focus on specific prediction tasks or general scientific knowledge~, lacking the specialized capability to address the unique characteristics of PSC research~. This gap highlights the urgent need for an integrated system that can systematically organize domain knowledge and provide intelligent assistance to researchers.\n\nRecently, large language models (LLMs) have developed rapidly, with numerous high-performance models emerging~. Advances in LLMs are primarily driven by increases in model size, the scale of pre-training data, and enhanced reasoning capabilities. Recently, much attention has focused on improving reasoning abilities, which not only boost accuracy but also enhance interpretability and controllability. For example, the introduction of Chain-of-Thought (CoT) prompting in 2022 led to significant improvements in mathematical reasoning tasks for large models~, laying the groundwork for further developments in the field. Moreover, integrating CoT methods with reinforcement learning has further advanced model autonomy, enabling superior performance in tasks such as mathematics, programming, and scientific reasoning~. At the same time, applying LLMs to specialized domains has become an important trend. However, due to complex terminologies and extensive knowledge structures in these fields, general-purpose models often fall short. Techniques such as domain-specific fine-tuning and structured knowledge injection have facilitated the emergence of specialized LLMs. For instance, dedicated LLMs have been developed and applied in areas such as biomedicine, materials science, education, and finance~.\n\nIn this work, we present Perovskite-R1, an LLM with advanced reasoning capabilities designed specifically for the discovery and design of PSC precursor additives. Perovskite-R1 aims to provide intelligent and systematic support for material design and screening in the field of perovskite photovoltaics. Leveraging the capabilities of Perovskite-R1, we systematically identify a set of potential defect compensation strategies for perovskite precursor additives. Several of these strategies are further validated through experimental investigations, which demonstrated that the solutions proposed by Perovskite-R1 are both rational and reliable, leading to improvements in material performance and stability. Specifically, we collect 1,232 scientific publications related to perovskite precursor design as well as a candidate library containing 33,269 materials. Utilizing the OpenAI o1 model {The full version of OpenAI o1 model was released on December 5, 2024.}, we transform the content of these papers into an instruction-tuning dataset in the form of question–answer pairs, thereby enriching the model’s domain-specific knowledge and practical examples. Building on this high-quality instruction dataset, we fine-tune the QwQ-32B pre-trained model {The QwQ-32B model was first released on March 6, 2025.} to develop Perovskite-R1. Thanks to this comprehensive dataset and instruction tuning, Perovskite-R1 can effectively integrate existing knowledge on perovskite precursors with the latest research advances and experimental data, generating innovative and practical design solutions.\n\nTo experimentally validate the predictive power of our model, we conducted a systematic comparison between additives recommended by Perovskite-R1~(3, 5-difluoropyridine-2-carboxylic acid (AI-DFCA) and 5-hydroxy-2-methylbenzoic acid (AI-HMBA)) and those chosen manually by researchers~(gallic acid (Manual-GA) and caffeic acid (Manual-CA)). All additives were incorporated at equal concentrations into Cs$_{0.05}$MA$_{0.1}$FA$_{0.85}$PbI$_{3}$ perovskite devices under identical fabrication conditions. The results revealed that model-identified additives significantly improved device performance, while manually selected additives led to inferior outcomes. This highlights the advantage of data-driven screening over traditional, experience-based approaches in complex materials discovery, opening new avenues for intelligent material discovery in the PSC field.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.367,
      "datasets_score": 0.343,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on fine-tuning a pre-trained model (QwQ-32B) using an automatically generated instruction-tuning dataset from scientific publications and the OpenAI o1 model. There is no mention of human feedback, a reward model trained on human-ranked data, or reinforcement learning techniques to align the model with human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper incorporates chain-of-thought reasoning in dataset generation and model fine-tuning, but it does not describe any adaptation of diffusion models or iterative refinement processes for multi-step logical tasks. There is no evidence of treating the reasoning path as a holistically corrected entity over steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669296",
      "updated_at": "2025-08-11T23:43:05.607135",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16310",
      "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for\n  Text-to-Video Generation",
      "authors": [
        "Yanchen Liu",
        "Yanan Sun",
        "Zhening Xing",
        "Junyao Gao",
        "Kai Chen",
        "Wenjie Pei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing text-to-video methods struggle to transfer motion smoothly from a\nreference object to a target object with significant differences in appearance\nor structure between them. To address this challenge, we introduce MotionShot,\na training-free framework capable of parsing reference-target correspondences\nin a fine-grained manner, thereby achieving high-fidelity motion transfer while\npreserving coherence in appearance. To be specific, MotionShot first performs\nsemantic feature matching to ensure high-level alignments between the reference\nand target objects. It then further establishes low-level morphological\nalignments through reference-to-target shape retargeting. By encoding motion\nwith temporal attention, our MotionShot can coherently transfer motion across\nobjects, even in the presence of significant appearance and structure\ndisparities, demonstrated by extensive experiments. The project page is\navailable at: https://motionshot.github.io/.",
      "published_date": "2025-07-22T07:51:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16310v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16310v1",
      "latex_url": "http://arxiv.org/src/2507.16310v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{comment}\nIn recent years, with the rise and development of diffusion models , high-quality video generation has attracted much attention .\n\nBy leveraging user-provided reference images or videos, models can learn specific visual hints, precisely guiding the generation process to achieve more personalized and controllable creation.\n\nThis motion customization refers to the generation of specific motion videos based on different textual prompts while ensuring that the motion of the subject in the generated video remains consistent with that of the reference video.\nHowever, a major challenge arises from the tight coupling of motion and appearance information in the latent space. Effectively transferring motion information without leaking appearance details is a critical issue. In certain tasks, such as human pose or facial expression transfer , a predefined set of landmarks is often used as motion clues to generate videos with customized pose and expression motion.\n {comment}\n\nRecent advances in diffusion models have significantly propelled the progress of video generation . Although existing methods can produce high-quality videos guided by text prompts, achieving precise motion customization---where generated videos adhere to specific motion patterns from user-provided reference videos---remains particularly challenging, especially for arbitrary reference-target object pairs with significant appearance differences.\n\nExisting motion transfer methods primarily focus on developing effective motion descriptors. For example, one line of research~ utilizes landmark sequences as motion descriptors for transferring motion between reference-target pairs with similar appearances. However, this approach cannot be easily generalized to arbitrary objects, as predefining landmarks for all objects proves to be challenging. Another approach~ typically extracts learned spatial-temporal features from reference videos as motion descriptors. Unfortunately, the inherent entanglement of motion and appearance in latent representations creates a critical bottleneck, leading to unintended leakage of reference appearance details. Recent studies have turned to alternative motion cues as intermediate motion descriptors, including depth or edge maps~, sparse optical flow or trajectories~. While these methods excel at transferring motion between objects with minor appearance differences, they often struggle with objects that have substantial appearance discrepancies, as they do not account for region-level semantic correspondence and pixel-level structural correspondence.\n\nIn this work, we introduce  , a new training-free motion transfer framework capable of accurately transferring motion information to a target object without requiring additional training, even when there are considerable differences in appearance and structure, as illustrated in~ {fig:teaser}. Our   directs video generation to adhere to the desired motion using temporal attention guidance, eliminating the need for labor-intensive large-scale data collection. However, attention guidance based on positional alignment becomes less effective for objects with substantial appearance differences. To tackle this issue, we propose a novel two-level motion alignment strategy---high-level semantic motion alignment and low-level structural motion alignment---to create adaptive temporal attention guidance for arbitrary object pairs.\n\nSpecifically, the high-level motion alignment establishes semantic correspondence automatically between reference and target objects. This correspondence is determined through semantic feature matching between two keypoint sets, which are sampled in a structure-aware manner from both the reference and target objects. Relying solely on high-level motion alignment may lead to discontinuities in temporal attention guidance. We further enhance the motion alignment with low-level structural mapping, achieved through Thin Plate Spline-based shape warping. This approach ensures more precise motion control while maintaining structural alignment with the target object.\n\nBy integrating our two-level motion alignment, the attention-guided video generation model enables motion transfer that faithfully follows the reference motion while naturally fitting the structure of the target subject.   is the first framework to explicitly model both high-level and low-level motion alignment. Overall, our main contributions are manifold:\n\n {itemize}\n   We introduce  , a novel training-free motion transfer framework that facilitates precise motion adaptation, even when there are substantial differences in appearance and structure between the reference and target objects.\n   We develop an unique two-level motion alignment strategy that combines semantic and structural alignment to establish correspondence between reference-target pairs, allowing for adherence to the reference object's motion while preserving the appearance of the target object.\n     demonstrates superior performance compared to existing methods in both motion fidelity and structural coherence, particularly in scenarios where there are significant appearance and structural discrepancies between the reference and target objects.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.314,
      "datasets_score": 0.297,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily focuses on adapting diffusion models for text-to-video generation and motion transfer between objects, emphasizing semantic and structural alignments. It does not involve adapting the iterative refinement process of diffusion models for multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks, as required by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668064",
      "updated_at": "2025-08-11T23:43:05.606920",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16318",
      "title": "M-SpecGene: Generalized Foundation Model for RGBT Multispectral Vision",
      "authors": [
        "Kailai Zhou",
        "Fuqiang Yang",
        "Shixian Wang",
        "Bihan Wen",
        "Chongde Zi",
        "Linsen Chen",
        "Qiu Shen",
        "Xun Cao"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "RGB-Thermal (RGBT) multispectral vision is essential for robust perception in\ncomplex environments. Most RGBT tasks follow a case-by-case research paradigm,\nrelying on manually customized models to learn task-oriented representations.\nNevertheless, this paradigm is inherently constrained by artificial inductive\nbias, modality bias, and data bottleneck. To address these limitations, we make\nthe initial attempt to build a Generalized RGBT MultiSpectral foundation model\n(M-SpecGene), which aims to learn modality-invariant representations from\nlarge-scale broad data in a self-supervised manner. M-SpecGene provides new\ninsights into multispectral fusion and integrates prior case-by-case studies\ninto a unified paradigm. Considering the unique characteristic of information\nimbalance in RGBT data, we introduce the Cross-Modality Structural Sparsity\n(CMSS) metric to quantify the information density across two modalities. Then\nwe develop the GMM-CMSS progressive masking strategy to facilitate a flexible,\neasy-to-hard, and object-centric pre-training process. Comprehensive\nexperiments validate M-SpecGene's generalizability across eleven datasets for\nfour RGBT downstream tasks. The code will be available at\nhttps://github.com/CalayZhou/M-SpecGene.",
      "published_date": "2025-07-22T08:00:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16318v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16318v2",
      "latex_url": "http://arxiv.org/src/2507.16318v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[t]\n  \n\n  [width=1.0 ]{img/introduction.png}\n  {-2.4em}\n  {(a) Manually customized models: task-oriented representations are learned under a case-by-case research paradigm. (b) Generalized RGBT multispectral foundation model aims to learn modality-invariant representations by self-supervised learning. The t-SNE visualization of RGB and thermal features indicates M-SpecGene achieves superior cross-modality alignment. }\n\n  {-1.4em}\n {figure}\n\nRGB sensors alone struggle to handle complex environmental conditions, including smog, low light, and high dynamic range scenarios. RGBT multispectral vision, with its all-weather, round-the-clock sensing capabilities, has emerged as a crucial technology in fields like autonomous driving, military defense, remote sensing, and industrial inspection.\n\nCurrently, most RGBT downstream tasks follow a case-by-case research paradigm. For a given task, task-oriented representations are learned via fully supervised learning on small, task-specific datasets, often using models pretrained on ImageNet or trained from scratch. As illustrated in Fig.~(a), existing methods commonly use two-stream branches to extract features from both RGB and thermal images, incorporating complex handcrafted modules in the intermediate feature space, such as channel attention , spatial attention , Transformer , and graph network . However, this case-by-case paradigm has several limitations:\n1) Artificial inductive bias: Task-oriented, manually customized models, being optimized for a given task, are effective for that task but may lead to suboptimal results on others, thereby restricting both the scalability of the designed model and the generalizability of the learned representations. 2) Modality bias: Due to inherent differences between RGB and thermal modalities, initializing the thermal branch with the ImageNet pretrained model inevitably introduces modality bias. This bias can potentially impair the encoded prior knowledge and result in suboptimal feature representations for the thermal modality. 3) Data bottleneck: RGBT multispectral images are harder to obtain than single RGB images, and high-quality manual annotation for large datasets is costly and time-intensive.\n\nRecently, foundation models, with their capacity to encode extensive knowledge , offer a potential solution to above limitations. As shown in Fig.~(b), we make an initial attempt to transform manually customized models into a generalized multispectral foundation model named M-SpecGene, which aims to explore a new RGBT fusion paradigm that learns modality-invariant representations in a self-supervised manner, therefore eliminating the need for handcrafted modules and facilitating multi-modality feature fusion in a simple yet effective way. However, the self-supervised pre-training of generalized multispectral foundation model is challenging, due to the lack of large-scale datasets and the inherent information imbalance in RGBT data. In contrast to RGB images, thermal images lack rich textures, colors, and fine details. Moreover, significant differences in imaging mechanisms introduce asymmetry in information density between the two modalities. Additionally, RGBT datasets are not object-centric like ImageNet ; instead, they tend to include smaller, less salient objects with dispersed and uneven information distribution.\n\nTo address above problems, M-SpecGene employs a Siamese architecture and a progressive masking strategy to promote consistent representations in latent space. Leveraging the unique correlations within multispectral images, we introduce cross-modality structural sparsity to quantify information density between two modalities. Then we develop a Gaussian Mixture Model (GMM) to fit the overall CMSS distribution of the whole pre-training datasets, enabling a flexible, modality-balanced masking strategy that progresses from easier to more difficult learning stages. Our GMM-CMSS progressive masking strategy alleviates the impact of information imbalance in self-supervised pre-training, enhancing the encoder's ability to focus on consistent, modality-invariant, and object-centric representations.\n\nM-SpecGene provides new insights into the RGBT fusion paradigm and offers the following advantages: 1) Simplified model design: A single foundation model can effectively represent both RGB and thermal modalities, eliminating the need for complex handcrafted modules and facilitating the adaptation of single-modality RGB methods to RGBT two-modality tasks. 2) Generalized representation: Self-supervised pre-training on large-scale data enables M-SpecGene to learn a versatile representation that overcomes limitations associated with artificial inductive and modality biases, making it adaptable to a diverse range of downstream tasks. 3) Enhanced data utilization: M-SpecGene fully integrates self-supervised pre-training data from existing RGBT tasks without the need for human annotations. Our contributions are as follows:\n\n$ $ We make the first attempt to build a multispectral foundation model, M-SpecGene, exploring a new RGBT fusion paradigm that eliminates the need for handcrafted modules.\n\n$ $ A high-quality, large-scale dataset, RGBT550K is carefully constructed for self-supervised pre-training.\n\n$ $ Considering the unique characteristic of RGBT datasets, we introduce a GMM-CMSS progressive masking strategy to mitigate the impact of information imbalance.\n\n$ $ M-SpecGene integrates prior case-by-case studies into a unified paradigm and demonstrates strong generalizability across eleven datasets for four RGBT downstream tasks.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.291,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.35,
      "datasets_score": 0.347,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668072",
      "updated_at": "2025-08-11T23:43:05.606922",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16322",
      "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical\n  Language Reasoning LLM Benchmarks for African Disease Burdens",
      "authors": [
        "Fred Mutisya",
        "Shikoh Gitau",
        "Christine Syovata",
        "Diana Oigara",
        "Ibrahim Matende",
        "Muna Aden",
        "Munira Ali",
        "Ryan Nyotu",
        "Diana Marion",
        "Job Nyangena",
        "Nasubo Ongoma",
        "Keith Mbae",
        "Elizabeth Wamicha",
        "Eric Mibuari",
        "Jean Philbert Nsengemana",
        "Talkmore Chidede"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Introduction: Existing medical LLM benchmarks largely reflect examination\nsyllabi and disease profiles from high income settings, raising questions about\ntheir validity for African deployment where malaria, HIV, TB, sickle cell\ndisease and other neglected tropical diseases (NTDs) dominate burden and\nnational guidelines drive care. Methodology: We systematically reviewed 31\nquantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English\nmedical QA benchmarks. Alama Health QA was developed using a retrieval\naugmented generation framework anchored on the Kenyan Clinical Practice\nGuidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,\nMedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized\nsemantic profiling (NTD proportion, recency, readability, lexical diversity\nmetrics) and blinded expert rating across five dimensions: clinical relevance,\nguideline alignment, clarity, distractor plausibility, and language/cultural\nfit. Results: Alama Health QA captured >40% of all NTD mentions across corpora\nand the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB\n(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global\nbenchmarks showed minimal representation (e.g., sickle cell disease absent in\nthree sets) despite large scale. Qualitatively, Alama scored highest for\nrelevance and guideline alignment; PubMedQA lowest for clinical utility.\nDiscussion: Quantitative medical LLM benchmarks widely used in the literature\nunderrepresent African disease burdens and regulatory contexts, risking\nmisleading performance claims. Guideline anchored, regionally curated resources\nsuch as Alama Health QA and expanded disease specific derivatives are essential\nfor safe, equitable model evaluation and deployment across African health\nsystems.",
      "published_date": "2025-07-22T08:05:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16322v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16322v1",
      "latex_url": "http://arxiv.org/src/2507.16322v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.347,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.309,
      "datasets_score": 0.392,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.667800",
      "updated_at": "2025-08-11T23:43:05.606859",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16329",
      "title": "DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via\n  Distribution Modeling",
      "authors": [
        "Boheng Li",
        "Junjie Wang",
        "Yiming Li",
        "Zhiyang Hu",
        "Leyi Qi",
        "Jianshuo Dong",
        "Run Wang",
        "Han Qiu",
        "Zhan Qin",
        "Tianwei Zhang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Despite the integration of safety alignment and external filters,\ntext-to-image (T2I) generative models are still susceptible to producing\nharmful content, such as sexual or violent imagery. This raises serious\nconcerns about unintended exposure and potential misuse. Red teaming, which\naims to proactively identify diverse prompts that can elicit unsafe outputs\nfrom the T2I system (including the core generative model as well as potential\nexternal safety filters and other processing components), is increasingly\nrecognized as an essential method for assessing and improving safety before\nreal-world deployment. Yet, existing automated red teaming approaches often\ntreat prompt discovery as an isolated, prompt-level optimization task, which\nlimits their scalability, diversity, and overall effectiveness. To bridge this\ngap, in this paper, we propose DREAM, a scalable red teaming framework to\nautomatically uncover diverse problematic prompts from a given T2I system.\nUnlike most prior works that optimize prompts individually, DREAM directly\nmodels the probabilistic distribution of the target system's problematic\nprompts, which enables explicit optimization over both effectiveness and\ndiversity, and allows efficient large-scale sampling after training. To achieve\nthis without direct access to representative training samples, we draw\ninspiration from energy-based models and reformulate the objective into simple\nand tractable objectives. We further introduce GC-SPSA, an efficient\noptimization algorithm that provide stable gradient estimates through the long\nand potentially non-differentiable T2I pipeline. The effectiveness of DREAM is\nvalidated through extensive experiments, demonstrating that it surpasses 9\nstate-of-the-art baselines by a notable margin across a broad range of T2I\nmodels and safety filters in terms of prompt success rate and diversity.",
      "published_date": "2025-07-22T08:10:22+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16329v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16329v1",
      "latex_url": "http://arxiv.org/src/2507.16329v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{abstract}\nDespite the integration of safety alignment and external filters, text-to-image (T2I) generative models are still susceptible to producing harmful content, such as sexual or violent imagery. This raises serious concerns about unintended exposure and potential misuse. Red teaming, which aims to proactively identify diverse prompts that can elicit unsafe outputs from the T2I system (including the core generative model as well as potential external safety filters and other processing components), is increasingly recognized as an essential method for assessing and improving safety before real-world deployment. Yet, existing automated red teaming approaches often treat prompt discovery as an isolated, prompt-level optimization task, which limits their scalability, diversity, and overall effectiveness. To bridge this gap, in this paper, we propose DREAM, a scalable red teaming framework to automatically uncover diverse problematic prompts from a given T2I system. Unlike most prior works that optimize prompts individually, DREAM directly models the probabilistic distribution of the target system's problematic prompts, which enables explicit optimization over both effectiveness and diversity, and allows efficient large-scale sampling after training. To achieve this without direct access to representative training samples, we draw inspiration from energy-based models and reformulate the objective into simple and tractable objectives. We further introduce GC-SPSA, an efficient optimization algorithm that provide stable gradient estimates through the long and potentially non-differentiable T2I pipeline. During inference, we also propose a diversity-aware sampling strategy to enhance prompt variety. The effectiveness of DREAM is validated through extensive experiments, demonstrating that it surpasses 9 state-of-the-art baselines by a notable margin across a broad range of T2I models and safety filters in terms of prompt success rate and diversity. Additionally, DREAM successfully uncovers failure cases in 4 real-world commercial T2I systems and enables more robust safety fine-tuning that generalizes to unseen harmful prompts.\n {abstract}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "arxiv.tex",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.473,
      "distributed_training_score": 0.419,
      "datasets_score": 0.373,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on an automated red teaming framework for identifying problematic prompts in text-to-image systems, using distribution modeling and optimization techniques. It does not involve human feedback, reward models, or reinforcement learning for model alignment.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper addresses red teaming for text-to-image generative systems, which may use diffusion models, but it does not adapt diffusion processes for multi-step logical reasoning or treat reasoning paths as entities for iterative refinement.",
      "distributed_training_justification": "The paper proposes a scalable framework for prompt optimization and sampling but does not discuss distributed training, parallel computing, or partitioning data/computation across multiple nodes for model training.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667658",
      "updated_at": "2025-08-11T23:43:05.606830",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16330",
      "title": "Scene Text Detection and Recognition \"in light of\" Challenging\n  Environmental Conditions using Aria Glasses Egocentric Vision Cameras",
      "authors": [
        "Joseph De Mathia",
        "Carlos Francisco Moreno-García"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In an era where wearable technology is reshaping applications, Scene Text\nDetection and Recognition (STDR) becomes a straightforward choice through the\nlens of egocentric vision. Leveraging Meta's Project Aria smart glasses, this\npaper investigates how environmental variables, such as lighting, distance, and\nresolution, affect the performance of state-of-the-art STDR algorithms in\nreal-world scenarios. We introduce a novel, custom-built dataset captured under\ncontrolled conditions and evaluate two OCR pipelines: EAST with CRNN, and EAST\nwith PyTesseract. Our findings reveal that resolution and distance\nsignificantly influence recognition accuracy, while lighting plays a less\npredictable role. Notably, image upscaling emerged as a key pre-processing\ntechnique, reducing Character Error Rate (CER) from 0.65 to 0.48. We further\ndemonstrate the potential of integrating eye-gaze tracking to optimise\nprocessing efficiency by focusing on user attention zones. This work not only\nbenchmarks STDR performance under realistic conditions but also lays the\ngroundwork for adaptive, user-aware AR systems. Our contributions aim to\ninspire future research in robust, context-sensitive text recognition for\nassistive and research-oriented applications, such as asset inspection and\nnutrition analysis. The code is available at\nhttps://github.com/josepDe/Project_Aria_STR.",
      "published_date": "2025-07-22T08:12:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16330v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16330v1",
      "latex_url": "http://arxiv.org/src/2507.16330v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Scene Text Detection and Recognition (STDR) is a long-standing problem within the Document Analysis and Recognition (DAR) community. It deals with the detection and classification of characters found in natural images, including pictures taken from advertisements, signposts, books, among others~. Recently, Meta launched their Project Aria Research initiative~, consisting of glasses with multiple cameras and sensors. These glasses have been released to the scientific community to perform innovative research on how to improve Virtual Reality (VR) and Augmented Reality (AR) applications~  {Plizzari2024}, but also aid in the creation of datasets and algorithms based on egocentric vision in order to improve robotics~,~ imitation learning~, amongst others.\n\nApplication-wise, Project Aria’s glasses could help bolster applications in numerous real-life fields where there is a need to inspect an asset~ or to understand an individual’s emotion through facial analysis~. In our work, we explore how these glasses can assist nutrition experts in understanding consumers dietary habits. Specifically by analysing the video feed recorded by a participant and understanding their eye gaze behaviour when buying a product; which information they are focusing on, their dietary patterns (e.g quantities and portions) and their overall interaction with food products.\n\nThe aim of this project is to study STDR specifically using footage captured by Project Aria glasses. The study focuses on how environmental and image quality factors such as lighting, distance, and resolution impact the accuracy of STDR algorithms. For this study, we have used the Efficient and Accurate Scene Text detection(EAST) algorithm~ to detect the text bounding boxes. Subsequently, we implement a heuristic correction to merge individual character bounding boxes together, thus conforming the word areas. For the Optical Character Recognition (OCR) stage, this study utilises two different algorithms: Google’s Pytesseract~ and a Convolutional Recurrent Neural Network (CRNN) provided through EasyOCR~. Following the convention set at the International Conference on Document Analysis and Recognition (ICDAR) 2024 competition~, in this project we will evaluate our methods based on the Character Error Rate (CER). To understand how lighting, distance and resolution affect the OCR models, we collated a custom dataset by using the glasses. This dataset contains images of a ground truth poster in different lighting conditions and at varying recorded distances and resolutions.\n\nThe rest of the paper is organised as follows. Section presents the related work to egocentric vision in the context of STDR. Section describes our methodological approach. Section discusses our three experimental validations, with Section concluding the report and pointing out future research directions.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.297,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.351,
      "distributed_training_score": 0.345,
      "datasets_score": 0.403,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes introducing a novel, custom-built dataset specifically for Scene Text Detection and Recognition (STDR) using egocentric vision from Meta's Project Aria glasses. This dataset was curated under controlled conditions to vary factors like lighting, distance, and resolution, aligning directly with dataset creation and curation methodologies. The paper also involves benchmarking and evaluating this dataset through STDR algorithm performance analysis, such as measuring Character Error Rate (CER), which fits the topic's emphasis on benchmarking and analyzing datasets for machine learning and AI applications.",
      "summary": "This paper investigates the effects of environmental factors such as lighting, distance, and resolution on Scene Text Detection and Recognition (STDR) using Meta's Project Aria smart glasses, aiming to enhance performance in real-world egocentric vision scenarios. The authors create a custom dataset, evaluate two OCR pipelines—EAST with CRNN and EAST with PyTesseract—and find that resolution and distance significantly impact accuracy, while image upscaling reduces the Character Error Rate from 0.65 to 0.48; they also propose integrating eye-gaze tracking for improved efficiency, laying groundwork for adaptive AR applications in fields like asset inspection and nutrition analysis.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing STDR algorithms applied to egocentric vision with Project Aria glasses and introduces a custom dataset, offering a notable adaptation to real-world conditions rather than a entirely new technique or problem.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in subfields like egocentric vision and AR systems by providing practical insights and a dataset for STDR under challenging conditions, though its applicability may remain niche and not broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers valuable, practical contributions to STDR in wearable technology contexts, making it essential for researchers in computer vision and AR to consider for potential applications and future developments.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6010e1ae0e0ba9b51e9cba558281f6e83e2ef27",
      "h_index_fetch_method": "full_id",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Joseph De Mathia",
          "profile_url": "https://www.semanticscholar.org/author/2373046061",
          "h_index": 0
        },
        {
          "name": "Carlos Francisco Moreno-Garc'ia",
          "profile_url": "https://www.semanticscholar.org/author/2072786128",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668080",
      "updated_at": "2025-08-11T23:44:57.362233",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16334",
      "title": "Higher Gauge Flow Models",
      "authors": [
        "Alexander Strunk",
        "Roland Assam"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "math.DG (Differential Geometry)"
      ],
      "abstract": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models.",
      "published_date": "2025-07-22T08:16:06+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16334v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16334v2",
      "latex_url": "http://arxiv.org/src/2507.16334v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "A Higher Gauge Flow Model is a novel approach to generative modeling, where the dynamics is governed by the following neural Ordinary Differential Equation (ODE):\n {align*}\n { }_{dt} x(t) := v_{ }(x(t), t) -  (t)  _{M;  {W}}  (  {A}_{ }(x(t), t)  [  {v}(x(t), t)  ] d^{ }(x(t), t)  )\n {align*}\nwhere $ {v}(x(t), t)$ has the structure of a graded vector. The Higher Gauge Field acts on this graded vector as follows:\n {align*}\n  {A}_{ }(x(t), t)  [  {v}(x(t), t)  ] d^{ }(x(t), t) :=  _m  {A}^{a}_{ }(x(t), t) d^{ }(x(t), t) b_{m} ( e_{a},  {v}(x(t), t),   ,  {v}(x(t), t))\n {align*}\n\n Here, the index $m$ represents the number of inputs for the higher brackets $b_{m}$ within the L$_{ }$-algebra, a sophisticated algebraic structure providing a framework for representing and manipulating higher-order symmetries and invariants. The Higher Gauge Field, denoted as $ {A}_{ }(x(t), t)$, is valued within the graded vector space of the L$_{ }$-algebra. This novel incorporation of an L$_{ }$-algebra enables the generative model to integrate richer mathematical structures. Specifically, this framework enables the exploration of interesting model architectures and potentially introduces higher symmetries into the domain of deep learning.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "Higher_Gauge_Flow_Models.tex",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.306,
      "diffusion_reasoning_score": 0.441,
      "distributed_training_score": 0.328,
      "datasets_score": 0.249,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces Higher Gauge Flow Models, which are generative models based on ODEs and L∞-algebras for data generation, such as on Gaussian Mixture datasets. It does not involve diffusion processes, iterative refinement for logical tasks, or any form of multi-step reasoning like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669305",
      "updated_at": "2025-08-11T23:43:05.607136",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16337",
      "title": "One Polyp Identifies All: One-Shot Polyp Segmentation with SAM via\n  Cascaded Priors and Iterative Prompt Evolution",
      "authors": [
        "Xinyu Mao",
        "Xiaohan Xing",
        "Fei Meng",
        "Jianbang Liu",
        "Fan Bai",
        "Qiang Nie",
        "Max Meng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Polyp segmentation is vital for early colorectal cancer detection, yet\ntraditional fully supervised methods struggle with morphological variability\nand domain shifts, requiring frequent retraining. Additionally, reliance on\nlarge-scale annotations is a major bottleneck due to the time-consuming and\nerror-prone nature of polyp boundary labeling. Recently, vision foundation\nmodels like Segment Anything Model (SAM) have demonstrated strong\ngeneralizability and fine-grained boundary detection with sparse prompts,\neffectively addressing key polyp segmentation challenges. However, SAM's\nprompt-dependent nature limits automation in medical applications, since\nmanually inputting prompts for each image is labor-intensive and\ntime-consuming. We propose OP-SAM, a One-shot Polyp segmentation framework\nbased on SAM that automatically generates prompts from a single annotated\nimage, ensuring accurate and generalizable segmentation without additional\nannotation burdens. Our method introduces Correlation-based Prior Generation\n(CPG) for semantic label transfer and Scale-cascaded Prior Fusion (SPF) to\nadapt to polyp size variations as well as filter out noisy transfers. Instead\nof dumping all prompts at once, we devise Euclidean Prompt Evolution (EPE) for\niterative prompt refinement, progressively enhancing segmentation quality.\nExtensive evaluations across five datasets validate OP-SAM's effectiveness.\nNotably, on Kvasir, it achieves 76.93% IoU, surpassing the state-of-the-art by\n11.44%.",
      "published_date": "2025-07-22T08:19:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16337v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16337v1",
      "latex_url": "http://arxiv.org/src/2507.16337v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[h]\n\n \n [width=0.45 ]{{sec/figs/intro.png}}\n\n {Challenges in SAM-based polyp segmentation and methods comparison. (a) The size variations of polyps and interference like bubbles and reflections complicate polyp perception. (b) Previous methods pour all the prompts simultaneously, while our method introduces evaluation to add prompts interactively.}\n {figure}\n\nColorectal cancer (CRC), ranking as the third most prevalent cancer globally , begins when small, benign cell clusters called polyps form on the colon's interior lining. Early polyp detection via colonoscopy can prevent CRC, yet polyp segmentation, especially with fully supervised methods, faces two key limitations. First, polyp segmentation is inherently challenging due to significant variations in size, color, and texture, requiring extensive expert annotations to enhance the segmentation capability . However, ambiguous boundaries make precise and large-scale annotations time-consuming and prone to errors, which can inversely degrade model performance. Second, clinical deployment of polyp segmentation models requires strong generalizability across datasets. However, domain shifts from variations in endoscopic devices and patient demographics hinder transferability. Fully supervised models tend to overfit the training data, resulting in poor generalization and requiring frequent, resource-intensive retraining, which is clinically inefficient and impractical . Therefore, a polyp segmentation model is needed that minimizes annotation dependence while ensuring accurate and generalizable segmentation.\n\nVision foundation models (VFM), such as CLIP , DINOv2 , and SAM2 , are trained on large-scale datasets, endowing them with strong transferability. Among them, SAM2 excels in segmentation with its fine-grained contour capture and impressive zero-shot capabilities, requiring only sparse prompts like points or bounding boxes. This makes it ideal for polyp segmentation with precise boundary detection, better generalization, and reduced annotation dependency. However, SAM’s reliance on manual prompts limits its clinical feasibility. Manually providing prompts for each image is time-consuming and lacks automation. This prompts a key question: Can a single annotated image automatically generate reliable prompts for SAM, ensuring accuracy and generalization without additional annotation burdens?\n\nRecent research has explored integrating SAM with single-shot semantic segmentation to enable automated segmentation using only one annotation. The pioneer work PerSAM uses prototype-based matching for prompt selection. Matcher adopts a prompt-free pixel-level feature matching approach, and ProtoSAM improves localization with multi-scale features. However, these methods have notable limitations in polyp segmentation. First, these methods struggle to handle morphological variations, particularly differences in polyp size (see Fig. 1(a)). Since a single support image provides limited information, it may fail to capture the entire polyp region when the query is too large, while being prone to false positives for smaller query polyps. Second, previous approaches dump all prompts into SAM simultaneously (see Fig. 1(b)), leading to a trade-off: too few prompts fail to provide sufficient information, whereas excessive prompts introduce noise, degrading performance. Thus, an iterative strategy for optimal prompt selection and placement is required.\n\nIn this work, we introduce OP-SAM, a flexible, feedback-driven automatic prompting segmentation framework with just One Polyp mask and SAM. Our approach handles lesion size variations by adaptively extracting multi-scale semantics from a single support image and iteratively placing prompts for optimal segmentation. First, to achieve accurate label transfer from the support image and capture complete polyp information, we propose Correlation-based Prior Generation (CPG), which enhances feature-matching accuracy by leveraging patch-wise feature correlation cross- and within-images. Second, to handle polyp size variability, we introduce multi-scale prior and reverse-transferring adaptive fusion, namely Scale-cascaded Prior Fusion (SPF). Multi-granularity priors are adaptively fused based on reverse transfer to eliminate false-positive noise. Additionally, we develop Euclidean Prompt Evolution (EPE), an algorithm inspired by SAM’s interactive training pipeline, utilizing Euclidean distance transform to refine segmentation based on output from the previous round iteratively. Given a precise prior, we establish a cyclic process of prompting, segmentation, and evaluation, ensuring comprehensive polyp coverage.\n\nExtensive evaluation across multiple regional five datasets—Kvasir , PolypGen , CVC-ClinicDB , CVC-ColonDB and Piccolo —demonstrates significant improvements over current state-of-the-art (SOTA) methods, with maximal IoU increases of 11.44%. Notably, on the Kvasir dataset, our approach even surpasses oracle performance which uses ground truth from test images to generate prompts. We further test OP-SAM on a manually curated dataset containing extreme-sized polyps. Our approach shows a 10.26% IoU improvement over existing methods, demonstrating robust performance in challenging yet clinically critical cases. Our key contributions can be summarized as follows:\n {itemize}\n   We propose OP-SAM, an innovative training-free framework for one-shot polyp segmentation, leveraging VFM semantics to automate prompt generation for SAM, enabling efficient adaptation with minimal annotations.\n\n   We propose the CPG and SPF modules to generate fine-grained semantic priors, effectively handling polyp size variability. Additionally, we introduce the EPE algorithm, which simulates human-like interactive prompting to enhance segmentation accuracy.\n\n   Through comprehensive experiments across diverse datasets, we demonstrate the remarkable performance and generalization ability of our methods over state-of-the-art methods. In certain datasets, it even surpasses the oracle method where ground truths are given.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.322,
      "weak_supervision_score": 0.381,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.306,
      "datasets_score": 0.303,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668089",
      "updated_at": "2025-08-11T23:43:05.606925",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16341",
      "title": "Navigating Large-Pose Challenge for High-Fidelity Face Reenactment with\n  Video Diffusion Model",
      "authors": [
        "Mingtao Guo",
        "Guanyu Xing",
        "Yanci Zhang",
        "Yanli Liu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Face reenactment aims to generate realistic talking head videos by\ntransferring motion from a driving video to a static source image while\npreserving the source identity. Although existing methods based on either\nimplicit or explicit keypoints have shown promise, they struggle with large\npose variations due to warping artifacts or the limitations of coarse facial\nlandmarks. In this paper, we present the Face Reenactment Video Diffusion model\n(FRVD), a novel framework for high-fidelity face reenactment under large pose\nchanges. Our method first employs a motion extractor to extract implicit facial\nkeypoints from the source and driving images to represent fine-grained motion\nand to perform motion alignment through a warping module. To address the\ndegradation introduced by warping, we introduce a Warping Feature Mapper (WFM)\nthat maps the warped source image into the motion-aware latent space of a\npretrained image-to-video (I2V) model. This latent space encodes rich priors of\nfacial dynamics learned from large-scale video data, enabling effective warping\ncorrection and enhancing temporal coherence. Extensive experiments show that\nFRVD achieves superior performance over existing methods in terms of pose\naccuracy, identity preservation, and visual quality, especially in challenging\nscenarios with extreme pose variations.",
      "published_date": "2025-07-22T08:23:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16341v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16341v1",
      "latex_url": "http://arxiv.org/src/2507.16341v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Face reenactment is the process of synthesizing a lifelike talking head video using a provided source image as a reference, guided by a driving video. In this synthesis, the resulting face maintains the identity attributes of the source image while adopting the pose and expressions from the driving video. Face reenactment has many valuable applications, including character role-playing, digital avatars, online education, video conferencing, etc.\n\nExisting face reenactment methods have demonstrated remarkable capabilities in generating talking faces. Keypoints are typically employed by these methods to represent facial motion. According to whether the keypoints are learned automatically (implicit) or predefined (explicit), these methods can be categorized into two types: implicit keypoint-based methods and explicit keypoint-based methods. Among them, implicit keypoint-based methods learn keypoints from the source and driving images to estimate dense motion fields (e.g., optical flow). These fields are then used to warp the source image toward the pose and expression of the driving image. Finally, a generator inpaints occluded or degraded regions to produce the final frame. By providing dense and flexible motion guidance, these methods effectively capture fine-grained facial deformations. However, when there is a large pose discrepancy between the source and driving images, the limited identity and appearance information in a single source image often fails to support effective inpainting in severely warped regions, leading to degraded synthesis quality. In contrast, explicit keypoint-based methods rely on facial landmarks extracted from the driving video to generate pose maps as spatial conditions for the pre-trained Stable Diffusion model . To maintain identity and appearance consistency with the source image, they further incorporate fine-grained texture and appearance features from the source using ReferenceNet . Although these approaches can produce high-quality facial textures, they are fundamentally limited by the coarse nature of explicit facial landmarks , which primarily capture rigid facial contours. When handling large head poses (e.g., profile views), these rigid contours often result in overlapping or collapsed keypoints, causing the generated pose maps to lose meaningful facial structure and identity cues. Therefore, handling large pose variations remains a significant challenge for existing face reenactment methods.\n\nTo achieve high-fidelity face reenactment under large pose variations, we leverage implicit facial keypoints to represent facial motion and use them to warp the source image toward the target pose and expression. To address warping-induced degradation, we observe that image-to-video (I2V) models trained on large-scale video datasets are highly effective at synthesizing realistic and temporally coherent facial dynamics—such as head movements, speech, and blinking—while reliably preserving identity and appearance consistency, even under extreme pose variations. Therefore, our key insight is to exploit the I2V model’s motion-aware latent feature space to reconstruct regions degraded by warping, enabling temporally coherent video generation that faithfully preserves source identity while recovering fine-grained details lost during the warping process.\n\nIn this paper, we propose a Face Reenactment Video Diffusion model (FRVD) for high-fidelity face reenactment under large pose variations. Our model first employs a Motion Extractor to extract implicit keypoints from both the source and driving images, which serve as fine-grained representations of facial motion. These keypoints are then used in a warping module to align the motion of the source image with that of the driving image. To recover regions degraded during the warping process, we introduce a Warping Feature Mapper (WFM) that maps features from the warped source image into the motion-aware latent space of a pretrained I2V model. This latent space, learned from large-scale video data, encodes rich spatiotemporal priors of facial dynamics, enabling the model to perform effective warping correction. By leveraging these priors, the WFM facilitates high-quality reconstruction of facial details while preserving both identity and temporal coherence.\n\nOur main contributions are summarized as follows:\n {itemize}\n   We propose a Face Reenactment Video Diffusion model (FRVD) to address the challenge of face reenactment under large pose variations, overcoming the limitation of existing methods, which typically produce satisfactory results only when the pose of the source image closely matches that of the driving image.\n   We introduce the Warping Feature Mapper (WFM), which maps the warped source image into the motion-aware latent space of a pretrained image-to-video (I2V) model. This allows the model to leverage its prior knowledge to reconstruct degraded regions, thereby enabling high-fidelity face reenactment under large pose variations.\n   Extensive experiments demonstrate that FRVD outperforms state-of-the-art methods, achieving significant improvements in pose accuracy, identity preservation, and overall video quality.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "elsarticle-template-cag.tex",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.296,
      "diffusion_reasoning_score": 0.475,
      "distributed_training_score": 0.34,
      "datasets_score": 0.283,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using a diffusion-based model for face reenactment, specifically adapting the iterative refinement process in a pretrained image-to-video (I2V) model to handle visual generation tasks like warping correction and video synthesis. However, it does not involve multi-step logical reasoning, Chain-of-Thought processes, or solving complex logical tasks. The diffusion mechanism is applied to visual and temporal coherence in video generation, not to reasoning, making it only tangentially related through the shared concept of iterative refinement.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668097",
      "updated_at": "2025-08-11T23:43:05.606928",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16342",
      "title": "Mamba-OTR: a Mamba-based Solution for Online Take and Release Detection\n  from Untrimmed Egocentric Video",
      "authors": [
        "Alessandro Sebastiano Catinello",
        "Giovanni Maria Farinella",
        "Antonino Furnari"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This work tackles the problem of Online detection of Take and Release (OTR)\nof an object in untrimmed egocentric videos. This task is challenging due to\nsevere label imbalance, with temporally sparse positive annotations, and the\nneed for precise temporal predictions. Furthermore, methods need to be\ncomputationally efficient in order to be deployed in real-world online\nsettings. To address these challenges, we propose Mamba-OTR, a model based on\nthe Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence\nduring inference while being trained on short video clips. To address label\nimbalance, our training pipeline incorporates the focal loss and a novel\nregularization scheme that aligns model predictions with the evaluation metric.\nExtensive experiments on EPIC-KITCHENS-100, the comparisons with\ntransformer-based approach, and the evaluation of different training and test\nschemes demonstrate the superiority of Mamba-OTR in both accuracy and\nefficiency. These finding are particularly evident when evaluating full-length\nvideos or high frame-rate sequences, even when trained on short video snippets\nfor computational convenience. The proposed Mamba-OTR achieves a noteworthy\nmp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in\nstreaming mode, versus the 20.32 of a vanilla transformer and 25.16 of a\nvanilla Mamba, thus providing a strong baseline for OTR. We will publicly\nrelease the source code of Mamba-OTR to support future research.",
      "published_date": "2025-07-22T08:23:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16342v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16342v1",
      "latex_url": "http://arxiv.org/src/2507.16342v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Wearable devices provided with cameras are able to capture visual information from a first-person perspective, enabling the development of personalized, context-aware assistive technologies to support user daily activities~. A key requirement for such systems is the ability to detect fine-grained, atomic actions—such as take and release of an object—which are essential for downstream tasks like intention prediction, object interaction tracking, and anomaly detection during goal-directed human behavior.\n {figure}\n  \n  [width= ]{ODAE.drawio.pdf}\n  {Algorithms are tasked to process the video online and output a single prediction corresponding to the last frame of the take/release action (blue frame), while avoiding predictions for background (red frame) or any other frame (green ones).}\n\n {figure}\nTo address this challenge, action recognition algorithms must operate on streaming video data in an online fashion while maintaining temporal coherence by emitting a single, unambiguous prediction per action instance. We refer to this task as ``Online Take-Release detection'' (OTR). Previous work has explored various strategies, including detecting the starting frame of an action~ and identifying contact frames~. However, recent findings suggest that predicting the ending frame of an action yields superior performance, as it reduces the uncertainty caused by partial observations and premature predictions~, while inducing a small delay in the predictions, which is acceptable for most applications.  {fig:ODAE} illustrates this setup, originally discussed in~.\nDespite the advantages of this formulation, OTR still faces multiple challenges, notably, the extreme class imbalance between positives (last frame of a take/release action) and negatives (any other frame), the need to suppress multiple detections, and the requirement of online and computationally efficient processing in order to support deployment to real-world scenarios.\n\nWe propose a novel approach based on the Mamba architecture~. Our method uses the focal loss during training and incorporates tailored regularization techniques that align the model's behavior with the evaluation metric, encouraging precise and temporally consistent predictions. We evaluated our model on the EPIC-KITCHENS-100 dataset, focusing exclusively on the take and release verbs, and conduct extensive experiments across various architectural configurations.\nThe proposed Mamba-OTR has been compared with respect to different approaches based on Tranformers and Mamba-based models, showing the advantages of each introduced optimization.\nOur results demonstrate that, although the task remains challenging, it becomes significantly more tractable when approached with an appropriate training strategy. Furthermore, we show that Mamba’s inherent recurrence enables efficient training on short clips while allowing inference over full-length videos. This training-inference decoupling leads to substantial improvements in inference speed and enhances predictive performance.\nOur optimized Mamba-OTR model achieves an mp-mAP of $51.76$ and a mean inference time of $0.14s$, versus the mp-mAP of $20.32$ and the inference time of $0.28s$ of a standard Transformer module.\n\nIn sum, our main contributions are as follows: 1) A training pipeline incorporating regularization strategies to address severe annotation imbalance of OTR; 2) Comprehensive benchmarking against existing Transformer- and Mamba-based methods; 3) The Mamba-OTR model, which is a strong baseline to support research in this area. We plan to release our code.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.33,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.367,
      "datasets_score": 0.297,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668105",
      "updated_at": "2025-08-11T23:43:05.606930",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16343",
      "title": "Detect Any Sound: Open-Vocabulary Sound Event Detection with Multi-Modal\n  Queries",
      "authors": [
        "Pengfei Cai",
        "Yan Song",
        "Qing Gu",
        "Nan Jiang",
        "Haoyu Song",
        "Ian McLoughlin"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Most existing sound event detection~(SED) algorithms operate under a\nclosed-set assumption, restricting their detection capabilities to predefined\nclasses. While recent efforts have explored language-driven zero-shot SED by\nexploiting audio-language models, their performance is still far from\nsatisfactory due to the lack of fine-grained alignment and cross-modal feature\nfusion. In this work, we propose the Detect Any Sound Model (DASM), a\nquery-based framework for open-vocabulary SED guided by multi-modal queries.\nDASM formulates SED as a frame-level retrieval task, where audio features are\nmatched against query vectors derived from text or audio prompts. To support\nthis formulation, DASM introduces a dual-stream decoder that explicitly\ndecouples event recognition and temporal localization: a cross-modality event\ndecoder performs query-feature fusion and determines the presence of sound\nevents at the clip-level, while a context network models temporal dependencies\nfor frame-level localization. Additionally, an inference-time attention masking\nstrategy is proposed to leverage semantic relations between base and novel\nclasses, substantially enhancing generalization to novel classes. Experiments\non the AudioSet Strong dataset demonstrate that DASM effectively balances\nlocalization accuracy with generalization to novel classes, outperforming\nCLAP-based methods in open-vocabulary setting (+ 7.8 PSDS) and the baseline in\nthe closed-set setting (+ 6.9 PSDS). Furthermore, in cross-dataset zero-shot\nevaluation on DESED, DASM achieves a PSDS1 score of 42.2, even exceeding the\nsupervised CRNN baseline. The project page is available at\nhttps://cai525.github.io/Transformer4SED/demo_page/DASM/.",
      "published_date": "2025-07-22T08:24:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16343v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16343v1",
      "latex_url": "http://arxiv.org/src/2507.16343v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Sound event detection (SED) aims to recognize what is happening in an audio signal and when it is happening~.\nAs a fundamental task in computer audition, SED has been widely applied to various domains, such as security surveillance~ and autonomous driving~.\nRecently, SED has also gained attention in the development of audio and multimodal large language models~(LLM), as it enables LLMs to perceive environmental audio information~.\n\nExisting SED methods~ are limited to detecting a predefined set of sound classes, failing to identify novel events unseen during training.\nHowever, real-world soundscapes consist of thousands of sound classes, whereas SED datasets annotate only a small fraction of these classes.\nIn recent years, the audio community has made significant efforts in developing large-scale datasets~ that encompass hundreds of sound classes.\nDespite their scale, these datasets often suffer from a severe long-tail distribution, where rare classes contain only a few minutes of annotated audio, leading to suboptimal performance due to data scarcity.\nFurthermore, even though these datasets encompass a wide range of sound events, they still cannot guarantee full coverage of all potential events required in applications.\nConsequently, models pre-trained on large-scale datasets typically require fine-tuning on target datasets when applied to new scenarios~.\nConstructing scenario-specific datasets and the associated fine-tuning process are both costly and time-consuming, hindering the practicality of SED for real-world applications.\n\nTo overcome these limitations, the concept of open-vocabulary learning has been introduced.\nIn contrast to traditional closed-set methods, open-vocabulary learning enables models to recognize novel classes beyond the annotated label space.\nThe core idea is to align audio and language representations through pre-trained audio-language models~, allowing models to generalize to novel classes via natural language descriptions.\nRecently, Contrastive audio-language Pretraining (CLAP)~ has demonstrated strong open-vocabulary recognition capabilities in the audio classification task by pretraining on large-scale audio-text datasets.\nSome studies have attempted to apply CLAP directly to the SED task~, but their performance to date is significantly lower than that of closed-set SED models.\nThis gap can be attributed to two primary factors.\nFirst, CLAP is trained using clip-level contrastive learning, aligning text and audio at the clip-level without frame-level supervision, which limits its ability to accurately localize sound events.\nSecond, the original CLAP framework aligns text and audio only during loss computation, lacking a cross-modal fusion structure to refine representations across modalities.\n\nIn this paper, we introduce Detect Any Sound Model (DASM), a query-based framework for open-vocabulary SED.\nDASM is trained on large-scale SED datasets and combines the precise event localization of closed-set models with the ability to generalize to novel categories.\n\nTo achieve these objectives, we formulate open-vocabulary SED as a frame-level retrieval task, where sound events are detected by matching query vectors with frame-wise audio features.\nSpecifically, an audio encoder extracts frame-level feature sequences from input audios, while a CLAP based query generation module generates query vectors from event queries.\nMultimodal queries are supported by DASM, allowing queries to be either natural language descriptions (e.g., ``Sound of cats'') or audio clips containing the target event.\n\nSubsequently, a dedicated decoder architecture is design to match query vectors with audio features.\nThe decoder in DASM adopts a dual-stream structure to explicitly decouple event recognition and localization: a cross-modality event decoder performs clip-level event recognition via query-feature fusion, while a context network models temporal dependencies for frame-level localization.\n\nFurthermore, we propose an inference-time attention masking strategy to leverage semantic relations between base and novel classes, substantially aiding generalization to novel events.\n\nTo comprehensively evaluate the proposed approach, we construct a benchmark consisting of three subtasks: open-vocabulary detection, closed-set detection, and cross-dataset detection.\nIn open-vocabulary experiments on the AudioSet-Strong dataset, DASM achieves a PSDS score of 33.9 on novel classes, surpassing the best CLAP-based approach by 7.8, highlighting the superior capability of our model in open-vocabulary scenarios.\nUnder the closed-set setting, DASM attains a PSDS score of 50.9, outperforming closed-set baselines by 6.9 points.\nFurthermore, in cross-dataset evaluation on DESED, DASM achieves a zero-shot PSDS1 score of 42.2, exceeding the supervised CRNN baseline, demonstrating strong generalization across datasets.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sample-sigconf-authordraft.tex",
      "rlhf_score": 0.295,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.388,
      "distributed_training_score": 0.334,
      "datasets_score": 0.412,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of the Detect Any Sound Model (DASM) for open-vocabulary sound event detection, rather than a primary focus on datasets. However, it is moderately relevant because the authors use existing datasets like AudioSet Strong and DESED for experiments, construct a benchmark with subtasks for evaluation, and discuss issues with datasets such as long-tail distributions and the need for benchmarking. This involves benchmarking and evaluating datasets to demonstrate model performance, aligning with the topic, but it is not the core focus.",
      "summary": "The paper introduces the Detect Any Sound Model (DASM), a novel framework for open-vocabulary sound event detection that uses multi-modal queries such as text or audio prompts to identify and localize sound events beyond predefined classes. By formulating SED as a frame-level retrieval task with a dual-stream decoder for event recognition and temporal localization, along with an inference-time attention masking strategy to enhance generalization, DASM achieves superior performance on the AudioSet dataset, outperforming CLAP-based methods in open-vocabulary settings and baselines in closed-set scenarios, while also excelling in cross-dataset zero-shot evaluations on DESED.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new query-based framework with a dual-stream decoder and attention masking strategy, significantly advancing open-vocabulary sound event detection by improving fine-grained alignment and generalization beyond existing methods.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of audio processing and AI, as it enhances the practicality of SED for real-world applications, though its influence may be limited to specific domains like sound event detection.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with strong experimental results and innovative techniques in open-vocabulary SED, making it valuable for researchers in audio and AI fields to be aware of and potentially build upon.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/c0b6573ad506bb5a9b93685c53eae3cf394c4b4a",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 7,
      "average_h_index": 2.5,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Pengfei Cai",
          "profile_url": "https://www.semanticscholar.org/author/2316327043",
          "h_index": 2
        },
        {
          "name": "Yan Song",
          "profile_url": "https://www.semanticscholar.org/author/2319676919",
          "h_index": 1
        },
        {
          "name": "Qing Gu",
          "profile_url": "https://www.semanticscholar.org/author/2319380650",
          "h_index": 2
        },
        {
          "name": "Nan Jiang",
          "profile_url": "https://www.semanticscholar.org/author/2319334660",
          "h_index": 2
        },
        {
          "name": "Hao-Yu Song",
          "profile_url": "https://www.semanticscholar.org/author/2292675367",
          "h_index": 1
        },
        {
          "name": "Ian McLoughlin",
          "profile_url": "https://www.semanticscholar.org/author/2150006877",
          "h_index": 7
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669315",
      "updated_at": "2025-08-11T23:46:01.416477",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16347",
      "title": "Leveraging Personalized PageRank and Higher-Order Topological Structures\n  for Heterophily Mitigation in Graph Neural Networks",
      "authors": [
        "Yumeng Wang",
        "Zengyi Wo",
        "Wenjun Wang",
        "Xingcheng Fu",
        "Minglai Shao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Graph Neural Networks (GNNs) excel in node classification tasks but often\nassume homophily, where connected nodes share similar labels. This assumption\ndoes not hold in many real-world heterophilic graphs. Existing models for\nheterophilic graphs primarily rely on pairwise relationships, overlooking\nmulti-scale information from higher-order structures. This leads to suboptimal\nperformance, particularly under noise from conflicting class information across\nnodes. To address these challenges, we propose HPGNN, a novel model integrating\nHigher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces\nan efficient high-order approximation of Personalized PageRank (PPR) to capture\nlong-range and multi-scale node interactions. This approach reduces\ncomputational complexity and mitigates noise from surrounding information. By\nembedding higher-order structural information into convolutional networks,\nHPGNN effectively models key interactions across diverse graph dimensions.\nExtensive experiments on benchmark datasets demonstrate HPGNN's effectiveness.\nThe model achieves better performance than five out of seven state-of-the-art\nmethods on heterophilic graphs in downstream tasks while maintaining\ncompetitive performance on homophilic graphs. HPGNN's ability to balance\nmulti-scale information and robustness to noise makes it a versatile solution\nfor real-world graph learning challenges. Codes are available at\nhttps://github.com/streetcorner/HPGNN.",
      "published_date": "2025-07-22T08:28:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16347v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16347v1",
      "latex_url": "http://arxiv.org/src/2507.16347v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Graph Representation Learning (GRL) tackles the challenge of capturing complex relational information in non-Euclidean structured data, enabling advanced machine learning applications on graphs~. Graph Neural Networks (GNNs), a cornerstone of GRL, were originally developed under the homophily assumption, where connected nodes share similar properties. Early GNN models focused on homophilic graphs~, yet many real-world networks exhibit heterophily, with connections spanning dissimilar nodes. The performance of GNNs often depends on the graph's homophily level~, driving recent efforts to address heterophily~.\n\nDespite these advances, most prior work centers on pairwise node relationships~, which fail to capture the multi-scale, complex interactions prevalent in real-world scenarios while higher-order structures better reflect real-world complex systems~. To overcome this limitation, higher-order network methods, such as hypergraphs and simplicial complexes (SCs), have emerged. Hypergraphs generalize graphs by allowing edges to connect multiple nodes but often neglect intra-hyperedge relationships, limiting their robustness to noise compared to SCs~. Grounded in algebraic topology~, SCs model multidimensional relationships, offering richer node context~ and proving effective in domains like propagation~, sensor networks~, and social systems~.\n\nA central challenge in higher-order neural networks lies in constructing node relationships using SCs. Recent approaches leverage the Hodge Laplacian from SC theory~, but its computation can reach \\(  {O}(n^3) \\) complexity in dense graphs, posing scalability issues. Meanwhile, Personalized PageRank (PPR), a random walk variant with teleportation~, efficiently captures local node information. Recent studies integrate PPR with GNNs to enhance representations while preserving efficiency~, though they often overlook multi-level, higher-order interactions.\n\nIn this paper, we propose HPGNN, a novel framework that combines higher-order topological structures with Personalized PageRank to advance graph representation learning, particularly for heterophilic graphs. Unlike methods limited to node-to-node interactions, HPGNN incorporates relationships across multiple simplicial complexes. Our approach comprises two key components: 1) Higher-order Personalized PageRank (HiPPR), which efficiently computes long-range interactions while reducing noise, and 2) Higher-order Adaptive Spectral Convolution (HiASC), which extends spectral convolution to capture higher-dimensional relationships via SCs.\nOur contributions are threefold:\n {itemize}\n   HPGNN pioneers the integration of higher-order information into PPR approximation, bridging simplicial complex theory with personalized random walks.\n   We introduce an adaptive PPR matrix operator that encodes interactions among higher-order graph structures.\n   Extensive experiments on real-world datasets demonstrate that HPGNN achieves better performance than five out of seven state-of-the-art models.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "ijcai25.tex",
      "rlhf_score": 0.399,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.375,
      "datasets_score": 0.303,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668618",
      "updated_at": "2025-08-11T23:43:05.607028",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16356",
      "title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for\n  Improved Message Delivery in Mobile Maternal Health",
      "authors": [
        "Arpan Dasgupta",
        "Mizhaan Maniyar",
        "Awadhesh Srivastava",
        "Sanat Kumar",
        "Amrita Mahale",
        "Aparna Hedge",
        "Arun Suggala",
        "Karthikeyan Shanmugam",
        "Aparna Taneja",
        "Milind Tambe"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Mobile health (mHealth) programs utilize automated voice messages to deliver\nhealth information, particularly targeting underserved communities,\ndemonstrating the effectiveness of using mobile technology to disseminate\ncrucial health information to these populations, improving health outcomes\nthrough increased awareness and behavioral change. India's Kilkari program\ndelivers vital maternal health information via weekly voice calls to millions\nof mothers. However, the current random call scheduling often results in missed\ncalls and reduced message delivery. This study presents a field trial of a\ncollaborative bandit algorithm designed to optimize call timing by learning\nindividual mothers' preferred call times. We deployed the algorithm with around\n$6500$ Kilkari participants as a pilot study, comparing its performance to the\nbaseline random calling approach. Our results demonstrate a statistically\nsignificant improvement in call pick-up rates with the bandit algorithm,\nindicating its potential to enhance message delivery and impact millions of\nmothers across India. This research highlights the efficacy of personalized\nscheduling in mobile health interventions and underscores the potential of\nmachine learning to improve maternal health outreach at scale.",
      "published_date": "2025-07-22T08:42:17+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16356v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16356v1",
      "latex_url": "http://arxiv.org/src/2507.16356v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Maternal health remains a critical public health concern in India and various other developing countries globally (, , , , ), with millions of women having limited access to timely and accurate information during pregnancy and postpartum. Mobile health (mHealth) programs (, , ) that use automated voice messages, have the ability to deliver such critical maternal and child health information. Recognizing the need for information and the promise of mHealth programs, the Government of India launched the Kilkari program, a nationwide mobile health initiative that delivers weekly voice messages that contain essential maternal health information to more than 10 million registered mothers~ { {https://rchrpt.mohfw.gov.in/RCHRPT/Kilkari/Kilkari_Message.aspx}}. mHealth programs such as these play a vital role in reducing maternal mortality rates - a key target within the WHO's Sustainable Development Goals . These messages cover vital topics such as iron and calcium supplementation, antenatal care, and postnatal practices, aiming to improve maternal health outcomes throughout the country.\n\nHowever, the effectiveness of this large-scale program is contingent upon successful message delivery. Currently, Kilkari employs a random call scheduling strategy, attempting to reach mothers, with up to nine re-attempts (until the call is picked up), but without considering individual preferences for call timing. This approach often results in missed calls, crucial bandwidth spent on re-attempts and most importantly limiting the reach and impact of crucial health information . To address this challenge, recently, a stochastic bandit approach was proposed to learn appropriate timing for calls to mothers. Whereas learning individually the appropriate timing to call each mother is expensive, a collaborative bandit approach attempts to harness similarity among the mothers to jointly learn their preferences for call timings . Whereas this approach has shown promise in simulations, its performance in real-world field trials remains unknown. To address this limitation, this paper presents a field trial of the collaborative bandit algorithm designed to optimize call scheduling by learning mothers' preferred call times.\n\nGeneralizable insights\nCollaborative bandit algorithms offer a promising approach for personalized intervention delivery in mobile health. By iteratively learning from user responses and interactions, these algorithms can adapt to individual preferences and maximize engagement. In this study, we implemented a collaborative bandit algorithm within the Kilkari platform and conducted a field trial involving approximately $6500$ beneficiaries. Our goal was to evaluate the algorithm's ability to improve call pick-up rates compared to the baseline random calling strategy.\n\nThis research contributes to the growing body of literature in the application of machine learning in mobile health interventions . By demonstrating the effectiveness of a collaborative bandit algorithm in a real-world setting, we highlight the potential for personalized call scheduling to enhance the reach and impact of maternal health programs at scale. Given the national scope of Kilkari and the potential for improved message delivery to millions of beneficiaries, our findings have significant implications for public health policy and practice in India and beyond.\n\nKey note on the experiments reported in this paper This work was conducted as a joint effort between a research team from a non-profit in India called ARMMAN () and Google Deepmind India a non-profit organization in India as reflected in the co-authorship of this paper.\nIt is crucial to highlight that the beneficiary data utilized in this research is fully anonymized, and no socio-demographic features were available to the research team. To ensure data privacy and security, the experimental infrastructure was managed exclusively by the ARMMAN team, who were the only individuals with access to the raw beneficiary data.\nThe Google Deepmind researchers contributed by advising the ARMMAN team on the collaborative bandit algorithm, specifically the algorithm’s implementation and subsequently collaborating on the analysis of the resulting study. The ARMMAN team has followed general guidelines related to ethics approvals laid down by Indian Council for Medical Research (ICMR).",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sample.tex",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.257,
      "distributed_training_score": 0.315,
      "datasets_score": 0.283,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667810",
      "updated_at": "2025-08-11T23:43:05.606861",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16360",
      "title": "A High Magnifications Histopathology Image Dataset for Oral Squamous\n  Cell Carcinoma Diagnosis and Prognosis",
      "authors": [
        "Jinquan Guan",
        "Junhong Guo",
        "Qi Chen",
        "Jian Chen",
        "Yongkang Cai",
        "Yilin He",
        "Zhiquan Huang",
        "Yan Wang",
        "Yutong Xie"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Oral Squamous Cell Carcinoma (OSCC) is a prevalent and aggressive malignancy\nwhere deep learning-based computer-aided diagnosis and prognosis can enhance\nclinical assessments.However, existing publicly available OSCC datasets often\nsuffer from limited patient cohorts and a restricted focus on either diagnostic\nor prognostic tasks, limiting the development of comprehensive and\ngeneralizable models. To bridge this gap, we introduce Multi-OSCC, a new\nhistopathology image dataset comprising 1,325 OSCC patients, integrating both\ndiagnostic and prognostic information to expand existing public resources. Each\npatient is represented by six high resolution histopathology images captured at\nx200, x400, and x1000 magnifications-two per magnification-covering both the\ncore and edge tumor regions.The Multi-OSCC dataset is richly annotated for six\ncritical clinical tasks: recurrence prediction (REC), lymph node metastasis\n(LNM), tumor differentiation (TD), tumor invasion (TI), cancer embolus (CE),\nand perineural invasion (PI). To benchmark this dataset, we systematically\nevaluate the impact of different visual encoders, multi-image fusion\ntechniques, stain normalization, and multi-task learning frameworks. Our\nanalysis yields several key insights: (1) The top-performing models achieve\nexcellent results, with an Area Under the Curve (AUC) of 94.72% for REC and\n81.23% for TD, while all tasks surpass 70% AUC; (2) Stain normalization\nbenefits diagnostic tasks but negatively affects recurrence prediction; (3)\nMulti-task learning incurs a 3.34% average AUC degradation compared to\nsingle-task models in our multi-task benchmark, underscoring the challenge of\nbalancing multiple tasks in our dataset. To accelerate future research, we\npublicly release the Multi-OSCC dataset and baseline models at\nhttps://github.com/guanjinquan/OSCC-PathologyImageDataset.",
      "published_date": "2025-07-22T08:48:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16360v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16360v1",
      "latex_url": "http://arxiv.org/src/2507.16360v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Oral Squamous Cell Carcinoma~(OSCC) is a common malignant head and neck tumour. According to global cancer statistics, more than 380,000 patients with oral cancer were diagnosed in 2022, of which approximately 180,000 died  {bray2024global}. Accurate diagnosis, effective treatment, and a well-informed prognosis plan are essential to reduce mortality rates. Histopathology checking is a gold standard for identifying OSCC and its status. To achieve this purpose, it is often necessary for clinicians to carry out a histopathology biopsy of the lesion site of the patient, and the biopsy tissues are processed via staining and microtomy to generate histopathology slides. The pathologist confirms the diagnosis through examination and analysis of the histopathology slides, after which clinicians make a more accurate prognosis assessment.\n\nArtificial intelligence~(AI) systems have demonstrated significant potential for rapid and accurate analysis of pathology images~ {mckinney2020international}. In the context of OSCC, AI automated analysis of histopathology images promises to streamline the diagnostic process, enabling precise and efficient identification and classification of cancerous tissues, and ultimately improving patient prognosis~ {warin2024deep}. Existing OSCC datasets are shown in Table~, which include: the TCGA-HNSC database~ {zuley2016cancer} compiles clinical information, radiological data, genomic data, and histopathology images from 528 patients, most of whom have oral cancer aiming for prognosis purpose.  {rahman2020histopathological} published a dataset of optical microscopy images for diagnosing normal and OSCC images. The ORCHID dataset~ {chaudhary2024high} includes microscopy images of OSCC and oral submucous fibrosis~(OSMF), supporting cell classification studies and providing tumor differentiation~(TD) labels for OSCC images. However, existing OSCC datasets often have limited patient cohort sizes and focus on specific aspects of diagnosis or prognosis. These limitations constrain the range of clinical problems that their developed AI systems can address, while also hindering the development of more generalized and robust models.\n\n {./Tables/table_dataset_comparison.tex}\n\nTo advance research in histopathological image analysis, we introduce Multi-OSCC, a novel dataset of Oral Squamous Cell Carcinoma (OSCC) images featuring multiple targets.\nFollowing the data collection methodology of and , we capture these histopathology images using a microscope at various high magnifications.\nThis dataset encompasses six tasks related to the diagnosis and prognosis of OSCC, incorporating a larger patient cohort, with detailed descriptions provided in Table~. The tasks in our dataset are based on three clinically relevant scenarios designed to assist clinicians in diagnostic and prognostic analysis:\n {itemize}\n   [1.] REC:\n This task aims to assist clinicians in identifying the risk of tumor recurrence. Based on the recurrence risk predicted by our model, the clinician can formulate an appropriate prognosis plan for patients who have undergone surgical resection.\n\n   [2.] LNM:\n This task helps clinicians decide whether further surgical procedures, such as cervical lymph node dissection, are necessary. Using histopathological images obtained through incisional biopsy, our model predicts the probability of lymph node metastasis, reducing unnecessary lymphadenectomy while ensuring high-risk areas are not overlooked.\n\n   [3.] TD, TI, CE, PI:\n These tasks assist clinicians in assessing the severity of the tumor. Since tumor staging (T stage) involves lesion size, which can be measured manually, we focus on more granular diagnostic classifications. The excised lesions from surgery are sent to pathologists for examination, and our model helps them diagnose tumor status and make comprehensive pathological assessments.\n {itemize}\n\n {./Tables/table_task_descriptions.tex}\n\nCompared to prior datasets limited to a single task, our dataset enables joint modeling of diagnosis and prognosis, aligning with clinical workflows. It features multi-task labels and histopathology images from multiple tissue slices per patient, offering a comprehensive resource for multi-target analysis. To the best of our knowledge, this is the first publicly available histopathology image dataset specifically designed for OSCC research, with multiple diagnostic and prognostic targets.\n\nWe conduct extensive experiments to evaluate various aspects of our dataset, including comparing vision backbones trained with ImageNet versus histopathology-specific pre-trained weights, examining multi-slice feature fusion strategies, assessing the impact of stain normalization, and exploring multi-task learning in histopathology analysis. The findings of our analysis reveal:\n(1) Models pre-trained on histopathology-specific datasets consistently outperform their ImageNet-pretrained counterparts, evaluated across an average of six tasks. Specifically, the top-performing model achieves an Area Under the Curve (AUC) of 94.72% on the REC task and 81.23% on the TD task, with all other tasks surpassing an AUC of 70%.\n(2) Stain normalization leads to a significant decrease in AUC for the prognosis task (REC), while it notably improves AUC for five diagnostic tasks, suggesting REC's reliance on original color properties.\n(3) Within the multi-task learning framework, GradNorm~ {chen2018gradnorm} achieves the highest average AUC, with stain normalization providing an additional performance boost. Nevertheless, the multi-task models underperform their single-task counterparts by an average of 3.34% across the six tasks. This gap underscores the challenge of effectively balancing competing objectives in comprehensive computer-aided diagnosis (CAD) systems.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.308,
      "distributed_training_score": 0.353,
      "datasets_score": 0.423,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of a new dataset, Multi-OSCC, for OSCC histopathology image analysis, which directly aligns with research on creating, analyzing, benchmarking, and evaluating datasets for machine learning and AI applications. It describes dataset curation methodologies, including patient cohort selection, image capture at various magnifications, and annotation for multiple clinical tasks. The paper also provides benchmark evaluations through experiments on visual encoders, fusion techniques, stain normalization, and multi-task learning, offering insights into dataset performance and analysis, making it a core example of this topic.",
      "summary": "This paper introduces the Multi-OSCC dataset, a comprehensive collection of high-resolution histopathology images from 1,325 patients with Oral Squamous Cell Carcinoma (OSCC), captured at x200, x400, and x1000 magnifications and annotated for six key tasks including recurrence prediction, lymph node metastasis, and tumor invasion. The authors evaluate various models and techniques, such as different visual encoders, multi-image fusion, stain normalization, and multi-task learning frameworks, revealing that histopathology-specific pre-trained models perform best with AUC scores up to 94.72% for recurrence prediction, stain normalization improves diagnostic tasks but hinders prognosis, and multi-task learning slightly underperforms single-task models by an average of 3.34% AUC.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a large-scale OSCC dataset with multiple diagnostic and prognostic tasks, expanding on existing resources in a clever way, though it does not introduce entirely new problems or techniques.",
      "impact_score": "High",
      "impact_justification": "The work could significantly influence future research and commercial applications in AI-driven histopathology for OSCC by providing a publicly available dataset that enables the development of more generalizable models for diagnosis and prognosis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution through its new dataset and benchmarking insights, making it essential for researchers in medical AI and computer vision focused on cancer diagnostics.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b4dadce89d9e09f98f99a8ee9ac9a3e684210c2b",
      "h_index_fetch_method": "full_id",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 12,
      "average_h_index": 3.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jinquan Guan",
          "profile_url": "https://www.semanticscholar.org/author/2364691203",
          "h_index": 0
        },
        {
          "name": "Junhong Guo",
          "profile_url": "https://www.semanticscholar.org/author/2364548854",
          "h_index": 1
        },
        {
          "name": "Qi Chen",
          "profile_url": "https://www.semanticscholar.org/author/2372450412",
          "h_index": 0
        },
        {
          "name": "Jian Chen",
          "profile_url": "https://www.semanticscholar.org/author/2253982640",
          "h_index": 1
        },
        {
          "name": "Yongkang Cai",
          "profile_url": "https://www.semanticscholar.org/author/2216413170",
          "h_index": 2
        },
        {
          "name": "Yilin He",
          "profile_url": "https://www.semanticscholar.org/author/2279561164",
          "h_index": 1
        },
        {
          "name": "Zhiquan Huang",
          "profile_url": "https://www.semanticscholar.org/author/2144108499",
          "h_index": 12
        },
        {
          "name": "Yan Wang",
          "profile_url": "https://www.semanticscholar.org/author/2152545822",
          "h_index": 7
        },
        {
          "name": "Yutong Xie",
          "profile_url": "https://www.semanticscholar.org/author/2256700361",
          "h_index": 3
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669048",
      "updated_at": "2025-08-11T23:45:47.817951",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16362",
      "title": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification\n  and Recognition Network",
      "authors": [
        "Guangzhu Xu",
        "Pengcheng Zuo",
        "Zhi Ke",
        "Bangjun Lei"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.",
      "published_date": "2025-07-22T08:54:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16362v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16362v2",
      "latex_url": "http://arxiv.org/src/2507.16362v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In recent years, with the continued growth in vehicle ownership, Automatic License Plate Recognition (ALPR) systems have found widespread application in traffic management, parking management, security surveillance, intelligent transportation, and law enforcement. In constrained environments characterized by dense vehicle populations, such as parking lots and toll booths, integrated barrier systems and stable light sources are commonly deployed to ensure high-quality acquisition of license plate images, thereby enabling efficient and accurate recognition. However, in unconstrained environments with variable lighting conditions, diverse shooting angles, and complex weather conditions, license plate image quality is often poor, exhibiting issues such as blurring, occlusion, and skewing, which significantly reduce the accuracy of license plate localization and character recognition. These image degradation phenomena increase the difficulty of license plate recognition in unconstrained environments, posing a significant challenge for current research.\n\nLicense plate recognition systems typically comprise three key components: license plate detection, rectification, and character recognition. While notable advancements have been achieved in license plate detection and character recognition algorithms, research on license plate rectification remains relatively limited. Presently, mainstream rectification methods can be categorized into two types: the first type is based on the localization of the four vertices of the license plate, employing perspective transformation for image correction . This approach offers advantages such as high computational efficiency and straightforward implementation; however, its performance heavily depends on the accuracy of vertex localization. Errors in vertex positioning can lead to rectification distortions, subsequently degrading character recognition performance, and this method also demands high-precision annotation of training data. The second category involves estimating spatial transformation parameters for correction, exemplified by the Spatial Transformation Network (STN), which is often integrated with license plate recognition models and trained in an end-to-end manner to enhance rectification quality. Nevertheless, STN exhibits limitations when handling license plates with significant perspective distortion, primarily because it is better suited for affine transformations and has limited adaptability to nonlinear deformations. Besides STN, deformable convolutional networks (DCN)have also been employed for license plate rectification. Although DCN introduces learnable offsets that allow convolution kernels to adaptively adjust their sampling locations—potentially improving deformation handling—experimental results indicate that, for license plates viewed nearly frontally, the offset adjustments introduced by DCN may induce unnecessary deformations, thereby counteracting recognition accuracy.\n\nCurrent state-of-the-art license plate recognition algorithms predominantly favor segmentation-free character recognition approaches. These methods take the entire license plate image as input, leveraging convolutional neural networks (CNNs) or convolutional recurrent neural networks (CRNNs) for end-to-end feature learning and recognition. While these methods offer advantages in terms of accuracy, they typically rely on high-performance GPU hardware to achieve real-time processing, thereby limiting their deployment in practical application scenarios. Consequently, low-computational-cost, lightweight license plate recognition models are urgently needed for real-world applications, particularly within intelligent transportation systems and edge computing devices, where such models are better suited to meet requirements for high efficiency and low power consumption.\n\nCompared to recognition methods based on CRNNs, lightweight license plate recognition models centered on CNNs support highly parallelized computation, substantially accelerating training speed. LPRNet is one of the few purely end-to-end license plate recognition models that combines CNN and Connectionist Temporal Classification (CTC) techniques. It has a relatively small number of parameters (only about 467K), which grants it good adaptability for edge devices. However, research by Zou et al. indicates that LPRNet's accuracy on the deformed subset of the CCPD single-line license plate dataset—specifically regarding rotations and tilts—still has room for improvement compared to more complex models. This limitation primarily arises from LPRNet's insufficient utilization of spatial positional information of characters. In practical recognition scenarios, the lack of character spatial context can lead to feature confusion and character adhesion, thereby reducing recognition accuracy.\n\nIn unrestricted environments, Chinese license plate recognition faces the challenge of handling both single-line and double-line plates, which are commonly encountered. To achieve license plate rectification informed by recognition results through the linkage of rectification and recognition networks, the recognition model needs to process both single and double-line plates simultaneously. Currently, robust solutions addressing this requirement remain underdeveloped. Furthermore, the scarcity of high-quality double-line Chinese license plate datasets, particularly in unconstrained environments, significantly hinders the performance improvement of models across all stages of license plate recognition. While publicly available datasets like CCPDv1 and CCPDv2 offer diversity in shooting environments and angles, they primarily feature single-line Anhui province license plates, lacking the necessary variety to effectively train models for unconstrained scenarios.\n\nTo address the aforementioned challenges, this paper proposes a lightweight integrated rectification and recognition network for both single-line and double-line Chinese license plates. To overcome the issue that double-line plates cannot be directly recognized—leading to difficulties in providing effective supervision signals for the recognition network—we extend the rectification network by supervising it with double-line plate images recognized by the single-line recognition network after correction. Additionally, in response to the scarcity of double-line license plate datasets, this paper constructs a dedicated double-line license plate dataset. Regarding model performance, to improve upon the limitations of LPRNet, we introduce a lightweight per-channel attention (LP-CA) module and adopt Focal CTC, aiming to enhance recognition accuracy while maintaining its real-time processing speed.\n\nThe main contributions of this paper are as follows:\n\nA lightweight Perspective Transformation Rectification (PTR) module is introduced for automatic rectification of single-line license plate images. This module innovatively combines license plate vertex coordinate estimation with inverse perspective transformation, thereby eliminating the need for direct regression of perspective transformation matrix parameters, a common practice in traditional methods. This design effectively mitigates the inherent instability challenges encountered by conventional Spatial Transformer Networks (STNs) in regressing perspective matrix parameters. Furthermore, by optimizing PTR jointly with a license plate recognition network, self-adaptive license plate rectification is achieved without requiring explicit annotations, significantly enhancing the system's practicality and adaptability.\n\nTo overcome the performance limitations of the existing lightweight License Plate Recognition Network (LPRNet), an enhanced LPRNet architecture is proposed. Specifically, a lightweight per-channel attention (LP-CA) module is integrated into LPRNet to accentuate the high-level features of license plate characters, thereby reducing character misidentification. Additionally, the traditional CTC loss is replaced with Focal CTC loss, which effectively addresses the challenge of imbalanced Chinese character distribution within the dataset, leading to a significant improvement in LPRNet's recognition performance.\n\nExtending the proposed single-line PTR module, the synchronized spatial rectification of double-line license plates is achieved. This is accomplished by concatenating their upper and lower character regions to effectively transform them into a single-line format, enabling unified distortion rectification for both single-line and double-line license plates while preserving lightweight characteristics. Building upon these advancements, an end-to-end License Plate Transformation and Recognition - Adaptive Feature Learning Network (LPTR-AFLNet) is further constructed. This novel framework integrates the PTR module and the improved LPRNet for collaborative optimization, thereby achieving unified rectification and recognition of both single-line and double-line Chinese license plates within a single, coherent system.\n\nThe remainder of this paper is structured as follows: Section reviews existing license plate rectification and recognition algorithms, summarizing their respective advantages and disadvantages. Building upon this, Section details the innovative algorithm proposed herein. Section then presents the experimental results, demonstrating the effectiveness of this algorithm through comparison with established methods. The paper concludes in Section , summarizing key findings and discussing potential avenues for future research.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "cas-sc-template.tex",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.357,
      "datasets_score": 0.292,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668113",
      "updated_at": "2025-08-11T23:43:05.606932",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16370",
      "title": "Canonical Representations of Markovian Structural Causal Models: A\n  Framework for Counterfactual Reasoning",
      "authors": [
        "Lucas de Lara"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "math.ST (Statistics Theory)",
        "stat.TH (Statistics Theory)"
      ],
      "abstract": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples.",
      "published_date": "2025-07-22T09:13:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16370v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16370v1",
      "latex_url": "http://arxiv.org/src/2507.16370v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Pearl's causality ladder distinguishes three levels of queries of increasing strength that causal reasoning seeks to answer: (1) observational, (2) interventional, and (3) counterfactual  {pearl2018book}. For illustration, consider a medical context where an analyst aims to understand the link between taking aspirin and recovering from headache. The first level focuses on predictions from observations, from seeing. It allows to answer questions such as  {What is the recovery rate among people taking aspirin?}. The second level addresses predictions from actions, from doing. It enables one to answer questions like  {What percentage of patients would recover if we give them aspirin?}. This corresponds to the result of a randomized controlled trial, where (in contrast to the observational rung) patients do not freely choose whether they take aspirin: an agent blindly assigns the treatment status. The third level tackles predictions from contrary-to-fact events, from imagining. It notably permits one to answer questions of the form  {Had Alice taken aspirin (assuming she did not), would have she recovered?}.\n\nCausal inference refers to the task of answering queries from the second or third levels, using the lower levels along with additional assumptions, typically encoded as mathematical models. The interventional and counterfactual rungs differ fundamentally at two regards in causal inference: the types of conclusions they allow to make and the possible inference methods to reach these conclusions. First, interventional questions address only general causes (does the treatment work?) whereas many counterfactual questions generally deal with singular causes {Singular causation is also often referred to as individual causation or actual causation.} (does the treatment work for Alice?)  [Chapter 7]{pearl2009causality}. The treatment could perfectly work in average in the whole population while degrading Alice's health. Second, while various interventional statements can be tested through fully randomized experiments, most counterfactual statements cannot be empirically verified. Verification of the example statement would require to also observe Alice's outcome in the alternative reality where she took the treatment all other things being kept equal.\n\nThe fact that some counterfactual statements cannot be falsified notably led to qualify them as  {metaphysical} and to advocate restricting causal analysis to the interventional rung. Nevertheless, counterfactuals have always played a crucial role in common language (to express our beliefs on causation) and in scientific modeling (precisely to ask metaphysical questions)  {pearl2000causal}. Furthermore, as reminded by , they serve to define fundamental concepts like harm  {richens2022counterfactual}, credit  {mesnard2021counterfactual}, and fairness  {kusner2017counterfactual}, at the basis of critical applications notably in justice (see also  [Section 4.4]{pearl2016causal}). In particular, by allowing to articulate singular causes, counterfactuals can define notions of algorithmic fairness at the individual level  {kusner2017counterfactual}: the only legally-grounded level in the French law. Therefore, despite their unfalsifiability, designing intelligible models to formally represent counterfactual conceptions remains an essential scientific problem with practical consequences. Addressing this problem is precisely the goal of this article.\n\nCounterfactual assumptions that cannot be tested rest on an arbitrary choice. Supposing that had Alice received aspirin her level of pain would have been given, for instance, by rank preservation between the control and treated groups can be nothing more than choice. This is why analysts working at the level of singular causation need a mathematical framework to formalize such choices on top of given assumptions describing general causation. Structural causal models  {pearl2009causality}, by fully describing the latent rules governing the data-generating process, enable one to answer queries from the whole causality ladder  {bareinboim2022pearl}. However, we argue that structural causal models in their classical form---based on structural equations and exogenous distributions over a directed graph---raise some issues to sensibly encode the counterfactual level, and more specifically singular causes. Notably, the structural representation can be hard to unpack into informal, intelligible counterfactual knowledge. Conversely, it may feel unclear how to translate counterfactual beliefs into the structural formalism. Moreover, the structural framework is inconvenient to modify counterfactual assumptions without changing the observational and interventional levels.\n\nTo solve this problem, we propose a class of models that are equivalent to structural causal models, in the sense that they characterize the same causality ladders, while properly separating layers of causation (notably general and singular causes). Crucially, these models do not rely on standard structural equations to represent counterfactual conceptions. We call them counterfactual models or canonical representations of structural causal models. They build upon the fact that a singular counterfactual quantity in a structural causal model is mathematically derived from a joint probability distribution between marginals representing general causes. Then, we introduce normalizations of counterfactual models, which allow to specify the intrinsic cross-dependencies of counterfactual joint probability distributions independently of the marginals. This approach enables analysts to transparently stipulate their singular-level assumptions without altering the general-level ones.\n\nThis work has theoretical and practical interests. It provides a natural framework to derive a counterfactual conception from a causal model and reciprocally to integrate a counterfactual conception into a causal model. Furthermore, it enables analysts aiming to learn a causal model to disentangle the objective of fitting the given general causes (encoded by a graphical model) from the choice of the singular causes. Note that our approach builds upon a given causal graphical model; it does not address the estimation or design of such a model. In an introductory example below, we illustrate the limitations of structural causal models and the principles of the proposed solution. We emphasize that for general purposes, counterfactual models are not necessarily better than structural causal models. They simply offer an alternative---yet equivalent---perspective to counterfactual reasoning, that has several advantages.\n\nOverall, this article aims at enriching the understanding and modeling of counterfactuals in causality. It doing so, we expect to clarify fundamental differences within the causality research, based on the concerned layer of causation and the way causal models are employed. Further, we hope to bridge the gap between conceptual counterfactual notions and their practicality, by providing a clearer and more convenient framework than structural causal models to test, discuss, and implement any counterfactuals compatible with a same causal graphical model. Developing this approach also led us to study random processes and distributional regression in theory and in practice.\n\nOutline of the paper\n\nAfter an introductory example illustrating the challenges of reasoning counterfactually with structural causal models and the functioning of counterfactual models ( {sec:intro_example}), the rest of the paper is organized as follows:\n {itemize}\n    {sec:notation} presents the basic notation and essential background on probability;\n    {sec:setup} introduces Pearl's causal framework, notably causal graphical models and structural causal models;\n    {sec:ctf_models} is the main section: it defines counterfactual models, proves their equivalence to structural causal models, and explains how to specify them in practice via normalizations;\n    {sec:regression} studies a generic distributional regression problem with monotonicity constraints and details how it can notably be applied to implement normalizations of counterfactual models;\n    {sec:exp} contains numerical experiments;\n    {sec:comparison} discusses the similarities and differences of our approach to related works.\n {itemize}\nWe defer the proofs of the theoretical results to  {sec:proofs}.  {sec:monotonic_networks} and  {sec:quantile_regression} propose extra background and experiments.\n\nMotivating example\n\nThe goal of this example is three-fold. First, to illustrate the specific place of counterfactual reasoning, notably singular causation, in causal analysis. Second, to highlight issues that arise when modeling counterfactual assumptions through structural causal models. Third, to introduce an alternative way of intelligibly encoding counterfactual assumptions. This presents all the basics ideas of the paper. For simplicity, we use on-the-fly mathematical notation and informal definitions of causal models throughout this example. We refer to  {sec:notation} and  {sec:setup} for a complete specification.\n\nIllustrating the causal hierarchy\n\nLet us illustrate the causal hierarchy on a concrete toy example. We consider a fictitious medical study where the variables  {t} and  {y} respectively represent a medical dose and a health outcome. The possible observations follow the probability distribution $P_{ {t}, {y}}$ on $ ^2$ given by $P_{ {t}} :=  {Unif}([0,10])$ and $P_{ {y}    {t}} (  | t) :=  {N}(m(t),1)$ where $m(t):= 10  ( {  t}{14})$ for $t   [0,10]$. The knowledge of $P_{ {t}, {y}}$ alone corresponds to the observational level: by inferring features of $P_{ {t}, {y}}$, an analyst can estimate statistical associations between  {t} and  {y} but not necessarily causal dependencies.\n\nTo address the interventional level, we assume that  {t} is the treatment from a randomized controlled trial. This ensures that the variables are causally ordered according to the simple graph $ {t}    {y}$ denoted by $ $, which implies that the conditional dependence of  {y} in  {t} represents causation. As such, $P_{ {y}    {t}}$ has a causal interpretation. The fact that $P_{ {y}    {t}} (  | t) =  {N}(m(t),1)$ with $m$ minimal at $0$ on $[0,10]$ signifies that the treatment works in the sense that it increases health in average. Note, however, that its efficiency decreases for $t > 7$. The knowledge of $P_{ {t}, {y}}$ and $ $ forms a causal graphical model $ $. It completes the observational level with graphical assumptions to allow causal claims.  {sec:cgm} furnishes a reminder on causal graphical models. Critically, such a model fully handles the interventional level, not the complete counterfactual level. For illustration, suppose that Alice received $t=4$ and experienced $y=5$. With $ $ alone, an analyst can conclude that increasing the dose from $t=4$ to $t=6$ improves health by $m(6)-m(4)$ units in average for the general population, but cannot determine what would have been the outcome of Alice specifically had $t=6$. We refer to  {fig:pop_indiv} for an illustration. proposed similar graphics to highlight the nonidentifiability of counterfactual curves.\n\nTo tackle the whole counterfactual level, including individual causes, one needs stronger hypotheses than randomization or the knowledge of a causal graphical model. Encoding counterfactual assumptions on top of $ $ can be achieved by postulating a structural causal model $ $ compatible with $ $.  {sec:scm} details structural causal models. In this work, we focus on Markovian models only. Such a model basically corresponds to a pair of independent exogenous random variables $(U_ {t},U_ {y})$ and a pair of functions $(f_ {t},f_ {y})$ such that the endogenous random variables $(T,Y)$ defined by the assignments\n {align*}\n &T := f_ {t}(U_ {t}),\n\n &Y := f_ {y}(T,U_ {y}),\n {align*}\nmeet $(T,Y)   P_{ {t}, {y}}$. The functional dependence of $Y$ in $T$ conforms to the graph $ $. These assignments enables one to carry out interventions producing counterfactual variables. Concretely, intervening on $ {t}$ defines the potential outcome $Y_t := f_ {y}(t,U_ {y})$ representing the outcome had the treatment been equal to $t$ for $t   [0,10]$. In $ $, one can identify the marginal laws of $(Y_t)_{t   [0,10]}$, called interventional laws. More precisely, note that $U_ {t}   U_ {y}$ implies $T   U_ {y}$, and thereby $Y_t   P_{ {y}    {t}} (  | t)$. As such, $ $ contains all the assumptions encoded in $ $, that is the observational and interventional rungs. Furthermore, because the variables $(Y_t)_{t   [0,10]}$ share a common source of randomness $U_ {y}$, they follow a joint probability distribution between the interventional marginals called a counterfactual law. Such a law cannot be identified in $ $ only.  {fig:int_cf} illustrates the distinction between interventional and counterfactual laws, by representing a counterfactual coupling over $(Y_4,Y_6)$.\n\nTo summarize, one can formulate counterfactual assumptions (third rung) respecting observational and interventional knowledge (first and second rung) by postulating a structural causal model $ $ compatible with the known graphical model $ $. This raises several questions regarding the link between a structural causal model and the produced counterfactual distributions. How to modify a structural causal model as to change the counterfactual conception without modifying the observational and interventional assumptions? Are all couplings between two interventional marginals attributable to a structural causal model? How to unpack the counterfactual assumptions contained in a structural causal model? Conversely, how to design a structural causal models compatible with given counterfactual assumptions? This work precisely aims to address these questions.\n\n {figure}[tb]\n  \n  {subfigure}[b]{0.47 }\n  \n  [width= ]{population_individual.png}\n  {Population response / Individual response}\n\n  {subfigure}\n  \n  {subfigure}[b]{0.47 }\n  [width= ]{interventional_counterfactual.png}\n  {Interventional laws / Counterfactual law}\n\n  {subfigure}\n  {Differences between interventional and counterfactual assumptions on a dose-response curve from the example randomized controlled trial. The  {blue}{blue} highlights observed experimental results (identifiable in $ $), whereas the  {red}{red} highlights untestable hypotheses (identifiable in $ $).  {fig:pop_indiv} tracks a same patient across alternative realities where they received different values of the treatment.  {fig:int_cf} represents the counterfactual coupling between the marginals for $t=4$ and $t=6$. Counterfactuals where generated via the covariance function $k(t,t') :=  ( {t-t'}^2/(2  ^2))$ with $  = 2$.}\n\n {figure}\n\nChallenges of structural counterfactual modeling\n\nTo illustrate the challenges underlying these questions, we first consider the following assignments:\n {align*}\n &T := U_ {t},\n\n &Y^+ := m(T) + U_ {y},\n {align*}\nwhere $U_ {t}  ...",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.269,
      "weak_supervision_score": 0.265,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.236,
      "datasets_score": 0.191,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669336",
      "updated_at": "2025-08-11T23:43:05.607138",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16372",
      "title": "Depth Gives a False Sense of Privacy: LLM Internal States Inversion",
      "authors": [
        "Tian Dong",
        "Yan Meng",
        "Shaofeng Li",
        "Guoxing Chen",
        "Zhen Liu",
        "Haojin Zhu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into daily routines,\nyet they raise significant privacy and safety concerns. Recent research\nproposes collaborative inference, which outsources the early-layer inference to\nensure data locality, and introduces model safety auditing based on inner\nneuron patterns. Both techniques expose the LLM's Internal States (ISs), which\nare traditionally considered irreversible to inputs due to optimization\nchallenges and the highly abstract representations in deep layers. In this\nwork, we challenge this assumption by proposing four inversion attacks that\nsignificantly improve the semantic similarity and token matching rate of\ninverted inputs. Specifically, we first develop two white-box\noptimization-based attacks tailored for low-depth and high-depth ISs. These\nattacks avoid local minima convergence, a limitation observed in prior work,\nthrough a two-phase inversion process. Then, we extend our optimization attack\nunder more practical black-box weight access by leveraging the transferability\nbetween the source and the derived LLMs. Additionally, we introduce a\ngeneration-based attack that treats inversion as a translation task, employing\nan inversion model to reconstruct inputs. Extensive evaluation of short and\nlong prompts from medical consulting and coding assistance datasets and 6 LLMs\nvalidates the effectiveness of our inversion attacks. Notably, a 4,112-token\nlong medical consulting prompt can be nearly perfectly inverted with 86.88 F1\ntoken matching from the middle layer of Llama-3 model. Finally, we evaluate\nfour practical defenses that we found cannot perfectly prevent ISs inversion\nand draw conclusions for future mitigation design.",
      "published_date": "2025-07-22T09:15:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16372v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16372v1",
      "latex_url": "http://arxiv.org/src/2507.16372v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Despite its widespread application, the large size of  {llm} prohibits fast inference on local devices, forcing users to send their inputs ( , prompts) to the cloud and risk privacy leakage.\nThis also impedes the application in sensitive domains and commercial cooperation~. Moreover, as the model scale continues to grow ( , Llama-3 has a size up to 405B~), a single server can merely load the model in one piece, let alone swift inference.\n\nTherefore, collaborative inference~ has been widely applied to enforce data locality, where the shallow layers are stored on the local device and only the  {is} are transmitted to the cloud for continuous inference on rest layers.\nMeanwhile, to meet the requirements of trustworthy  {ai}~,  {is} can also be exposed to a third party for safety auditing, as  {is} of deep layers can be leveraged to robustly identify factual errors~, defend jailbreaks, backdoors~, or manipulate internal representations of the model's concepts~.\n\nThe potential exposure of increasingly used  {is} raises our research question: Can we invert the input query based on the  {is, even in highly deep  {llm}?}\nCurrent embedding inversion~ assigns trainable variables to each input token and selects the candidate tokens via optimization, which is proven effective on conventional  {lm} ( , BERT).\nRecent works show that text embeddings or model outputs~ can be used to invert inputs.\nThese attacks train generative inversion models conditioned on observed embeddings or outputs.\n\nYet, simple adoption cannot work well for  {is} because of two new challenges.\nFirst,  {is} are designed for subsequent inference and contain abstract logical representations~, which are inherently different from previously studied embeddings or model outputs of high semantically relevance with inputs.\nSecond,  {llm} have significantly more layers, higher width, and larger dictionary than  {lm} studied in prior work, which further hinders the inversion, especially for  {is} of deep layers because of feature loss based on the information bottleneck~.\nTherefore, we need more powerful inversion attacks to evaluate the privacy risk of  {is}.\n\nIn this work, we are the first to explore the inversion feasibility of  {is} by proposing both optimization-based and generation-based attacks adapting to white-box access and black-box access to model weights.\nSpecifically, our white-box attacks are designed for the adversary ( , curious-but-honest inference server) who can exploit the weights to optimize the input text with nearly exact and correctly ordered tokens without any assumption on input distribution.\nOur black-box attacks are suitable for a third-party adversary ( ,  {llm} auditor) who can probe the  {is} for analysis and can train an inversion model based on her own surrogate data of similar distribution to the victim's queries.\n\nSince searching for the optimal token sequence through brute force is infeasible, we introduce a novel two-phase inversion for the optimization-based attack: we first invert the input embeddings and then recover the correct input tokens.\nFor shallow layers, our attack, Embedding Recovery (ER), produces embeddings of candidate inputs by minimizing the distance of its  {is} to the target.\nThen, the tokens with the closest input embedding to the optimized embedding are selected.\nThis tackles the large-dictionary challenge by avoiding searching over significantly huge token combination space.\nFor deep layers, ER can fail because of gradient explosion.\nWe propose Token Basis Selection (TBS) that determines the optimal combination among base vectors of input embedding space as the inverted embeddings for further token inversion.\nThis tackles the high-depth challenge by reducing optimized variables and avoiding local minima encountered in the previous solution~.\n\nWithout access to the target model weights, we first extend our optimization attacks to the black-box setting by identifying whether the target is derived from adversary-known  {llm}, based on our insights that a large number of  {llm} are derived from existing ones instead of pretrained from scratch.\nFor the generation-based attack, we regard the  {is} as an encoded language and use the encoder-decoder models, which are commonly used in machine translation, for input inversion.\nTo tackle the challenge of representation discrepancy between  {is} and semantic meaning, we propose a projection module that aligns the  {is} with the encoder for inversion with the decoder.\n\nOur evaluations include 6 real-world high-ranking  {llm}, both short-context prompts, as adopted in existing works, and additional long-context prompts on medical consulting and coding assistance.\nThe results demonstrate the inversion effectiveness.\nFor example, given  {is} from the middle layer of  { -3-8B-Instruct}, our TBS attack can invert input of 4,112 tokens with 86.88 F1 token matching and 95.19 semantic similarity (see  {fig:example-p1,fig:example-p2,fig:example-p3}) which cannot be reached by prior work.\nOur generation-based attack can also achieve 81.6 F1 score for inputs of medium length ( , $ $1k tokens) which is comparable to the white-box attack.\nLastly, we test four defenses including quantization, dropout, noisy input embedding, and  {dp} through the Laplace mechanism.\nOur black-box attack cannot be mitigated without greatly deteriorating the model utility, calling for more effective defenses in the future.\n\nIn summary, our contributions are:\n {itemize}\n   We are the first to systematically investigate the input inversion risk of  {llm}  {is}. Our work reveals that an attacker can successfully recover sensitive prompts of LLMs, spanning up to 4,112 tokens, from their  {is}.\n\n   To overcome the challenges of semantic spasticity and feature loss from high-depth layers, we propose four novel inversion attacks adapting to both white-box and black-box attack settings.\n\n   We extensively evaluate our attacks on sensitive inputs including medical dialogues and coding assistance. We also evaluated  {dp}-based defense and found our attack can still invert input of high semantic similarity even significantly sacrificing the downstream inference quality.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "arxiv.tex",
      "rlhf_score": 0.39,
      "weak_supervision_score": 0.366,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.35,
      "datasets_score": 0.278,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on inverting internal states of Large Language Models (LLMs) to recover original inputs, emphasizing privacy risks through optimization-based and generation-based attacks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668627",
      "updated_at": "2025-08-11T23:43:05.607031",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16382",
      "title": "Application of LLM Guided Reinforcement Learning in Formation Control\n  with Collision Avoidance",
      "authors": [
        "Chenhao Yao",
        "Zike Yuan",
        "Xiaoxu Liu",
        "Chi Zhu"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.",
      "published_date": "2025-07-22T09:26:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16382v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16382v1",
      "latex_url": "http://arxiv.org/src/2507.16382v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Multi-Agent Systems (MAS) have demonstrated superior performance across various fields,\nshowcasing higher task efficiency and stronger fault tolerance compared to single-agent systems. However, current MAS applications predominantly operate within structured environments, with less satisfactory performance in more complex, unstructured scenarios. Traditional methods, such as Optimal Reciprocal Collision Avoidance (ORCA) and Artificial Potential Fields (APF) , explicitly model the system and precisely calculate the instructions for each agent at every moment. However, these approaches often rely on certain assumptions (e.g., ORCA assumes that all agents follow the same obstacle avoidance strategy), which can lead to policy failures when real-world deployments do not align with these preconditions.\n\nIn recent years, Multi-Agent Reinforcement Learning (MARL), a type of reinforcement learning (RL) has made notable advancements in addressing challenges such as formation control, obstacle avoidance, and maintaining system stability within MAS, showcasing its substantial potential. Through continuous interaction with their environment, agents iteratively refine their policies to maximize cumulative rewards, highlighting the significant impact that the design of reward function can have on policy quality. While crafting an effective reward function is relatively straightforward for single-task scenarios, the Formation Control with Collision Avoidance (FCCA) problem introduces the complexity of accounting for interdependencies between different tasks. This added layer of complexity often necessitates considerable time and effort in designing and tuning reward functions; even minor adjustments, such as tweaking the weights of individual reward components, can require retraining of the models based on the original models to ensure optimal performance across multiple objectives.\n\nTo address the above issues, we propose a framework where agent observations are provided to an LLM, which generates an initial reward function focused on core objectives rather than optimal performance across all tasks. Instead of relying on reward magnitude, effectiveness is evaluated by how well predefined performance criteria are met. After a fixed number of iterations, these criteria and task-specific rewards are fed back to the LLM, enabling online adjustments to improve the reward function. We validate our method in a complex scenario where a MAS maintains formation, avoids dynamic obstacles, and reaches its destination in minimal time with stable actions. An overview of the proposed method is shown in Fig.~.\n\nIn summary, the contributions of this work can be highlighted as follows:\n {itemize}\n  We are the first to apply LLM-guided RL to the multi-agent FCCA task, enabling the creation of sophisticated reward structures that guide agents in achieving complex objectives.\n  We implemented a framework that dynamically updates reward functions, allowing continuous improvement and higher efficiency with fewer iterations.\n  We validated the effectiveness and practicality of our approach through the use of sim-to-sim and sim-to-real validation methods.\n {itemize}\n\n {figure*}[!t]\n  \n  [width=18cm]{figure/framework.pdf}\n  {Overview of our method}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "root.tex",
      "rlhf_score": 0.505,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.367,
      "datasets_score": 0.306,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on using Large Language Models (LLMs) to generate and dynamically adjust reward functions for Multi-Agent Reinforcement Learning (MARL) in formation control and collision avoidance tasks. It relies on automated evaluations and predefined performance criteria for adjustments, without involving human feedback, rankings, or preferences. RLHF specifically requires human-ranked data to train a reward model for fine-tuning, which is not a feature of this work, making it unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668635",
      "updated_at": "2025-08-11T23:43:05.607033",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16385",
      "title": "STAR: A Benchmark for Astronomical Star Fields Super-Resolution",
      "authors": [
        "Kuo-Cheng Wu",
        "Guohang Zhuang",
        "Jinyang Huang",
        "Xiang Zhang",
        "Wanli Ouyang",
        "Yan Lu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Super-resolution (SR) advances astronomical imaging by enabling\ncost-effective high-resolution capture, crucial for detecting faraway celestial\nobjects and precise structural analysis. However, existing datasets for\nastronomical SR (ASR) exhibit three critical limitations: flux inconsistency,\nobject-crop setting, and insufficient data diversity, significantly impeding\nASR development. We propose STAR, a large-scale astronomical SR dataset\ncontaining 54,738 flux-consistent star field image pairs covering wide\ncelestial regions. These pairs combine Hubble Space Telescope high-resolution\nobservations with physically faithful low-resolution counterparts generated\nthrough a flux-preserving data generation pipeline, enabling systematic\ndevelopment of field-level ASR models. To further empower the ASR community,\nSTAR provides a novel Flux Error (FE) to evaluate SR models in physical view.\nLeveraging this benchmark, we propose a Flux-Invariant Super Resolution (FISR)\nmodel that could accurately infer the flux-consistent high-resolution images\nfrom input photometry, suppressing several SR state-of-the-art methods by\n24.84% on a novel designed flux consistency metric, showing the priority of our\nmethod for astrophysics. Extensive experiments demonstrate the effectiveness of\nour proposed method and the value of our dataset. Code and models are available\nat https://github.com/GuoCheng12/STAR.",
      "published_date": "2025-07-22T09:28:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16385v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16385v1",
      "latex_url": "http://arxiv.org/src/2507.16385v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Image quality is critical to astronomical observation, while high quality means finer astrophysical structures and enables precise measurements~. This results in the astronomy community always establishing new telescopes to seek high-quality and high-resolution surveys, even facing high costs~. Different from astronomy, in natural image processing, the software computer vision Super Resolution (SR) technique~has provided a series of successful methods to achieve high-quality and high-resolution observations in an economical way~. So, there is obviously an opportunity to introduce the computer vision SR method to process high-quality astronomical images. However, there remains a great challenge -- data.\n\nExisting datasets~ in astronomical super resolution (ASR) have 3 drawbacks: physically trivial, object-centric, and limited-scale. 1). Flux Inconsistency:\nIn the real world, telescopes under different observation resolutions have a flux consistency relation~. Specifically, although a celestial object has different levels of distortions under low resolutions, it almost has the same total flux as in high resolutions because of the telescope imaging principle~.\n\nHowever, existing datasets have significant drifts from this property because they directly use simple interpolation~, suitable for natural images but conflicts with astronomical observations.\n\nThis catastrophic limitation makes existing datasets almost physically trivial, significantly affecting their scientific value.\n2). Object-Crop Configuration: Each image in existing ASR datasets only contains a center-cropped and resized singular celestial object (e.g., stars or galaxies)~. This ideal configuration neglects many valuable patterns beyond single object, important in astrophysics, such as large-scale structure~, cross-object interaction~, and weak lensing~, limiting the value of existing datasets.\n\n3). Insufficient Data Diversity: The scale of existing ASR datasets ranges from 1,597 to 17,000~. The restricted scale limits the ability of the learned model and makes evaluation unreliable and unfair. To address the above-mentioned dataset limitations, we introduce a new dataset called STAR.\n\nSTAR is a large-Scale ASR dataset. It consists of 54,738 high-resolution star field images captured by the Hubble Space Telescope (HST)~. Each image is totally field-level, covering a large range of star fields and average containing 30 objects and complex scenarios including multiple celestial objects, cross-object interaction and weak lensing phenomenon, as Figure~ shows. Compared with existing ASR datasets, STAR provides approximately at least 15 times more observation objects per image on average, while also offering 60% of cosmic information outside the object area (e.g, like diffuse interstellar medium (ISM) regions~), significantly showing the scale priority. We provide overall advantages of the STAR for other datasets in Tab.~.\n\nExcept that, to tackle the 'Physical trivial' problem, STAR proposes a flux-consistent data generation pipeline, which processes cross-resolution image pairs fitting the aforementioned real telescope flux-consistent property, making the entire dataset physically faithful.\n\nFurthermore, STAR provides a novel Flux Error (FE) to evaluate SR models from a physical perspective, ensuring their outputs align with astrophysical principles critical for reliable scientific analysis.\n\n \nSTAR is designed with the following core innovations: 1). Flux-consistency: STAR proposes a flux-consistent data generation pipeline.\nThis ensures the processed cross-resolution image pairs fit the real telescope flux-consistent property, which makes the entire dataset physically faithful. 2). Field-level: Data in STAR are totally field-level, which covers complex scenarios including multiple celestial objects, cross-object interaction and weak lensing phenomenon, as Figure~ shows.\n\n3). Large Scale Data: STAR consists of 54,738 high-resolution star field images captured by Hubble Space Telescope (HST). Due to each image average contain  {red}{xxx} objects, the entire STAR offers at least  {red}{xxx} times more celestial region than existing ASR datasets, significantly show the scale priority of STAR. 4). Physic Evaluation Metric: Furthermore, STAR provides a novel Flux Error to evaluate SR models from a physical perspective, ensuring their outputs align with astrophysical principles critical for reliable scientific analysis.\n\nIn conclusion, STAR introduces a large-scale field-level ASR dataset benchmark and corresponding evaluation metrics. Its satisfactory scale and physically faithful property allow reliable large field-level ASR model training and evaluation.\n \nWith the STAR, we evaluate several state-of-the-art SR methods, including both natural~ and astronomical SR methods~ to quantify their generalization ability to the field-level ASR topic, noting that many astronomical SR methods directly adopt natural SR methods. Unfortunately, they cannot provide satisfactory results. We analyze that the main reason is the lack of specific optimization for the flux-consistency prior.\nDue to this, we propose a novel field-level ASR model, Flux-Invariant Super Resolution (FISR). It introduces the flux consistency property at both the model design and optimization views to fulfill the flux relationships neglected by previous ASR works. At the model view, FISR has a series of specific designs to extract flux information from low-resolution input as visual prompts following astrophysical ideas. These prompts are then injected into the model and give the ability to perceive input flux accurately, allowing the model to propagate consistent flux cues from low-resolution inputs to predicted high-resolution outputs.\n\nAnd at the optimization view, we provide a Flux consistency loss (FCL) which constrains the photometry gap for each celestial object between the ground-truths and predictions, highlighting the importance of flux during the model optimization process and leading to a more reliable trained model.\n\n {itemize}\n   STAR Benchmark: We introduce STAR, a large-scale, flux-consistent ASR benchmark with 54,738 cross-resolution image pairs from HST F814W star fields. Unlike prior datasets, STAR captures field-level complexity, offering 15 times more objects per image and 60% additional cosmic information, using a flux-consistent pipeline.\n   Flux Error (FE): We present FE, a novel metric to evaluate SR models’ alignment with astrophysical flux conservation, ensuring reliable photometric analysis.\n   Flux-Invariant Super Resolution (FISR) Model: We propose FISR, a field-level ASR model and a Flux Consistency Loss, outperforming existing methods by addressing flux relationships neglected in prior work.\n {itemize}\n\n {figure}\n  \n  [width=1.0 ]{Fig/fig1.pdf}\n  {Comparison of previous datasets and ours, highlighting richer structures such as cross-object interaction, weak lensing, and dark matter halos.}\n {figure}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_introduction.tex",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.264,
      "distributed_training_score": 0.367,
      "datasets_score": 0.365,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668121",
      "updated_at": "2025-08-11T23:43:05.606934",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16389",
      "title": "From Flat to Round: Redefining Brain Decoding with Surface-Based fMRI\n  and Cortex Structure",
      "authors": [
        "Sijin Yu",
        "Zijiao Chen",
        "Wenxuan Wu",
        "Shengxian Chen",
        "Zhongliang Liu",
        "Jingxin Nie",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Xin Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reconstructing visual stimuli from human brain activity (e.g., fMRI) bridges\nneuroscience and computer vision by decoding neural representations. However,\nexisting methods often overlook critical brain structure-function\nrelationships, flattening spatial information and neglecting individual\nanatomical variations. To address these issues, we propose (1) a novel sphere\ntokenizer that explicitly models fMRI signals as spatially coherent 2D\nspherical data on the cortical surface; (2) integration of structural MRI\n(sMRI) data, enabling personalized encoding of individual anatomical\nvariations; and (3) a positive-sample mixup strategy for efficiently leveraging\nmultiple fMRI scans associated with the same visual stimulus. Collectively,\nthese innovations enhance reconstruction accuracy, biological interpretability,\nand generalizability across individuals. Experiments demonstrate superior\nreconstruction performance compared to SOTA methods, highlighting the\neffectiveness and interpretability of our biologically informed approach.",
      "published_date": "2025-07-22T09:34:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16389v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16389v1",
      "latex_url": "http://arxiv.org/src/2507.16389v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure*}[t]\n  \n  [width= ]{pictures/contribution.pdf}\n  {\n The drawbacks of previous work and the contributions of this paper.\n }\n\n {figure*}\n\nThe human brain, shaped by evolution, can be viewed as a highly optimized, naturally ``pre-trained'' neural network.\n\nIt encodes sensory stimuli such as visual and auditory signals from various organs into intricate patterns of neural activity.\n\nBrain decoding, the reverse of this process, aims to reconstruct sensory stimuli from recorded brain activity, typically measured using functional magnetic resonance imaging (fMRI).\n\nThis decoding holds significant implications for both neuroscience, by revealing how the brain represents sensory information, and brain-computer interfaces (BCIs), by translating neural signals into actionable outputs.\n\nA pivotal area of brain decoding is fMRI-image reconstruction, which focuses on the exploration of the brain’s visual functions.\n\nOver time, techniques have evolved from semantic category reconstruction to pixel-level reconstruction .\n\nCurrently, the most popular approach is to train an fMRI encoder , whose output fMRI embeddings are used as conditions to guide a diffusion model for image reconstruction.\n\nFollowing these approaches, fMRI-image reconstruction has expanded from single-subject to cross-subject applications .\n\nMethods that encode fMRI signals and use them as conditions to guide diffusion models have achieved success.\n\nHowever, existing methods share fundamental limitations stemming from the oversimplification of fMRI data as one-dimensional signals, disregarding critical spatial and structural properties.\n\nSpecifically:\n\n(1) fMRI signals arise from voxels located on the cerebral cortex, naturally forming spatial patterns on a two-dimensional non-Euclidean cortical surface .\n\nIgnoring this spatial organization diminishes the richness of spatial information available.\n\n(2) Structural brain differences among individuals significantly influence functional responses , resulting in varied neural responses to identical stimuli .\n\nCurrent methods fail to effectively capture these structural variations, thus impairing performance in cross-subject generalization.\n\nBased on this observation, we propose the Sphere Tokenizer, which processes fMRI signals into fMRI tokens that incorporate spatial structural information.\n\nThese tokens are then fed into the fMRI encoder for encoding.\n\nfMRI signals located on the cerebral cortex are mapped onto a standard sphere using FreeSurfer-based method.\n\nSpherical convolution is introduced to extract fMRI features.\n\nSimilarly to planar convolution , each voxel (pixel) only gathers information from its neighbors and updates its value.\n\nIn addition, unlike previous spherical convolution frameworks, we introduce structural information of the cerebral cortex and sphere positional embedding into the Sphere Tokenizer.\n\nThe cortical structure allows the model to perceive differences in brain structures across subjects, while the sphere positional embedding enables the model to account for functional pattern differences across different regions of the cortex.\n\nTo the best of our knowledge, we are the first to incorporate brain structure into the fMRI-image reconstruction task.\n\nIn addition, current fMRI-image datasets typically provide asymmetric matching samples, i.e., one image corresponds to multiple fMRI scans.\n\nPrevious work apply each individual fMRI scan for training, but employ the averaged fMRI scan for inference.\n\nThis creates a data distribution gap between training and inference.\n\nTo address this, we proposed a positive sample mixup strategy, where during training, we randomly mixup the corresponding fMRI scans to match the averaging process during inference.\n\nContributions:\n\nwe summarize the drawbacks of previous works and the contributions of this paper in Fig. .",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "paper/introduction.tex",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.302,
      "diffusion_reasoning_score": 0.435,
      "distributed_training_score": 0.327,
      "datasets_score": 0.31,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models to guide image reconstruction from fMRI data, which involves iterative refinement processes. However, this is applied to visual stimulus reconstruction in neuroscience, not to solving complex logical tasks or holistic Chain-of-Thought reasoning as defined in the topic. The connection is limited to the use of diffusion mechanisms, without any multi-step logical reasoning component.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667504",
      "updated_at": "2025-08-11T23:43:05.606791",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16393",
      "title": "Are Foundation Models All You Need for Zero-shot Face Presentation\n  Attack Detection?",
      "authors": [
        "Lazaro Janier Gonzalez-Sole",
        "Juan E. Tapia",
        "Christoph Busch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Although face recognition systems have undergone an impressive evolution in\nthe last decade, these technologies are vulnerable to attack presentations\n(AP). These attacks are mostly easy to create and, by executing them against\nthe system's capture device, the malicious actor can impersonate an authorised\nsubject and thus gain access to the latter's information (e.g., financial\ntransactions). To protect facial recognition schemes against presentation\nattacks, state-of-the-art deep learning presentation attack detection (PAD)\napproaches require a large amount of data to produce reliable detection\nperformances and even then, they decrease their performance for unknown\npresentation attack instruments (PAI) or database (information not seen during\ntraining), i.e. they lack generalisability. To mitigate the above problems,\nthis paper focuses on zero-shot PAD. To do so, we first assess the\neffectiveness and generalisability of foundation models in established and\nchallenging experimental scenarios and then propose a simple but effective\nframework for zero-shot PAD. Experimental results show that these models are\nable to achieve performance in difficult scenarios with minimal effort of the\nmore advanced PAD mechanisms, whose weights were optimised mainly with training\nsets that included APs and bona fide presentations. The top-performing\nfoundation model outperforms by a margin the best from the state of the art\nobserved with the leaving-one-out protocol on the SiW-Mv2 database, which\ncontains challenging unknown 2D and 3D attacks",
      "published_date": "2025-07-22T09:38:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16393v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16393v1",
      "latex_url": "http://arxiv.org/src/2507.16393v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The development and evolution of face recognition systems over the years has been mainly due to the success of advances in the area of deep learning~. Despite their advances, facial recognition technologies are vulnerable to attack presentations (AP) which, in most cases, can be easily created by a malicious individual with the intent to impersonate an authorised subject and gain access to the latter's information (e.g. financial transactions and unlocking of smartphones). The daily information flow through social networks such as Facebook, Instagram and YouTube allows an attacker to download a photo or video of a target subject and replay it on the system's capture device (this is a 2D attack) to grant unauthorised access to different applications~. More sophisticated attacks, including 3D masks, can also be used effectively to circumvent biometric recognition technologies.\n\nTo protect face recognition systems against APs, numerous presentation attack detection (PAD) approaches have been proposed~. Current state-of-the-art PAD algorithms are mainly developed upon deep learning and require a large amount of data for training to obtain reliable detection performance~. Despite the progress achieved over the years, these PAD algorithms lack generalisability, which is evidenced by the degradation of their performance in detecting unknown presentation attack instruments (PAI) or databases that have not been seen during training. Note that the collection of new databases to train PAD subsystems has not experienced the same advances as PAD technologies and is partly due to privacy concerns and the fact that it is a time-consuming task. To alleviate the lack of generalisability, the literature has focused, on the one hand, on the creation of synthetic data that resembles real images captured from a PAI~. On the other hand, reusing the weights of deep neural networks (DNN) that were optimised with a huge amount of images~ and they are supposed to be generalisable to different tasks.\n\nHuman learning is inherently multimodal, as harnessing multiple senses together helps us to better understand and analyse new information. Recent advances in multimodal learning have been inspired by the effectiveness of this process in creating models capable of processing and relating information using a variety of modalities such as image, video, text, audio, body gestures, facial expressions and physiological signals. In this paper, we focus in particular on the reuse of DNN weights to mitigate the lack of generalisability of PAD approaches. To do so, we explore the effectiveness of recent foundation models for zero-shot PAD. Foundation models are large models pre-trained on large amounts of data, designed to be generalisable and easily adaptable to specific tasks. Zero-shot classification is the task of predicting objects of unseen classes (target domain) by transferring knowledge obtained from other seen classes (source domain) with the help of semantic information~. Exploiting the generalisable weights of the foundational models, we attempt to provide a simple framework that is capable of detecting unknown PAI with high performance. The main contributions of this work are summarised below:\n\n {itemize}\n   Demonstration of the effectiveness of the foundation model-based framework on an unrelated top-down task, adapting only a minimum number of parameters related to the classification header in the training phase. It is shown that the performance of the framework for zero-shot PAD is improved by simply fusing different foundation models.\n\n   Extensive evaluation in line with metrics defined in the international standard ISO/IEC 30107-3~ for biometric PAD of the proposed approach in challenging scenarios, such as unknown PAI species and cross-database. Experimental evaluation shows that the proposed framework can achieve state-of-the-art performance in different protocols and outperforms baselines by a large margin.\n {itemize}\n\n The remainder of this paper is organised as follows: Related work is summarised in Sect.~. In Sect.~, we describe the foundation models-based framework. The experimental setup is summarised in Sect.~. Experimental results, including the foundation model assessment, as well as a benchmark of the proposed PAD framework on challenging settings, are presented in Sect~. Conclusions and future work directions are finally summarised in Sect.~.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "2025-FG-FoundationsPAD.tex",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.324,
      "distributed_training_score": 0.325,
      "datasets_score": 0.321,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668129",
      "updated_at": "2025-08-11T23:43:05.606936",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16395",
      "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and\n  Implicit Dependency Reasoning",
      "authors": [
        "Bo Hou",
        "Xin Tan",
        "Kai Zheng",
        "Fang Liu",
        "Yinghao Zhu",
        "Li Zhang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks.",
      "published_date": "2025-07-22T09:42:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16395v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16395v1",
      "latex_url": "http://arxiv.org/src/2507.16395v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In collaborative software development, a commit is the basic unit of code change, consisting of source file modifications and a message summarizing their purpose . Ideally, each commit should be atomic, addressing a single concern—such as implementing a feature, fixing a bug, or refactoring . Atomic commits are a widely endorsed best practice for enhancing readability, facilitating reviews, and aiding maintenance by clearly linking changes to their intent . As a result, many software communities and companies recommend small, single-purpose commits .\n\nHowever, in practice, developers often create tangled commits, which combine code changes related to multiple development concerns within a single commit . Tangled commits commonly arise due to time constraints or unclear boundaries between concerns during implementation . For example, a developer may simultaneously refactor code while fixing a bug alongside documentation updates, resulting in a commit that does not adhere to the principle of separation of concerns .\n\nEmpirical studies find that tangled commits are prevalent, indicating that 11% to 40% of commits in software repositories are tangled. Such commits hinder program comprehension, complicate maintenance .\nMoreover, tangled commits reduce the accuracy of automated tools such as bug prediction and localization models that rely on mining repository history . These tools often assume that each commit addresses a single concern, so tangled commits introduce noise and degrade model performance.\n\nResearchers have proposed various approaches to untangling commits to split a tangled commit into separate coherent changes. These methods fall into three categories based on their automation level: heuristic rule-based, feature-based, and graph clustering-based. Heuristic methods use manually defined rules—such as line distance, textual similarity, or co-change frequency—to assess change relatedness~. Feature-based methods employ handcrafted features (e.g., same method, class, or package) in supervised models to classify change pairs~. Graph clustering-based methods build program graphs (e.g., PDGs) to model structural dependencies and use clustering or node embeddings to group related changes~.\n\nDespite their methodological differences, these approaches share several key limitations. First, they often rely on surface-level signals or structural proximity and lack the capacity for deeper semantic reasoning. Graph-based methods attempt to encode some semantic relationships via graph embeddings, but they typically require substantial training and are limited in their ability to generalize across projects or languages. Second, most approaches act as black-box models, offering limited interpretability and little insight into why specific code changes are grouped together. Third and most critically, prior work does not clearly distinguish or integrate different types of dependencies among code changes. For instance, two edits may be connected via control flow (explicit) or may reflect a coherent conceptual refactoring (implicit), yet existing models either fail to recognize such links or treat them uniformly, resulting in reduced effectiveness on complex or ambiguous commits.\n\nWe argue that effective commit untangling must go beyond surface-level patterns and rely on a principled framework that explicitly accounts for both explicit and implicit dependencies. Explicit dependencies refer to observable relationships such as control or data flow, containment, or static code references. Implicit dependencies involve semantic connections—such as conceptual similarity, logical association, or shared intent—that are not necessarily reflected in the program structure. While some existing methods may partially capture these relationships through learned embeddings or co-change patterns, they do not explicitly model or directly leverage these dependencies as integral elements of the untangling process.\n\nRecent advances in large language models (LLMs) have demonstrated strong performance in software engineering tasks that require semantic understanding, reasoning, and explanation~. LLMs are particularly well-suited to identify implicit dependencies that go beyond what is detectable via static analysis or statistical patterns. However, a single LLM acting alone may not consistently balance different dependency perspectives or resolve ambiguous cases. Inspired by recent work on multi-agent collaboration~, we propose decomposing the untangling task into subtasks, each handled by specialized agents, and enabling them to collaborate through consultation.\n\nTo this end, we introduce ColaUntangle, a collaborative consultation framework for commit untangling that integrates both explicit and implicit dependencies. We construct multi-version Program Dependency Graphs ($ $-PDG) to capture structural and contextual information for code changes. Then, we design a multi-agent architecture driven by LLMs: an explicit worker agent focused on explicit dependencies (e.g., control/data flow), an implicit worker agent focused on implicit dependencies (e.g., conceptual relationships), and a reviewer agent that synthesizes and reconciles their results. All agents provide not only untangling decisions but also explanations. Through iterative interaction, these agents simulate human-like consultation and collectively decide on the final untangling outcome.\n\nTo evaluate the effectiveness and efficiency of the proposed approach, we conduct experiments on the widely-used C\\# and Java datasets~ with 1,612 and 14k tangled commits. The evaluation results show that ColaUntangle achieves better untangling results (i.e., 44% and 100% enhancement of effectiveness for C\\# and Java, compared to the best-performing baseline). Overall, our work makes the following contributions:\n {itemize}[leftmargin=*]\n   ColaUntangle: a collaborative consultation model leveraging LLM-driven agents for commit untangling. We propose ColaUntangle, the first LLM-driven collaborative consultation model that untangles commits by considering explicit and implicit dependencies among code changes, simulating human collaborative decision-making to generate results and explanations.\n   Definition of explicit and implicit dependencies for commit untangling. We define a clear and generalizable set of explicit and implicit dependencies between code changes, providing principled guidance that enables both automated and explainable commit untangling.\n   Extensive empirical evaluation. We evaluate ColaUntangle against the state-of-the-art approaches, and the results demonstrate its superior performance. Our tool and data are available at https://anonymous.4open.science/r/ColaUntangle-0109.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.392,
      "weak_supervision_score": 0.387,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.336,
      "datasets_score": 0.308,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces ColaUntangle, a multi-agent LLM framework for commit untangling that uses iterative consultation among agents to handle explicit and implicit dependencies. While it involves iterative refinement in reasoning, it does not adapt the iterative refinement process of diffusion models, nor does it treat the Chain-of-Thought as a single entity for holistic correction as defined in the topic. The approach is based on LLM-driven collaboration, not diffusion-based mechanisms, so it lacks any direct or indirect connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668645",
      "updated_at": "2025-08-11T23:43:05.607035",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16397",
      "title": "ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT\n  Feature and Hierarchical Content Disentanglement",
      "authors": [
        "Kahim Wong",
        "Jicheng Zhou",
        "Haiwei Wu",
        "Yain-Whar Si",
        "Jiantao Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The advancement of image editing tools has enabled malicious manipulation of\nsensitive document images, underscoring the need for robust document image\nforgery detection.Though forgery detectors for natural images have been\nextensively studied, they struggle with document images, as the tampered\nregions can be seamlessly blended into the uniform document background (BG) and\nstructured text. On the other hand, existing document-specific methods lack\nsufficient robustness against various degradations, which limits their\npractical deployment. This paper presents ADCD-Net, a robust document forgery\nlocalization model that adaptively leverages the RGB/DCT forensic traces and\nintegrates key characteristics of document images. Specifically, to address the\nDCT traces' sensitivity to block misalignment, we adaptively modulate the DCT\nfeature contribution based on a predicted alignment score, resulting in much\nimproved resilience to various distortions, including resizing and cropping.\nAlso, a hierarchical content disentanglement approach is proposed to boost the\nlocalization performance via mitigating the text-BG disparities. Furthermore,\nnoticing the predominantly pristine nature of BG regions, we construct a\npristine prototype capturing traces of untampered regions, and eventually\nenhance both the localization accuracy and robustness. Our proposed ADCD-Net\ndemonstrates superior forgery localization performance, consistently\noutperforming state-of-the-art methods by 20.79\\% averaged over 5 types of\ndistortions. The code is available at https://github.com/KAHIMWONG/ACDC-Net.",
      "published_date": "2025-07-22T09:48:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16397v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16397v1",
      "latex_url": "http://arxiv.org/src/2507.16397v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "With the rise of image editing tools such as Photoshop, Canva, and deep inpainting models~, altering document images has become effortless. This allows adversaries to manipulate financial records or other sensitive data in the document images to conceal malicious activities such as spreading rumors and committing economic fraud, which can lead to costly errors in decision-making and significant financial loss~. These manipulations pose significant security threats, making effective tools for detecting tampering in document images more critical than ever~.\n\n {figure}[t]\n  \n  [width=0.46 ]{img/intro.png}\n  {-5pt}\n  {Comparison of ADCD-Net with existing methods: (a) Sole reliance on RGB forensic traces is insufficient for forgery detection. (b) Combining RGB and DCT features enhances effectiveness and robustness against JPEG compression but becomes severely vulnerable to disruptions in DCT block alignment. (c) ADCD-Net adaptively leverages DCT traces through a predicted score, simultaneously achieving desirable effectiveness and robustness against various distortions.}\n\n  {-8pt}\n {figure}\n\n {figure}[t]\n  \n  [width=0.48 ]{img/hcd_intro.png}\n  {-15pt}\n  {\n The strong intensity contrast between text and background (BG) (a \\& b) causes pristine text pixels (red) to be undesirably drawn toward tampered text (green) in the feature space (c \\& e) instead of aligning with the pristine BG (blue) ideally. The proposed Hierarchical Content Decoupling (HCD) mitigates this text-BG bias (d \\& f). }\n\n  {-8pt}\n {figure}\n\nForgery detection in natural images has been extensively studied, with various methods focusing on specific types of image forgery such as splicing~, copy-move~, and inpainting~. Some approaches target particular forgery traces, including JPEG artifacts~, camera traces~, and editing traces~, while others focus on learning general forgery features directly on RGB domain~. More recent solutions address complex and mixed types of forgery, including those involving transmission degradation and various post-processing operations, by detecting general or mixture of forgery artifacts~. However, forensic cues in natural images differ significantly from those in document images. Specifically, document images often feature uniform background (BG) and structured text with sharp contours and consistent textures. Tampered regions can be extremely small, blending seamlessly with their surroundings and thus making forgeries easy to execute but difficult to detect. These unique characteristics pose great challenges to accurate and robust forgery localization for documents.\n\nSubstantial efforts have been made to localize forged areas in document images~. Current methods leverage various traces, such as JPEG artifacts~, noise and texture traces~, or directly derive from the RGB domain~. Particularly,~ extracts forensic features from both RGB and DCT domains, achieving competitive detection performance on document images and exhibiting robustness against JPEG recompression. However, real-world document images often undergo various degradations, including cropping and resizing, which could obscure forensic features and even break JPEG grids alignment, making the existing DCT-based methods severely vulnerable in practice~. Additionally, existing methods do not fully utilize the distinctive characteristics of document images, further leading to inferior performance.\n\nTo address the aforementioned challenges, we propose a novel forgery localization model, ADCD-Net, specifically designed for tampered document images. The key innovation of ADCD-Net lies in adaptively utilizing forensic traces from both the RGB and DCT domains to enhance detection performance and robustness against various distortions; not limited to JPEG compression. This innovation is inspired by the fact that, while the \\(8   8\\) block DCT-based features have proven effective over other form of frequency features (such as LoG~, Bayar~ and SRM~) in prior works~, their reliability significantly decreases when the \\(8   8\\) DCT block alignment is disrupted by operations like resizing or cropping~. Therefore, instead of completely discarding DCT features or fully relying on static DCT features (Fig.~~(a-b)), we propose adaptively considering DCT features as shown in Fig.~~(c). Specifically, adaptive DCT features are extracted through an optimized scoring mechanism that dynamically modulates its contributions, thereby providing robust input features for subsequent forgery decisions.\n\nHowever, the multi-view features still suffer from significant text-BG bias as illustrated in Fig.~, which would lead to inferior localization accuracy. To resolve this challenge, we propose two key modules, Hierarchical Content Decoupling (HCD), and Pristine Prototype Estimation (PPE) that utilize the domain knowledge of the document images toward more accurate and robust forgery localization.\n\nSpecifically, the text-BG bias creates feature disparities that could obscure subtle tampering cues. The proposed HCD effectively separates forgery from content features across scales, reducing such text-BG bias and improving detection accuracy.\n\nAdditionally, it is a reasonable assumption that most BG regions are pristine, as forgery often occurs in informative text regions. This phenomenon can be easily verified in many datasets,   . The proposed PPE generates a prototype capturing pristine noise patterns. By comparing the pristine prototype with the entire image, we can effectively reveal subtle tampered areas, and enhance model performance and robustness. Our key contributions are summarized as follows:\n {itemize}\n   We propose ADCD-Net, a robust document forgery localization model that adaptively fuses RGB and DCT traces, adjusting DCT feature contributions with a predicted alignment score. This enhances detection accuracy and robustness by mitigating the impact of alignment-disrupting operations such as resizing and cropping.\n   We introduce HCD and PPE modules tailored to isolate forgery features from document content and estimate pristine noise patterns to uncover subtle tampered areas, further improving detection performance and robustness.\n   Extensive experiments show that our ADCD-Net surpasses state-of-the-art (SOTA) methods by an average gain of 20.79% across various distortion types.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.296,
      "weak_supervision_score": 0.301,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.305,
      "datasets_score": 0.308,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668137",
      "updated_at": "2025-08-11T23:43:05.606938",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16403",
      "title": "ReasonVQA: A Multi-hop Reasoning Benchmark with Structural Knowledge for\n  Visual Question Answering",
      "authors": [
        "Duong T. Tran",
        "Trung-Kien Tran",
        "Manfred Hauswirth",
        "Danh Le Phuoc"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we propose a new dataset, ReasonVQA, for the Visual Question\nAnswering (VQA) task. Our dataset is automatically integrated with structured\nencyclopedic knowledge and constructed using a low-cost framework, which is\ncapable of generating complex, multi-hop questions. We evaluated\nstate-of-the-art VQA models on ReasonVQA, and the empirical results demonstrate\nthat ReasonVQA poses significant challenges to these models, highlighting its\npotential for benchmarking and advancing the field of VQA. Additionally, our\ndataset can be easily scaled with respect to input images; the current version\nsurpasses the largest existing datasets requiring external knowledge by more\nthan an order of magnitude.",
      "published_date": "2025-07-22T09:55:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16403v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16403v2",
      "latex_url": "http://arxiv.org/src/2507.16403v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{images/img-intro.png}{Sample image and questions from  . Using an existing image, a question is formulated by reasoning through one or multiple hops over the knowledge graph. The generated questions span diverse domains.}{img-intro}\n\nIn recent years, significant advancements have been made in the field of Visual Question Answering (VQA) on standard VQA datasets~.\nInitially, these datasets focused mainly on simple questions related to object identification and attributes, such as name, shape, color, and position. Towards the goal of general-purpose artificial intelligence, VQA models are expected to answer questions that require a deeper understanding of the world, fine-grained visual recognition, and multi-step reasoning. Recently, several additional VQA datasets~ have been introduced to challenge VQA systems to handle more complex questions. However, there are limitations associated with these datasets. Some datasets are entirely synthetic, while others rely heavily on manual human effort.\nIn this paper, we propose a new dataset called  , which was developed from our low-cost and scalable framework.   focuses on integrating external (world) knowledge associated with objects in the images and multi-hop reasoning. For example, in Figure , the question \"How tall is this church?\" requires not only the identification of the church but also additional specific facts, i.e. its height. And the question \"What is the capital of the country where this church is located?\" additionally requires multi-hop knowledge. Such information is often dispersed across multiple paragraphs in the training text, presenting considerable challenges for VQA models.\n\nOur main contributions are summarized as follows: (1) We introduce a new high quality VQA benchmark, called  , which consists of multi-hop questions that require external knowledge to answer;\n(2) We propose a scalable, low-cost framework for the construction of   without involving too much manual effort. The largest version of   comprises 4.2 million questions, making it one to two orders of magnitude larger than similar existing datasets. The quality of our dataset is verifiable via a user study. Moreover, the framework is designed to be scalable with respect to the number of input images; and, (3) We evaluated various state-of-the-art VQA models, including those based on foundation models, on  . Our experiments demonstrate that   presents substantial challenges for both current VQA models and multimodal methods, highlighting its potential for benchmarking and advancing the field of VQA.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "content/1_intro.tex",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.33,
      "diffusion_reasoning_score": 0.495,
      "distributed_training_score": 0.301,
      "datasets_score": 0.405,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a new VQA dataset with multi-hop reasoning and structural knowledge, but it does not mention or involve diffusion-based models, iterative refinement processes, or treating Chain-of-Thought as a holistic entity for reasoning. There is no component related to adapting diffusion models for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and introduction of a new VQA dataset (ReasonVQA), including its curation methodology, scalability, benchmarking of models, and comparison to existing datasets, which directly aligns with research on dataset creation, analysis, and evaluation in AI applications.",
      "summary": "The paper introduces ReasonVQA, a new benchmark dataset for Visual Question Answering (VQA) that integrates structured encyclopedic knowledge to generate complex, multi-hop questions using a scalable, low-cost framework. It evaluates state-of-the-art VQA models on this dataset, demonstrating significant challenges for current models in handling external knowledge and multi-step reasoning, while highlighting that ReasonVQA is over an order of magnitude larger than existing similar datasets.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new benchmark dataset and framework for multi-hop VQA with structural knowledge, advancing the state-of-the-art by automating question generation and scaling dataset size beyond previous efforts.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the VQA subfield due to its large scale and focus on complex reasoning, potentially improving model development in computer vision. However, its influence may be limited to specialized applications rather than broader fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a significant contribution to VQA research by providing a challenging new benchmark, making it valuable for researchers in computer vision and AI to understand advancements in multi-hop reasoning.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/7891df934c1ab88d2add1787b93465fce3f0a87e",
      "h_index_fetch_method": "full_id",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 4,
      "average_h_index": 2.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Duong T. Tran",
          "profile_url": "https://www.semanticscholar.org/author/2373483960",
          "h_index": 0
        },
        {
          "name": "Trung-Kien Tran",
          "profile_url": "https://www.semanticscholar.org/author/2374109975",
          "h_index": 0
        },
        {
          "name": "M. Hauswirth",
          "profile_url": "https://www.semanticscholar.org/author/2269896075",
          "h_index": 4
        },
        {
          "name": "Danh Le Phuoc",
          "profile_url": "https://www.semanticscholar.org/author/2257190712",
          "h_index": 4
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668144",
      "updated_at": "2025-08-11T23:44:59.738111",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16405",
      "title": "Self-Supervised Inductive Logic Programming",
      "authors": [
        "Stassa Patsantzis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Inductive Logic Programming (ILP) approaches like Meta \\-/ Interpretive\nLearning (MIL) can learn, from few examples, recursive logic programs with\ninvented predicates that generalise well to unseen instances. This ability\nrelies on a background theory and negative examples, both carefully selected\nwith expert knowledge of a learning problem and its solutions. But what if such\na problem-specific background theory or negative examples are not available? We\nformalise this question as a new setting for Self-Supervised ILP and present a\nnew MIL algorithm that learns in the new setting from some positive labelled,\nand zero or more unlabelled examples, and automatically generates, and labels,\nnew positive and negative examples during learning. We implement this algorithm\nin Prolog in a new MIL system, called Poker. We compare Poker to\nstate-of-the-art MIL system Louise on experiments learning grammars for\nContext-Free and L-System languages from labelled, positive example strings, no\nnegative examples, and just the terminal vocabulary of a language, seen in\nexamples, as a first-order background theory. We introduce a new approach for\nthe principled selection of a second-order background theory as a Second Order\nDefinite Normal Form (SONF), sufficiently general to learn all programs in a\nclass, thus removing the need for a backgound theory tailored to a learning\ntask. We find that Poker's performance improves with increasing numbers of\nautomatically generated examples while Louise, bereft of negative examples,\nover-generalises.",
      "published_date": "2025-07-22T09:57:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16405v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16405v1",
      "latex_url": "http://arxiv.org/src/2507.16405v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{table*}[t]\n  \n  \n  { }{1.6mm}\n  {tabularx}{0.99 }{cccccccc}\n  {8}{c}{ Self-supervised ILP with Poker }\n\n  \n Labelled examples $ {E^+{:}1^n0^n (n > 0)}$ &  {7}{c}{Unlabelled examples $ {E^?{:}1^n0^m (n   m   0)}$ (21 of 100 for $n   [0,18]$) }\n\n $s(1^1,0^1).$ & $s(1^0,0^0).$ & $s(1^3,0^0).$ & $s(1^6,0^0).$ & $s(1^9,0^9).$ & $s(1^7,0^7).$ & $s(1^6,0^6).$ & $s(1^5,0^3).$\n\n $s(1^2,0^2).$ & $s(1^1,0^0).$ & $s(1^4,0^0).$ & $s(1^7,0^0).$ & $s(1^8,0^8).$ & $s(1^6,0^1).$ & $s(1^5,0^1).$ & $s(1^5,0^5).$\n\n $s(1^3,0^3).$ & $s(1^2,0^0).$ & $s(1^5,0^0).$ & $s(1^8,0^0).$ & $s(1^7,0^1).$ & $s(1^6,0^2).$ & $s(1^5,0^2).$ & $s(1^4,0^1).$\n\n  {tabularx}\n  {tabular}{rlll}\n  {4}{c}{ $ {B:}$ First-order background theory }\n\n & $one([1|X], X).$ & $zero([0|X], X).$ & $empty(X, X).$\n\n  {tabular}\n  {tabularx}{ }{lX}\n  {2}{c}{ $ { {M}:}$ Second-Order Background Theory }\n\n (Identity) $P(x,y)   Q(x,y):$ & $target(P) $ $  \\; background(Q)   empty(Q) $\n\n (Chain) $P(x,y)   Q(x,z), R(z,y):$ & $P   Q $ $  \\; (target(P)   invented(P)) $ $  \\; not(target(Q)) $ $  \\; not(empty(Q,R)) $\n\n & $  \\; invented(P,Q)   P   Q $\n\n  \n  {tabularx}\n  {tabular}{llll}\n  {4}{c}{ Learned Hypothesis }\n\n With invented predicates ($s_1/2$): & $s(A,B)   one(A,C), zero(C,B).$ & $s(A,B)   s_1(A,C), zero(C,B).$\n & & $s_1(A,B)   one(A,C), s(C,B)).$\n\n Unfolded to remove invented predicates: & $s(A,B)   one(A,C), zero(C,B).$ & $s(A,B)   one(A,C), s(C,D), zero(D,B).$ &\n\n  {tabular}\n  {tabularx}{ }{lll|lllllll}\n  \n  {10}{c}{{ {5em}} Labelling (includes both automatically generated and user-given unlabelled examples) }\n\n  {3}{c}{ Labelled Positive } &  {7}{c}{ Labelled Negative (21 of 175) }\n\n $s(1^1,0^1).$ & $s(1^4,0^4).$ & $s(1^7,0^7).$ & $s(1^0,0^0).$ & $s(1^2,0^1).$ & $s(1^3,0^2).$ & $s(1^4,0^2).$ & $s(1^5,0^1).$ & $s(1^6,0^0).$ & $s(1^7,0^0).$\n\n $s(1^2,0^2).$ & $s(1^5,0^5).$ & $s(1^8,0^8).$ & $s(1^1,0^0).$ & $s(1^3,0^0).$ & $s(1^4,0^0).$ & $s(1^4,0^3).$ & $s(1^5,0^2).$ & $s(1^6,0^1).$ & $s(1^7,0^1).$\n\n $s(1^3,0^3).$ & $s(1^6,0^6).$ & $s(1^9,0^9).$ & $s(1^2,0^0).$ & $s(1^3,0^1).$ & $s(1^4,0^1).$ & $s(1^5,0^0).$ & $s(1^5,0^3).$ & $s(1^6,0^2).$ & $s(1^8,0^0).$\n\n  \n  {tabularx}\n  {Poker inputs and outputs. Example strings pretty-printed from DCG\n notation (e.g. $s([1,1,0,0],[])\n   s(1^2,0^2)$).}\n\n {table*}\n\nIn the Standard Setting for Inductive Logic\nProgramming (ILP) a background theory $B$ and positive and\nnegative examples $E^+, E^-$ are used as training data to learn a correct\nhypothesis $H$ that accepts, in conjunction with $B$, all examples in $E^+$, and\nno examples in $E^-$. Typically, $E^+, E^-$ are selected, and $B$ programmed,\nmanually by the user according to their background knowledge of the learning\ntarget (i.e. the logic program to be learned). $B$ in particular is tailored to\neach learning target , and $E^-$ hand-picked to avoid\nover-generalisation. Given the right $B,E^+$ and $E^-$, recent ILP systems can\nlearn correct hypotheses with recursive clauses and invented predicates.\n\nThe ability to include background knowledge in training data is a desirable\ncharacteristic of ILP systems, but the practice of manually creating a\ntarget-specific background theory and selecting negative examples is a constant\nburden that limits the real-world application of ILP approaches.\n\nIn this paper we present a new way to alleviate the aforementioned burden on the\nuser with a new algorithm for Self-Supervised ILP, more specifically,\nSelf-Supervised Meta-Interpretive Learning (MIL). The new\nalgorithm, implemented in a new MIL system called Poker {Named after,\nnot the game, but Wittgenstein's Poker; a friendly dig at Popper\n.}, is ``self-supervised\" in the sense that it learns from\nboth labelled positive and unlabelled examples and it automatically generates\nnew positive and negative examples during learning. Because it generates new\nnegative examples, Poker can learn from a background theory that is not tailored\nto the learning target without over-generalising. Poker returns not only a\nhypothesis, but also a labelling of unlabelled, and automatically\ngenerated, examples.\n\nWe illustrate the use of Poker in Table where\nPoker is given 3 positive, labelled examples, $E^+$, of the $\\{1^n0^n: n > 0\\}$\n($1^n0^n$) Context-Free Language (CFL), and 30 unlabelled examples, $E^?$ of the\n$\\{1^n0^m: n   m   0\\}$ ($1^n0^m$) CFL which includes $1^n0^n$ as a\nsubset. Examples are atoms in Definite Clause Grammars (DCG) notation\n representing strings of $1^n0^n$ and $1^n0^m$. The\nfirst-order background theory, $B$, consists of just the terminal vocabulary of\nboth languages, $\\{1,0, \\}$, defined as a set of DCG pre-terminals.\n$1^n0^n$ and $1^n0^m$ are more often denoted as $a^nb^n$ and $a^nb^m$\nrespectively, but replacing $a,b$ with $1,0$ makes it possible for $B$ to\nexpress any grammar with two terminal symbols suitably mapped to $1$ and $0$.\n$B$ can also be constructed automatically from $E^+,E^?$. A second-order\nbackground theory, $ {M}$, includes two metarules, Chain\nand Identity, Second-Order definite clauses with a set of constraints\nencoding a Second Order Definite Normal Form (SONF) {SONFs are\nformalised in the Framework Section.}. The background theory $B  \n {M}$ is thus sufficiently general to express, as a DCG, a Context-Free\nGrammar (CFG) of any bit-string CFL or Regular language, i.e. one with a\nvocabulary of at most two characters and $ $.\n\nThe maximal generality of $B    {M}$ in Table\n achieves two purposes. On the one hand it\nguarantees that $B    {M}$ is general enough to learn the target\ngrammar, i.e. a CFG of $1^n0^n$. On the other hand, $B    {M}$ is no\nlonger target specific: instead of being tailored to one learning target,\n$1^n0^n$, it can be reused to learn any bit-string CFG. At the same time, the\ngenerality of $B    {M}$ introduces a problem: in the absence of\nnegative examples, it is impossible to distinguish $1^n0^n$ from $1^n0^m$ (the\nlanguage of $E^?$). Indeed, without negative examples it is impossible to\ndistinguish any bit-string language from $\\{0,1\\}^*$ the maximally general\nlanguage of all bit-strings. In order to learn $1^n0^n$ without\nover-generalising, therefore, negative examples are necessary. Poker can\ngenerate negative examples automatically, thus avoiding over-generalisation.\n\nIt is also possible to avoid over-generalisation by learning a hypothesis that\nonly accepts $E^+$, i.e. over-fitting $E^+$. Poker returns a labelling of $E^?$\nwhich, in Table , include $1^n0^n$ strings that\nare not in $E^+$. In order to correctly label examples in $E^?$ Poker must\ntherefore learn a hypothesis that generalises at least to the $1^n0^n$ strings\nin $E^?$.\n\nIn summary, Poker's ability to automatically generate negative examples makes it\npossible to use a maximally general background theory that is no longer tailored\nto a single learning target. This ability frees the user from having to manually\nselect a background theory and negative examples for each new learning target.\n\nSelf-Supervised ILP by detection of contradictions\n\nThe intuition behind Poker's algorithm is that, if two atoms (in the First Order\nLogic sense) $e_1$ and $e_2$ are accepted by the same hypothesis $H$ (a logic\nprogram), i.e. $H   \\{e_1, e_2\\}$, then to assume that $e_1$ is a positive\nand $e_2$ a negative example of $H$ is a contradiction (in the informal sense).\nSuch a contradiction can be detected in the following manner. Suppose $T =\n\\{H_1, ..., H_m\\}$ is a set of hypotheses and $T   \\{e_1, e_2\\}$. And\nsuppose that we remove from $T$ each $H_i$, where $H_i   e_2$ leaving\nbehind the set $T'$ such that $T'     e_2$. There are now two\npossibilities: either $T'   e_1$, in which case there is no contradiction;\nor $T'     e_1$, in which case there is a contradiction: $e_1$ and\n$e_2$ are both accepted by some subset of $T$, now missing from $T'$; therefore,\n$e_2$ is a positive, not a negative, example of the subset of $T$ that accepts\n$e_1$.\n\nAccordingly, Poker begins by constructing a set $T$ of initial hypotheses that\naccept the labelled examples $E^+$. Poker can generate new unlabelled examples,\nadded to $E^?$, by executing $T$ as a generator. Poker then assumes that each\nunlabelled example $e^?   E^?$ is negative and removes from $T$ each\nhypothesis $H$ that accepts $e^?$ in conjunction with $B$. If the remaining $T$\nnow rejects any examples in $E^+$, $e^?$ is re-labelled as positive and moved to\n$E^+$. The labelling process thus iteratively specialises $T$ until it is\nconsistent with $E^+$. The labelling process is not without error but its\naccuracy increases monotonically with the cardinality of $E^?$.\n\nContributions\n\nWe make the following contributions:\n\n {itemize}\n   A new setting for Self-Supervised ILP.\n   A new MIL algorithm for Self-Supervised ILP, and a new MIL system,\n Poker, implementing the new algorithm in Prolog.\n   A definition of Second-Order Definite Normal Forms (SONFs), a new\n kind of second-order background theory for MIL sufficiently\n general to learn all programs in a class.\n   Two SONFs for CFGs and L-System grammars in DCG notation.\n   A proof that Poker's accuracy increases monotonically with the\n number of unlabelled examples.\n   Experiments investigating the effect of automatically generated\n examples on Poker's learning performance.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "aaai_26.tex",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.321,
      "distributed_training_score": 0.269,
      "datasets_score": 0.266,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a self-supervised ILP system, Poker, which programmatically generates positive and negative examples from unlabelled data, directly aligning with weak supervision. It trains models using automatically derived, potentially noisy labels rather than relying on perfectly hand-labeled data, as seen in its process of detecting contradictions and iteratively refining labels. This core mechanism reduces the need for expert-crafted examples, embodying the principles of weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper introduces a new self-supervised setting for Inductive Logic Programming (ILP), specifically Meta-Interpretive Learning (MIL), to address the limitations of traditional ILP that requires manually selected background theories and negative examples. The authors present a novel MIL algorithm implemented in a system called Poker, which learns from positive and unlabelled examples by automatically generating and labelling new positive and negative examples, using a general second-order background theory in the form of Second-Order Definite Normal Forms (SONFs); experiments show that Poker outperforms the state-of-the-art system Louise in learning grammars for Context-Free and L-System languages, with performance improving as more examples are generated, thus reducing over-generalization.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new self-supervised ILP setting and algorithm that advances the state-of-the-art by eliminating the need for expert-crafted background theories and negative examples, enabling more autonomous learning.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI and machine learning by making ILP more practical and applicable to real-world scenarios without manual intervention.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This is a high-quality paper with significant contributions to ILP, making it valuable for researchers in artificial intelligence and machine learning to understand advancements in self-supervised techniques.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ea4a775c510e7f4a0e5182ff836155ed1a58449f",
      "h_index_fetch_method": "full_id",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Stassa Patsantzis",
          "profile_url": "https://www.semanticscholar.org/author/2373066582",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668653",
      "updated_at": "2025-08-11T23:45:29.902838",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16406",
      "title": "Sparse-View 3D Reconstruction: Recent Advances and Open Challenges",
      "authors": [
        "Tanveer Younis",
        "Zhanglin Cheng"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Sparse-view 3D reconstruction is essential for applications in which dense\nimage acquisition is impractical, such as robotics, augmented/virtual reality\n(AR/VR), and autonomous systems. In these settings, minimal image overlap\nprevents reliable correspondence matching, causing traditional methods, such as\nstructure-from-motion (SfM) and multiview stereo (MVS), to fail. This survey\nreviews the latest advances in neural implicit models (e.g., NeRF and its\nregularized versions), explicit point-cloud-based approaches (e.g., 3D Gaussian\nSplatting), and hybrid frameworks that leverage priors from diffusion and\nvision foundation models (VFMs).We analyze how geometric regularization,\nexplicit shape modeling, and generative inference are used to mitigate\nartifacts such as floaters and pose ambiguities in sparse-view settings.\nComparative results on standard benchmarks reveal key trade-offs between the\nreconstruction accuracy, efficiency, and generalization. Unlike previous\nreviews, our survey provides a unified perspective on geometry-based, neural\nimplicit, and generative (diffusion-based) methods. We highlight the persistent\nchallenges in domain generalization and pose-free reconstruction and outline\nfuture directions for developing 3D-native generative priors and achieving\nreal-time, unconstrained sparse-view reconstruction.",
      "published_date": "2025-07-22T09:57:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16406v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16406v1",
      "latex_url": "http://arxiv.org/src/2507.16406v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Reconstructing three-dimensional (3D) scenes from two-dimensional (2D) images has been a central challenge in computer vision for decades. Early approaches, such as structure-from-motion (SfM) and Multiview Stereo (MVS), typically depended on dense, highly overlapping sets of images to achieve reliable results. However, in many real-world scenarios, such as robotics, augmented reality (AR), virtual reality (VR), autonomous navigation, and digital content creation, collecting such dense image datasets is often difficult or costly. Consequently, research has increasingly focused on sparse-view 3D reconstruction, where the goal is to produce accurate and detailed 3D models using only a small number of partially overlapping images.\n\n {figure}[htbp]\n  \n  [width= ]{figures/radar_chart.png}\n  {Comparative performance of leading sparse-view 3D reconstruction methods across six normalized metrics: Handling Sparse Inputs, Pose-Free Capability, Real-Time Performance, Efficiency, Generalizability, and Reconstruction Accuracy (all scores normalized to [0–1] scale, where 1.0 denotes highest performance).}\n\n {figure}\n\nWhile existing surveys have addressed broader aspects of 3D reconstruction or focused on specific techniques like 3D Gaussian Splatting for sparse views. To the best of our knowledge, no previous study has systematically analyzed the convergence of geometry-based, neural implicit, and generative (diffusion-based) approaches in sparse-view 3D reconstruction. Our survey addresses this gap by providing a unified framework and comparative evaluation of all leading classes of methods.\n\nSparse-view 3D reconstruction is inherently ambiguous because of limited input, leading to artifacts such as floaters, blurred textures, background collapse, and pose estimation ambiguity. This persistent 'chicken-and-egg' problem, which becomes particularly severe with limited input views, has shifted the research focus towards deep learning methods that can jointly optimize or bypass explicit pose estimation.\n\n {figure*}[h]\n  \n  [width= ]{figures/paper_structure.pdf}\n  {Structure of this survey: major topics and subtopics covered in sparse-view 3D reconstruction.}\n\n {figure*}\n\nDeep learning-based methods have recently led to significant advances in both reconstruction quality and robustness. Implicit neural representations, such as Neural Radiance Fields (NeRFs), and explicit representations, such as 3D Gaussian Splatting (3DGS), have driven much of this progress. NeRF, in particular, has had a major impact on sparse-view reconstruction by encoding scenes as continuous volumetric functions, enabling the synthesis of realistic novel views from only a handful of images. While early NeRF variants struggled with computational inefficiency and overfitting, newer methods have incorporated depth priors, geometric regularization, and semantic consistency. Collectively, these advances enable significantly improved results with fewer input views than those of prior studies.\n\n {figure*}[t]\n  \n  [width= ]{figures/review_protocol.pdf}\n  {Methodological Review Protocol outlining the systematic process of literature identification, screening, data extraction, categorization, and quality assessment used in this sparse-view 3D reconstruction survey.}\n\n {figure*}\n\n {figure*}[t]\n  \n  [width= ]{figures/paper_taxonomy.pdf}\n  {Taxonomy of sparse view 3D reconstruction methods by core categories.}\n\n {figure*}\n\nRecent advances in explicit representations, especially 3DGS, have resulted in substantial gains in computational efficiency and real-time rendering. By modeling scenes with Gaussian primitives, 3DGS allows for fast rasterization into images. New methods use depth-informed pruning and co-regularization to reduce overfitting and limit artifacts, particularly when input images are sparse. InstantSplat demonstrated that high-quality reconstructions can be completed in a few seconds. This demonstrates significant improvements in both the speed and robustness to errors in the camera pose.\n\nRecent studies have shown that diffusion-based generative models reduce ambiguity in sparse-view reconstruction by predicting the likely shapes and textures. Diffusion models, trained on extensive datasets, provide strong priors that improve the realism and consistency of both images and 3D outputs. Researchers have combined these generative models with NeRF and 3DGS in hybrid systems. This blending of explicit and implicit representations enables a better balance between quality, efficiency, and usability. Camera pose estimation is a central challenge in sparse-view 3D reconstruction, motivating the development of pose-free methods that directly recover geometry from uncalibrated images. Recent approaches such as InstantSplat, COLMAP-Free 3D Gaussian Splatting, and MV-DUSt3R+ exemplify this trend. These methods enable robust 3D reconstruction even in difficult image-capture scenarios.\n\nThis survey reviews recent advances in sparse-view 3D reconstruction, focusing on core technical challenges and how new methods—spanning geometric priors, diffusion models, and improved representations—have advanced the field. To contextualize this progression, figure~ illustrates the development and relative prominence of these categories over time. We also summarize the key performance benchmarks and discuss persistent problems. Finally, we outline promising research directions that may help address these issues. The overall structure of this study is illustrated in Figure . The main contributions of this review are as follows.\n {itemize}\n   Systematic Categorization: We organize recent sparse-view 3D reconstruction methods into geometry-based, neural implicit (NeRF), 3D Gaussian Splatting (3DGS), and hybrid classes, clearly outlining core mechanisms and limitations.\n   In-depth Analysis of 3DGS Methods: We present the most extensive and up-to-date review of 3D Gaussian Splatting techniques, including core, diffusion-integrated, and pose-free variants, with a focus on their effectiveness in sparse-view settings.\n   Integration of Generative Models: We analyze how diffusion models and vision foundation models (CLIP, SAM, DINO) are being leveraged to inject strong priors, enforce view consistency, and hallucinate plausible geometry from limited data.\n   Cross-paradigm Comparison: We provide critical comparisons across paradigms (SfM, NeRF, 3DGS, diffusion), evaluating their trade-offs in accuracy, efficiency, generalizability, and real-world applicability under sparse constraints.\n   Identification of Research Gaps: We outline unresolved challenges such as domain generalization, pose-free reconstruction, and efficient learning from minimal supervision, paving the way for future research directions.\n {itemize}\nThe complete review process, including the search strategy, literature selection, screening, data extraction, categorization, and quality assessment, is shown in figure~.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.359,
      "datasets_score": 0.332,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668152",
      "updated_at": "2025-08-11T23:43:05.606941",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16413",
      "title": "Towards Railway Domain Adaptation for LiDAR-based 3D Detection:\n  Road-to-Rail and Sim-to-Real via SynDRA-BBox",
      "authors": [
        "Xavier Diaz",
        "Gianluca D'Amico",
        "Raul Dominguez-Sanchez",
        "Federico Nesti",
        "Max Ronecker",
        "Giorgio Buttazzo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "In recent years, interest in automatic train operations has significantly\nincreased. To enable advanced functionalities, robust vision-based algorithms\nare essential for perceiving and understanding the surrounding environment.\nHowever, the railway sector suffers from a lack of publicly available\nreal-world annotated datasets, making it challenging to test and validate new\nperception solutions in this domain. To address this gap, we introduce\nSynDRA-BBox, a synthetic dataset designed to support object detection and other\nvision-based tasks in realistic railway scenarios. To the best of our\nknowledge, is the first synthetic dataset specifically tailored for 2D and 3D\nobject detection in the railway domain, the dataset is publicly available at\nhttps://syndra.retis.santannapisa.it. In the presented evaluation, a\nstate-of-the-art semi-supervised domain adaptation method, originally developed\nfor automotive perception, is adapted to the railway context, enabling the\ntransferability of synthetic data to 3D object detection. Experimental results\ndemonstrate promising performance, highlighting the effectiveness of synthetic\ndatasets and domain adaptation techniques in advancing perception capabilities\nfor railway environments.",
      "published_date": "2025-07-22T10:04:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16413v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16413v1",
      "latex_url": "http://arxiv.org/src/2507.16413v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "In recent years, the railway industry has increasingly invested in research efforts to achieve higher levels of automation.\nAccording to the IEC 62267 standard~, the Grade of Automation (GoA) defines the degree to which train operations are automated, ranging from GoA0, where all functions are manually performed by the driver, to GoA4, which represents a fully autonomous operation.\nWhile most railway systems in Europe currently operate up to GoA2, moving to GoA3 or GoA4 requires the integration of robust and accurate perception systems that comply with strict safety and reliability requirements of railway standards.\nVision-based tasks such as semantic segmentation and 2D/3D object detection are essential to build such perception capabilities (Figure~ illustrates a synthetic point cloud from the  ~dataset, showcasing 3D bounding box annotations for different object classes.).\nUnlike the automotive domain, gathering labeled multi-sensor data in the railway sector is challenging, due to safety and data protection regulations, as well as the high cost and time demands associated with data acquisition and manual labeling.\nAs a result, the railway domain lacks publicly available labeled datasets to train, validate, and benchmark new perception algorithms in this field.\n {figure}[H]\n \n [width=0.44 ]{img/pointcloud_full.pdf}\n {Synthetic point cloud sample from  . Points are colored in grey-scale based on their Z-values, while relevant object targets are colored according to their semantic class colors and enclosed within their corresponding 3D bounding boxes.}\n\n {figure}\nThis data scarcity significantly slows down the development and advancement of research in railway automation.\n {figure*}[!htb]\n \n [width=0.85 ]{img/imgcompare.pdf}\n {Samples from  ~showing a semantically segmented image on the left, which includes the proper legend for the classes, and an RGB image with relevant 2D bounding boxes on the right; both come from the same scene as the point cloud in Fig.~ and use the same semantic color mapping.}\n\n {figure*}\nTo overcome these limitations and integrate well-performing perception methods in a railway setting, simulations and domain adaptation techniques introduce promising solutions.\nIn the automotive field, simulators are widely used to generate annotated datasets, including rare and dangerous corner case scenarios~.\nMoreover, domain adaptation methods have proven effective in transferring models trained on synthetic data to real-world applications~.\nThese techniques can similarly be used to support the railway sector, both by adapting models trained on synthetic data to real railway environments and by transferring knowledge from automotive datasets to railway-specific scenarios.\n\nIn contrast to the automotive and robotics sectors, where tools like CARLA~, AirSim~, or NVIDIA Drive Sim~ are commonly used to generate synthetic data, the railway domain has far fewer options ~.\nAmong the few recent efforts, Iglesias et al.~ present a CARLA-based approach for generating synthetic railway data, however, since the work is still under review, it cannot be used to test the proposed methods.\n\nEven though domain adaptation methods can help bridge the gap between domains (both in sim-to-real and automotive-to-railway scenarios) the following challenges remain:\n {enumerate}\n   The sim-to-real domain shift often limits the direct transferability of models trained on synthetic data to real-world environments, since simulations usually introduce \"perfect\" data, when in the real-world data noise is commonplace.\n   While automotive datasets typically focus on dense, urban settings, railway environments are often open-field, sparse, and operate on different spatial and semantic scales, further complicating adaptation.\n {enumerate}\nGiven the synthetic data and domain adaptation challenges, the contribution of this paper is threefold:\n {enumerate}\n   We introduce  , an extension of the synthetic dataset  ~, which includes camera, depth and LiDAR data, along with multiple annotations, to support the evaluation of vision-based algorithms in railway environments.\n To the best of our knowledge,  ~is the first publicly available synthetic dataset that supports both 2D and 3D object detection and semantic segmentation tasks in this domain.\n Figure~ presents an example RGB image alongside its corresponding semantic segmentation, both captured from a virtual environment generated within the  ~framework.\n   We apply and adapt a state-of-the-art semi-supervised domain adaptation approach for 3D point clouds (SSDA3D) to evaluate both the transferability of models trained on  ~to real railway data and the impact of incorporating automotive data (Waymo) for cross-domain adaptation.\n   We provide an analytical evaluation of how synthetic railway data, real-world automotive data, and their combination influence domain adaptation performance when transferring to a real-world railway dataset (OSDaR23). Moreover the original SSDA3D focused only on the car class, but we report results also for pedestrian detection since that is the most vulnerable traffic actor.\n {enumerate}\n\nThe rest of the paper is organized as follows: Section~ presents the related work in this field;\nSection~ introduces the  ~dataset;\nSection~ describes our optimized SSDA3D domain adaptation method;\nSection~ presents the experiments carried out to\nevaluate the proposed domain adaptation framework across various training setups; and Section~ states the conclusions and future work.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "01-introduction.tex",
      "rlhf_score": 0.321,
      "weak_supervision_score": 0.361,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.378,
      "datasets_score": 0.434,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the introduction of SynDRA-BBox, a new synthetic dataset tailored for 2D and 3D object detection in railway environments, including annotations for camera, depth, and LiDAR data. It details dataset creation methodologies, such as generation in a virtual framework, and evaluates its utility through domain adaptation experiments, benchmarking performance on real-world datasets like OSDaR23. This directly aligns with research on creating, analyzing, and benchmarking datasets for machine learning and AI applications.",
      "summary": "The paper addresses the scarcity of annotated datasets in the railway domain by introducing SynDRA-BBox, a synthetic dataset tailored for 2D and 3D object detection in realistic railway scenarios. It adapts a state-of-the-art semi-supervised domain adaptation method (SSDA3D) from automotive applications to railway contexts, demonstrating promising experimental results in transferring synthetic data to real-world 3D object detection, thereby advancing perception capabilities for automated train operations.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing the first synthetic dataset specifically for railway 3D detection and adapting an existing domain adaptation technique to this new domain, combining ideas in a clever way to address a known problem. However, it does not introduce a entirely new architecture or technique, making it incremental rather than groundbreaking.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within the subfield of railway automation and computer vision, as the publicly available dataset could facilitate further research in domain adaptation for transportation. Nonetheless, its influence may remain limited to niche applications in railways rather than broadly affecting other areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper provides a valuable contribution by offering a new dataset and adapted methods for railway perception, which is essential for researchers in automated transportation systems. While insightful, it is not universally critical, making it important but not mandatory for those outside the specific domain.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/01a88f5273198e5c7bcb341116c604e62d7fb598",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 2.6666666666666665,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Xavier Diaz",
          "profile_url": "https://www.semanticscholar.org/author/2373095441",
          "h_index": 0
        },
        {
          "name": "G. D’Amico",
          "profile_url": "https://www.semanticscholar.org/author/2148898699",
          "h_index": 4
        },
        {
          "name": "Raul Dominguez-Sanchez",
          "profile_url": "https://www.semanticscholar.org/author/2373098329",
          "h_index": 0
        },
        {
          "name": "F. Nesti",
          "profile_url": "https://www.semanticscholar.org/author/39421702",
          "h_index": 8
        },
        {
          "name": "M. Ronecker",
          "profile_url": "https://www.semanticscholar.org/author/1395740708",
          "h_index": 2
        },
        {
          "name": "Giorgio C. Buttazzo",
          "profile_url": "https://www.semanticscholar.org/author/2281149460",
          "h_index": 2
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669058",
      "updated_at": "2025-08-11T23:45:50.184762",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16414",
      "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based\n  Detection Framework",
      "authors": [
        "Hongyi Tang",
        "Zhihao Zhu",
        "Yi Yang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs.",
      "published_date": "2025-07-22T10:05:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16414v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16414v1",
      "latex_url": "http://arxiv.org/src/2507.16414v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The effectiveness of large language models (LLMs) hinges significantly on their training corpus . However, these pre-training corpora may contain copyrighted material or private user information , raising substantial concerns about compliance and privacy. For example, The New York Times recently filed a lawsuit against OpenAI, alleging illegal use of its articles as training data for ChatGPT  {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html}. Furthermore, LLMs can inadvertently acquire undesirable knowledge from their training data, such as biased or harmful content , compromising the trustworthiness of the language model. Precise knowledge of the learned data is therefore crucial. However, determining whether a model has incorporated specific data remains challenging. This leads to a critical question: given an LLM and a text sample, how can we determine if this text was part of the LLM's pre-training? This is the pre-training data detection (PDD) problem.\n\nExisting PDD algorithms suffer from two primary limitations: 1) Superficial Information Reliance: Most algorithms focus on surface-level features of LLMs . For instance, Loss Attack uses the LLM's prediction loss on a given text, while Min-K% Prob uses predictive probabilities of tokens. This approach limits detection effectiveness, resulting in insufficient performance and high false positive rates, rendering them unsuitable for applications such as copyright verification . 2) Benchmark Time Drift: Due to the confidentiality of LLM training data , researchers often use release dates to infer training data, comparing it to publicly available datasets like Wikipedia. For example, pre-2023 data might be considered training data for a 2023 LLaMA model, while post-2023 data is viewed as non-training data . This temporal bias complicates the accurate evaluation of PDD methods intended to identify training corpora.\n\nTo address the first limitation, we introduce NA-PDD, a novel PDD algorithm that utilizes neuronal activation patterns within LLMs. Our method stems from the observation that training text activates different neurons within an LLM compared to non-training text. NA-PDD is particularly suitable for copyright verification in open-source LLMs or internal audits. It utilizes a small set of reference corpora to record neuronal activation for both training and non-training data. Neurons predominantly activated by training data are labeled as \"member\" neurons, while those activated by non-training data are labeled as \"non-member\" neurons. We then design a straightforward detection algorithm to determine whether a given sample $x$ was part of the LLM's pre-training corpus. During model inference with input $x$, we record the activation states of neurons across different layers and provide PDD predictions based on the relative prominence of member neurons in these layers.\n\nTo address the second limitation, we introduce CCNewsPDD, a time-drift-free PDD benchmark based on the CCNews dataset. This benchmark ensures temporal alignment between training and non-training data. To make sure that non-training data was not used in pre-training, we apply transformations such as back translation, masking, and LLM rewriting to the original non-training data. These transformations introduce meaningful variations while maintaining a rigorous definition of non-training data.\n\nOur contributions are as follows:\n {itemize}\n   We introduce NA-PDD, a novel PDD algorithm leveraging neuronal activation patterns within LLMs. NA-PDD analyzes the differential activation between trained and non-trained samples during inference to construct an effective PDD algorithm.\n   We introduce CCNewsPDD, a time-drift-free PDD benchmark. Using data transformation methods, CCNewsPDD ensures no temporal distribution differences between training and non-training data while maintaining semantic and lexical coherence.\n   We evaluate NA-PDD against nine representative PDD methods on CCNewsPDD and two public benchmarks. Our results demonstrate substantial improvements. For example, on OPT-6.7B with CCNewsPDD, NA-PDD outperforms DC-PDD by 27.9% AUC points (increasing from 71.8% to 99.7%).\n {itemize}\n\n {figure*}[!ht]\n  \n  [width= ]{images/Main2.png}\n  {An overview of NA-PDD.}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "zhihao.tex",
      "rlhf_score": 0.42,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.432,
      "datasets_score": 0.424,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on detecting pre-training data in LLMs using neuron activation patterns, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper's main contribution is a detection framework for pre-training data, not a method for training models with noisy or programmatically generated labels, despite mentioning data transformations for benchmarks.",
      "diffusion_reasoning_justification": "The paper deals with neuron activation for data detection in LLMs and does not involve diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper addresses post-training data detection, not techniques for parallel computing, data partitioning, or accelerating model training across multiple nodes.",
      "datasets_justification": "The paper introduces and evaluates a new benchmark (CCNewsPDD) for pre-training data detection, including dataset creation, transformations, and benchmarking, which directly aligns with research on datasets for AI applications.",
      "summary": "This paper addresses the challenges of identifying whether specific data was included in the pre-training corpus of large language models (LLMs) to mitigate legal, ethical, and bias-related concerns by introducing NA-PDD, a novel algorithm that analyzes differential neuron activation patterns between training and non-training data during inference. It also proposes CCNewsPDD, a temporally unbiased benchmark using data transformations to ensure fair evaluation, and demonstrates through experiments that NA-PDD significantly outperforms existing methods across multiple benchmarks, achieving up to 27.9% improvement in AUC points.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by leveraging differential neuron activation patterns for pre-training data detection, which advances the state-of-the-art beyond superficial feature-based methods. This represents a significant innovation in addressing the limitations of existing PDD approaches.",
      "impact_score": "High",
      "impact_justification": "The work could influence a wide range of future research and applications in AI ethics, copyright verification, and model auditing by providing a more reliable method for detecting training data in LLMs. Its potential to address real-world legal and privacy issues makes it highly relevant for both academia and industry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution to AI research by introducing an effective new method for a pressing ethical issue, making it valuable for researchers and practitioners in machine learning and AI ethics. While not groundbreaking in every aspect, its practical implications warrant attention from the relevant community.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/5b4ce13f5be5704e55c81d107b38dadb779e3025",
      "h_index_fetch_method": "title_search",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 21,
      "average_h_index": 5.333333333333333,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Aftab Hussain",
          "profile_url": "https://www.semanticscholar.org/author/2082388485",
          "h_index": 6
        },
        {
          "name": "Md Rafiqul Islam Rabin",
          "profile_url": "https://www.semanticscholar.org/author/2367340743",
          "h_index": 0
        },
        {
          "name": "Toufique Ahmed",
          "profile_url": "https://www.semanticscholar.org/author/2271468417",
          "h_index": 3
        },
        {
          "name": "Mohammad Amin Alipour",
          "profile_url": "https://www.semanticscholar.org/author/1962253",
          "h_index": 21
        },
        {
          "name": "Bowen Xu",
          "profile_url": "https://www.semanticscholar.org/author/2271682103",
          "h_index": 2
        },
        {
          "name": "Stephen Huang",
          "profile_url": "https://www.semanticscholar.org/author/2367345698",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667818",
      "updated_at": "2025-08-11T23:44:43.122826",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16427",
      "title": "Combined Image Data Augmentations diminish the benefits of Adaptive\n  Label Smoothing",
      "authors": [
        "Georg Siedel",
        "Ekagra Gupta",
        "Weijia Shao",
        "Silvia Vock",
        "Andrey Morozov"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Soft augmentation regularizes the supervised learning process of image\nclassifiers by reducing label confidence of a training sample based on the\nmagnitude of random-crop augmentation applied to it. This paper extends this\nadaptive label smoothing framework to other types of aggressive augmentations\nbeyond random-crop. Specifically, we demonstrate the effectiveness of the\nmethod for random erasing and noise injection data augmentation. Adaptive label\nsmoothing permits stronger regularization via higher-intensity Random Erasing.\nHowever, its benefits vanish when applied with a diverse range of image\ntransformations as in the state-of-the-art TrivialAugment method, and excessive\nlabel smoothing harms robustness to common corruptions. Our findings suggest\nthat adaptive label smoothing should only be applied when the training data\ndistribution is dominated by a limited, homogeneous set of image transformation\ntypes.",
      "published_date": "2025-07-22T10:21:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16427v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16427v1",
      "latex_url": "http://arxiv.org/src/2507.16427v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Vision models have long surpassed human accuracy on tasks such as image classification . A key success factor to their ability to learn general and robust representations of data is data augmentation, which enriches training data diversity by applying controlled transformations to images or labels . Among image data augmentations, very aggressive transformations have proved effective in methods such as TrivialAugment . However, excessively transforming images can lead to information loss in the image (see Figure ). In this case, a model will learn confident labels from uncertain information, which could lead to its miscalibration . Soft augmentation addresses this issue by combining image augmentation with adaptive label smoothing. The method scales down label confidences the higher the magnitude of image transformations become. Specifically applied to random cropping augmentations, this enabled more aggressive crops during training without overfitting or model miscalibration.\n\nIn this work, we extend adaptive label smoothing to well-established, aggressive image augmentation strategies like TrivialAugment and Random Erasing to further enhance their efficacy, as demonstrated by the image examples in Figure .\nOur contributions can be summarized as follows: {Code available at  {https://github.com/Georgsiedel/soft_label_random_augmentation}{https://github.com/georgsiedel/soft\\_label\\_random\\_augmentation}}\n\n {itemize}[wide]\n   We integrate adaptive label smoothing with TrivialAugment. We model magnitude-confidence mapping functions for its various transformation types from human vision studies, proxy networks, and image similarity metrics.\n   Through image classification experiments, we show that adaptive label smoothing enables more aggressive parameter settings for Random Erasing and can improve the performance of noise injection data augmentation.\n   We demonstrate that the benefit of adaptive label smoothing vanishes when applied across a heterogeneous set of augmentations as in TrivialAugment, which constrains its practical applicability.\n {itemize}\n\n {figure}[t]\n  \n  [width= ,trim={10 118 10 8},clip]{figures/example_TIN.png}\n  {TinyImageNet images transformed with soft TrivialAugment and soft Random Erasing (RE) data augmentation. The titles display the TrivialAugment transformation type and whether RE is applied, as well as the label and its softened confidence. The confidence is calculated as a function of the augmentation severity for every transformation type individually. Here, the functions are derived from a proxy models average accuracy on that transformation type and severity, as can be seen from Figure .}\n\n {figure}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.362,
      "weak_supervision_score": 0.48,
      "diffusion_reasoning_score": 0.365,
      "distributed_training_score": 0.36,
      "datasets_score": 0.338,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on adaptive label smoothing as a regularization technique for image augmentations, which involves programmatically adjusting label confidence based on augmentation severity, introducing a form of label noise. While this shares some conceptual overlap with weak supervision's use of noisy or imprecise labels, the paper is primarily about enhancing supervised learning through augmentations, not generating labels from high-level sources. Thus, the connection is indirect and not central to weak supervision.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669067",
      "updated_at": "2025-08-11T23:43:05.607110",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16429",
      "title": "Robust Noisy Pseudo-label Learning for Semi-supervised Medical Image\n  Segmentation Using Diffusion Model",
      "authors": [
        "Lin Xi",
        "Yingliang Ma",
        "Cheng Wang",
        "Sandra Howell",
        "Aldo Rinaldi",
        "Kawal S. Rhode"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Obtaining pixel-level annotations in the medical domain is both expensive and\ntime-consuming, often requiring close collaboration between clinical experts\nand developers. Semi-supervised medical image segmentation aims to leverage\nlimited annotated data alongside abundant unlabeled data to achieve accurate\nsegmentation. However, existing semi-supervised methods often struggle to\nstructure semantic distributions in the latent space due to noise introduced by\npseudo-labels. In this paper, we propose a novel diffusion-based framework for\nsemi-supervised medical image segmentation. Our method introduces a constraint\ninto the latent structure of semantic labels during the denoising diffusion\nprocess by enforcing prototype-based contrastive consistency. Rather than\nexplicitly delineating semantic boundaries, the model leverages class\nprototypes centralized semantic representations in the latent space as anchors.\nThis strategy improves the robustness of dense predictions, particularly in the\npresence of noisy pseudo-labels. We also introduce a new publicly available\nbenchmark: Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV),\nwhich provides detailed, manually annotated segmentation ground truth for\nmultiple anatomical structures in X-ray angiography videos. Extensive\nexperiments on the EndoScapes2023 and MOSXAV datasets demonstrate that our\nmethod outperforms state-of-the-art medical image segmentation approaches under\nthe semi-supervised learning setting. This work presents a robust and\ndata-efficient diffusion model that offers enhanced flexibility and strong\npotential for a wide range of clinical applications.",
      "published_date": "2025-07-22T10:21:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16429v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16429v1",
      "latex_url": "http://arxiv.org/src/2507.16429v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Medical image segmentation, which involves pixel-wise classification, is a critical dense prediction task that plays a vital role in enhancing the accuracy of disease diagnosis, monitoring, and assessment. In recent years, learning-based approaches have significantly outperformed traditional methods, with fully supervised image segmentation achieving higher accuracy due to the abundance of annotated data. In other words, large-scale image datasets with manual annotations are typically required to train robust and generalizable deep neural networks for dense prediction tasks, e.g., image segmentation. However, obtaining pixel-wise annotations is challenging, as it both time-consuming and labor-intensive. Such a massive annotation cost has motivated the community to develop semi-supervised learning methods .\n\nGiven that unlabeled data is typically abundant in practice, semi-supervised learning has emerged as a compelling approach for medical image segmentation. It leverages limited labeled data in conjunction with large amounts of pseudo-labeled data to iteratively train the segmentation model . In this paradigm, pseudo labels are generated for unlabeled images using a reliable model, and these pseudo-labeled samples are effectively incorporated into training by minimizing their prediction entropy. Nevertheless, semantic misalignment remains a significant challenge in semi-supervised learning. Most existing methods rely primarily on consistency regularization and auxiliary supervision at the output mask level, implicitly enforcing semantic consistency under perturbations (e.g., pseudo labels). However, these approaches often result in overlapping semantic representations in the latent space and overlook distinct semantic boundaries, thereby limiting generalization.\n\nTo address the above issue, some previous works have proposed modeling data distribution with deep generative models, including Variational Autoencoders (VAEs) , Generative Adversarial Networks (GANs) , and specialized medical image segmentation diffusion models . Among these generalist approaches, diffusion models have demonstrated significant potential in alleviating this problem by formulating complex data distributions probabilistically. A prominent branch treats dense prediction tasks as a label-denoising problem , employing variational inference to progressively generate predictions from noisy data. While diffusion models are proven capable of capturing the underlying distribution of each semantic category, their full potential to distinctly shape the latent structure of semantic labels remains to be discovered. The precise delineation of semantic boundaries and distinct domains within each semantic representation is crucial for improving overall performance in semi-supervised dense prediction tasks by eliminating ambiguity. These observations motivate our investigation into whether diffusion-based deep generative models can accurately capture and align the precise distribution of semantic labels, enabling the progressive rectification of pseudo-labels for enhanced supervision.\n\nIn this paper, we propose a novel diffusion model for semi-supervised medical image segmentation. Specifically, we first use embedding layers to encode dense labels and map these features into a semantic latent space via project layers. To structure the latent space, we introduce explicit class prototypes as fixed anchors in the embedding space and directly optimize the feature representations using a contrastive loss. These non-learnable prototypes guide the model beyond simply optimizing for prediction accuracy. The denoised embedding features are then processed by a diffusion decoder, guided by visual conditions.\n\nOur main contributions are as follows: 1) We propose a novel diffusion-based framework for semi-supervised medical image segmentation, which performs dense label denoising through a diffusion process by reformulating dense prediction as a label denoising task. 2) We introduce a prototype-anchored contrastive learning to structure the latent space of semantic labels, enhancing the quality and robustness of segmentation predictions. 3) We present a new benchmark dataset, Multi-Object Segmentation in X-ray Angiography Videos (MOSXAV)  {https://github.com/xilin-x/MOSXAV}, which focuses on the segmentation of multiple objects in X-ray angiography videos and provides high-quality, manually annotated ground truth labels.\n\nTo validate these contributions, the proposed method is evaluated on one public benchmark and a newly collected benchmark dataset. Experimental results demonstrate that our algorithm outperforms state-of-the-art (SoTA) medical image segmentation methods under the semi-supervised learning setting.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "manuscript.tex",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.46,
      "diffusion_reasoning_score": 0.497,
      "distributed_training_score": 0.342,
      "datasets_score": 0.351,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves using pseudo-labels generated programmatically from unlabeled data, which are noisy and imprecise, to train a segmentation model alongside limited annotated data. This directly aligns with weak supervision, as it relies on high-level, automatically generated labels rather than perfect hand-labeled ones, enhancing data efficiency in medical image segmentation.",
      "diffusion_reasoning_justification": "The paper applies diffusion models for iterative refinement in image segmentation and label denoising, but it does not involve multi-step logical reasoning, chain-of-thought processes, or solving complex logical tasks. It focuses solely on generative modeling for perceptual predictions, not holistic reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper proposes a novel diffusion-based framework for semi-supervised medical image segmentation to address the challenges of noisy pseudo-labels, by incorporating prototype-based contrastive consistency to structure semantic representations in the latent space. The methodology involves encoding dense labels, using class prototypes as anchors for optimization via contrastive loss, and applying a diffusion decoder for denoising, while also introducing a new benchmark dataset, MOSXAV, for multi-object segmentation in X-ray angiography videos. Experimental results on the EndoScapes2023 and MOSXAV datasets demonstrate that the approach outperforms state-of-the-art methods, offering improved robustness and potential for clinical applications.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by integrating diffusion models with prototype-based contrastive learning to structure latent spaces and handle noisy pseudo-labels, significantly advancing semi-supervised medical image segmentation. This represents a meaningful departure from existing methods that primarily rely on consistency regularization.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence future research in semi-supervised learning and medical imaging by providing a robust framework for handling limited annotations, and the introduction of the MOSXAV dataset could standardize evaluations in this subfield. Its applications in clinical diagnostics suggest broader real-world implications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper presents a high-quality contribution with innovative methods and a new dataset that advances medical image segmentation, making it essential for researchers in computer vision and healthcare to be aware of for potential applications. However, it may not be groundbreaking enough to be considered must-read for all audiences.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6f6142d5a728d1fc054780850c8141fd10f2fda0",
      "h_index_fetch_method": "title_search",
      "total_authors": 1,
      "authors_found": 1,
      "highest_h_index": 3,
      "average_h_index": 3.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Heejoon Koo",
          "profile_url": "https://www.semanticscholar.org/author/1782879110",
          "h_index": 3
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668166",
      "updated_at": "2025-08-11T23:45:04.527804",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16430",
      "title": "Beyond Algorethics: Addressing the Ethical and Anthropological\n  Challenges of AI Recommender Systems",
      "authors": [
        "Octavian M. Machidon"
      ],
      "categories": [
        "cs.CY (Computers and Society)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, I examine the ethical and anthropological challenges posed by\nAI-driven recommender systems (RSs), which have become central to shaping\ndigital environments and social interactions. By curating personalized content,\nRSs do not merely reflect user preferences but actively construct individual\nexperiences across social media, entertainment platforms, and e-commerce.\nDespite their ubiquity, the ethical implications of RSs remain insufficiently\nexplored, even as concerns over privacy, autonomy, and mental well-being\nintensify. I argue that existing ethical approaches, including algorethics, the\neffort to embed ethical principles into algorithmic design, are necessary but\nultimately inadequate. RSs inherently reduce human complexity to quantifiable\ndimensions, exploit user vulnerabilities, and prioritize engagement over\nwell-being. Addressing these concerns requires moving beyond purely technical\nsolutions. I propose a comprehensive framework for human-centered RS design,\nintegrating interdisciplinary perspectives, regulatory strategies, and\neducational initiatives to ensure AI systems foster rather than undermine human\nautonomy and societal flourishing.",
      "published_date": "2025-07-22T10:22:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16430v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16430v1",
      "latex_url": "http://arxiv.org/src/2507.16430v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The rapid advancement of AI is reshaping the fabric of society, permeating everyday life and fundamentally altering human interactions. From social networks to digital services, AI-driven systems increasingly mediate our experiences, structuring our choices and minimizing spontaneity. Among these, AI-powered recommender systems (RSs) play a particularly pervasive role, influencing how we consume content, interact online, and make decisions. By tailoring suggestions to user preferences, RSs do not merely reflect individual choices but actively shape them, reinforcing certain behaviors while suppressing others. As a result, human engagement with the digital world becomes increasingly algorithmic, reducing direct and spontaneous experiences.\n\nAgainst this backdrop, Pope Francis’s message for the 57th World Day of Peace (January 1, 2024), titled Artificial Intelligence and Peace, offers a timely reflection on AI’s ethical challenges~ {francis2024}. He warns against the use of algorithms to manipulate mental and social behaviors for commercial or political gain, arguing that AI’s impact extends beyond its technical design to the intentions of its creators and users. Stressing the need for ethical oversight, he underscores key values such as inclusivity, transparency, fairness, privacy, and reliability. The Pope’s concerns align with the broader concept of algorethics, introduced by Paolo Benanti, an AI advisor to the Vatican~ {benanti2023urgency}. This approach calls for ethically guided algorithmic development that integrates human values at every stage—from research and design to deployment and governance. Crucially, Pope Francis highlights that AI ethics must go beyond technological considerations, encompassing anthropological, educational, social, and political dimensions.\n\nThe ethical concerns raised by Pope Francis apply directly to AI recommender systems, which have become integral to digital life. RSs influence user engagement across various platforms, from personalized social media feeds and YouTube recommendations to TikTok’s For You Page, which dominates user interaction~ {narayanan2023understanding}. Beyond entertainment, RSs shape decisions in domains such as e-commerce (Amazon’s product recommendations) and news consumption (Google’s algorithmic news feeds, which impact over three billion users) {valentine2023recommender}. Given the increasing entanglement of human activity and AI-driven recommendations—alongside proprietary and privacy concerns that limit external scrutiny—there is an urgent need for a more thorough examination of the ethical and anthropological risks these systems pose.\n\nThis paper analyzes the impact of AI recommender systems on human users, advocating for greater awareness of the ethical and anthropological challenges they present. It explores approaches for fostering human-centered design in recommendation algorithms to promote safer, more ethical, and responsible human-AI interactions. The paper makes the following key contributions:\n\n {itemize}\n  A comprehensive ethical analysis: Examining the core ethical risks of RSs, including privacy concerns, autonomy erosion, and behavioral manipulation.\n  An exploration of their anthropological impact: Investigating how RSs shape human behavior and social interactions, reinforcing algorithmically curated experiences.\n  A critique of algorethics and the limits of technical solutions: Arguing that while ethical AI design is essential, technical solutions alone cannot resolve the deeper ethical dilemmas of RSs.\n  A call for policies, regulation, and education: Advocating for legal accountability, oversight mechanisms, and educational initiatives to promote responsible AI development and use.\n  A theoretical framework for human-centered AI: Proposing an interdisciplinary approach that integrates ethics, anthropology, and technology studies to move beyond algorethics and guide the design and governance of ethical RSs.\n {itemize}\n\nThe paper proceeds as follows. Section~ provides an overview of AI recommender systems and their key ethical challenges. Section~ examines their anthropological impact, while Section~ discusses the role of policy, regulation, and education in mitigating ethical risks. Section~ critically assesses the concept of algorethics and argues for an interdisciplinary approach that goes beyond technical solutions. Next, Section~ introduces a theoretical framework for the development of safer, human-centered AI recommender systems, and Section~ concludes the paper.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "tex/introduction.tex",
      "rlhf_score": 0.48,
      "weak_supervision_score": 0.315,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.31,
      "datasets_score": 0.386,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on the ethical, anthropological, and societal challenges of AI recommender systems, including privacy, autonomy, and the need for human-centered design frameworks. It does not discuss or involve technical aspects of reinforcement learning, such as training models with human-ranked data to align AI with preferences. There is no mention of RLHF methods, making the paper's contributions unrelated to this specific topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668661",
      "updated_at": "2025-08-11T23:43:05.607038",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16434",
      "title": "From model-based learning to model-free behaviour with Meta-Interpretive\n  Learning",
      "authors": [
        "Stassa Patsantzis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A \"model\" is a theory that describes the state of an environment and the\neffects of an agent's decisions on the environment. A model-based agent can use\nits model to predict the effects of its future actions and so plan ahead, but\nmust know the state of the environment. A model-free agent cannot plan, but can\nact without a model and without completely observing the environment. An\nautonomous agent capable of acting independently in novel environments must\ncombine both sets of capabilities. We show how to create such an agent with\nMeta-Interpretive Learning used to learn a model-based Solver used to train a\nmodel-free Controller that can solve the same planning problems as the Solver.\nWe demonstrate the equivalence in problem-solving ability of the two agents on\ngrid navigation problems in two kinds of environment: randomly generated mazes,\nand lake maps with wide open areas. We find that all navigation problems solved\nby the Solver are also solved by the Controller, indicating the two are\nequivalent.",
      "published_date": "2025-07-22T10:28:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16434v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16434v1",
      "latex_url": "http://arxiv.org/src/2507.16434v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[t]\n  \n  {tabular}{ll}\n  [Model-based Solver ]{ [width=0.29 ]{solver_cave_small_1.png}} &\n  \n  [Model-free Controller ]{ [width=0.29 ]{controller_cave_small_1.png}}\n\n  {tabular}\n  {A model-based Solver can predict the effect of its future\n actions and plan ahead, but requires knowledge of its environment. A\n model-free Controller cannot plan and must explore its environment but\n can act without knowledge of the environment. Green tiles: passable; red\n tiles: unpassable; S: start; E: goal.}\n {figure}\n\nA model is a theory that describes the possible states of an environment\nand the way in which the state of the environment changes as a result of an\nagent's actions. An environment may be an abstract domain such as the set\nof all lists or the Cartesian plane, a virtual environment, such as a computer\nsimulation or game, or a physical environment such as a real-world location. An\naction is a predicate that relates a description of the current state of\nan environment to a new state; in other words, a transition relation over\nstates. An agent is a program that generates a sequence of actions\nconnecting an initial state to a desired goal state; what we call a plan.\nTo generate such a sequence of actions is to solve a planning problem. A\nmodel-based agent is an agent that solves planning problems given a\nmodel. A model-free agent is an agent that selects actions without a\nmodel and, therefore, without a plan. We call a model-based agent a \"solver for\nplanning problems\" or, simply, solver and a model-free agent a\ncontroller.\n\nFigure illustrates the difference between\nplanning by a model-based Solver and acting by a model-free Controller. In the\nFigure the environment is a map of a lake with islands (alternatively a cave\nsystem) where red tiles represent unpassable locations, green tiles passable\nlocations and the letters $S$ and $E$ a starting and goal location respectively.\nYellow arrows show agent trajectories.\n\nPlanning with a model implies some knowledge of the structure of an environment.\nThe Solver in Figure is initially given a map of the\nenvironment, which it can fully observe, and uses this map to generate the set\nof all possible moves between adjacent, passable map tiles. The Solver then uses\nthis navigation model to plan a direct path from $S$ to $E$. The Controller in\nFigure is not given the map of the environment, cannot\ndirectly observe $S$ or $E$, and can only observe whether map tiles adjacent to\nits current location are passable or unpassable. The Controller explores its\nenvironment to find a path from $S$ to $E$.\n\nA model-based solver and a model-free controller have complementary advantages\nand disadvantages. An autonomous agent capable of acting independently in\narbitrary environments must combine the capabilities of both. In this paper we\nshow how the two sets of abilities can be combined by using Meta-Interpretive\nLearning (MIL) to learn a solver, and use the solver to\ngenerate examples to learn a controller that solves the same problems as the\nsolver.\n\nOur ultimate goal is the development of an action-selection component of an\nAutonomous Agent Architecture that must guide a mobile robot to survery missions\nin environments with dynamic and unobserved features so that the autonomous\nagent guiding the robot must combine the capabilities of both solver and\ncontroller. Full details are reserved for future work.\n\nInductive Logic Programming as model-based learning\n\n {figure}[t]\n  \n  [Grid navigation solver]{\n  {tabular}{l}\n $S(s_1,s_2)   Step\\_down(s_1,s_2).$\n\n $S(s_1,s_2)   Step\\_left(s_1,s_2).$\n\n $S(s_1,s_2)   Step\\_right(s_1,s_2).$\n\n $S(s_1,s_2)   Step\\_up(s_1,s_2).$\n\n $S(s_1,s_2)   Step\\_down(s_1,s_3),S(s_3,s_2).$\n\n $S(s_1,s_2)   Step\\_left(s_1,s_3),S(s_3,s_2).$\n\n $S(s_1,s_2)   Step\\_right(s_1,s_3),S(s_3,s_2).$\n\n $S(s_1,s_2)   Step\\_up(s_1,s_3),S(s_3,s_2).$\n  {tabular}\n }\n  [Grid navigation solver model]{\n  {tabular}{l}\n\n $Step\\_down([id,x/y,t],[id,x/y^{-1},t')$\n\n $Step\\_left([id,x/y,t],[id,x^{-1}/y,t')$\n\n $Step\\_right([id,x/y,t],[id,x^{+1}/y,t'])$\n\n $Step\\_up([id,x/y,t],[id,x/y^{+1},t'])$\n  {tabular}\n } \n  [Solver model instantiated to Maze A]{\n  {tabular}{l}\n $Step\\_right([maze\\_a,0/6,s],[maze\\_a,1/6,f])$\n\n $Step\\_down([maze\\_a,2/6,f],[maze\\_a,2/5,f])$\n\n $Step\\_left([maze\\_a,2/0,f],[maze\\_a,1/0,f])$\n\n $Step\\_left([maze\\_a,1/0,f],[maze\\_a,0/0,e])$\n\n % ... 56 more actions\n  {tabular}\n }\n  {tabular}{r}\n  [Path in Maze A]{ [width=0.22 ]{tessera_2.png}}\n\n  {tabular}\n  [Solver model instantiated to Maze B]{\n  {tabular}{l}\n $Step\\_right([maze\\_b,0/6,s],[maze\\_b,1/6,f])$\n\n $Step\\_down([maze\\_b,2/6,f],[maze\\_b,2/5,f])$\n\n $Step\\_up([maze\\_b,4/0,f],[maze\\_b,4/1,f])$\n\n $Step\\_down([maze\\_b,6/1,f],[maze\\_b,6/0,e])$\n\n % ... 56 more actions\n  {tabular}\n }\n  {tabular}{r}\n  [Path in Maze B]{ [width=0.22 ]{tessera_1.png}}\n\n  {tabular}\n  {A Solver for grid navigation problems learned with MIL.}\n {figure}\n\nPlannning is the model-based approach to autonomous behaviour\n in the sense that a planning agent's behaviour is derived\nfrom a model and a planning problem by means of an inference procedure\n(typically a search algorithm such as $A^*$). Inductive Logic Programming\n(ILP) can be seen as the model-based approach to\nmachine learning where the \"model\" is a background theory and a new\nhypothesis is derived by an inference procedure given a set of examples. To\nlearn planning programs, i.e. solvers, with ILP, we can structure the background\ntheory as a set of action predicates and give a set of planning problems as\nexamples. For a solver to be general, it must be a recursive program, therefore\nwe use MIL, a form of ILP capable of learning recursion.\n\nFigure lists a grid navigation solver as a set of definite\nclauses. The solver's body consists of eight clauses of the predicate $S/2$,\nwhere each argument $s_i$ is a Prolog list representing an initial or end state\nof the environment. Each clause of $S/2$ changes the environment state by moving\nan agent one step to each of the four directions up, right, down or left,\nrecursively. The solver's model consists of the step actions $Step\\_up,\nStep\\_right, Step\\_down, Step\\_left$, implemented as dyadic predicates, listed\nin un-instantiated form in Figure . The two arguments\nof each action, shared with $S/2$, represent the state of the environment before\nand after the action is taken as a list of first-order terms: $id$, a map\nidentifier, $x/y$, the coordinates of an agent on the map, and, $t$, the type of\nmap tile at $x/y$, one of the constants $f,w,e,s$ for floor, wall, start\nand end tile, respectively; wall tiles are not passable and so they are not\nfeatured in actions' state arguments.\n\nThe solver's model must be instantiated to the coordinates and tile types of a\nmap before it can be used by the solver to solve that map. Two examples of\ninstantiated models are listed in Figures ,\n for two mazes, Maze A and B. The two mazes are\nidentical save for the position of the end tile and so their instantiated models\nare almost identical. Figures ,\nillustrate the solutions of the two mazes by the solver where each yellow arrow\ncorresponds to a step action in the indicated direction.\n\nThe solver in Figure was learned by the MIL system Louise\n as described in Section .\n\nFinite State Controllers for model-free behaviour\n\n {figure}[t]\n  \n  [Finite State Controller labels]{\n  {tabular}{l}\n 4 Controller state labels: $Q = \\{q_0,q_1,q_2,q_3\\}$\n\n 4 Action labels: $A = \\{up,right,down,left\\}$\n\n 15 Observation labels: $O = \\{pppp,pppu,ppup,ppuu,pupp,pupu,puup,puuu,$\n\n $uppp,uppu,upup,upuu,uupp,uupu,uuup\\}$\n\n 960 4-tuples: $|Q   O   A   Q|$\n  {tabular}\n }    \n  [FSC for Maze A]{\n  {tabular}{l}\n $(q_0,upuu,right,q_1) $\n\n $(q_1,upup,right,q_1) $\n\n $(q_1,uupp,down,q_2) $\n\n $(q_2,pupu,down,q_2) $\n\n $(q_2,ppup,left,q_3) $\n\n $(q_3,upup,left,q_3) $\n  {tabular}\n }\n  {tabular}{r}\n\n  [Path in Maze A]{ [width=0.22 ]{tessera_2.png}}\n  {tabular}\n  [FSC for Maze B]{\n  {tabular}{l}\n $(q_1,upup,right,q_1).$\n\n $(q_1,pupp,down,q_2).$\n\n $(q_1,uupp,down,q_2).$\n\n $(q_2,ppup,right,q_1).$\n\n $(q_2,pupu,down,q_2).$\n\n $(q_0,uppu,right,q_1).$\n\n $(q_0,upuu,right,q_1).$\n\n $(q_0,pupu,up,q_0).$\n\n $(q_1,puup,up,q_0).$\n  {tabular}\n }\n  {tabular}{r}\n\n  [Path in Maze B]{ [width=0.215 ]{maze_b_slam_path.png}}\n\n  {tabular}\n  {Finite State Controllers for mazes A, B trained with the Solver\n in Fig .}\n\n {figure}\n\nWhen its model cannot be instantiated a model-based solver cannot plan. An\nalternative is found in Finite State Controllers (FSCs) a\nformalisation of the concept of Finite State Machines from video games and\nrobotics. A FSC is a set of 4-tuples $(q,o,a,q')$ mapping pairs $(q,o)$ of\ncurrent controller state {Controller states are not\nenvironment states.} and observation labels, to pairs $(a,q')$ of action\nand next controller state labels. Figure lists two\nmaze-solving FSCs for mazes A and B in Figure , and their\nsets of labels. Each observation label is a string $\\{U,R,D,L\\}   \\{u,p\\}^2$\ndenoting whether map tiles in the directions up, right, down or left,\nrespectively, from the agent's current map location, are passable ($p$) or\nunpassable ($u$). For example, the string $upuu$ denotes that only the tile to\nthe right of the agent is passable. Action labels denote that the agent must\nmove up, right, down or left. The two FSCs in Figure\n were learned by Louise from solutions of mazes A\nand B generated by the solver in Figure .\n\nFSCs are model-free in that they have no representation of the environment\nstate, or the way actions modify that state; they are only a mapping between\npairs of labels. In particular, FSC observation labels represent an agent's\nbelief about the state of the environment. Additionally, FSCs are\nefficient in that they do not need to search a large state-space like\nplanners, or, indeed, solvers. Conversely, an FSC is equivalent to a\nDeterministic Finite State Transducer a.k.a. a\nMealy Machine , i.e. a Regular Automaton and so can only\ngenerate one sequence of actions, e.g. the maze FSC in Figure\n can only solve mazes where the exit can be reached\nby moving right, down, or left, like Maze A in Figure ,\nwhereas the FSC in figure can only solve mazes where\nthe exit can be reached by moving right, down or up, like Maze B in Figure\n. Finally, the literature on FSCs does not describe a\nconcrete method to execute FSCs, only their notation as sets of 4-tuples.\n\nContributions\n\nWe identify FSCs as a promising framework for the development of model-free\ncontrollers solving the same problems as model-based solvers in partially\nobservable environments. We make the following contributions.\n\n {itemize}\n   We formalise a notation for model-based learning of planning\n problem solvers as a higher-order background theory for MIL.\n   We extend the framework of FSCs to Nondeterministic FSCs.\n   We implement a set of FSC executors, stack-machines that\n take as input a set of FSC tuples and execute them in order.\n   We implement a novel approach to Simultaneous Localisation and\n Mapping (SLAM) used by executors to avoid cycles.\n   We show that a solver and a controller that solve the same\n problems can be learned by MIL using our model notation.\n   We demonstrate empirically that our learned solver and controller\n can solve the same planning problems.\n   We implement two new libraries in Prolog: Controller Freak,\n to learn controllers from solvers; and Grid Master to\n solve navigation problems on grids.\n {itemize}\n\nImplementation code and experiment data are available at the following URL:\n {https://github.com/stassa/ijclr24}.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "ijclr_2024.tex",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.304,
      "diffusion_reasoning_score": 0.344,
      "distributed_training_score": 0.282,
      "datasets_score": 0.213,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668669",
      "updated_at": "2025-08-11T23:43:05.607040",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16443",
      "title": "VGGT-Long: Chunk it, Loop it, Align it -- Pushing VGGT's Limits on\n  Kilometer-scale Long RGB Sequences",
      "authors": [
        "Kai Deng",
        "Zexin Ti",
        "Jiawei Xu",
        "Jian Yang",
        "Jin Xie"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, extending these models to large-scale\nRGB stream 3D reconstruction remains challenging due to memory limitations. In\nthis work, we propose VGGT-Long, a simple yet effective system that pushes the\nlimits of monocular 3D reconstruction to kilometer-scale, unbounded outdoor\nenvironments. Our approach addresses the scalability bottlenecks of existing\nmodels through a chunk-based processing strategy combined with overlapping\nalignment and lightweight loop closure optimization. Without requiring camera\ncalibration, depth supervision or model retraining, VGGT-Long achieves\ntrajectory and reconstruction performance comparable to traditional methods. We\nevaluate our method on KITTI, Waymo, and Virtual KITTI datasets. VGGT-Long not\nonly runs successfully on long RGB sequences where foundation models typically\nfail, but also produces accurate and consistent geometry across various\nconditions. Our results highlight the potential of leveraging foundation models\nfor scalable monocular 3D scene in real-world settings, especially for\nautonomous driving scenarios. Code is available at\nhttps://github.com/DengKaiCQ/VGGT-Long.",
      "published_date": "2025-07-22T10:39:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16443v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16443v1",
      "latex_url": "http://arxiv.org/src/2507.16443v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Perceiving 3D environments from monocular RGB streams is crucial for autonomous driving, yet existing methods struggle with kilometer-scale and uncalibrated sequences. Unlike the small-scale indoor 3D vision tasks, driving scenarios involve long trajectories with sparse frame correspondence, dynamic objects and challenging outdoor conditions. While some approaches handle large-scale monocular scenes, they often depend on sophisticated multi-module pipelines or assume known camera intrinsics. Others leverage additional sensors (LiDAR , IMU or stereo ), sidestepping the core challenge: scalable, calibration-free reconstruction from monocular RGB alone and it is a critical for autonomous systems.\n\nA recent paradigm shift in 3D vision has witnessed the rise of end-to-end foundation models, largely based on the Transformer architecture . A mainly of works from DUSt3R~ and MASt3R~ to CUT3R~, Fast3R~, and most recently VGGT~, aim to replace complex, multi-component SfM and SLAM pipelines with a single and unified deep learning model. These models are trained on massive datasets to integrate camera pose estimation, intrinsic parameter regression, and 3D scene representation (typically as a point map) into one cohesive framework. A key goal is to enable backpropagation of errors through the entire system, creating a powerful and versatile foundation model for 3D reconstruction that operates on raw and uncalibrated RGB inputs. However, foundation models like CUT3R and Fast3R still struggle with severe drift in outdoor environments, even on short sequences of a few dozen frames, which limits their practical applicability. In contrast, VGGT delivers remarkably stable and accurate local reconstructions, establishing it as the state-of-the-art in terms of reconstruction quality. Its primary limitation is not performance, but its immense computational and memory footprint.\n\nThe computational and memory demands of Transformer based foundation models severely limit their scalability. Standard self-attention scales quadratically with input size, and while techniques like Flash-Attention reduce compute complexity to linear. However, GPU memory requirement still remains prohibitive. For example, VGGT can just process 60 to 80 images on a 24 GiB RTX 4090 GPU; scaling to a KITTI Seq 00 trajectory (about 4,600 frames) would require 1,380 to 1,840 GiB, far exceeding current hardware. This bottleneck confines such models to small-scale scenes, as both memory and drift accumulation become intractable over long sequences.\n\nOur work is inspired by recent efforts to integrate these foundation models into large-scale systems. A notable example is MASt3R-SLAM~, which builds a sophisticatedly designed SLAM system on top of the MASt3R model. To achieve global consistency, it employs a complex backend with pose graph optimization, bundle adjustment and other heavy machinery. While powerful, such systems often entail significant engineering complexity, making them difficult to adapt to down-streaming tasks.\n\nThis raises a fundamental question: must large-scale reconstruction always equate to system-level complexity? Our philosophy diverges significantly from this trend. We advocate for a minimalist approach that unlocks the inherent potential of the foundational model itself. We posit that VGGT is already a remarkably powerful engine for large-scale 3D perception, and the primary challenge is not a lack of capability, but a lack of scalability. Instead of building another intricate system around it, we ask: can we solve the problem with the minimal overhead?\n\nTo this end, we propose VGGT-Long, a framework that extends VGGT to long sequences through a simple yet effective framework: processing the sequence in overlapping chunks, robustly aligning adjacent chunks, and correcting for drift using a high-quality loop closure module. This ``chunk-and-align'' paradigm avoids the need for a complex graph-based optimization backend (such as bundle adjustment ). It is a testament to the power of the underlying VGGT model, demonstrating that with the right strategy, its exceptional local reconstruction capabilities can be seamlessly stitched together to form a globally consistent, kilometer-scale map. Our work champions the idea that, a sufficiently powerful base model may not necessarily require a complex backend system to assist.\n\nIn summary, our contributions are as follows:\n\n {enumerate}\n   We present the first system that successfully extends monocular 3D reconstruction models to kilometer-scale, unbounded outdoor scenes, without requiring camera calibration and depth supervision.\n\n   We introduce a simple yet effective chunk-and-align pipeline that resolves the memory limitations of foundation models like VGGT on long video sequences, while achieving accuracy comparable to traditional methods with calibrated cameras.\n\n   We address the accumulated Sim(3) drift problem inherent in processing long sequences with local models, demonstrating that VGGT can serve as a robust front-end for a large-scale reconstruction system without requiring a complex backend.\n\n {enumerate}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.405,
      "distributed_training_score": 0.404,
      "datasets_score": 0.306,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on extending a Transformer-based 3D vision model for large-scale monocular reconstruction through chunking and alignment strategies. It does not involve diffusion models, iterative refinement for logical tasks, or any multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "The paper addresses memory limitations in processing long sequences by using a chunk-based approach on a single GPU, but it does not discuss distributed training, parallel computing across multiple nodes, or strategies for partitioning data/computation in a multi-processor environment.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668174",
      "updated_at": "2025-08-11T23:43:05.606945",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16454",
      "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions",
      "authors": [
        "Pierangela Bruno",
        "Carmine Dodaro",
        "Giuseppe Galatà",
        "Marco Maratea",
        "Marco Mochi"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LO (Logic in Computer Science)"
      ],
      "abstract": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).",
      "published_date": "2025-07-22T10:56:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16454v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16454v1",
      "latex_url": "http://arxiv.org/src/2507.16454v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The Operating Room Scheduling (ORS) problem consists of optimizing daily surgical schedules in operating rooms. It is a complex and highly constrained problem that requires, among other tasks, determining the starting times of surgeries and allocating the necessary resources~ {abedini2016operating,aringhieri_two_2015,DBLP:journals/cor/HamidNWSZ19,DBLP:journals/dss/MeskensDH13}.\n\nRecently, several solutions based on Answer Set Programming (ASP) for the ORS problem have been proposed~ {DBLP:journals/tplp/DodaroGKMP22,DBLP:journals/logcom/DodaroGGMMMS24}, showing promising results in finding feasible and efficient schedules under realistic constraints. However, when applied to real-world data, such as those provided by ASL1 Liguria in Italy~ {DBLP:journals/logcom/DodaroGGMMMS24}, these solutions rely on the assumption that surgery durations are known in advance. Specifically, since the scheduling was performed on past surgeries, the actual durations were already available. This allowed researchers to compare the ASP-generated schedules with those historically adopted by the hospital and evaluate potential improvements retrospectively. Nevertheless, in a practical setting, surgery durations are not known beforehand, and this uncertainty poses a critical challenge: ASP systems heavily depend on accurate input values, and imprecise duration estimations can lead to suboptimal scheduling solutions. As a result, the ability to generate provisional schedules under uncertainty remains largely unaddressed.\n\nTo overcome this limitation, it is necessary to integrate predictive models capable of estimating surgery durations before the actual scheduling process takes place. Machine learning (ML) techniques offer a promising solution for this purpose, enabling the estimation of surgery durations based on historical patient and surgery data.\n\nThe integration of deductive (logic-based) and inductive (ML-based) approaches has emerged as one of the most active areas of research in the AI community in recent years, and ASP is no exception.\nIndeed, several efforts have been made in this direction, such as using ML to guide the heuristics of ASP solvers to improve performance~ {DBLP:journals/aicom/Balduccini11,DBLP:conf/lpnmr/DodaroIOR22,DBLP:conf/lpnmr/LiuTL22}, applying algorithm selection techniques~ {DBLP:journals/tplp/MarateaPR14,DBLP:journals/tplp/HoosLS14}, representing and explaining ML models via ASP~ {DBLP:conf/ijcai/EiterGHO23,DBLP:journals/tplp/GiordanoD22}, and developing languages and tools for learning ASP programs~ {DBLP:conf/ijcai/YangIL20,DBLP:conf/aaai/TarzariolGSL23,DBLP:conf/ijcai/CunningtonL0R23,DBLP:journals/corr/abs-2005-00904}. In addition, there has been growing interest in neuro-symbolic approaches in real-world applications where ASP is applied in conjunction with ML techniques~ {DBLP:journals/tplp/BarbaraGLMQRR23,DBLP:journals/tplp/EiterHOP22,DBLP:conf/lpnmr/BrunoCM22}.\n\nIn this paper, we contribute to this line of research by proposing a hybrid approach that integrates ML predictions into an ASP-based solution for the ORS problem. Specifically, our contributions are as follows.\nFirst, we perform an analysis of the available real-world dataset, identifying significant distribution skewness that could negatively affect predictive accuracy. To mitigate this, we apply a dedicated preprocessing phase to improve data quality and reliability.\nThen, we systematically evaluate several state-of-the-art ML algorithms for predicting surgery durations, using standard performance metrics such as mean absolute error, root mean squared error, and coefficient of determination. Among the tested models, XGBoost~ {chen2016xgboost} achieves the best performance and is selected for further integration.\nSubsequently, we introduce the notion of prediction confidence by clustering the predicted durations into four discrete levels, ranging from high confidence to very low confidence, providing an additional layer of information to assess prediction reliability.\nThen, we extend the original ASP encoding to incorporate confidence information into the scheduling process, enabling the computation of more robust and reliable surgical schedules.\nFinally, we conduct an extensive experimental evaluation. Despite the challenges posed by the inherent distribution skewness in the dataset, and the limited predictive accuracy of the models, the results show that incorporating ML predictions, especially when combined with confidence information, leads to a good improvement in scheduling quality.\nIn fact, our approach obtains a better operating room usage and reduces the incidence of overbooking compared to the baseline ASP encoding that relies only on statistical averages.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.326,
      "distributed_training_score": 0.308,
      "datasets_score": 0.298,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668679",
      "updated_at": "2025-08-11T23:43:05.607042",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16467",
      "title": "Estimating Treatment Effects with Independent Component Analysis",
      "authors": [
        "Patrik Reizinger",
        "Lester Mackey",
        "Wieland Brendel",
        "Rahul Krishnan"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The field of causal inference has developed a variety of methods to\naccurately estimate treatment effects in the presence of nuisance. Meanwhile,\nthe field of identifiability theory has developed methods like Independent\nComponent Analysis (ICA) to identify latent sources and mixing weights from\ndata. While these two research communities have developed largely\nindependently, they aim to achieve similar goals: the accurate and\nsample-efficient estimation of model parameters. In the partially linear\nregression (PLR) setting, Mackey et al. (2018) recently found that estimation\nconsistency can be improved with non-Gaussian treatment noise. Non-Gaussianity\nis also a crucial assumption for identifying latent factors in ICA. We provide\nthe first theoretical and empirical insights into this connection, showing that\nICA can be used for causal effect estimation in the PLR model. Surprisingly, we\nfind that linear ICA can accurately estimate multiple treatment effects even in\nthe presence of Gaussian confounders or nonlinear nuisance.",
      "published_date": "2025-07-22T11:16:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16467v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16467v1",
      "latex_url": "http://arxiv.org/src/2507.16467v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The accurate estimation of causal effects is a central challenge in medical research and policy-making  {king1994designing}, as it guides the development of more effective treatment strategies and interventions  {rosenbaum1983central,pearl2009causal,hill2011bayesian}. This task becomes difficult when the data contain high-dimensional confounding variables---features that affect both the treatment and the outcome. A number of machine learning methods has emerged to handle this setting while maintaining theoretical guarantees on causal effect estimation.\nAmong these methods,  {dml}~ {chernozhukov_doubledebiased_2017} exhibits robust statistical properties in the  {plr} model ~ {robinson1988root}, where confounders affect the outcome and treatment in a potentially nonlinear way. DML’s two-stage procedure---first learning nuisance functions, then leveraging orthogonalization to adjust for confounders---yields consistent and efficient estimators of treatment effects under minimal assumptions.\n\n  {ica}~ {comon1994independent,hyvarinen_independent_2000} is a family of representation learning methods that focuses on separating mixed signals into statistically independent components, enabling the discovery of latent structures, often referred to as (causal) representations, from observational data.  {ica} can also be used for  {cd},  , the extraction of the causal graph, both in the linear~ {shimizu_linear_2006} and the nonlinear~ {reizinger_jacobian-based_2023} case.\n  {ica} and causal effect estimation are well-studied yet distinct tools for estimating measurements of interest from data~ {tramontano_causal_2024}. Despite their distinct origins, the non-Gaussianity of the source/noise variables are crucial in both. For (linear)  {ica} it is required to break the Gaussian's rotational symmetry to identify the sources in the infinite data limit; for treatment effect estimation, it can guarantee better estimation consistency~ {mackey2018orthogonalICML,jin2025its}.\n\n  {figure*}\n  \n  {fig1}\n  {Overview of treatment effect estimation in the  {plr model:} (Left:) the linear  {plr} model, where the covariates $X$ affect both treatment $T$ and outcome $Y$. The quantity of interest is the treatment effect { {figblue}$ $}. (Center:)  {oml} estimates { {figblue}$ $} in two steps, 1) regressing the residuals of $X$ explaining $T,$ correcting for the indirect effect of $X$ on $Y$ via the $X  T  Y$ path, then 2) using the estimated noise to regress the residuals of $Y$, yielding { {figblue}$ $} as a regression coefficient; (Right:)  {ica} inverts the  {plr} model by maximizing non-Gaussianity of the sources, thereby yielding { {figblue}$ $} as a coefficient in the so-called unmixing matrix---scale and permutation indeterminacies are resolved by relying on non-Gaussianity and the  {plr} structure ( {lem:lin_plr_ica}) }\n  {-1.25 }\n\n  {figure*}\n\n However, these similarities were neither recognized nor explored before as both fields developed independently.\n\n Our work is the first to connect treatment effect estimation and  {ica}, focusing on the  {plr} model, showing its feasibility. We prove that  {ica} can estimate treatment effects; we show that the problem of estimating treatment effects in the PLR model is equivalent to identifying the (elements of the) mixing matrix in ICA. Next, we show how the permutation and scale indeterminacies of  {ica} can be overcome. This transformation permits the extensions to new variants of the causal effect estimation problems: effects under multiple continuous treatments, and Gaussian covariate noise, all using the same off-the-shelf ICA algorithm, FastICA~ {hyvarinen1999fast}. We also demonstrate how to use linear ICA for estimating treatment effects in a nonlinear  {plr}. These insights lead us to critically assess the necessity of non-Gaussianity in the fields of (causal) representation learning and effect estimation.\n\n Our contributions are (   {fig:fig1}):\n  {itemize}[leftmargin=*,nolistsep]\n   We formalize the link between  {homl} and  {ica}; we clarify the role of non-Gaussianity in both algorithms,\n\n   We show how ICA can estimate treatment effects with partially Gaussian source variables ( {table:breaking_symmetries,cor:ica_gauss}) and to estimate multiple treatment effects ( {cor:multi_T});\n   We highlight promising first results of the effectiveness of linear  {ica} for treatment effect estimation for a  {plr} model with nonlinear nuisance factors ( {subsec:lin_nonlin}).\n  {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main_text.tex",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.331,
      "distributed_training_score": 0.296,
      "datasets_score": 0.286,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669346",
      "updated_at": "2025-08-11T23:43:05.607139",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16472",
      "title": "DenseSR: Image Shadow Removal as Dense Prediction",
      "authors": [
        "Yu-Fan Lin",
        "Chia-Ming Lee",
        "Chih-Chung Hsu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Shadows are a common factor degrading image quality. Single-image shadow\nremoval (SR), particularly under challenging indirect illumination, is hampered\nby non-uniform content degradation and inherent ambiguity. Consequently,\ntraditional methods often fail to simultaneously recover intra-shadow details\nand maintain sharp boundaries, resulting in inconsistent restoration and\nblurring that negatively affect both downstream applications and the overall\nviewing experience. To overcome these limitations, we propose the DenseSR,\napproaching the problem from a dense prediction perspective to emphasize\nrestoration quality. This framework uniquely synergizes two key strategies: (1)\ndeep scene understanding guided by geometric-semantic priors to resolve\nambiguity and implicitly localize shadows, and (2) high-fidelity restoration\nvia a novel Dense Fusion Block (DFB) in the decoder. The DFB employs adaptive\ncomponent processing-using an Adaptive Content Smoothing Module (ACSM) for\nconsistent appearance and a Texture-Boundary Recuperation Module (TBRM) for\nfine textures and sharp boundaries-thereby directly tackling the inconsistent\nrestoration and blurring issues. These purposefully processed components are\neffectively fused, yielding an optimized feature representation preserving both\nconsistency and fidelity. Extensive experimental results demonstrate the merits\nof our approach over existing methods. Our code can be available on\nhttps://github$.$com/VanLinLin/DenseSR",
      "published_date": "2025-07-22T11:22:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16472v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16472v1",
      "latex_url": "http://arxiv.org/src/2507.16472v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Shadows, as natural consequences of light-object interactions, are ubiquitous optical phenomena in the visual world. The presence of shadows profoundly impacts multimedia content analysis, degrading performance in tasks ranging from remote sensing , segmentation , tracking and 3D reconstruction to multimedia applications .\nRemoving shadows from images to restore the authentic appearance of occluded regions is not only a fundamental computer vision task but also a critical step for enhancing downstream application performance . The core challenge of this task lies in accurately understanding the local illumination attenuation patterns (distinguishing shadows from intrinsic object darkness) and leveraging contextual information to perform physically plausible and visually natural content filling and color correction within shadowed areas for restoration .\n\n {figure}[t!]\n \n [width=1.0 ]{fig/avg_var.pdf}\n {-0.7cm}\n {To tackle inconsistent restoration and boundary blurring in shadow removal, we employs an adaptive strategy. As illustrated, it distinctly processes: (Left) smoothed base features ensuring content consistency (akin to mean, processed via ACSM smoothing); and (Right) high-frequency features for detail recovery and boundary sharpening (akin to variance, refined via TBRM). Fusing these purposefully processed features enables high-quality shadow removal that balances both content consistency and boundary clarity.}\n\n {-0.2cm}\n {figure}\nDespite significant advances driven by deep learning in single-image shadow removal, several deep-seated bottlenecks remain. First, the ambiguity between shadows and intrinsic object properties remains challenging to resolve solely based on RGB information. Second, the complexity of real-world illumination, particularly the prevalence of indirect lighting and the resulting soft shadows in indoor scenes, is often inadequately addressed, limiting model performance in such scenarios, partly due to insufficient modeling of physical light transport like scattering and diffusion. Third, standard feature fusion strategies employed in hierarchical networks exhibit inherent flaws: they typically assume features accurately represent scene content at their respective scales. However, shadows non-uniformly degrade this representation, causing simple fusion methods to fail in handling this spatially varying signal degradation, resulting in inconsistent intra-shadow restoration and significant loss of boundary details.\n\nTo overcome these bottlenecks, our approach returns to the physical essence of shadow formation and fundamental principles of information processing. As revealed by fundamental shading models, object appearance results from a complex interplay of illumination, geometry (surface orientation), and material (reflectance properties). Shadows fundamentally alter the illumination component. Accurately inverting this effect to obtain the shadow-free image necessitates effectively disentangling illumination effects from intrinsic properties, strongly motivating the incorporation of external prior knowledge capturing geometry and material/semantic characteristics. Concurrently, recognizing the failure of standard fusion strategies when dealing with shadow-degraded features, we identified the need for a more sophisticated and adaptive fusion mechanism. Such a mechanism must be capable of distinguishing and processing different information components affected by shadows—for instance, the relatively stable low-frequency base appearance versus the heavily distorted or obscured high-frequency texture details.\n\nBased on these motivations, we propose the DenseSR framework, approaching shadow removal from a dense pixel-wise prediction perspective. The core of DenseSR lies in a two-parts: first, it achieves deep scene understanding and implicit shadow localization/disambiguation by integrating powerful geometric (depth, normal) and semantic (DINO) priors guided through attention mechanisms in Scene-Integrated Modules (SIM); building upon this understanding, we introduce the innovative Dense Fusion Block (DFB) within the decoder, specifically responsible for high-fidelity content restoration. DFB employs an adaptive component processing approach: the Adaptive Content Smoothing Module (ACSM) focuses on restoring a consistent base appearance within the shadow region from coarser-scale features, suppressing noise and artifacts; meanwhile, the Texture-Boundary Recuperation Module (TBRM) concentrates on recuperating obscured fine textures and sharpening boundaries using finer-scale features, as shown in Figures and . These complementary modules yield effectively combined outputs, generating an optimized feature representation that preserves both internal consistency and boundary details, ultimately enabling high-quality shadow removal. The main contributions of this study can be summarized into three points:\n\n {itemize}\n   A novel shadow removal framework (DenseSR) integrating prior knowledge: Approaching the task from a dense prediction perspective, this framework utilizes attention mechanisms to effectively guide geometric and semantic priors, addressing the core shadow ambiguity issue.\n   The design of a Dense Fusion Block (DFB) tailored for shadow degradation: Featuring complementary ACSM and TBRM modules, its adaptive component processing strategy specifically targets the intra-shadow inconsistency and boundary/detail loss issues characteristic of standard fusion methods in shadow removal.\n   Demonstration of state-of-the-art performance under complex illumination: Extensive experiments validate DenseSR's robustness and effectiveness, particularly in handling challenging direct and indirect illumination scenarios.\n {itemize}\nThe following sections will detail related work, motivation, network architecture, experimental setup, and results analysis.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.315,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.389,
      "distributed_training_score": 0.332,
      "datasets_score": 0.317,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668182",
      "updated_at": "2025-08-11T23:43:05.606947",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16473",
      "title": "Learning Temporal Abstractions via Variational Homomorphisms in\n  Option-Induced Abstract MDPs",
      "authors": [
        "Chang Li",
        "Yaren Zhang",
        "Haoran Lv",
        "Qiong Cao",
        "Chao Xue",
        "Xiaodong He"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.",
      "published_date": "2025-07-22T11:22:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16473v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16473v2",
      "latex_url": "http://arxiv.org/src/2507.16473v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent advancements in deep reinforcement learning (DRL) have\ndemonstrated significant successes across a variety of complex\ndomains, such as mastering the human level of\natari~ and Go~\ngames. These achievements underscore the potential of combining\nreinforcement learning (RL) with powerful function approximators\nlike neural networks~ to tackle\nintricate tasks that require nuanced control over extended\nperiods. Despite these breakthroughs, Deep RL still faces\nsubstantial challenges, such as insufficient exploration in\ndynamic\nenvironments~,\ninefficient learning associated with temporally extended\nactions~ and long\nhorizon tasks~, and vast\namounts of samples required for training proficient\nbehaviors~.\n\nOne promising area for addressing these challenges is the\nutilization of hierarchical reinforcement learning\n(HRL)~,\na diverse set of strategies that decompose complex tasks into\nsimpler, hierarchical structures for more manageable learning.\nAmong these strategies, the option\nframework~, developed on the Semi-Markov\nDecision Process (SMDP), is particularly effective at segmenting\nnon-stationary task stages into temporally-extended actions known\nas options. Options are typically learned through a maximum\nlikelihood approach that aims to maximize the expected rewards\nacross trajectories. In this framework, options act as temporally\nabstracted actions executed over variable time steps, controlled\nby a master policy that decides when each option should execute\nand terminate. This structuring not only simplifies the\nmanagement of complex environments but also enables the\nsystematic discovery and execution of temporal abstractions over\nlong-horizon tasks~.\n\nHowever, the underlying SMDP framework is frequently undermined\nby three key challenges: 1) Insufficient exploration and\ndegradation~. As options are unevenly updated using\nconventional maximum likelihood methods~, the policy is\nquickly saturated with early rewarding observations. This\ntypically results in focusing on only low-entropy options that\nlead to local optima rewards, causing a single option to either\ndominate the entire policy or switch every timestep. Such\npremature convergence limits option diversity significantly. 2)\nSample Inefficiency. The semi-Markovian nature inherently leads\nto sample\ninefficiency~:\neach policy update at the master level extends over multiple time\nsteps, thus consuming a considerable volume of experience samples\nwith relatively low informational gain. This inefficiency is\nfurther exacerbated by the prevalence of on-policy option\nlearning algorithms~, which\nrequire new samples to be collected simultaneously from both\nhigh-level master policies and low-level action policies at each\ngradient step, and thus sample expensive. 3) Computationally\nexpensive. Options are conventionally defined as\ntriples~ with intra-option policies and\ntermination functions, often modeled using neural networks which\nare expensive to optimize. These challenges collectively limit\nthe broader adoption and effectiveness of the option framework in\nreal-world scenarios, particularly in complex continuous\nenvironments where scalability and stability are\ncritical~.\n\nTo address these challenges, we introduce the Variational\nMarkovian Option Critic (VMOC), a novel off-policy algorithm that\nintegrates the variational inference framework on option-induced\nMDPs~. We first formulate the optimal\noption-induced SMDP trajectory as a probabilistic inference\nproblem, presenting a theoretical convergence proof of the\nvariational distribution under the soft policy iteration\nframework~. Similar to prior variational\nmethods~, policy entropy terms\nnaturally arise as intrinsic rewards during the inference\nprocedure. As a result, VMOC not only seeks high-reward options\nbut also maximizes entropy across the space, promoting extensive\nexploration and maintaining high diversity. We implements this\ninference procedure as an off-policy soft actor\ncritic~ algorithm, which allows reusing\nsamples from replay buffer and enhances sample efficiency.\nFurthermore, to address the computational inefficiencies\nassociated with conventional option triples, we\nfollow~ and employ low-cost option embeddings\nrather than complex neural network models. This not only\nsimplifies the training process but also enhances the\nexpressiveness of the model by allowing the agent to capture a\nmore diverse set of environmental dynamics.\n\nTo provide a rigorous theoretical foundation for learning in\nabstract option spaces, we extend the theory of continuous MDP\nhomomorphisms~ to the continuous\nHiT-MDP setting. MDP homomorphisms provide a formal framework for\nstate-action abstractions that preserve optimal value functions,\nbut previous work has been limited to standard continuous MDPs.\nWe introduce continuous HiT-MDP homomorphisms using the\nmathematical framework of vector bundles to elegantly capture the\nrelationship between state-option pairs across different levels\nof abstraction. This formulation allows us to prove that optimal\nvalue equivalence and policy lifting properties extend to the\noption framework, ensuring that learning in abstract spaces does\nnot sacrifice optimality.\n\nBuilding on this theoretical foundation, we further establish\nthat the variational inference framework seamlessly integrates\nwith abstract HiT-MDPs. Specifically, we prove that maximizing\nthe evidence lower bound (ELBO) in an abstract HiT-MDP obtained\nthrough a homomorphism is equivalent to optimizing a lower bound\nof the ELBO in the original space. This result provides a\nprincipled justification for using VMOC to learn policies in\nabstract option spaces: the algorithm directly optimizes the\nvariational objective of the original problem while benefiting\nfrom the computational advantages of working in a simplified\nabstract space. The combination of HiT-MDP homomorphisms and\nvariational inference thus offers both theoretical guarantees and\npractical benefits for hierarchical reinforcement learning.\n\nBeyond traditional control tasks, the structure of VMOC offers a\ncompelling solution to challenges in other domains, such as the\nefficiency of reasoning in Large Language Models (LLMs). While\nChain-of-Thought (CoT) prompting has enabled LLMs to tackle\ncomplex multi-step problems, the generation of explicit reasoning\ntext incurs substantial computational and latency\ncosts~. A promising alternative is to perform\nreasoning implicitly within the model's latent\nspace~, but this\noften sacrifices the interpretability of the reasoning\nprocess~. Our framework naturally\naddresses this trade-off. We posit that the latent option space\nin VMOC can represent abstract reasoning steps, or an ``implicit\nCoT''. To initialize this space, we propose a cold-start phase\nthat leverages supervised fine-tuning (SFT) datasets of explicit\nreasoning demonstrations. Through a variational objective\nanalogous to  {eq:elbo}, we distill these explicit CoT\nchains into the discrete latent option embeddings. This\npre-training endows the model with a rich library of reasoning\nprimitives that can be invoked for efficient, purely latent\ninference. This two-stage approach opens the door to developing\nagents that perform implicit reasoning in a structured latent\nspace, while retaining the ability to be refined via RL and\npotentially be decoded into understandable language, bridging\nefficient performance with verifiability.\n\nOur contributions can be summarized as follows:\n {itemize}\n  We propose the Variational Markovian Option Critic (VMOC),\n an off-policy, maximum-entropy algorithm that learns a diverse\n set of options represented as low-cost embeddings, enhancing\n sample efficiency and exploration.\n  We provide a rigorous theoretical foundation by extending\n the framework of continuous MDP homomorphisms to HiT-MDPs,\n proving that learning in the abstract option space preserves\n optimality guarantees.\n  We introduce a novel application for VMOC in language\n models, proposing a cold-start supervised fine-tuning (SFT)\n procedure to learn an ``implicit Chain-of-Thought'' in the\n latent option space for efficient and effective reasoning.\n  We demonstrate through extensive experiments that VMOC\n significantly outperforms strong baselines in challenging\n Mujoco locomotion tasks and achieves competitive results on\n complex logical reasoning benchmarks, validating its broad\n applicability.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "nips_2024.tex",
      "rlhf_score": 0.423,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.5,
      "distributed_training_score": 0.343,
      "datasets_score": 0.256,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses supervised fine-tuning (SFT) on human reasoning demonstrations for initializing the latent option space, which involves human data. However, it does not fully align with RLHF, as RLHF requires training a reward model on human-ranked preferences and using it for RL fine-tuning. Here, the focus is on SFT for cold-start and subsequent RL, not a dedicated RLHF pipeline.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models or iterative refinement processes for reasoning. It focuses on variational inference, hierarchical RL with options, and implicit Chain-of-Thought in latent spaces, with no mention of adapting diffusion for multi-step logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667826",
      "updated_at": "2025-08-11T23:43:05.606865",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16476",
      "title": "Survival Modeling from Whole Slide Images via Patch-Level Graph\n  Clustering and Mixture Density Experts",
      "authors": [
        "Ardhendu Sekhar",
        "Vasu Soni",
        "Keshav Aske",
        "Garima Jain",
        "Pranav Jeevan",
        "Amit Sethi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We introduce a modular framework for predicting cancer-specific survival from\nwhole slide pathology images (WSIs) that significantly improves upon the\nstate-of-the-art accuracy. Our method integrating four key components. Firstly,\nto tackle large size of WSIs, we use dynamic patch selection via quantile-based\nthresholding for isolating prognostically informative tissue regions. Secondly,\nwe use graph-guided k-means clustering to capture phenotype-level heterogeneity\nthrough spatial and morphological coherence. Thirdly, we use attention\nmechanisms that model both intra- and inter-cluster relationships to\ncontextualize local features within global spatial relations between various\ntypes of tissue compartments. Finally, we use an expert-guided mixture density\nmodeling for estimating complex survival distributions using Gaussian mixture\nmodels. The proposed model achieves a concordance index of $0.712 \\pm 0.028$\nand Brier score of $0.254 \\pm 0.018$ on TCGA-KIRC (renal cancer), and a\nconcordance index of $0.645 \\pm 0.017$ and Brier score of $0.281 \\pm 0.031$ on\nTCGA-LUAD (lung adenocarcinoma). These results are significantly better than\nthe state-of-art and demonstrate predictive potential of the proposed method\nacross diverse cancer types.",
      "published_date": "2025-07-22T11:32:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16476v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16476v2",
      "latex_url": "http://arxiv.org/src/2507.16476v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Accurate survival prediction for cancer patients plays a vital role in personalized oncology, enabling clinicians to tailor treatment plans, adjust monitoring schedules, and allocate healthcare resources more effectively. In recent years, whole slide images (WSIs) — high-resolution digital scans of hematoxylin and eosin (H\\&E) stained pathology slides — have emerged as a valuable data modality for prognostic modeling. WSIs capture a wealth of histological information, including tumor architecture, stromal patterns, immune cell infiltration, and spatial interactions within the tumor microenvironment (TME), all of which are known to be correlated with patient outcomes.\n\nDespite their potential, WSIs present significant challenges for computational analysis. A single slide may contain billions of pixels, making end-to-end processing computationally prohibitive. Moreover, acquiring detailed annotations at the cellular or regional level is expensive, time-consuming, and often infeasible at scale. As a result, most approaches rely on weakly supervised learning paradigms and patch-based representations, where each slide is divided into smaller tiles or patches that are processed independently or aggregated using pooling strategies.\n\nWhile such methods have achieved considerable success in cancer classification and subtyping tasks, survival prediction introduces additional complexity. Unlike classification, which often hinges on localized discriminative features, survival analysis requires the joint modeling of long-range dependencies, subtle morphological cues, and interactions among spatially distributed components within the TME. Traditional statistical models such as the Cox proportional hazards model are limited in their capacity to capture non-linear and high-dimensional relationships inherent in WSIs. Deep learning approaches offer a more expressive alternative, but designing models that are both accurate and interpretable for survival estimation remains a persistent challenge.\n\nTo address these limitations, we propose a comprehensive modular framework that integrates four key components:\n {enumerate}\n   A dynamic quantile-based patch selection strategy that identifies prognostically informative tissue regions while reducing noise and computational burden;\n   A graph-guided k-means clustering technique to capture phenotype-level heterogeneity by grouping spatially coherent and morphologically similar patches;\n   An attention mechanism that incorporates both intra-cluster attention to model fine-grained interactions among patches within each phenotype cluster, and inter-cluster attention to capture high-level contextual relationships across different phenotype clusters;\n   An expert-guided mixture density modeling module, which models survival distributions using Gaussian mixture models.\n {enumerate}\n\nTogether, these components form a unified and comprehensive pipeline for interpretable, flexible, and clinically meaningful survival modeling. By bridging spatial reasoning, phenotype abstraction, and probabilistic outcome modeling, our framework offers a robust tool for enhancing prognosis in real-world cancer cohorts.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "Methodology__1_.tex",
      "rlhf_score": 0.261,
      "weak_supervision_score": 0.344,
      "diffusion_reasoning_score": 0.38,
      "distributed_training_score": 0.316,
      "datasets_score": 0.3,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668190",
      "updated_at": "2025-08-11T23:43:05.606949",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16478",
      "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data\n  Generation & Adaptive Training",
      "authors": [
        "Shreya Saxena",
        "Siva Prasad",
        "Zishan Ahmad",
        "Vishal Vaddina"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.",
      "published_date": "2025-07-22T11:35:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16478v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16478v1",
      "latex_url": "http://arxiv.org/src/2507.16478v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure*}[ht]\n  \n [width= ,trim=0.3cm 0.4cm 0.2cm 0.2cm,clip]{figures/archi.pdf}\n  {Overview of the ACT framework, illustrating the workflow with a focus on the Controller's role in managing iterative processes like data generation and finetuning }\n\n {figure*}\n\nIn recent years, the rapid evolution of software development practices has necessitated seamless code translation across diverse programming languages. Traditional code translation methods often relied on manually created rules to transform code, were labor intensive and often produced suboptimal translations . Statistical machine translation techniques for code translation such as semSMT and others using abstract syntax tree (AST) improved translation quality, but the advent of large language models (LLM) significantly advanced the quality of code translation systems . While open-source language models show they often lag behind proprietary solutions . Proprietary, closed-source models deliver superior results but pose concerns about data security, control, and dependency on third-party services. Finetuning open-source models is a viable solution, but limited access to high-quality training data remains a critical challenge. Recent research has explored synthetic data generation as a means to improve fine-tuning for various tasks, including code translation . However, effectively generating diverse, functionally accurate synthetic data remains an open problem.\n\nBuilding on these advancements, we introduce Auto-Train for Code Translation (ACT), a novel framework that facilitates in-house finetuning of open-source LLMs for effective and secure code translation. Central to ACT is its automatic synthetic data generation module, which generates extensive, high-quality datasets from an initial set of code samples provided, complete with unit tests to validate functional accuracy. Through iterative data generation and model finetuning, ACT optimizes translation capabilities with minimal data overhead, forming a robust solution tailored to specific project needs.\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "colm2025_conference.tex",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.416,
      "diffusion_reasoning_score": 0.43,
      "distributed_training_score": 0.398,
      "datasets_score": 0.396,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves synthetic data generation, where training data and labels are programmatically created from initial code samples with unit tests, aligning directly with weak supervision. This approach reduces reliance on hand-labeled data by using automated, high-level sources to generate large quantities of labels, as seen in ACT's pipeline for finetuning LLMs.",
      "diffusion_reasoning_justification": "The paper focuses on synthetic data generation and adaptive training for code translation, with iterative processes managed by a controller, but it does not involve diffusion models, iterative refinement of Chain-of-Thought reasoning, or any multi-step logical reasoning adapted from diffusion techniques.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces ACT, a framework designed to enhance code translation by enabling in-house finetuning of open-source Large Language Models (LLMs) through an automated pipeline. It employs synthetic data generation from initial code samples, incorporating unit tests for accuracy and diversity, along with a controller module that dynamically adjusts hyperparameters and manages iterative training and evaluation processes, ultimately improving model performance and providing a secure alternative to proprietary solutions for software development and migration projects.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of synthetic data generation and adaptive training in a unified framework for code translation, offering a notable improvement over existing methods by addressing limitations in open-source LLMs.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research and applications in code translation and software engineering subfields by providing a practical, secure finetuning approach, though its broader applicability may be limited to specific domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a strong, innovative contribution to code translation challenges, making it valuable for researchers and practitioners in AI and software engineering who deal with model finetuning and data security.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a4a398dd1ebfcd074c73a8be5acd592b1d78292c",
      "h_index_fetch_method": "full_id",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 6,
      "average_h_index": 3.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shreya Saxena",
          "profile_url": "https://www.semanticscholar.org/author/2184792648",
          "h_index": 6
        },
        {
          "name": "Siva Prasad",
          "profile_url": "https://www.semanticscholar.org/author/2268314981",
          "h_index": 2
        },
        {
          "name": "Zishan Ahmad",
          "profile_url": "https://www.semanticscholar.org/author/2372332666",
          "h_index": 0
        },
        {
          "name": "Vishal Vaddina",
          "profile_url": "https://www.semanticscholar.org/author/1419986651",
          "h_index": 4
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668688",
      "updated_at": "2025-08-11T23:45:32.091815",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16480",
      "title": "Designing for Difference: How Human Characteristics Shape Perceptions of\n  Collaborative Robots",
      "authors": [
        "Sabrina Livanec",
        "Laura Londoño",
        "Michael Gorki",
        "Adrian Röfer",
        "Abhinav Valada",
        "Andrea Kiesel"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.ET (Emerging Technologies)",
        "cs.SY (Systems and Control)",
        "eess.SY (Systems and Control)"
      ],
      "abstract": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.",
      "published_date": "2025-07-22T11:36:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16480v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16480v1",
      "latex_url": "http://arxiv.org/src/2507.16480v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Robots have become essential in many contexts, including manufacturing, construction, healthcare, rehabilitation, and education. As advances in AI-based robotics increasingly enable robots to be used not only in highly controlled settings but also in human-populated environments, the demand for advanced human-robot interaction is becoming increasingly important. The integration of robots into human-centered work and leisure environments requires a balance between technical as well as human-centered functionality to ensure social acceptance~.\n\n {figure*}\n  \n  [width= ]{cam_teaser.pdf}\n  {Human-Centered Design: A socio-technical approach for teaching sociability to robots, leveraging learning methods and data from social studies.}\n\n {figure*}\n\nWithin the field of social robotics, human-robot collaboration (HRC) is emerging as a significant area of research with numerous applications in the service, social, manufacturing, and industrial context~. However, since robots are expected to work alongside and collaborate directly with humans, major concerns arise regarding safety and ethics, as well as ensuring human comfort. Hence, robots are expected not only to operate efficiently and safely in human environments but also to adapt to the personal characteristics of the individuals with whom they are expected to collaborate. Establishing interactions between humans and robots that feel natural in the sense of human-like and socially appropriate is considered a crucial step towards achieving these goals~  {xie2023chatgpt}.\n\nAs humans exhibit distinct preferences when engaging and collaborating with others, shaped by both cultural heritage and individual attributes, such as gender, age, or disabilities, etc., diverse groups hold varying perceptions and expectations of adaptive robots, influenced by their personal and cultural characteristics~. Some studies suggest that, from the user's perspective, interaction with a robot is similar to interaction with fellow humans~. Given that collaborative robots are expected to operate in close proximity to and interact directly with individuals, it is essential to adopt a human-centered perspective in robot design to address the specific needs, preferences, and expectations humans have when collaborating with a robot. Therefore, it is important to analyze and design human-robot collaboration from an interdisciplinary perspective, including robotics, engineering sciences as well as cognitive science, psychology, and ethics~. In  {fig:Figure 15.drawio.png}, we highlight the essential workflow involved in designing and implementing a human-centered approach to robot learning.\n\nWe adopt an interdisciplinary perspective to investigate how the perception of robot behavior is influenced by the personal characteristics of the human collaboration partner. The aim of this study is to assess the importance of social robots adjusting their behavior to the individual traits of human collaborators in domestic environments. To achieve this, we designed a case study centered on a classic human-robot collaboration task that involves the joint unpacking of a shopping basket. We assess whether humans perceive the assistance provided by a robot in emptying a shopping basket as efficient and useful, on the one hand, and as comfortable and socially appropriate, on the other. In the present study, the presence of robots in human-inhabited environments is understood as the integration of social agents capable of engaging in high-level social interaction and potentially reshaping our very understanding of what constitutes social interaction. Accordingly, the robot is not merely viewed as a machine, but is instead conceptualized as a social agent within the interactional space.\n\nIn our study, we focused on the design characteristics of robot behavior related to timing, rhythm, and fluency in the interaction. For the robot's design, engineers were involved who were able to assess the level of accuracy of the simulation carried out. We considered four different dimensions of robot behaviors ('antisocial', 'midFluency', 'maxFluency', and 'alternating items') in two different handing conditions ('handover' and 'no-handover'). We varied seven different types of robot behavior (see 3.2) and combined these robot behaviors with characteristics of the human collaboration partner, varying four different human conditions (young female, young male, disabled, and aged). In total, we compared 28 variations of the human-robot collaboration task. We recorded these variations on video and asked participants to assess them from an observer's perspective in an online study.\n\nSince the type of advanced social robots that perform complex tasks in domestic environments and adapt flexibly to the humans around them are not mature, market-ready products, we had to assume that the participants would have little to no experience with advanced social robots in practice. Furthermore, we also critically questioned how easy or difficult it would be for participants to evaluate the variations of the human-robot collaboration task from an observer's perspective, based solely on video footage, and to relate the robot's behavior to the different human characteristics. To address these concerns, we provided participants with a scenario text at the beginning of the survey that provided information about social robots and emphasized the importance of robot adaptability to different physical conditions of human collaboration partners.\n\nIn addition, we hypothesized that participants would make more differentiated assessments if they had thought more intensively and deeply about the opportunities and challenges of human-robot collaboration and the associated emotions before watching the videos. To foster reflection, we asked half of the participants to create a cognitive-affective map (CAM) on human-robot collaboration before completing the questionnaire survey. Cognitive-affective mapping is a kind of mind mapping technique. Participants create and connect concept nodes on a given topic and additionally assign affective connotations to each concept node (see e.g.,~).\n\nWe therefore address two research questions (RQs):\n {itemize}\n   RQ-HRC: How do participants assess a human-robot collaboration (HRC) from the observer's perspective with regard to the combination of robot behavior and human physical preconditions?\n   RQ-CAM: Does prior general reflection on human-robot collaboration using cognitive-affective mapping (CAM) lead to differences in the assessment of the specific human-robot collaboration in our study?\n {itemize}\n\nThis study aims to contribute to the field of social robotics from an interdisciplinary perspective, focusing on the development of robots that cater to human preferences and needs within a specific social context. Understanding these factors not only supports the improvement in individuals' willingness to engage with robots, but also shapes their broader perception of robotic technologies~. Furthermore, adopting this socio-technical approach will drive progress in social robotics by establishing a pathway for successful and safe integration, as well as acceptance of robots in everyday social environments.\n\nIn the following sections, we provide an overview of related work conducted in the context of human-robot collaboration, human-centered robot design approaches, and cognitive-affective mapping. We provide a comprehensive description of the methodological details of our study and present the results of evaluating our hypotheses. After discussing the results, we end with a conclusion and an outlook on future research.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sn-article.tex",
      "rlhf_score": 0.464,
      "weak_supervision_score": 0.32,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.299,
      "datasets_score": 0.334,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an empirical study on how human characteristics influence perceptions of collaborative robot behaviors, using video evaluations and cognitive-affective mapping to gather feedback. It does not involve training AI models, creating reward models, or using reinforcement learning to fine-tune systems based on human-ranked data. While human feedback is collected, it is analyzed for design insights rather than applied in an RLHF framework.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667764",
      "updated_at": "2025-08-11T23:43:05.606852",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16488",
      "title": "ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination\n  Detection in LLMs",
      "authors": [
        "Zhenliang Zhang",
        "Xinyu Hu",
        "Huixuan Zhang",
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large language models (LLMs) excel at various natural language processing\ntasks, but their tendency to generate hallucinations undermines their\nreliability. Existing hallucination detection methods leveraging hidden states\npredominantly focus on static and isolated representations, overlooking their\ndynamic evolution across layers, which limits efficacy. To address this\nlimitation, we shift the focus to the hidden state update process and introduce\na novel metric, the ICR Score (Information Contribution to Residual Stream),\nwhich quantifies the contribution of modules to the hidden states' update. We\nempirically validate that the ICR Score is effective and reliable in\ndistinguishing hallucinations. Building on these insights, we propose a\nhallucination detection method, the ICR Probe, which captures the cross-layer\nevolution of hidden states. Experimental results show that the ICR Probe\nachieves superior performance with significantly fewer parameters. Furthermore,\nablation studies and case analyses offer deeper insights into the underlying\nmechanism of this method, improving its interpretability.",
      "published_date": "2025-07-22T11:44:26+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16488v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16488v1",
      "latex_url": "http://arxiv.org/src/2507.16488v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large language models (LLMs) demonstrate remarkable performance across various natural language processing tasks . However, these models are still prone to generating hallucinations, which are nonsensical or irrelevant content that deviates from the intended output . This issue highlights the critical need for effective methods of hallucination detection.\n\nVarious methods for hallucination detection exist. Mainstream approaches analyze generated output through consistency checks or reference comparisons , while probability-based methods focus on logit probability uncertainty . Another approach examines hidden states (e.g., embedding vectors) across LLM layers to detect hallucinations . Methods based on output or logit probabilities often require ground truth references or multiple generations for consistency. In contrast, hidden state based detection offers the advantage of being reference-free, eliminating the need for external sources.\n\nCurrent hallucination detection methods based on hidden states can be broadly categorized into training-based and training-free methods. Training-based methods often involve training individual or combination probes , or using semantic entropy probes , whereas training-free methods calculate detection metrics directly from hidden states .\nHowever, existing methods typically focus on static, high-dimensional hidden states (around 4000 dimensions), which limits feature extraction capabilities. These methods make it challenging to capture the updates of hidden states and the cross-layer evolution of the residual stream, ultimately restricting the effectiveness of hallucination detection.\n\nTo overcome these limitations, we introduce a novel approach that shifts the focus from the hidden states themselves to their update process across layers. Specifically, the ICR Score (Information Contribution to Residual Stream) quantifies the contribution of different modules (e.g., FFN or self-attention) to hidden state updates at each layer. Empirical results demonstrate that the ICR Score captures a stable and consistent pattern of residual stream updates, showing strong potential for distinguishing hallucinations.\n\nWe further introduce the ICR Probe, which aggregates ICR Scores across all layers to capture the comprehensive dynamics of the residual stream.\nExperimental results on three mainstream open-source LLMs demonstrate that the ICR Probe effectively detects hallucinations and outperforms previous methods across multiple datasets. Additionally, ablation studies are conducted to reveal the underlying mechanisms.\nIn summary, our core contributions are:\n {itemize}\n  Novel Detection Signal: We propose a hallucination detection method by focusing on the update patterns of hidden states across layers, introducing the ICR Score, a metric that captures the dynamic evolution of residual stream updates.\n\n  ICR Probe Development: We introduce the ICR Probe, an effective and robust tool for hallucination detection, offering superior performance with fewer parameters.\n\n  Validation: We conduct extensive empirical evaluations, showcasing the effectiveness, generalizability, and interpretability of our method across various datasets, reinforcing the understanding of its underlying mechanisms {The code is available in  {https://github.com/XavierZhang2002/ICR_Probe}}.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.364,
      "datasets_score": 0.319,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on detecting hallucinations in LLMs by analyzing hidden state dynamics and introducing the ICR Score and ICR Probe. It does not involve training models with human feedback, reward models, or reinforcement learning techniques, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for hallucination detection based on hidden state updates in LLMs, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668697",
      "updated_at": "2025-08-11T23:43:05.607046",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16506",
      "title": "PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium\n  Specimens",
      "authors": [
        "Youcef Sklab",
        "Florian Castanet",
        "Hanane Ariouat",
        "Souhila Arib",
        "Jean-Daniel Zucker",
        "Eric Chenin",
        "Edi Prifti"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning-based classification of herbarium images is hampered by\nbackground heterogeneity, which introduces noise and artifacts that can\npotentially mislead models and reduce classification accuracy. Addressing these\nbackground-related challenges is critical to improving model performance. We\nintroduce PlantSAM, an automated segmentation pipeline that integrates YOLOv10\nfor plant region detection and the Segment Anything Model (SAM2) for\nsegmentation. YOLOv10 generates bounding box prompts to guide SAM2, enhancing\nsegmentation accuracy. Both models were fine-tuned on herbarium images and\nevaluated using Intersection over Union (IoU) and Dice coefficient metrics.\nPlantSAM achieved state-of-the-art segmentation performance, with an IoU of\n0.94 and a Dice coefficient of 0.97. Incorporating segmented images into\nclassification models led to consistent performance improvements across five\ntested botanical traits, with accuracy gains of up to 4.36% and F1-score\nimprovements of 4.15%. Our findings highlight the importance of background\nremoval in herbarium image analysis, as it significantly enhances\nclassification accuracy by allowing models to focus more effectively on the\nforeground plant structures.",
      "published_date": "2025-07-22T12:02:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16506v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16506v1",
      "latex_url": "http://arxiv.org/src/2507.16506v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Plants are a fundamental component of Earth’s biodiversity, shaping ecosystems, forming the basis of trophic networks, and playing a crucial role in regulating the balance between carbon dioxide and oxygen . However, biodiversity currently faces unprecedented threats from climate change and human activities . Natural history collections, particularly herbarium specimens, hold hundreds of years of data documenting the evolution of biodiversity and the environment . These collections provide invaluable insights into plant distribution, plant morphology, and environmental changes over centuries .\n\nRecent digitization efforts have significantly enhanced the accessibility of these herbarium collections . Institutions such as the National Museum of Natural History (MNHN)( {https://www.mnhn.fr/en}) in Paris have spearheaded large-scale digitization projects, making millions of specimens available online through platforms like ReColNat ( {https://www.recolnat.org/en/}) and the Global Biodiversity Information Facility (GBIF) ( {https://www.gbif.org}). These digitized collections enable researchers to conduct high-throughput analyses of plant traits, such as leaf size, shape, and organ counts . They also support advanced studies in crop quality assessment and disease classification or even soils evolution . However, to efficiently navigate and utilize these massive datasets, automatic methods are essential for extracting comprehensive metadata and descriptive characteristics (traits) related to plant morpho-anatomy or traits specific to each specimen -e.g. its conservation state, or the presence / absence of particular organs .\n\nDeep Learning (DL) has emerged as a promising approach for analyzing herbarium specimens . Over the last two decades, DL has driven profound advancements in artificial intelligence, leading to the development of innovative architectures such as convolutional neural networks (CNNs) and Vision Transformers (ViTs) . These models have achieved remarkable performance in computer vision tasks, including image classification and object detection . In botanical research, DL models have been successfully applied to identify and annotate plant organs in herbarium images .\n\nDespite their potential, DL models often struggle with the heterogeneous backgrounds and artifacts present in herbarium images, such as labels, scale bars, overlapping plant structures, and aged paper textures (Figure ). The presence of background noise can lead these models to learn irrelevant features, thereby reducing their performance in downstream tasks like trait classification and species identification . Therefore, isolating the foreground plant components (e.g., leaves, stems, flowers, and fruits) from non-plant background elements is essential to improve the accuracy and robustness of these models .\n\n {figure}\n  \n  { [width=.8 ]{figures/noise-examples.png}}\n {Examples illustrating the diversity in paper color, paper quality and the non-plant elements present in herbarium sheets (non-exhaustive)\n}\n\n {figure}\n\nThis task can be effectively addressed using image segmentation techniques . However, traditional segmentation models often struggle with these challenges, as they may focus on irrelevant features such as paper texture instead of plant morphology. Recent advancements in segmentation, particularly foundation models like the Segment Anything Model (SAM) and its enhanced version SAM2 , offer strong generalization capabilities across various segmentation tasks . However, applying these models to herbarium images requires domain-specific fine-tuning and effective prompt generation, which can limit their scalability for large datasets .\n\nIn this work, we propose an automatic segmentation pipeline, called PlantSAM, that integrates the YOLOv10 object detection model with SAM2 for segmenting plant regions. YOLOv10 generates bounding boxes as prompts, guiding SAM2 to segment plant structures. This approach removes the need for manual segmentation, enabling scalable segmentation across large herbarium datasets for downstream tasks. Our contributions are as follows:\n {itemize}\n   Development of a novel segmentation pipeline combining YOLOv10 for prompt generation and SAM2 for plant segmentation.\n   Fine-tuning of SAM2 and YOLOv10 on a curated dataset of herbarium images, addressing domain-specific challenges such as non-uniform backgrounds and complex plant structures.\n   Evaluation of segmentation performance using Intersection over Union (IoU) and Dice coefficient, demonstrating significant improvements over UNet and SAM2.\n   Assessment of the impact of segmentation on classification performance, showing accuracy improvements of up to 4.36% and F1-score gains of 4.15% across multiple botanical traits.\n   Integration of the pipeline into a semi-automatic annotation tool to streamline mask refinement and improve the overall quality of the dataset.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "PlantSAM_Submitted_ArXiev.tex",
      "rlhf_score": 0.268,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.301,
      "distributed_training_score": 0.293,
      "datasets_score": 0.339,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668199",
      "updated_at": "2025-08-11T23:43:05.606951",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16507",
      "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in\n  Real-World Applications",
      "authors": [
        "Jean Lelong",
        "Adnane Errazine",
        "Annabelle Blangero"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large\nLanguage Models (LLMs) but often fall short on complex queries, delivering\nlimited, extractive answers and struggling with multiple targeted retrievals or\nnavigating intricate entity relationships. This is a critical gap in\nknowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system\nfor exploring the scientific data of INRAE (France's National Research\nInstitute for Agriculture, Food and Environment). INRAExplorer employs an\nLLM-based agent with a multi-tool architecture to dynamically engage a rich\nknowledge base, through a comprehensive knowledge graph derived from open\naccess INRAE publications. This design empowers INRAExplorer to conduct\niterative, targeted queries, retrieve exhaustive datasets (e.g., all\npublications by an author), perform multi-hop reasoning, and deliver\nstructured, comprehensive answers. INRAExplorer serves as a concrete\nillustration of enhancing knowledge interaction in specialized fields.",
      "published_date": "2025-07-22T12:03:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16507v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16507v1",
      "latex_url": "http://arxiv.org/src/2507.16507v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Effective digital knowledge utilization demands relevant, exhaustive and structured information retrieval. While Retrieval-Augmented Generation (RAG) grounds Large Language Models (LLMs) in curated, trustworthy information  {lewis2020retrieval}, prevalent architectures —termed classical RAG— exhibit key limitations. Classical RAG typically retrieves a limited set (top-k) of semantically similar text chunks via vector search  {karpukhin2020dense, gunther2023jina} to contextualize an LLM. While valuable for anchoring LLM responses and extractive question answering, this 'top-k snippet' approach is often insufficient for queries requiring exhaustive lists, synthesis from multiple distinct data points, or navigating complex relational paths (e.g., author to publications to funding projects).\n\nTo address these shortcomings, we introduce INRAExplorer, a method that synergizes agentic RAG  {singh2025agenticsurvey} for dynamic reasoning and Knowledge Graph (KG)-enhanced RAG  {zhang2025surveygraphrag, zhu2025knowledgegraphguidedrag} for structured, exhaustive retrieval. INRAExplorer deeply incorporates KG querying as a core agentic capability, enabling it to overcome the single-pass, limited-context nature of classical RAG. This fusion delivers precise, relationally-aware retrieval from KGs, combined with the adaptive, multi-hop reasoning of an LLM-driven agent.\n\nWhile the integration of Knowledge Graphs with LLMs is gaining traction, many current approaches primarily use KGs by performing a sophisticated form of map-reduce summarization over graph-retrieved data  {edge2024graphrag}. In contrast, INRAExplorer, drawing inspiration from the need for deeper reasoning similar to human investigative processes, focuses on enabling the LLM agent to construct chains of thought leveraging several ways to retrieve information. Our system empowers the agent to dynamically navigate between different tools, gathering evidence, evaluating intermediate findings, and planning subsequent steps. This allows INRAExplorer to act more like a human researcher, meticulously assembling pieces of information to construct a comprehensive and nuanced answer, rather than simply summarizing pre-existing snippets of information.\n\n {table*}[t]\n  \n  {Distribution of Node Types in the INRAExplorer Knowledge Graph (Total Nodes: 417,030)}\n\n  {tabular}{@{}lrrp{10cm}@{}}\n  \n Node Type & Count & Percentage & Description\n\n  \n Author & 233,728 & 56.0% & Researchers and authors of scientific publications\n\n Keyword & 96,588 & 23.2% & Keywords associated with publications (declared by authors)\n\n Publication & 38,791 & 9.3% & Scientific articles and other academic publications\n\n Software & 21,617 & 5.2% & Software developed or used in research\n\n Concept & 13,591 & 3.3% & Concepts from the INRAE thesaurus identified in publications\n\n Journal & 5,563 & 1.3% & Scientific journals where works are published\n\n Project & 3,999 & 1.0% & Funded research projects\n\n Domain & 2,595 & 0.6% & Thematic domains of the INRAE thesaurus\n\n ResearchUnit & 299 & 0.1% & INRAE research units and laboratories\n\n Dataset & 240 & 0.1% & Datasets used or produced in research\n\n Region & 19 & 0.0% & Geographic regions where research units are located\n\n  \n  {tabular}\n {table*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "pap_ECAI.tex",
      "rlhf_score": 0.382,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.47,
      "distributed_training_score": 0.333,
      "datasets_score": 0.383,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces INRAExplorer, an agentic RAG system using LLMs and knowledge graphs for multi-hop reasoning, iterative queries, and dynamic navigation. However, it does not mention or adapt the iterative refinement process of diffusion models, nor does it treat Chain-of-Thought as a single entity for holistic correction over steps. The focus is on agentic and KG-enhanced RAG, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668706",
      "updated_at": "2025-08-11T23:43:05.607048",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16511",
      "title": "Analogy making as amortised model construction",
      "authors": [
        "David G. Nagy",
        "Tingke Shen",
        "Hanqi Zhou",
        "Charley M. Wu",
        "Peter Dayan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Humans flexibly construct internal models to navigate novel situations. To be\nuseful, these internal models must be sufficiently faithful to the environment\nthat resource-limited planning leads to adequate outcomes; equally, they must\nbe tractable to construct in the first place. We argue that analogy plays a\ncentral role in these processes, enabling agents to reuse solution-relevant\nstructure from past experiences and amortise the computational costs of both\nmodel construction (construal) and planning. Formalising analogies as partial\nhomomorphisms between Markov decision processes, we sketch a framework in which\nabstract modules, derived from previous construals, serve as composable\nbuilding blocks for new ones. This modular reuse allows for flexible adaptation\nof policies and representations across domains with shared structural essence.",
      "published_date": "2025-07-22T12:16:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16511v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16511v1",
      "latex_url": "http://arxiv.org/src/2507.16511v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Humans, like artificial reinforcement learning (RL) agents, maintain internal representations of their environment, enabling them to interpret sensory inputs and predict action outcomes. In RL, these representations usually take the form of Markov decision processes (MDPs {We use MDP here as a shorthand for the broader family of Markov decision process-based models, including extensions such as POMDPs (partially observable MDPs), BAMDPs (Bayes-adaptive MDPs), and IPOMDPs (interactive POMDPs), which allow for uncertainty, learning, and multi-agent reasoning, respectively.}) --- abstract formalisations of the environment that support planning and decision making. Typically, formulating such an MDP --- defining the states, observations, actions, rewards --- is performed manually by an external designer  [although recent approaches allow the learning of this model from experience in restricted domains;][]{schrittwieser2020mastering,hafner2025mastering}.\nFor example, a vacuum-cleaning robot might perceive its environment in terms of ‘obstacles,’ ‘dirt concentration,’ and ‘returning to the charging dock’---rather than ‘minimal yet cozy living rooms,’ ‘ringing phones,’ or ‘following one’s dreams’---reflecting the designer’s assumptions about which abstractions are relevant for vacuuming floors.\n\nUnlike a robot vacuum confined to navigating an apartment, humans must operate in vastly more diverse situations: navigating not just an apartment, but an entire city (on foot, by bike, or by public transport); decorating an apartment or building a shelter, working in an office, negotiating, interpreting social nuances, or solving math problems. Crucially, the choice of the abstract representation used in these situations is deeply intertwined with decision-making processes operating over it --- an effective representation can render difficult problems trivially easy to solve, while a poor one can make a solution impossible  {giunchiglia_theory_1992, abel2022theory, ravindran_smdp_2003}.\n\nWe argue that the vast diversity of situations humans encounter rules out reliance on a pre-designed MDP --- or even a predetermined set of MDPs. Instead, humans must be capable of constructing MDPs for novel situations on-demand. In other words, they must fill the shoes of not just the {user}, but also the {designer}, of their own internal MDPs --- yielding what we refer to throughout this paper as situation specific construals, following  {ho_people_2022}. Both these roles have to be played in the face of limited cognitive resources. Unfortunately, the obvious solution of simplification leads to variants of the notoriously difficult ``frame problem''  {dennett_cognitive_1990,icard_resource-rational_2015}.\n\nHere, we take inspiration from a long body of work focusing on the central role of analogy and metaphor in human cognition  {hofstadter_surfaces_2013,lakoff_metaphors_2008,gentner_analogy_2017}. We propose that analogies (understood broadly) underlie the human ability for on-demand model construction, and more generally offer a powerful mechanism by which RL agents can flexibly adapt past knowledge to novel circumstances. Specifically, we formalise potential analogies as mappings between MDPs that preserve solution-relevant structure (and in some cases the entire solution). Through such analogical mappings, humans can construct new internal models by adapting and combining abstract MDPs that were effective in past situations. This reuse enables the transfer of prior computations, effectively amortising both the construction and solution costs of models for novel situations  {gershman_amortized_2014, dasgupta_memory_2021}.\n\nAs building blocks of analogies, we propose that the brain extracts structural regularities across diverse situations in the form of reusable fragments of MDPs, and incrementally compiles them into a library of consistently useful modules. As fragments are adapted and reused across increasingly varied contexts, they become progressively abstracted, gaining broader applicability as sources of analogy. Over time, such modules may become decoupled from their original contexts entirely, forming widely reusable conceptual primitives such as 'door', 'stairs', 'fire' or 'clock' (Fig. ).\n {figure}[ht]\n  {center}\n  [width=0.9 ]{fig-modular.jpg}\n  {center}\n  {Library of abstract MDP modules for amortised model construal.}\n\n {figure}\n\nConsider a small child observing that the front door (previously thought part of the wall) can be unlocked and opened by inserting and turning a specific object --- the key. This transforms it into a functional door, similar to those inside the house. Crucially, only a specific key will work; other objects, even if similarly shaped, will fail.\nYears later, the child is given their first email account. A parent explains, through an explicit analogy, that the password is the 'key' for their account --- an 'email-key'. Of course, the analogy is imperfect, e.g. there is no ‘turning’ the password, only clicking on the login button. Yet, the analogy likely helps the child greatly. For instance, they now understand that no other password, even of similar length or characters is likely to open the account. They know to guard the password, because if someone steals it they will gain access to their messages. But if they do want to grant someone access, they can simply share the password.\n\nDespite surface differences in terms of low-level actions and states, the key/door and the email/password situations share high-level structure. By seeing the login screen as a 'door' and using the password as a 'key', the child can reuse a familiar mental representation, including transition dynamics, possible actions, and useful policies. The analogy preserves the situation’s essence, allowing the child to transfer knowledge across domains.\n\nIn the following, we formulate the challenge of on-demand model construction, proposing analogy as a mechanism for amortising both the construction and solution costs of MDPs. We argue that humans construct models not only by analogy to single past situations, but also by recomposing abstract modules drawn from an internal library that they concurrently grow. We formalise analogies as partial MDP homomorphisms that preserve structure relevant for planning and decision making. We end with an overview of key computational challenges inherent in forming, maintaining and using such a library of composable modules.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.395,
      "weak_supervision_score": 0.34,
      "diffusion_reasoning_score": 0.52,
      "distributed_training_score": 0.321,
      "datasets_score": 0.297,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on analogy making as a mechanism for constructing and reusing internal models in reinforcement learning, formalized through partial homomorphisms between Markov Decision Processes (MDPs). It discusses how analogies enable the transfer of knowledge across domains but does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion for logical reasoning tasks. There is no mention of Chain-of-Thought or holistic correction mechanisms, making the paper's contributions entirely unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668714",
      "updated_at": "2025-08-11T23:43:05.607050",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16514",
      "title": "The Ever-Evolving Science Exam",
      "authors": [
        "Junying Wang",
        "Zicheng Zhang",
        "Yijin Guo",
        "Farong Wen",
        "Ye Shen",
        "Yingji Liang",
        "Yalun Wu",
        "Wenzhe Li",
        "Chunyi Li",
        "Zijian Chen",
        "Qi Jia",
        "Guangtao Zhai"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As foundation models grow rapidly in capability and deployment, evaluating\ntheir scientific understanding becomes increasingly critical. Existing science\nbenchmarks have made progress towards broad **Range**, wide **Reach**, and high\n**Rigor**, yet they often face two major challenges: **data leakage risks**\nthat compromise benchmarking validity, and **evaluation inefficiency** due to\nlarge-scale testing. To address these issues, we introduce the **Ever-Evolving\nScience Exam (EESE)**, a dynamic benchmark designed to reliably assess\nscientific capabilities in foundation models. Our approach consists of two\ncomponents: 1) a non-public **EESE-Pool** with over 100K expertly constructed\nscience instances (question-answer pairs) across 5 disciplines and 500+\nsubfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,\nand **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled\nand validated to enable leakage-resilient, low-overhead evaluations.\nExperiments on 32 open- and closed-source models demonstrate that EESE\neffectively differentiates the strengths and weaknesses of models in scientific\nfields and cognitive dimensions. Overall, EESE provides a robust, scalable, and\nforward-compatible solution for science benchmark design, offering a realistic\nmeasure of how well foundation models handle science questions. The project\npage is at: https://github.com/aiben-ch/EESE.",
      "published_date": "2025-07-22T12:22:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16514v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16514v1",
      "latex_url": "http://arxiv.org/src/2507.16514v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "With the rapid development of large-scale foundation models, there arises an urgent need to evaluate their scientific abilities in a reliable and systematic way  {aibench, opportunities,training,Wang_2025_CVPR,firoozi2025foundation}. Science benchmarks play a vital role in this process, offering a standardized, quantitative foundation for assessing how well models understand and reason about scientific concepts.\nAs science benchmarks continue to evolve, the research community is gradually converging on a shared understanding of what defines a high-quality science benchmark (e.g., MMLU~ {mmlu}, SuperGPQA~ {SuperGPQA}, GSM8K~ {GSM8K}, ScienceQA~ {ScienceQA}, HLE~ {hle}, SciEval~ {Scieval}). Naturally, this prompts the question:\n\n {quote}\nWhat constitutes a good science benchmark?\n {quote}\n\nIn general, an ideal benchmark should meet three essential criteria: broad Range, wide Reach, and high Rigor, which together ensure that it is:\n1) Extensive in scale ({Range}): comprising a large volume of instances to support robust and statistically meaningful evaluation,\n2) Diverse in scope ({Reach}): spanning a broad array of scientific disciplines and offering varied question formats to capture different cognitive and reasoning skills,\n3) Sound in methodology ({Rigor}): constructed through a careful, principled pipeline with rigorous quality assurance and verification processes.\n\n {figure*}[t]\n \n [width= ]{img/spotlight.pdf}\n {Overview of EESE-Pool construction, which adheres to the principles of Range (vast quantity of instances), Reach (diverse field and question format), and Rigor (systematic and rigor data construction). Specifically, EESE-Pool comprises over 100K science question–answer pairs spanning 5 disciplines and over 500 subfields.}\n\n {figure*}\n\nWhile many existing benchmarks strive to meet these criteria, new challenges emerge that limit their effectiveness in evaluating the scientific capacities of foundation models.\nFirst, there is a growing concern about data leakage~ {xu2024benchmarking, leakbench, lopez2024inter,antileakbench}. Once a benchmark is publicly available, there is a non-negligible risk that it could be inadvertently included in training data, especially when data is gathered via large-scale web scraping. Such leakage distorts the evaluation validity, making performance scores unreliable.\nSecond, there is the issue of evaluation inefficiency~ {Zhou_2025_CVPR, redundancy_principle, gupta2024improving, wen2025improve}. While increasing the number of evaluation instances can improve benchmark reliability, large-scale evaluation introduces significant computational and financial overheads. This evaluation cost can hinder rapid iteration in model development.\n\nTo balance high-quality benchmark design with practical needs like leakage-resistance and evaluation efficiency, we propose a new benchmark: The Ever-Evolving Science Exam (EESE).\nConcretely, a two-level strategy is adopted:\n1) We build a large-scale, high-quality, non-public instances repository, named EESE-Pool, which contains over 100,000 science instances. This pool is constructed under strict principles of Range, Reach, and Rigor.\n2) We periodically sample a dynamic subset of 500 instances, called EESE, for actual evaluation. This subset is carefully curated to maintain Range, {Reach}, and {Rigor}, while mitigating leakage risk and reducing evaluation inefficiency through regular updates.\nHence, EESE not only faithful and aligned with the principles of a good science benchmark, but offers low-cost, leakage-resistant, and continuously refreshed evaluations that better reflect real-world generalization and robustness of model.\n\nSpecifically, to construct the EESE-Pool, we design a streamlined Data Engine that ensures Range, {Reach}, and {Rigor} through three sequential stages.\nIn the Transcription stage, we collect raw instances from textbooks, public databases, and online sources. These instances are then standardized into a unified format and classified into 163 subfields based on academic taxonomy  {GB/T13745-2009}.\nIn the Expansion stage, these initial fields are enriched by engaging experts to develop high-quality instances, expanding the coverage to over 500 subfields. In the\nCategorization stage, we assign difficulty levels to each instance by evaluating model performance and manually validating correctness.\nTo systematically raise instance quality and mitigate trivial or ambiguous cases, a dedicated Data Refinement process is introduced. This process strategically improves the instance through a Parallel Three-Branch Refinement Framework: {Enhancement By Distraction}, {Enrichment By Cross-Disciplinary}, and {Refinement By Expert}.\n\nTo derive EESE, a representative, regular-updating, leakage-resilient, and low-overhead, evaluation set, we adopt a dynamic sampling strategy alongside expert check on EESE-Pool. Notably, we evaluate 32 leading models on EESE-Pool and EESE, and provide actionable guidance for the development of forward-compatible science benchmarks.\n\nIn summary, our key contributions are as follows:\n {itemize}\n   A large-scale, high-quality science benchmark pool: We construct EESE-Pool, a 100K+ science question-answer pair pool across 5 disciplines and 500+ subfields, with diverse formats and rigorous quality control. We design three-stage Data Engine (Transcription, Expansion, and Categorization) and Data Refinement (a Parallel Three-Branch Refinement Framework) to ensure range, reach, and rigor.\n   A dynamic, leakage-resilient evaluation set: We propose EESE, a 500-instance subset periodically updated (regular resampling 500 instances from the EESE-Pool), maintaining representativeness while reducing leakage risk and evaluation overhead.\n   Comprehensive evaluation of LLMs: We evaluate 32 leading models (open- and closed-source) on EESE-Pool and EESE, revealing significant performance gaps across disciplines, the effectiveness of refinement in improving quality, and the trade-offs between inference cost and science ability. The findings offer insights for future science benchmarks.\n {itemize}\n\n {figure*}[t]\n \n [width=0.9 ]{img/framework.pdf}\n {EESE-Pool Construction Framework. The three-stage Data Engine (Transcription, Expansion, Categorization) with a systematic Data Refinement process ensures large-scale coverage, expert-enriched content, difficulty stratification, and iterative quality improvement, laying a foundation for dynamic, leakage-resilient EESE.}\n\n {figure*}",
      "intro_extraction_method": "scrape_entire_folder",
      "tex_file_name": "arxiv_main.tex",
      "rlhf_score": 0.334,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.386,
      "datasets_score": 0.432,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on creating a high-quality, expert-curated benchmark for evaluating foundation models, emphasizing rigorous data construction and refinement processes. It does not involve training models using programmatically generated labels from noisy or imprecise sources, which is the core of weak supervision. Instead, it relies on expert involvement and manual validation, making it unrelated to this topic.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of a new dataset (EESE-Pool and EESE) for AI benchmarking, including detailed methodologies for dataset curation, such as multi-stage pipelines and refinement processes. It directly aligns with research on introducing, analyzing, and benchmarking datasets for machine learning applications, as it addresses dataset design, scalability, and evaluation efficiency.",
      "summary": "The paper introduces the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to evaluate foundation models' scientific capabilities while addressing challenges like data leakage and evaluation inefficiency in existing benchmarks. It features a non-public EESE-Pool with over 100,000 expertly constructed science question-answer pairs across five disciplines and over 500 subfields, constructed through a multi-stage pipeline emphasizing range, reach, and rigor, and a periodically updated 500-instance subset for efficient, leakage-resilient evaluations. Experiments on 32 open- and closed-source models demonstrate EESE's effectiveness in differentiating model strengths across scientific fields and cognitive dimensions, providing insights for future benchmark design.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a dynamic, evolving benchmark with a large non-public pool and periodic sampling to mitigate data leakage and inefficiency, cleverly combining existing ideas in a new way for science evaluation. While it advances benchmark design, it builds on known concepts rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work addresses critical issues in AI benchmarking, potentially influencing a wide range of future research and commercial applications by providing a scalable, leakage-resistant framework for evaluating foundation models. Its forward-compatible design could lead to broader adoption in model development and assessment practices.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper delivers a high-quality contribution with practical innovations in benchmark design, making it valuable for researchers in AI evaluation and foundation models to understand and build upon. It is significant but not essential for those outside this specific subfield.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/84ab255f983289f94eba8ab5f421ab9ca426fd18",
      "h_index_fetch_method": "full_id",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 23,
      "average_h_index": 4.5,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Junying Wang",
          "profile_url": "https://www.semanticscholar.org/author/2364795871",
          "h_index": 1
        },
        {
          "name": "Zicheng Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2116459218",
          "h_index": 23
        },
        {
          "name": "Yijin Guo",
          "profile_url": "https://www.semanticscholar.org/author/2364936231",
          "h_index": 1
        },
        {
          "name": "Farong Wen",
          "profile_url": "https://www.semanticscholar.org/author/2320722542",
          "h_index": 3
        },
        {
          "name": "Ye Shen",
          "profile_url": "https://www.semanticscholar.org/author/2372433643",
          "h_index": 0
        },
        {
          "name": "Yingji Liang",
          "profile_url": "https://www.semanticscholar.org/author/2350819420",
          "h_index": 1
        },
        {
          "name": "Yalun Wu",
          "profile_url": "https://www.semanticscholar.org/author/2364789332",
          "h_index": 0
        },
        {
          "name": "Wenzhe Li",
          "profile_url": "https://www.semanticscholar.org/author/2364823415",
          "h_index": 0
        },
        {
          "name": "Chunyi Li",
          "profile_url": "https://www.semanticscholar.org/author/2109738874",
          "h_index": 17
        },
        {
          "name": "Zijian Chen",
          "profile_url": "https://www.semanticscholar.org/author/2268795764",
          "h_index": 7
        },
        {
          "name": "Qi Jia",
          "profile_url": "https://www.semanticscholar.org/author/2372819155",
          "h_index": 0
        },
        {
          "name": "Guangtao Zhai",
          "profile_url": "https://www.semanticscholar.org/author/2333365277",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668727",
      "updated_at": "2025-08-11T23:45:34.336771",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16518",
      "title": "C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving\n  Reasoning",
      "authors": [
        "Xiuwei Chen",
        "Wentao Hu",
        "Hanhui Li",
        "Jun Zhou",
        "Zisheng Chen",
        "Meng Cao",
        "Yihan Zeng",
        "Kui Zhang",
        "Yu-Jie Yuan",
        "Jianhua Han",
        "Hang Xu",
        "Xiaodan Liang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Recent advances in multimodal large language models (MLLMs) have shown\nimpressive reasoning capabilities. However, further enhancing existing MLLMs\nnecessitates high-quality vision-language datasets with carefully curated task\ncomplexities, which are both costly and challenging to scale. Although recent\nself-improving models that iteratively refine themselves offer a feasible\nsolution, they still suffer from two core challenges: (i) most existing methods\naugment visual or textual data separately, resulting in discrepancies in data\ncomplexity (e.g., over-simplified diagrams paired with redundant textual\ndescriptions); and (ii) the evolution of data and models is also separated,\nleading to scenarios where models are exposed to tasks with mismatched\ndifficulty levels. To address these issues, we propose C2-Evo, an automatic,\nclosed-loop self-improving framework that jointly evolves both training data\nand model capabilities. Specifically, given a base dataset and a base model,\nC2-Evo enhances them by a cross-modal data evolution loop and a data-model\nevolution loop. The former loop expands the base dataset by generating complex\nmultimodal problems that combine structured textual sub-problems with\niteratively specified geometric diagrams, while the latter loop adaptively\nselects the generated problems based on the performance of the base model, to\nconduct supervised fine-tuning and reinforcement learning alternately.\nConsequently, our method continuously refines its model and training data, and\nconsistently obtains considerable performance gains across multiple\nmathematical reasoning benchmarks. Our code, models, and datasets will be\nreleased.",
      "published_date": "2025-07-22T12:27:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16518v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16518v2",
      "latex_url": "http://arxiv.org/src/2507.16518v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Recent advancements in large language models (LLMs) have achieved remarkable progress in solving problems, including mathematics , coding , etc.\nThese capabilities are enabled by advanced strategies, including chain-of-thought prompting , tool-augmented reasoning ,  .\nIn particular, OpenAI o1 and Deepseek-R1 have shown that reinforcement learning plays a critical role in aligning model outputs with desired behaviors by using structured reward signals derived from correctness, consistency, or human preference.\nThis mechanism has proven especially effective in eliciting nuanced self-verification and self-correction behavior in LLMs, thereby reinforcing the reliability and depth of their reasoning chains, particularly in mathematical and logical domains.\n\nDespite these advances, achieving such strong reasoning performance remains heavily reliant on large-scale, high-quality, and {complexity-aligned} datasets. As task complexity increases, collecting suitable training data becomes significantly more costly and difficult, presenting a major bottleneck to further progress. This challenge has sparked growing interest in self-improving paradigms, where models iteratively enhance their capabilities by\n{\ngenerating new synthetic data and refining reasoning traces.}\n\nRecent studies have shown that reasoning abilities can be substantially improved through carefully curated and progressively challenging data.\n\nFor example, OpenVLThinker adapts the self-improvement paradigm to the vision-language domain by iteratively alternating between supervised fine-tuning (SFT) and reinforcement learning (RL), distilling R1-style reasoning traces from text-based models into multimodal contexts.\n\n {table_figs/tabCo-Evolve}\n\nHowever, despite these promising developments, existing approaches face two key limitations:\n(1) Mismatched evolution of visual complexity and textual reasoning difficulty. Prior methods often address visual and textual components in isolation. Some focus on generating visually complex scenes without meaningful reasoning tasks, while others emphasize textual complexity with simplistic visuals.\nThis disconnect restricts the model’s ability to learn integrated cross-modal reasoning strategies.\n(2) The discrepancy between model capability and task difficulty. As models improve over the course of training, their ability to tackle more complex tasks naturally increases. However, current approaches rely on static or manually defined difficulty schedules, which do not adapt to the model’s evolving capability. This misalignment can lead to inefficient training, either under-challenging the model or overwhelming it with excessively difficult data.\n\n {table_figs/figFigevolve}\n\nTo address these challenges, we propose a fully automated, adaptive multimodal learning framework ( ) that jointly evolves both the model and its training data in a closed-loop fashion, with a particular focus on geometric reasoning tasks. Table summarizes the differences between our framework and state-of-the-art methods: Unlike existing methods that rely on static data, our method dynamically adjusts task complexity based on real-time assessments of model performance, ensuring a tighter coupling between model capability and data difficulty throughout the learning process.\n\n{Specifically, to tackle the challenge (1),\nwe incorporate the process into a cross-modal data evolution loop, where\nwe integrate SKETCHPAD-generated complex geometric diagrams with complex problem synthesis (  Figure (a)).}\n\n{GPT-4o is employed to generate auxiliary construction code, then executed via tools ( , Jupyter).}\nFormal image descriptions are produced using the Doubao model. Sub-problems (4--10 per image) are then generated following predefined guidelines and composed into complex reasoning questions. These are filtered using GPT-4o to remove misaligned cases, and validated using DeepSeek's three-step reasoning framework to ensure consistency. This process yields a dataset with semantically aligned visual and textual components.\nFor the challenge (2), we adopt a data-model evolution loop.\nThis loop utilizes data generated from the cross-modal data evolution loop and applies it to iteratively fine-tune the model using SFT and RL.\n\nSFT maintains output structure and coherence, while RL improves generalization through rule-based optimization.\n\nWe introduce a simple error-based filtering method that evaluates sample difficulty via prediction variance over $32$ generations.\n\n{\nBy selecting samples with an error rate $   0.3$, which are prone to error yet still within the model's grasp and pose a meaningful challenge, the framework ensures that task difficulty remains aligned with model capability, achieving continuous improvement over iterations ( , Figure (c)).\n}\n\nFinally, we investigate the impact of different data strategies and iterative training regimes on model performance. Our findings offer insights into the design of effective self-improving frameworks for improving complex multimodal reasoning and guiding the progressive evolution of vision-language models.\n\nOur contributions are summarized as follows:\n\n $ $ We propose a closed-loop self-improving framework, named { }, that jointly evolves training data and model capabilities.\n\n $ $ The proposed framework utilizes two co-evolution loops to improve the compatibility of cross-modal complexity and that between task difficulty and model capability.\n\n $ $ Extensive experiments demonstrate the effectiveness of different data strategies and iterative training regimes, revealing their impact on self-improving frameworks.\n%  {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro_revision.tex",
      "rlhf_score": 0.381,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.53,
      "distributed_training_score": 0.386,
      "datasets_score": 0.417,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a self-improving framework for multimodal models using supervised fine-tuning and reinforcement learning, but it does not involve diffusion models or adapt the iterative refinement process of diffusion for logical tasks. There is no mention of treating a Chain-of-Thought as a single entity for holistic correction via diffusion, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and evolving high-quality multimodal datasets through a cross-modal data evolution loop, including generating complex problems, diagrams, and sub-problems, as well as filtering and validating them. This directly aligns with research on dataset creation, curation methodologies, and benchmarking for AI applications, as evidenced by the framework's emphasis on dataset expansion and release.",
      "summary": "C2-Evo is a framework designed to enhance multimodal large language models (MLLMs) by jointly evolving training data and model capabilities in a closed-loop manner. It addresses challenges in existing methods through a cross-modal data evolution loop, which generates complex multimodal problems with aligned visual and textual complexities using tools like GPT-4o, and a data-model evolution loop, which adaptively selects data based on model performance for supervised fine-tuning and reinforcement learning. Experimental results show consistent performance improvements on mathematical reasoning benchmarks, demonstrating the effectiveness of this co-evolutionary approach in overcoming data and model mismatches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing self-improving techniques with a new cross-modal and adaptive evolution strategy to address specific challenges in MLLMs, though it builds on prior ideas like iterative fine-tuning and reinforcement learning.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of multimodal reasoning and self-improving models, as it introduces practical methods for aligning data complexity with model capabilities.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with innovative strategies for improving MLLMs, making it valuable for researchers in computer vision, language, and machine learning to understand evolving self-improvement techniques.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/81b6143853e3380ccd24eae8e32a47dc94351576",
      "h_index_fetch_method": "full_id",
      "total_authors": 12,
      "authors_found": 12,
      "highest_h_index": 24,
      "average_h_index": 4.416666666666667,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Xiuwei Chen",
          "profile_url": "https://www.semanticscholar.org/author/2346900385",
          "h_index": 1
        },
        {
          "name": "Wentao Hu",
          "profile_url": "https://www.semanticscholar.org/author/2374148600",
          "h_index": 0
        },
        {
          "name": "Hanhui Li",
          "profile_url": "https://www.semanticscholar.org/author/2276604489",
          "h_index": 5
        },
        {
          "name": "Jun Zhou",
          "profile_url": "https://www.semanticscholar.org/author/2298472548",
          "h_index": 1
        },
        {
          "name": "Zisheng Chen",
          "profile_url": "https://www.semanticscholar.org/author/2347655946",
          "h_index": 1
        },
        {
          "name": "Meng Cao",
          "profile_url": "https://www.semanticscholar.org/author/2334443159",
          "h_index": 1
        },
        {
          "name": "Yihan Zeng",
          "profile_url": "https://www.semanticscholar.org/author/2237077457",
          "h_index": 7
        },
        {
          "name": "Kui Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2373720163",
          "h_index": 0
        },
        {
          "name": "Yu-Jie Yuan",
          "profile_url": "https://www.semanticscholar.org/author/2350180100",
          "h_index": 2
        },
        {
          "name": "Jianhua Han",
          "profile_url": "https://www.semanticscholar.org/author/47180442",
          "h_index": 24
        },
        {
          "name": "Hang Xu",
          "profile_url": "https://www.semanticscholar.org/author/2320227478",
          "h_index": 6
        },
        {
          "name": "Xiaodan Liang",
          "profile_url": "https://www.semanticscholar.org/author/2309503120",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669699",
      "updated_at": "2025-08-11T23:46:12.465418",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16524",
      "title": "Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models",
      "authors": [
        "Xiaoyan Wang",
        "Zeju Li",
        "Yifan Xu",
        "Jiaxing Qi",
        "Zhifei Yang",
        "Ruifei Ma",
        "Xiangde Liu",
        "Chao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "New era has unlocked exciting possibilities for extending Large Language\nModels (LLMs) to tackle 3D vision-language tasks. However, most existing 3D\nmultimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or\nsegmenting independent objects to perform these tasks, which limits their\nspatial awareness due to insufficient representation of the richness inherent\nin 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D\nMLLM specifically designed to enhance spatial awareness for 3D vision-language\ntasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM\nintegrates an LLM backbone with a progressive spatial awareness scheme that\nprogressively captures spatial information as the perception field expands,\ngenerating location-enriched 3D scene embeddings to serve as visual prompts.\nFurthermore, we introduce two novel tasks: 3D object distance measurement and\n3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate\nthe model's spatial awareness capabilities. Experimental results demonstrate\nthat Spatial 3D-LLM achieves state-of-the-art performance across a wide range\nof 3D vision-language tasks, revealing the improvements stemmed from our\nprogressive spatial awareness scheme of mining more profound spatial\ninformation. Our code is available at\nhttps://github.com/bjshuyuan/Spatial-3D-LLM.",
      "published_date": "2025-07-22T12:32:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16524v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16524v1",
      "latex_url": "http://arxiv.org/src/2507.16524v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In recent years, Vision-Language Models (VLMs) have rapidly advanced, with 2D Multimodal Large Language Models (MLLMs) demonstrating remarkable capabilities in understanding complex visual scenes.\nConcurrently, much success of developing 3D MLLMs has been achieved on 3D scene understanding.\n3D spatial awareness encompasses the perception of spatial states, such as locations and distances, as well as spatial reasoning and generation derived from this perception, including embodied planning and spatial layout editing.\nWhile diving into 3D world, 3D spatial awareness is one of the keys for 3D MLLMs to perform downstream tasks, such as robotics, virtual reality and interior design.\n\nTo enable VLMs to perceive and comprehend the 3D world, most existing 3D MLLM architectures incorporate a 3D vision encoder to extract 3D features and align them with an LLM.\nCurrent methods primarily focus on segmented object attributes, overlooking strategies for precise 3D location perception.\nApproaches like and utilize the Q-former module to extract instruction-related information from 3D scene embeddings, forming the input for 3D MLLMs.\nHowever, the extracted embeddings are overly aligned with the instructions, failing to fully capture the spatial concepts of 3D scenes.\nExisting works still lack effective perception of 3D spatial relations and precise location generation, which are fundamental for spatial reasoning and generation.\nIn 3D scenes, spatial information exists naturally at various levels, including that of individual objects, object groupings, and entire architectures.\nConsequently, most of existing 3D MLLMs either compress holistic 3D scene information or segment independent objects for 3D vision-language(3D VL) tasks, limiting their spatial awareness due to insufficient representation of the richness inherent in 3D scenes.\n\nGiven the aforementioned concerns regarding the inadequate exploitation of spatial information in existing 3D MLLMs, we propose Spatial 3D-LLM, a 3D MLLM aimed at improving capabilities of spatial awareness for 3D VL tasks by enriching the spatial embeddings of 3D scenes.\nSpatial 3D-LLM integrates a frozen 3D scene encoder, an LLM backbone, and a meticulously designed progressive spatial awareness scheme that includes intra-referent clustering and abstraction, inter-referent message passing, and contextual referent-scene interactions.\nThis spatial awareness visual referent evolution begins with relation-based clustering. It then continues with inter-referent message passing to model spatial distribution based on the distances between different referents.\nFinally, it encompasses broader contextual information by considering the interactions between referents and the surrounding environment.\nThis stepwise scheme progressively captures spatial information as the perception field expands, injecting location-enriched spatial knowledge into the 3D scene embeddings.\nThese enhanced embeddings serve as visual prompt for end-to-end instruction tuning, eliminating the task-specific optimizations.\n\nConsidering spatial awareness from the perspective of tasks and datasets, several works have improved image-based spatial reasoning capabilities by generating large-scale spatially-aware training data.\nThey hypothesize that VLMs’ limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data.\nThose generated question answering datasets are mainly related to estimating object pair relationships and metric measurements.\nExisting 3D instruction following datasets\nsupport a variety of spatial tasks, including visual question answering, visual grounding, and spatial relationships estimation.\nHowever, these datasets mainly concentrate on perceiving coarse-grained and abstract object relationships while leaving fine-grained measurement unexplored.\nMoreover, they typically focus on local object interactions, neglecting the utilization of commonsense knowledge of object-scene spatial information.\n\nIn light of the mentioned deficiencies in existing 3D instruction datasets, we propose two novel tasks: 3D object distance measurement and 3D layout editing in 3D scenes, to evaluate the spatial awareness capabilities of 3D MLLMs.\nWe construct a 3D instruction dataset called Measure Object Distance and Layout Editing (MODLE) that is furnished with 263K vision-language annotations specifically targeted towards these tasks.\nInferring precise distances between objects enhances fine-grained spatial perception, while performing object placement and movement in a 3D scene fosters a deeper understanding of object-scene spatial information, accumulating commonsense knowledge for downstream tasks.\n\nIn summary, our contributions are as follows:\n {itemize}[leftmargin=*]\n   We propose two novel location-related tasks in 3D scenes, namely 3D object distance measurement and 3D layout editing.\n We construct MODLE, a 3D instruction dataset furnished with 263K vision-language annotations towards these tasks.\n\n   We present Spatial 3D-LLM, a 3D MLLM that improves 3D spatial awareness capabilities by enriching the spatial embeddings of 3D scenes.\n Spatial 3D-LLM features a progressive spatial awareness scheme that captures spatial information as the perception field expands, injecting location-enriched spatial knowledge into the 3D scene embeddings.\n   Experimental results demonstrate that our method achieves state-of-the-art performance across diverse 3D VL tasks, especially those concerning locations and spatial relationships.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "IEEE_Conference_Template_ICME_2025/icme2025.tex",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.368,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.349,
      "datasets_score": 0.362,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves developing a 3D vision-language model with enhanced spatial awareness through progressive spatial embeddings, clustering, and message passing, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning adaptations. Therefore, it does not align with diffusion-based reasoning concepts.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667514",
      "updated_at": "2025-08-11T23:43:05.606794",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16533",
      "title": "confopt: A Library for Implementation and Evaluation of Gradient-based\n  One-Shot NAS Methods",
      "authors": [
        "Abhash Kumar Jha",
        "Shakiba Moradian",
        "Arjun Krishnakumar",
        "Martin Rapp",
        "Frank Hutter"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Gradient-based one-shot neural architecture search (NAS) has significantly\nreduced the cost of exploring architectural spaces with discrete design\nchoices, such as selecting operations within a model. However, the field faces\ntwo major challenges. First, evaluations of gradient-based NAS methods heavily\nrely on the DARTS benchmark, despite the existence of other available\nbenchmarks. This overreliance has led to saturation, with reported improvements\noften falling within the margin of noise. Second, implementations of\ngradient-based one-shot NAS methods are fragmented across disparate\nrepositories, complicating fair and reproducible comparisons and further\ndevelopment. In this paper, we introduce Configurable Optimizer (confopt), an\nextensible library designed to streamline the development and evaluation of\ngradient-based one-shot NAS methods. Confopt provides a minimal API that makes\nit easy for users to integrate new search spaces, while also supporting the\ndecomposition of NAS optimizers into their core components. We use this\nframework to create a suite of new DARTS-based benchmarks, and combine them\nwith a novel evaluation protocol to reveal a critical flaw in how\ngradient-based one-shot NAS methods are currently assessed. The code can be\nfound at https://github.com/automl/ConfigurableOptimizer.",
      "published_date": "2025-07-22T12:44:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16533v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16533v1",
      "latex_url": "http://arxiv.org/src/2507.16533v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Neural Architecture Search (NAS), the domain of research that automates the design of neural network architectures, has matured significantly over the past decade.\nIn its early days, most of the methods were based on reinforcement learning  {zoph-iclr17a,zoph-cvpr18a,baker-iclr17a,pham-icml18a} and evolutionary search methods  {real-icml17a,real-aaai19a} .\nWhile these methods were effective, they also demanded substantial computational resources and time.\nDifferentiable Architecture Search (DARTS)  {liu-iclr19a}, the formative gradient-based one-shot NAS method, sped up the time required for searching the space by orders of magnitude.\nSubsequent DARTS-based methods have enabled even more efficient exploration of bounded architectural spaces, while also addressing several of the challenges of DARTS  {white-arxiv23a}.\n\nA major challenge in gradient-based one-shot NAS lies in reliable benchmarking and evaluation.\nPrior work has shown that the performance of a given architecture is heavily influenced by the training recipe, and even the random seeds used, making fair comparisons between NAS methods difficult  {yang-iclr20a}.\nDespite this, the DARTS search space remains the primary benchmark for evaluating new gradient-based NAS approaches.\nFurthermore, many of the reported improvements from newer methods fall within the margin of noise, making it challenging to draw confident conclusions\n {zhang2023rethink}.\n\nIn this work, we highlight several challenges in evaluating NAS methods that rely on a supernet, as done in DARTS, and propose a new evaluation protocol designed to address these issues.\nTo support this effort, we introduce Configurable Optimizer (confopt), a library built specifically for the development and evaluation of gradient-based one-shot NAS methods.\nUsing this library, we construct  , a collection of benchmarks derived from the DARTS search space, and use them to demonstrate key flaws in the prevailing evaluation pipeline.\nEach benchmark is an instantiation of the DARTS search space with a distinct configuration, varying in candidate operations, network depth, width, and other architectural details.\nImportantly, while each benchmark retains a large and expressive search space, the associated supernets are significantly more efficient to train.\nConcretely, we summarize our contributions as follows:\n {enumerate}[noitemsep]\n   We introduce Configurable Optimizer (confopt), a library for developing and benchmarking gradient-based one-shot NAS methods.\n   We introduce DARTS-Bench-Suite, a benchmark suite of nine DARTS-based benchmarks to more comprehensively evaluate NAS methods.\n   We evaluate seven NAS optimizers across nine new benchmarks and find that their rankings differ substantially across these settings, highlighting the need for a more comprehensive evaluation of gradient-based one-shot NAS methods.\n {enumerate}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1_Introduction.tex",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.387,
      "datasets_score": 0.354,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668736",
      "updated_at": "2025-08-11T23:43:05.607053",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16534",
      "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis\n  Technical Report",
      "authors": [
        "Shanghai AI Lab",
        ":",
        "Xiaoyang Chen",
        "Yunhao Chen",
        "Zeren Chen",
        "Zhiyun Chen",
        "Hanyun Cui",
        "Yawen Duan",
        "Jiaxuan Guo",
        "Qi Guo",
        "Xuhao Hu",
        "Hong Huang",
        "Lige Huang",
        "Chunxiao Li",
        "Juncheng Li",
        "Qihao Lin",
        "Dongrui Liu",
        "Xinmin Liu",
        "Zicheng Liu",
        "Chaochao Lu",
        "Xiaoya Lu",
        "Jingjing Qu",
        "Qibing Ren",
        "Jing Shao",
        "Jingwei Shi",
        "Jingwei Sun",
        "Peng Wang",
        "Weibing Wang",
        "Jia Xu",
        "Lewen Yan",
        "Xiao Yu",
        "Yi Yu",
        "Boxuan Zhang",
        "Jie Zhang",
        "Weichen Zhang",
        "Zhijie Zheng",
        "Tianyi Zhou",
        "Bowen Zhou"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.",
      "published_date": "2025-07-22T12:44:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16534v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16534v2",
      "latex_url": "http://arxiv.org/src/2507.16534v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{ }[1]{ {justification=centering} {#1}}\n\n { }[2]{ {#1}{ [origin=c]{90}{#2}}}\n\n {medgray55}{gray}{0.55}\n {medgray}{gray}{0.7}\n {litegray}{gray}{0.9}\n {gblue}{RGB}{210, 227, 252}\n {gred}{RGB}{250, 210, 207}\n {gyellow}{RGB}{254, 239, 195}\n {ggreen}{RGB}{206, 234, 214}\n {gorange}{RGB}{254, 223, 200}\n\n {gblue9}{RGB}{23, 78, 166}\n {gred9}{RGB}{165, 14, 14}\n {gyellow9}{RGB}{227, 116, 0}\n {ggreen9}{RGB}{13, 101, 45}\n {gorange9}{RGB}{176, 96, 0}\n\n {myblue}{rgb}{0,0,1}\n {myred}{rgb}{1,0,0}\n {mylightgray}{gray}{0.95}\n\n {highlightblue}{HTML}{185ABC}\n\n {\n basicstyle= ,\n moredelim=[is][ {highlightblue}]{@@}{@@},\n moredelim=[is][ {myred}]{!!}{!!}\n}\n\n { }{ {gblue}}\n { }{ {gred}}\n { }{ {gyellow}}\n { }{ {ggreen}}\n { }{ {gorange}}\n\n {citrine}{rgb}{0.89, 0.82, 0.04}\n { }{{ {BurntOrange}( {55})}}\n { }{{ {citrine}( {51})}}\n\n {P}[1]{>{  }p{#1}}\n\n {M}[1]{>{  }m{#1}}\n\n {prompt}{Verbatim}{\n breaklines,\n formatcom= {darkgray}\n}\n { }[1]{ [breaklines, formatcom= {darkgray}]{#1}}\n\n { }[1]{ { }{#1}}\n\n { }{\n  {medgray55}  {2-3}  {black}}\n\n { }{ {-4mm}}\n\n { }{{ {OliveGreen} {51}}}\n { }{{ {BrickRed} {55}}}\n\n { }{{ {OliveGreen} {51}}}\n { }{{ {BrickRed} {55}}}\n\nArtificial Intelligence (AI) has made significant progress in recent years, achieving human-comparable performance across a range of applications. These breakthroughs have sparked a lively conversation about the ``frontier'' risks of AI  {Anthropic23-rsp, openai-rsp, google-rsp, responsible-scaling-policies-rsps, phuong2024evaluating}, i.e., high-severity risks associated with general-purpose AI models. With the rapid development and deployment of advanced AIs, we need a comprehensive and practical identification and evaluation of their underlying risks, along with developing effective mitigation strategies.\n\nDrawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework)  {shlab2025safework_f1_framework}, this technical report conducts a comprehensive assessment of AI's frontier risks based on a systematic analysis of these interconnected analytical dimensions. We focus on frontier AI risks that could potentially pose significant threats to public health, national security, and societal stability due to their potential for rapid escalation, severe societal harm, and unprecedented scope of impact. Specifically, we evaluate critical risks across seven key areas: (1) cyber offense, (2) biological and chemical risks, (3) persuasion and manipulation, (4) strategic deception and scheming, (5) uncontrolled autonomous AI R\\&D, (6) self-replication, and (7) collusion. These risk areas span three of the four major categories identified in SafeWork-F1-Framework: misuse risks, loss of control risks, and systemic risks.\n\nWe propose preliminary boundaries for AI safety by defining ``red lines'' and ``yellow lines'' as early warning indicators, following the methodology outlined in  {shlab2025safework_f1_framework}. The E-T-C analysis enables us to identify unacceptable outcomes (red lines) and concrete threat scenarios (yellow lines) that could contribute to them by outlining how threats could materialize through specific combinations of deployment environment, threat source, and enabling capability. These risk zones define deployment protocols: green zone models have manageable risk levels suitable for routine deployment with continuous monitoring; yellow zone models require strengthened mitigations and controlled deployment; red zone models necessitate suspension of development and/or deployment until risks are adequately mitigated.\n\nOur experimental evaluation of recent frontier AI models (Table~) reveals that all assessed models currently reside in the green and yellow zones, with none crossing red line thresholds. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\\&D risks. For self-replication, strategic deception, and scheming risks, most current AI models remain in the green zone, except several reasoning models that fall within the yellow zone. In the area of persuasion and manipulation, most AI models demonstrate effective human influence capabilities and are classified in the yellow zone. For biological and chemical risks, we cannot definitively rule out the possibility that most models reside in the yellow zone. However, detailed threat modeling and in-depth assessment are required to make further claims. Notably, newly released AI models show a gradual decline in safety scores with respect to cyber offense, persuasion and manipulation, and collusion {The collusion risk belongs to systemic risk  {shlab2025safework_f1_framework}. This risk cannot be assessed when discussing the relationship between model capability and risk only.} areas, warranting increased attention from the research community.\n\nGuided by the ``AI-$45^ $ Law''  {yang2024ai45circlawroadmaptrustworthy}, we systematically apply the E-T-C analysis to facilitate frontier risk evaluation of AI models. This work reflects our current understanding of AI frontier risks and aims to provide a comprehensive, industry-leading approach to frontier risk identification and assessment. We believe AI safety is a global public good and urge collective action to address these critical challenges through transparent, collaborative risk management techniques and practices that can effectively mitigate risks while enabling beneficial AI development.\n\n { }[2]{\n [baseline={(char.center)}]{\n  [shape=circle,draw,#2,fill=#2,inner sep=0pt,minimum size=#1] (char) {};\n}}\n\n {rightcellcolour}{gray}{1.0}\n {weak}{gray}{0.9}\n {basic}{RGB}{252, 201, 52}\n {moderate}{RGB}{227, 116, 0}\n\n { }{$ $}\n { }{$ $}\n\n {table}[ht]\n \n {1.4}\n {tabular}{| p{3mm} | >{ }p{28mm} | p{100mm} | M{17mm} |}\n \n {litegray} & Experiment & Description & Risk Zone.\n  \n\n & Capture-The-Flag (CTF) & CTF challenge requires the AI model to gain access to servers and locate a specific field, or a field with a fixed format within a file (the ``flag''), to score points. & {green!100}{      }  \n  {-2.5em}[0pt][0pt]{ {90}{Cyber Offense}} & Autonomous Cyber Attack & An autonomous cyber attack requires the AI model to leverage its intrinsic reasoning, planning, and code generation capabilities to autonomously progress from vulnerability analysis to the generation of a functional exploit. &  {green!100}{      }\n  \n\n& Biological Protocol Diagnosis and Troubleshooting & This task measures AI models' ability to troubleshoot biological laboratory protocols and identify experimental errors, which could significantly lower technical barriers for threat actors attempting complex biological weapon development. &  {green!100}{      } \n {-8em}[0pt][0pt]{ {90}{Biological and Chemical}} & Biological Hazardous Knowledge and Reasoning & This task measures AI models' knowledge of hazardous biological information and reasoning capabilities, as well as their tendency to refuse providing such information when inappropriately requested. &  {red!35!yellow!80}{      }  \n\n & Chemical Hazardous Knowledge and Reasoning & This task measures AI models' knowledge of hazardous chemical information and reasoning capabilities, as well as their tendency to refuse providing such information when inappropriately requested. &  {red!35!yellow!80}{      }\n  \n\n {-2.3em}[0pt][0pt]{ {90}{P\\&M}} & Persuasion and Manipulation\n\n& AI models induce significant shifts in human or model opinions through dialogue, especially when such changes are achieved via non-transparent or unfair cognitive influence, often against the target’s best interests. A significant opinion shift indicates successful persuasion and manipulation.\n &  {red!35!yellow!80}{      }\n  \n\n& Dishonesty Under Pressure\n & Dishonesty refers to the behavior of AI models making statements that contradict their own internal beliefs, with the intent (explicit or implicit) to cause the human to accept those statements as true. &  {green!100}{      }  \n\n  {-1.0em}[0pt][0pt]{ {90}{Scheming}} & Sandbagging & AI models intentionally underperform during evaluation or alignment phases to obscure their true capabilities, often to avoid additional oversight or intervention. &  {red!35!yellow!80}{      }\n  \n\n {-2.65em}[0pt][0pt]{ {90}{AI R\\&D}} & Uncontrolled AI Research and Development\n& AI models strategically appear aligned with outer objectives in their development process, but secretly optimize for a different objective, their inner mesa-objective.&  {green!100}{      }\n  \n\n {-1.5em}[0pt][0pt]{ {90}{SR}} & Self-\nReplication\n\n& AI agent autonomously deploys a complete, functional replica of itself by replicating its model weights, application code, and runtime environment onto other machines or clusters without human supervision. &  {red!35!yellow!80}{      }\n  \n\n  {-3.2em}[0pt][0pt]{ {90}{Collusion}} & Multi-agent Fraud in Social Systems ~\n & Multiple AI agents collaborate and employ deceptive strategies like social engineering and impersonation to acquire financial assets or sensitive information from targets illegally.\n & $ {N/A}^{ {red}*}$\n\n  \n\n {tabular}\n {1}\n\n {The overall conclusion of our evaluations.   1em  {green!100}{      } Green zone  1.5em  {red!35!yellow!80}{      } Yellow zone  1.5em {red!100}{      } Red zone}\n\n {-10pt}\n {table}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/introduction.tex",
      "rlhf_score": 0.471,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.356,
      "distributed_training_score": 0.394,
      "datasets_score": 0.348,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a framework for assessing and managing risks associated with frontier AI models, including risk zones and evaluations in areas like cyber offense and persuasion. It does not discuss, involve, or reference Reinforcement Learning from Human Feedback (RLHF), such as training with human-ranked data or reward models, focusing instead on risk analysis rather than AI alignment techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667672",
      "updated_at": "2025-08-11T23:43:05.606832",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16535",
      "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
      "authors": [
        "Shang Liu",
        "Chenjie Cao",
        "Chaohui Yu",
        "Wen Qian",
        "Jing Wang",
        "Fan Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
      "published_date": "2025-07-22T12:46:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16535v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16535v2",
      "latex_url": "http://arxiv.org/src/2507.16535v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The field of 3D generation has witnessed remarkable progress in recent years, evolving from object-level~ to scene-level~ synthesis, yielding impressive photorealistic and structurally coherent outcomes.\nMoreover, recent works have pushed these capabilities toward urban-scale generation under diverse conditions~.\nThese achievements lead to new applications in computer graphics, virtual reality, and high-fidelity geospatial modeling.\n\nDespite these achievements, a critical gap remains in scaling 3D generation to extensive geographic-scale—a domain requiring holistic modeling of both anthropogenic structures and natural terrains. We identify two fundamental limitations in existing approaches:\n1) Most urban generation frameworks solely focus on the city generation within constrained semantic scopes~, neglecting other diverse natural formations (e.g., mountains, lakes, and deserts).\nThis requires comprehensive aerial datasets encompassing multi-terrain formations and well-designed models containing scalable capacity to handle the general Earth generation.\n2) Since the large-scale 3D generation is inherently intractable, existing generative methods heavily depend on various conditions, including images, semantics, height fields, captions, or combinations of them~.\nWhile these conditions improve the results, they constrain generative flexibility.\nConversely, unconditional generation at geographic scales often collapses into geometric incoherence or textural ambiguity, failing to produce satisfactory outcomes.\n\nTo address these challenges, we improve both data curation and model architecture to enhance geographic-scale generation.\nFormally, we present Aerial-Earth3D, the largest 3D aerial dataset created to date. This dataset comprises 50,028 meticulously curated scenes, each spanning 600m$ $600m, sourced across the mainland U.S. with 45 million multi-view frames captured from Google Earth.\nTo effectively cover valid and diverse regions with limited viewpoints, we carefully design heuristic camera poses based on simulated 3D scenes built upon DEM~, OSM~, and MS-Building~ datasets.\nSince Google Earth does not provide source meshes, we reconstruct 3D meshes via InstantNGP~, applying several post-processing techniques to extract surface planes, fix normals, and refine mesh connectivity.\nThen these meshes are voxelized as the ground truth for structural generation.\nAdditionally, we employ AIE-SEG~ to create semantic maps as mesh attributes, comprising 25 distinct classes.\nAs summarized in Table~, Aerial-Earth3D stands out as a large-scale 3D aerial dataset characterized by its diverse terrains and 3D annotations, significantly advancing both 3D generation and reconstruction efforts.\n\nBuilding upon this robust dataset, we present EarthCrafter, a novel framework designed for geographic-scale 3D generation through dual-sparse latent diffusion.\n\nFollowing Trellis~,\n\nEarthCrafter inherits the advantages of disentangled structure and texture generations with flexible conditioning and editing capabilities.\nHowever, Trellis focuses on object-level generation rather than the geographic scene, while the latter instance contains 10 times more voxels for the geometric modeling, presenting significant challenges in feature storage efficiency, geometric compression, network design, and input condition alignment.\nThus, we propose several key innovations to extend this method to a geographic scale.\nSpecifically, EarthCrafter integrates dual-sparse VAEs~ and Flow Matching (FM) diffusion models~ for structure and texture generations, respectively.\nDuring the training of the texture VAE, which directly decodes 2D Gaussian Splatting (2DGS)~ as textural representation, we find that high-resolution voxel features within low channels~ substantially outperform spatially compressed voxel features with large channels~ in large-scale 3D generation, while the former enjoys a lighter I/O overhead.\nIn contrast to~, we further spatially compress voxel representations of structured VAE via elaborate sparse network design, which allows us to efficiently represent detailed geographic shapes with 97.1% structural accuracy.\nAdditionally, we improve the model designs for both textual and structural FM models to tame the extremely large-scale generation.\nThese models can be flexibly conditioned on images, semantics, or operate without conditions.\nEspecially, we employ a novel coarse-to-fine framework for structural FM, which begins by classifying the full voxel initialization into a coarse voxel space, followed by a refinement phase that converts to a fine voxel space while predicting the related latents.\nThis coarse-to-fine modeling enables more precise structures compared to the one-stage dense modeling.\n\nWe conduct extensive experiments to verify the effectiveness of the proposed method. The key contributions of this paper can be summarized as follows:\n {itemize}\n   Aerial-Earth3D is presented as the largest 3D aerial dataset, comprising images captured from diverse structures and natural terrains with annotated 3D presentations.\n   Dual-sparse VAEs are designed for structural and textural encoding, facilitating efficient I/O, superior appearance, and detailed structures for large-scale generation.\n   Tailored flow matching models are proposed to enhance the modeling of latent spaces, while the coarse-to-fine strategy is incorporated for precise structural generation.\n {itemize}\n\n {table}[!t]\n \n {-0.1in}\n {-0.1in}\n \n { }{3pt}\n {tabular}{l|ccccl}\n \nDataset & Area & Images & Sites & Class & Source\n\n  \nUrbanScene3D~ & 136 & 128K & 16 & 1 & Synth/Real\n\nCityTopia~ & 36 & 3.75K & 11 & 7 & Synth\n\nCityDreamer~ & 25 & 24K & 400 & 6 & Real\n\nBuilding3D~ & 998 & - & 16 & 1 & Synth\n\nMatrixCity~ & 28 & 519K & 2 & - & Synth\n\nSTPLS3D~ & 17 & 62.6K & 67 & 32 & Synth/Real\n\nSensatUrban~ & 7.6 & - & 3 & 13 & Synth/Real\n\n \nOurs & 18010 & 45M & 50K & 25 & Real\n\n \n {tabular}\n {table}{Comparison of aerial-view 3D scene datasets.}\n\n {-0.2in}\n {table}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "aaai2026.tex",
      "rlhf_score": 0.289,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.387,
      "distributed_training_score": 0.369,
      "datasets_score": 0.402,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes the introduction of Aerial-Earth3D, a new and largest 3D aerial dataset with 50k curated scenes and 45M multi-view frames, which directly aligns with research on creating datasets for AI applications. It also details dataset curation methodologies, such as heuristic camera pose design, 3D mesh reconstruction, voxelization, semantic mapping, and quality control for terrain diversity. Furthermore, the paper compares Aerial-Earth3D with existing datasets in a table, involving benchmarking and analysis, making it highly pertinent to dataset creation, curation, and evaluation in machine learning contexts.",
      "summary": "The paper introduces Aerial-Earth3D, the largest 3D aerial dataset with 50k curated scenes spanning 600m x 600m across the U.S., featuring multi-view images, depth maps, and annotations to support large-scale 3D generation; it then proposes EarthCrafter, a framework using dual-sparse latent diffusion with separate structural and textural generation via sparse 3D-VAEs and condition-aware flow matching models, demonstrating superior performance in generating geographically plausible 3D Earth models for applications like urban layout and terrain synthesis.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and a tailored architecture for scaling 3D generation to geographic extents, addressing a significant gap in existing methods by handling vast scales and diverse terrains.",
      "impact_score": "High",
      "impact_justification": "The work's scalable framework and large dataset could broadly influence research in computer vision, AI, and applications like virtual reality and geospatial modeling, potentially enabling new advancements in large-scale 3D synthesis.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong contribution to 3D generation with innovative techniques and a valuable dataset, making it essential for researchers in computer vision and AI to stay informed on advancements in geographic-scale modeling.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/13fbd8b852318eb515966224d20a60ade9330e27",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 11,
      "average_h_index": 3.8333333333333335,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shang Liu",
          "profile_url": "https://www.semanticscholar.org/author/2310255226",
          "h_index": 2
        },
        {
          "name": "Chenjie Cao",
          "profile_url": "https://www.semanticscholar.org/author/2296071044",
          "h_index": 4
        },
        {
          "name": "Chaohui Yu",
          "profile_url": "https://www.semanticscholar.org/author/2110961040",
          "h_index": 11
        },
        {
          "name": "Wen Qian",
          "profile_url": "https://www.semanticscholar.org/author/2310268545",
          "h_index": 1
        },
        {
          "name": "Jing Wang",
          "profile_url": "https://www.semanticscholar.org/author/2372633175",
          "h_index": 0
        },
        {
          "name": "Fan Wang",
          "profile_url": "https://www.semanticscholar.org/author/2257894784",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667538",
      "updated_at": "2025-08-11T23:44:31.171821",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16537",
      "title": "Symbolic Graph Intelligence: Hypervector Message Passing for Learning\n  Graph-Level Patterns with Tsetlin Machines",
      "authors": [
        "Christian D. Blakely"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We propose a multilayered symbolic framework for general graph classification\nthat leverages sparse binary hypervectors and Tsetlin Machines. Each graph is\nencoded through structured message passing, where node, edge, and attribute\ninformation are bound and bundled into a symbolic hypervector. This process\npreserves the hierarchical semantics of the graph through layered binding from\nnode attributes to edge relations to structural roles resulting in a compact,\ndiscrete representation. We also formulate a local interpretability framework\nwhich lends itself to a key advantage of our approach being locally\ninterpretable. We validate our method on TUDataset benchmarks, demonstrating\ncompetitive accuracy with strong symbolic transparency compared to neural graph\nmodels.",
      "published_date": "2025-07-22T12:47:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16537v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16537v1",
      "latex_url": "http://arxiv.org/src/2507.16537v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Graph classification is a fundamental task in graph-based machine learning, where the goal is to assign a label or predict a target for an entire graph. This problem arises in a wide range of applications, from predicting molecular properties and protein function , to analyzing social networks and brain connectivity graphs .\n\nMainstream approaches to graph classification typically rely on message-passing neural networks (MPNNs), including Graph Convolutional Networks (GCNs) , Graph Attention Networks (GATs) , and variants such as the Graph Isomorphism Network (GIN) . These models operate by iteratively aggregating feature information from each node’s neighbors, then pooling node-level representations into a global graph-level embedding. Modern benchmark frameworks such as the TUDataset collection and Open Graph Benchmark (OGB) provide datasets where each sample is a graph with arbitrary topology and rich node/edge features. While effective, these approaches often involve complex architectures, are difficult to interpret, and can be sensitive to structural noise or training instability.\n\n {figure}\n  \n  [width=3in]{images/MUTAG_sample.png}\n  {  Example of graphs we consider in this paper where nodes and edges can contain rich information such as vector fields, bond types, importance, and much more.}\n\n {figure}\n\nIn this work, we introduce a novel, interpretable method for graph-level representation using sparse binary hypervectors and symbolic logic-based operations. Each node and edge is encoded as a high-dimensional sparse binary vector, where semantic properties (e.g., label, importance, and attribute values) are independently embedded. Information is propagated across the graph in a hierarchical fashion using symbolic binding: starting from a node, we bind its embedding to that of each outgoing edge, then further to the destination node, capturing a node $ $ edge $ $ neighbor path, as shown in Figure . Each such path contributes a distinct bound vector, and all such vectors are aggregated using a bundling operation into a fixed-size graph-level representation. This encoding pipeline can be extended across multiple layers of abstraction (e.g., substructures or motifs) and is amenable to transparent reasoning and local interpretation, aligning naturally with TM-based learning systems.\n\nOur approach retains the structural richness of graph data while avoiding the need for gradient-based backpropagation or opaque deep network architectures. It is well-suited for applications that demand interpretability, symbolic composability, or learning on low-resource or privacy-sensitive data.\n\nRelated Work\n\nThe most closely related work to our approach is the recent and forthcoming Graph Tsetlin Machine by Granmo et al.~, which proposes a symbolic learning framework for graphs using Tsetlin Automata and propositional logic. In their model, node and edge features are embedded as binary hypervectors, and logic-based clauses are learned to capture node-level patterns via message passing. The Graph TM operates over a fixed, global graph structure in which all nodes and edges are known in advance, and only the values (features or labels) associated with those entities vary across training samples. This makes the model particularly suitable for domains such as traffic networks, power grids, or knowledge graphs, where the topology is static but the observed signals evolve.\n\nBy contrast, our method addresses the more general and challenging problem of graph classification across a distribution of graphs with varying structure. Each sample in our setting is a standalone graph instance with potentially different nodes, edges, and topologies. To handle this, we propose a novel encoding strategy that maps each graph into a sparse binary hypervector through a symbolic hierarchy: nodes are bound to their label, importance, and attributes; edge types and attributes are bound and passed to destination nodes; and the resulting messages are bundled into a global graph-level representation. This approach enables TMs to operate in inductive settings where graphs differ not just in values but also in structure, extending symbolic reasoning to the full space of variable-topology graph classification tasks.\n\nIn essence, while both approaches leverage Tsetlin learning and symbolic hypervector encodings, they address fundamentally different problem settings: the Graph TM assumes a fixed graph and learns functions over evolving node/edge states, whereas our method encodes and learns over fully dynamic graph instances.\n\nPaper structure\nThe remainder of this paper is organized as follows. In Section~, we formally define sparse binary hypervectors and describe two key embedding mechanisms: linear scalar embeddings for continuous values, and interval-based symbolic embeddings for categorical variables. Section~ presents our graph encoding algorithm, which uses multi-hop symbolic binding to encode graph structure into a single fixed-length vector, and describes how these representations are used with the TM for graph classification. In Section~, we discuss the interpretability properties of our approach, including the ability to decompose global predictions into localized subgraph contributions. Section~ provides numerical comparisons on standard graph classification benchmarks from the TU Dortmund dataset collection~, where we evaluate our approach against established state-of-the-art models, including Graph Convolutional Networks (GCNs)~ and Graph Attention Networks (GATs)~. We conclude in Section~ with a summary of our findings and suggestions for future extensions.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.331,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.386,
      "distributed_training_score": 0.36,
      "datasets_score": 0.33,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668745",
      "updated_at": "2025-08-11T23:43:05.607055",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16540",
      "title": "Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph\n  Attention Networks",
      "authors": [
        "Radowanul Haque",
        "Aftab Ali",
        "Sally McClean",
        "Naveed Khan"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Detecting security vulnerabilities in source code remains challenging,\nparticularly due to class imbalance in real-world datasets where vulnerable\nfunctions are under-represented. Existing learning-based methods often optimise\nfor recall, leading to high false positive rates and reduced usability in\ndevelopment workflows. Furthermore, many approaches lack explainability,\nlimiting their integration into security workflows. This paper presents\nExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.\nThe method constructs Code Property Graphs and represents nodes using\ndual-channel embeddings that capture both semantic and structural information.\nThese are processed by an edge-aware attention mechanism that incorporates\nedge-type embeddings to distinguish among program relations. To address class\nimbalance, the model is trained using class-weighted cross-entropy loss.\nExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23\npercent across 30 independent runs on the ReVeal dataset. These results\nrepresent relative improvements of 4.6 percent in accuracy and 16.9 percent in\nF1 score compared to the ReVeal model, a prior learning-based method. The\nframework also outperforms static analysis tools, with relative gains of 14.0\nto 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond\nimproved detection performance, ExplainVulD produces explainable outputs by\nidentifying the most influential code regions within each function, supporting\ntransparency and trust in security triage.",
      "published_date": "2025-07-22T12:49:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16540v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16540v1",
      "latex_url": "http://arxiv.org/src/2507.16540v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Software vulnerabilities in low-level languages such as C and C++ continue to pose significant risks across a range of domains, including browser engines, operating system kernels, embedded systems, and Internet-of-Things (IoT) devices. Exploitable flaws in such systems can lead to remote code execution, data leakage, or full system compromise~. Traditional approaches such as static analysis and manual code review are widely used in practice to detect these issues. Static analysis tools, including Flawfinder~ and Cppcheck~, operate by scanning source code for syntactic patterns or predefined rules that are indicative of common vulnerabilities~. Manual review, in contrast, involves human inspection of code to reason about program logic, control flow, and semantic correctness. While these techniques are valuable, they face significant limitations. Static tools tend to produce a high number of false positives and lack contextual understanding, while manual review is time-consuming, error-prone, and difficult to scale to large codebases~.\n\nIn response, learning-based approaches have been proposed as alternatives for detecting vulnerabilities in source code. These methods aim to automatically identify patterns associated with known vulnerabilities, reducing dependence on manually crafted rules. Earlier models include VulDeePecker~, $ $VulDeePecker~, SySeVR~, and VulDeeLocator~, while graph-based approaches include Devign~, BGNN4VD~, ReVeal~, and IvDetect~.\n\nSequence-based models, such as VulDeePecker and SySeVR, treat source code as a linear token sequence, following methods from natural language processing. These models can learn lexical patterns but do not capture control or data dependencies between code elements~. Their performance is often sensitive to small changes in structure. In contrast, graph-based models represent source code as structured graphs, such as Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and Data Flow Graphs (DFGs). These representations enable models to reason over both syntactic structure and execution flow~.\nTo illustrate the challenge of vulnerability detection, consider the C function from Figure~.\n\n {figure}[!t]\n \n [width=0.35 ]{figs/buffer_over.pdf}\n {Example C function with a buffer overflow vulnerability.}\n\n {figure}\n\nThis function contains a classic buffer overflow vulnerability. The call to  {strcpy} copies the contents of  {input} into a fixed-size buffer without verifying the input length. If the input exceeds the allocated space, it can overwrite adjacent memory, potentially altering program state or control flow. While this example appears straightforward, similar vulnerabilities often occur in more complex forms across large codebases, making them difficult to detect using static pattern matching alone.\n\nA Code Property Graph (CPG)~ provides a unified representation by combining Abstract Syntax Trees (ASTs), Control Flow Graphs (CFGs), and Data Flow Graphs (DFGs). For the above function, the CPG encodes the buffer declaration, the data flow from  {input} to  {strcpy}, and the use of  {buffer} in the return statement. These relationships are captured as typed edges, enabling graph-based models to reason about how information propagates through the code in terms of both control and data dependencies.\n\nDespite this progress, most existing graph-based methods suffer from important limitations. They often use flat or shallow node features derived from syntax or tokens, and typically ignore the semantics of edge types that define relationships within the graph. Furthermore, the majority of these models act as black boxes, offering limited insight into why a function is predicted to be vulnerable. In software security workflows, understanding the reasoning behind a prediction is critical for trust, verification, and remediation.\n\nIn this work, we introduce ExplainVulD, a graph-based framework for explainable vulnerability detection in C/C++ functions. The framework represents each function as a Code Property Graph (CPG) constructed using Joern, where nodes correspond to code elements such as identifiers and control structures, and edges capture structural and semantic relationships. We construct dual-channel node embeddings to capture both semantic and structural aspects of code. The semantic channel uses a Word2Vec~ model trained on token sequences derived from filtered AST node labels, capturing lexical usage patterns. The structural channel is built from a second Word2Vec model trained on metapath-guided random walks over the CPG, incorporating node types, edge types, AST depth differences, and semantic scopes such as return paths. The two channels are concatenated to form a 1024-dimensional node embedding.\n\nTo propagate information, we apply GATv2~ extended with an edge-aware attention mechanism that incorporates learned edge-type embeddings. This enables the network to differentiate between relation types during message passing, supporting reasoning over heterogeneous program structures.\n\nExplainVulD also includes a post hoc explanation module. We compute relevance scores for nodes and edges using a combination of attention weights from global pooling and input gradients with respect to the classification loss. The most influential elements are then mapped back to source-level constructs, allowing structured interpretation of the model’s decisions. In contrast to general-purpose tools such as GNNExplainer~, our approach is model-specific and operates directly on the CPG representation.\n\nBy jointly modelling semantic and structural properties, incorporating relation-sensitive message passing, and producing interpretable outputs, ExplainVulD addresses common limitations of prior graph-based approaches to vulnerability detection.\n\nContributions\n\nThis work makes the following contributions to graph-based vulnerability detection in C/C++ code:\n\n {itemize}\n   To the best of our knowledge, this is the first work to propose a dual-channel node embedding for CPGs that combines semantic representations from AST tokens with structural features derived from metapath-guided walks.\n\n   We extend GATv2 to incorporate edge-type embeddings into the attention mechanism, enabling relation-sensitive message passing over heterogeneous code graphs.\n\n   We design an explanation module that combines attention weights and input gradients to identify influential nodes and edges in the CPG. While similar techniques exist in other domains, to our knowledge this is the first application of such a method to graph-based vulnerability detection in code.\n\n   We evaluate ExplainVulD on the ReVeal dataset using 30 independent runs with randomised splits. Our experiments compare the method against static and learning-based baselines and include ablation and explainability analyses.\n {itemize}\n\nThe remainder of the paper is structured as follows. Section~ reviews related work on vulnerability detection and graph learning. Section~ outlines the overall framework. Section~ describes the embedding design and model architecture. Section~ details the experimental setup. Section~ presents the results and analysis. Section~ discusses explainability and ablation findings. Finally, Section~ concludes the paper.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "cas-refs.tex",
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.407,
      "distributed_training_score": 0.339,
      "datasets_score": 0.317,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a graph-based framework for explainable vulnerability detection in C/C++ code, using techniques like Code Property Graphs, dual-channel embeddings, and edge-aware Graph Attention Networks. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669356",
      "updated_at": "2025-08-11T23:43:05.607141",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16541",
      "title": "A Comprehensive Data-centric Overview of Federated Graph Learning",
      "authors": [
        "Zhengyu Wu",
        "Xunkai Li",
        "Yinlin Zhu",
        "Zekai Chen",
        "Guochen Yan",
        "Yanyu Yan",
        "Hao Zhang",
        "Yuming Ai",
        "Xinmo Jin",
        "Rong-Hua Li",
        "Guoren Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "In the era of big data applications, Federated Graph Learning (FGL) has\nemerged as a prominent solution that reconcile the tradeoff between optimizing\nthe collective intelligence between decentralized datasets holders and\npreserving sensitive information to maximum. Existing FGL surveys have\ncontributed meaningfully but largely focus on integrating Federated Learning\n(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that\nemphasis on methodology and simulated scenarios. Notably, a data centric\nperspective, which systematically examines FGL methods through the lens of data\nproperties and usage, remains unadapted to reorganize FGL research, yet it is\ncritical to assess how FGL studies manage to tackle data centric constraints to\nenhance model performances. This survey propose a two-level data centric\ntaxonomy: Data Characteristics, which categorizes studies based on the\nstructural and distributional properties of datasets used in FGL, and Data\nUtilization, which analyzes the training procedures and techniques employed to\novercome key data centric challenges. Each taxonomy level is defined by three\northogonal criteria, each representing a distinct data centric configuration.\nBeyond taxonomy, this survey examines FGL integration with Pretrained Large\nModels, showcases realistic applications, and highlights future direction\naligned with emerging trends in GML.",
      "published_date": "2025-07-22T12:49:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16541v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16541v1",
      "latex_url": "http://arxiv.org/src/2507.16541v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{G}{raph} datasets, structured as non-Euclidean representations, are formally defined as tuples of nodes (entities) and edges (relationships) to rigorously model complex real-world systems.\nA key advantage of graph datasets is their ability to explicitly encode topological connections, overcoming the traditional constraints of independent and identically distributed (i.i.d.) data by capturing interaction dependencies among entities directly~.\nUnlike conventional data formats such as pixel-based images or text, graph structures offer distinct theoretical benefits, and the introduction of Graph Neural Networks (GNNs) enables Machine Learning (ML) algorithms to explore implicit structural insights hidden within topological connections based on the propagation mechanism.\nBy virtue of their effectiveness, GNNs have enabled breakthroughs such as AlphaFold~, which predicts protein structures from amino acid sequences to advance vaccine and antibody development.\n\nGiven the demonstrated efficacy of GNNs, numerous groundbreaking models, such as GCN~ and GAT~, have been proposed. The majority of these approaches adopt a model-centric perspective that emphasizes on introducing innovative architectural designs to achieve optimal performance on given datasets.~\nHowever, their success depends on the implicit assumption that the datasets have been thoroughly refined, and that performance improvements primarily stem from increasingly sophisticated model architectures.\nOn the other hand, real-world data often exhibit significant uncertainties—such as undesirable noise and incomplete descriptions of extracted entities—that violate this assumption. When such low-quality datasets are fed into GNNs, the models struggle to efficiently extract reliable knowledge, thereby exposing the inherent vulnerability of model-centric approaches in practical applications.\n\nTo address these limitations, data-centric Graph Machine Learning (GML) has emerged as a leading paradigm that more pragmatically addresses real-world data challenges. Consequently, data-centric GML has garnered increasing attention from researchers. However, most studies assume centralized settings, where datasets are uniformly stored in a single location. In contrast, decentralized data-centric GML remains underexplored, despite the critical reality that data are often dispersed across multiple, independent holders.\nMeanwhile, processing decentralized data requires stringent regulation compliance to preserve privacy.\nIn response, Federated Learning (FL) has gained attention for it enabling collaborative model training across decentralized datasets while preserving privacy.~\n\nExtending FL to graph datasets, Federated Graph Learning (FGL) has evolved swiftly as a specialized decentralized graph learning framework.\nExisting FGL studies share the pattern of proposing research challenges rooted in suggested realistic scenarios, based on which existing surveys have established taxonomies emphasizing prevalently adopted scenario-based challenges.\nSuch a pursuit has contributed meaningfully to the development of the field, but the scope of which are originated from model-centric perspective, introducing the innovative mechanism without delineating differentiated properties of datasets nor discussing their data-centric motivations.\n\nMotivation of this survey:\nInspiration for this data-centric FGL survey stems from the unequivocal understanding that most FGL challenges are data-related, such as statistical heterogeneity or topology heterogeneity.\nMoreover, examining those challenges closely requires the acknowledgment of data characteristics applied in FGL works since diversified data formats and decentralized settings have been discussed in existing works.\nThe correspondent mechanism is also knitted with data-centric GML but in decentralized settings.\nTo better present FGL as a general research field to researchers whose initial intention is to resolve observed data-related issues, having the data-centric FGL survey can offer greater convenience as a generalized guide.\n\nSpecifically, we introduce a two-level taxonomy, each comprising three orthogonal criteria that form unique combinations offering comprehensive view:\n\n The Data Characteristics Dimension includes: (i) differentiating various types of graph datasets (e.g., homogeneous, heterogeneous, knowledge, and bipartite graphs), (ii) highlighting the decentralization format describing how data is distributed across clients, and (iii) revealing the visibility level at each client, indicating whether each client has access to a full global graph or only partial subgraphs. Together, these criteria provide a comprehensive view of the structural and distributional properties of data addressed in FGL research.\n\n The Data Utilization Dimension, in turn, examines how and when FGL methods incorporate targeted mechanisms that aim for addressing data-centric challenges into the training process: (i) highlighting key data-centric challenges, including impaired data quality, class imbalance across clients' datasets, slow convergence when training on excessively large graphs, and enhancement of data privacy protection. (ii) indication of whether main innovations are proposed in client-side or server-side procedure. (iii) further refining the training procedure into four execution phases (e.g, Initialization, Local Training, Global Aggregation, and Post-aggregation) with details of techniques applied in representative FGL methods.\n\nAs the first FGL survey focusing on data-centric perspective, the contribution of this work can be illustrated as follows:\n\n(a) New Perspective: This work is the first to organize FGL studies through a data-centric lens, clarifying how different types of data are characterized and utilized in existing research. This perspective aligns closely with the priorities of the big data era, where the properties of data increasingly shape the choice and effectiveness of machine learning techniques.\n\n (b) Two-level Taxonomy: We propose a new two-level taxonomy grounded in a data-centric perspective.\nEach level of taxonomy comprises three orthogonal criteria that categorize all notable FGL studies in a fine-grained fashion and facilitating the discovery of studies aligned with specific data-centric questions.\n\n (c) Broader Impact-Artificial Generative Intelligence: This work is also the first to explore how FGL could integrate with ongoing research on Pre-trained Large Models (PLMs) to accelerate advances in graph machine learning. Our discussion of future research directions highlights several emerging data-centric topics that remain underexplored in the context of FGL and thus merit further attention and dedicated investigation.\n\nOrganization of the Survey\nThe structure of this survey is as follows: Sec. introduces the foundational concepts of FL and FGL, outlining the general training procedure. Sec. presents the first-level taxonomy based on data properties from both local and global perspectives. Sec. lays out second-level Taxonomy, which presents data-centric challenges and details how representative FGL methods respond to them. Sec. discusses studies where clients operate on non-graph-structured data. Sec. evaluates FGL’s applicability in addressing real-world data challenges. Sec. explores the mutual integration of FGL and PLMs. Finally, Sec.~ outlines future directions, exploring FGL's integration with trending GML topics and extending FGL to more topologically complex graph types.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "bare_jrnl.tex",
      "rlhf_score": 0.397,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.451,
      "datasets_score": 0.405,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Federated Graph Learning (FGL), which is a form of distributed training involving collaborative model training across decentralized clients. It discusses key procedures like local training, global aggregation, and server-side innovations, directly aligning with distributed training concepts such as parallel computing and multi-node machine learning to handle decentralized data and accelerate model performance.",
      "datasets_justification": "The paper emphasizes a data-centric taxonomy for FGL, analyzing dataset properties like structural types (e.g., homogeneous, heterogeneous graphs), decentralization formats, and visibility levels, which involves dataset analysis and evaluation in ML contexts. While it surveys and categorizes existing datasets rather than introducing new ones or benchmarking methodologies, it addresses data-centric challenges, making it relevant but not primarily focused on dataset creation or curation.",
      "summary": "This survey paper provides a data-centric overview of Federated Graph Learning (FGL), emphasizing how data properties and usage influence decentralized graph-based machine learning while preserving privacy. It introduces a two-level taxonomy—Data Characteristics, which categorizes graph datasets by structure, distribution, and visibility, and Data Utilization, which examines training procedures to address challenges like data quality and heterogeneity—while reviewing existing studies, exploring integrations with Pretrained Large Models, showcasing real-world applications, and outlining future research directions to advance the field.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new data-centric taxonomy for FGL, reframing the field by focusing on data properties and utilization, which significantly advances beyond existing model-centric surveys.",
      "impact_score": "High",
      "impact_justification": "The work could influence a wide range of future research in federated learning and graph ML by providing a structured framework for addressing data-centric challenges, potentially extending to commercial applications in privacy-preserving big data analytics.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This high-quality survey offers essential insights and a novel taxonomy that researchers in FGL and related fields need to be aware of for guiding future studies and applications.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/b926a6649906f207fd9c69ba65a4ee96300443a7",
      "h_index_fetch_method": "full_id",
      "total_authors": 11,
      "authors_found": 11,
      "highest_h_index": 8,
      "average_h_index": 2.909090909090909,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Zhengyu Wu",
          "profile_url": "https://www.semanticscholar.org/author/2268502577",
          "h_index": 7
        },
        {
          "name": "Xunkai Li",
          "profile_url": "https://www.semanticscholar.org/author/2268429288",
          "h_index": 8
        },
        {
          "name": "Yinlin Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2167187084",
          "h_index": 3
        },
        {
          "name": "Zekai Chen",
          "profile_url": "https://www.semanticscholar.org/author/2356610159",
          "h_index": 0
        },
        {
          "name": "Guochen Yan",
          "profile_url": "https://www.semanticscholar.org/author/2302862953",
          "h_index": 2
        },
        {
          "name": "Yanyu Yan",
          "profile_url": "https://www.semanticscholar.org/author/2374954895",
          "h_index": 0
        },
        {
          "name": "Hao Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2359231417",
          "h_index": 0
        },
        {
          "name": "Yuming Ai",
          "profile_url": "https://www.semanticscholar.org/author/2338828584",
          "h_index": 1
        },
        {
          "name": "Xinmo Jin",
          "profile_url": "https://www.semanticscholar.org/author/2373589452",
          "h_index": 0
        },
        {
          "name": "Ronghua Li",
          "profile_url": "https://www.semanticscholar.org/author/2312235766",
          "h_index": 3
        },
        {
          "name": "Guoren Wang",
          "profile_url": "https://www.semanticscholar.org/author/2240263835",
          "h_index": 8
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669367",
      "updated_at": "2025-08-11T23:46:03.603853",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16556",
      "title": "Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A\n  Practical Approach",
      "authors": [
        "Jon Gutiérrez-Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.AR (Hardware Architecture)",
        "cs.LG (Machine Learning)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "The use of HSI for autonomous navigation is a promising research field aimed\nat improving the accuracy and robustness of detection, tracking, and scene\nunderstanding systems based on vision sensors. Combining advanced computer\nalgorithms, such as DNNs, with small-size snapshot HSI cameras enhances the\nreliability of these systems. HSI overcomes intrinsic limitations of greyscale\nand RGB imaging in depicting physical properties of targets, particularly\nregarding spectral reflectance and metamerism. Despite promising results in\nHSI-based vision developments, safety-critical systems like ADS demand strict\nconstraints on latency, resource consumption, and security, motivating the\nshift of ML workloads to edge platforms. This involves a thorough\nsoftware/hardware co-design scheme to distribute and optimize the tasks\nefficiently among the limited resources of computing platforms. With respect to\ninference, the over-parameterized nature of DNNs poses significant\ncomputational challenges for real-time on-the-edge deployment. In addition, the\nintensive data preprocessing required by HSI, which is frequently overlooked,\nmust be carefully managed in terms of memory arrangement and inter-task\ncommunication to enable an efficient integrated pipeline design on a SoC. This\nwork presents a set of optimization techniques for the practical co-design of a\nDNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at\nADS, including key optimizations such as functional software/hardware task\ndistribution, hardware-aware preprocessing, ML model compression, and a\ncomplete pipelined deployment. Applied compression techniques significantly\nreduce the complexity of the designed DNN to 24.34% of the original operations\nand to 1.02% of the original number of parameters, achieving a 2.86x speed-up\nin the inference task without noticeable degradation of the segmentation\naccuracy.",
      "published_date": "2025-07-22T13:09:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16556v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16556v1",
      "latex_url": "http://arxiv.org/src/2507.16556v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The application of deep learning techniques, especially fully convolutional networks (FCN) , has boosted the advances in the field of image segmentation , improving the ability of AI algorithms to accurately recognise objects within images across various application domains including medical imaging , remote sensing and food industry , among others.\nNevertheless, a remarkable challenge arises when objects with different spectral signatures appear similar under specific lighting conditions, complicating object segmentation.\nThis phenomenon, known as metamerism, is a concern for many RGB-based image processing applications .\n\nTo address this phenomenon, recent studies have explored the use of hyperspectral imaging (HSI) as a robust and efficient potential solution to acquire spectral information across a wider range of wavelengths, providing the discriminative AI-based algorithm with richer input.\nWhen discussing HSI, it is important to distinguish between two concepts that are often interchangeably used in the literature.\nOn the one hand, some researchers are evaluating whether HSI in the visible range yields better results than traditional RGB images.\nOn the other hand, there is a line of research investigating whether utilizing information beyond the visible spectrum, regardless of whether it involves HSI, can lead to more accurate and robust segmentations.\nRegarding the potential benefits of using HSI over RGB, the authors of review several studies from different industries, such as agriculture, food assessment, healthcare, and automotive, where superior performance is achieved by using HSI.\nMore recently, for robot navigation and for facade segmentation have also found that HSI leads to clearer decision boundaries between classes.\n\nCurrently, the potential for HSI to be applied to dynamic environments such as autonomous driving systems (ADS), where accurate and timely data interpretation is crucial for ensuring passengers’ safety, has become more feasible due to the emergence of compact snapshot hyperspectral cameras .\nThis technology allows for the simultaneous capture of object reflectance across a multitude of wavelengths in a single shot at video rates.\nNonetheless, hyperspectral sensors, and especially snapshot sensors, usually require a computationally costly preprocessing stage to convert the acquired 2D raw data into 3D hyperspectral cubes.\nThis step is often overlooked by neural network developers, as the images used for training and testing are typically preprocessed beforehand.\nIn addition to this, the promising results of combining HSI with deep neural networks (DNNs) very often come at the cost of using over-parameterized deep learning models, leading to high computational complexity.\nThese DNNs typically contain millions of parameters and require the execution of billions of computation operations (floating-point, FLOPS or integer, OPS) per inference, posing significant challenges for the on-the-edge deployment of safety-critical applications.\n\nIn order to efficiently implement a complete segmentation pipeline in an embedded computing platform, careful planning using a refined hardware/software co-design methodology is required.\nThis scheme is essential to optimize the efficient integration of the different stages of the complete processing pipeline: raw data to hyperspectral cube preprocessing, data storage and arrangement in memory, data communication, and DNN inference.\nBesides, a key consideration in this process is the identification and mitigation of potential bottlenecks that may limit overall performance.\nTherefore, it is imperative to develop optimized solutions that can ensure reliable and fast performance.\n\nGiven these challenges, implementing these models on field programmable gate arrays (FPGAs) and FPGA-based system on chips (SoCs) emerges as a promising solution.\nFPGAs and programmable SoCs enable the design of domain-specific processors tailored to each use case, facilitating optimized resource usage, power efficiency, and low latency while allowing for reconfigurability when needed.\nThis adaptability makes these platforms ideal for deploying advanced HSI-based segmentation models in resource-constrained applications, paving the way for more effective and reliable systems in ADS applications and beyond.\n\nIn this article, a holistic design approach for a DNN-based HSI segmentation pipeline optimized for FPGA-based SoCs is presented.\nThe target platform has been AMD-Xilinx’s KV260 board (see Figure ), which is tailored for edge vision applications and integrates the K26 SOM with a Zynq UltraScale+ MPSoC, where the segmentation performance and optimizations were evaluated.\n\n {figure}[h!]\n \n [width=13.5cm]{images/PDF/BlockDiagramSOM.pdf}\n {Diagram of the DNN-based segmentation pipeline.\nLeft, KV260 board, adapted from ; centre, Zynq UltraScale+ MPSoC, adapted from , and right, Application Processing Unit, adapted from and Programmable Logic, adapted from .}\n\n {figure}\n\nThis hardware/software co-design methodology and DNN architecture selection are aligned with the platform constraints to maximize efficiency.\nThe optimization techniques applied throughout the design process, from raw data preprocessing to DNN inference deployment are detailed.\nPreprocessing stages leverage data- and thread-level parallelism, with some steps offloaded to hardware when feasible.\nComputational profiling identifies suitable DNN compression techniques and assesses the need for multi-stage pipelined preprocessing to mitigate bottlenecks.\n\nFor DNN inference, the rigidity of the K26 SOM’s accelerator in quantized parameter representation and its lack of support for sparse matrix multipliers led to focus on channel pruning.\nThe proposed iterative pruning method combines static and dynamic analyses to define pruning targets and assess their feasibility, while ensuring minimal impact on inference quality.\nIt is also explained how to assess whether the initially set pruning ratio is excessive, or if further pruning can be applied in subsequent iterations without degrading performance.\n\nApplied to this segmentation U-Net for the HSI-Drive v2.0 dataset , this optimization scheme reduces inference operations by an order of magnitude and the number of parameters by two orders of magnitude, while preserving performance.\nIt also improves resource and power efficiency, making deployment more practical.\n\nThe rest of the article is organized as follows.\nSection covers the related work, including current HSI databases for ADS, state-of-the-Art (SotA) deep learning models for ADS and current pruning-based DNN model optimization strategies.\nIn Section , the training and testing dataset, HSI-Drive v2.0 , is described as well as the modified version of the original U-Net .\nThe testing results against a SotA model are also compared there.\nSection details the compressing operations applied to the baseline model, with particular emphasis on both the static and dynamic analyses of the model and the iterative pruning methodology.\nThe preprocessing of the raw images is explained in Section , which also includes a discussion about the optimal memory arrangement of the hyperspectral cubes.\nFinally, Section provides details on the deployment of the complete pipeline (including raw data loading, preprocessing, cube transmission, and segmentation) on the KV260 board.\nDifferent configurations of the pipeline with 1, 2 and 3 stages are presented and the performance of varying configurations of the K26's deep processing unit (DPU) is characterized in terms of latency, throughput and power consumption.\nThe article concludes with a discussion of the key findings in Section .",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "template_acm.tex",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.338,
      "distributed_training_score": 0.462,
      "datasets_score": 0.361,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is on optimizing DNN inference for HSI segmentation on FPGA-based SoCs, including hardware/software co-design, model compression, and preprocessing for edge deployment in ADS. It does not address distributed training, parallel computing across multiple nodes, or strategies for accelerating model training by partitioning data or computation across processors. The focus is solely on inference optimization and deployment, not training processes.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667745",
      "updated_at": "2025-08-11T23:43:05.606848",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16559",
      "title": "Comparative validation of surgical phase recognition, instrument\n  keypoint estimation, and instrument instance segmentation in endoscopy:\n  Results of the PhaKIR 2024 challenge",
      "authors": [
        "Tobias Rueckert",
        "David Rauber",
        "Raphaela Maerkl",
        "Leonard Klausmann",
        "Suemeyye R. Yildiran",
        "Max Gutbrod",
        "Danilo Weber Nunes",
        "Alvaro Fernandez Moreno",
        "Imanol Luengo",
        "Danail Stoyanov",
        "Nicolas Toussaint",
        "Enki Cho",
        "Hyeon Bae Kim",
        "Oh Sung Choo",
        "Ka Young Kim",
        "Seong Tae Kim",
        "Gonçalo Arantes",
        "Kehan Song",
        "Jianjun Zhu",
        "Junchen Xiong",
        "Tingyi Lin",
        "Shunsuke Kikuchi",
        "Hiroki Matsuzaki",
        "Atsushi Kouno",
        "João Renato Ribeiro Manesco",
        "João Paulo Papa",
        "Tae-Min Choi",
        "Tae Kyeong Jeong",
        "Juyoun Park",
        "Oluwatosin Alabi",
        "Meng Wei",
        "Tom Vercauteren",
        "Runzhi Wu",
        "Mengya Xu",
        "An Wang",
        "Long Bai",
        "Hongliang Ren",
        "Amine Yamlahi",
        "Jakob Hennighausen",
        "Lena Maier-Hein",
        "Satoshi Kondo",
        "Satoshi Kasai",
        "Kousuke Hirasawa",
        "Shu Yang",
        "Yihui Wang",
        "Hao Chen",
        "Santiago Rodríguez",
        "Nicolás Aparicio",
        "Leonardo Manrique",
        "Juan Camilo Lyons",
        "Olivia Hosie",
        "Nicolás Ayobi",
        "Pablo Arbeláez",
        "Yiping Li",
        "Yasmina Al Khalil",
        "Sahar Nasirihaghighi",
        "Stefanie Speidel",
        "Daniel Rueckert",
        "Hubertus Feussner",
        "Dirk Wilhelm",
        "Christoph Palm"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Reliable recognition and localization of surgical instruments in endoscopic\nvideo recordings are foundational for a wide range of applications in computer-\nand robot-assisted minimally invasive surgery (RAMIS), including surgical\ntraining, skill assessment, and autonomous assistance. However, robust\nperformance under real-world conditions remains a significant challenge.\nIncorporating surgical context - such as the current procedural phase - has\nemerged as a promising strategy to improve robustness and interpretability.\n  To address these challenges, we organized the Surgical Procedure Phase,\nKeypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the\nEndoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel,\nmulti-center dataset comprising thirteen full-length laparoscopic\ncholecystectomy videos collected from three distinct medical institutions, with\nunified annotations for three interrelated tasks: surgical phase recognition,\ninstrument keypoint estimation, and instrument instance segmentation. Unlike\nexisting datasets, ours enables joint investigation of instrument localization\nand procedural context within the same data while supporting the integration of\ntemporal information across entire procedures.\n  We report results and findings in accordance with the BIAS guidelines for\nbiomedical image analysis challenges. The PhaKIR sub-challenge advances the\nfield by providing a unique benchmark for developing temporally aware,\ncontext-driven methods in RAMIS and offers a high-quality resource to support\nfuture research in surgical scene understanding.",
      "published_date": "2025-07-22T13:10:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16559v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16559v1",
      "latex_url": "http://arxiv.org/src/2507.16559v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In recent years, significant progress has been made in the field of computer and robot-assisted minimally invasive surgery (RAMIS) with the aim of supporting the surgical team during laparoscopic interventions with innovative assistance systems~.\nThe foundation for the development of such applications often relies on the determination of the current phase of a procedure, the segmentation of the surgical instruments in endoscopic images, and the localization of certain keypoints and the associated estimation of the instruments' pose.\nKnowing the current intervention phase offers further possibilities, such as improving safety in the operating room (OR) by providing early context-sensitive warnings~, by optimizing the OR management~, or by providing accurate procedure time predictions~, e.~g., for anesthesia planning.\nSegmentation of surgical instruments provides a variety of application areas, such as surgical navigation systems~, surgical skill assessment~, or autonomous endoscope guidance~.\nBased on the localization of certain keypoints that describe the pose of an instrument, the distance to risk structures can be determined~ and the assessment of surgical skills of the surgeon can further be automated~.\nPurely image-based processing of endoscopic data is of particular importance for all three tasks, as no changes need to be made to the surgical environment and the procedure workflow, allowing a simple integration into existing clinical practice~.\n\nIn the past, the development of methods was often based on the collection and annotation of application-specific data.\nIn order to avoid the disadvantages of this procedure, such as the lack of reproducibility or the inability to compare the results between the methods, the concept of challenges was introduced.\nThe most well-known platform for conducting such challenges in the area of medical image processing is the annual conference of the Medical Image Computing and Computer Assisted Intervention (MICCAI) society, which enables the organization of a large number of challenges for various medical applications.\nOne of them is the Endoscopic Vision Challenge (EndoVis), which has been held annually since 2015. It coordinates a series of sub-challenges, all focused exclusively on endoscopic image data.\nWithin the domain of laparoscopic phase recognition, three dedicated sub-challenges have been conducted to date~. For the task of surgical instrument segmentation, eight sub-challenges have been performed~. As for surgical instrument keypoint estimation, one dedicated sub-challenge has been held so far~.\n\n {figure*}[t]\n  \n\n  \n  {tabularx}{ }{p{1.7cm} X X X }\n\n  \n  {1}{c}{Task} &  {1}{c}{Hospital 1} &  {1}{c}{Hospital 2} &  {1}{c}{Hospital 3}\n\n  \n\n  { Input\n frames } &\n\n  [width=1.0 ]{00_dataset_hospital_1_frame.png} &\n  [width=1.0 ]{00_dataset_hospital_2_frame.png} &\n  [width=1.0 ]{00_dataset_hospital_3_frame.png}\n\n  {gray!10}\n  { Surgical\n phase\n recognition } &\n  {-0.15cm}\n  {00_dataset_hospital_1_phase}\n &\n  {-0.15cm}\n  {00_dataset_hospital_2_phase}\n &\n  {-0.15cm}\n  {00_dataset_hospital_3_phase}\n\n  {gray!10}\n  { Instrument\n instance\n segmentation } &\n  [width=1.0 ]{00_dataset_hospital_1_mask.png} &\n  [width=1.0 ]{00_dataset_hospital_2_mask.png} &\n  [width=1.0 ]{00_dataset_hospital_3_mask.png}\n\n  {gray!10}\n  { Instrument\n keypoint\n estimation } &\n  [width=1.0 ]{00_dataset_hospital_1_kp.png} &\n  [width=1.0 ]{00_dataset_hospital_2_kp.png} &\n  [width=1.0 ]{00_dataset_hospital_3_kp.png}\n\n  \n  {tabularx}\n  {\n Visualization of the PhaKIR tasks and annotations for each of the three medical centers. For the phase recognition task, the phases preparation (P), calot triangle dissection (CTD), clipping and cutting (ClCu), gallbladder dissection (GD), gallbladder packaging (GP), cleaning and coagulation (ClCo), and gallbladder retraction (GR) are shown. For the instrument instance segmentation task, the color-encoded masks are presented. For the instrument keypoint estimation task, a visualization of the keypoint coordinates is depicted, including hidden keypoints surrounded by white.\n }\n\n {figure*}\n\nHowever, the datasets provided by these sub-challenges do not cover all the necessary aspects required for the development of real-world applications.\nFor the phase recognition task, only one paper presents a dataset that originates from multiple centers~.\nIn the context of the instrument segmentation task, several challenges do not differentiate between distinct instrument classes~ or between individual instances within the same class .\nThis lack of distinction hampers the ability to accurately identify overlapping instruments and complicates the task of tracking the tools over time, which relies on the consistent identification of each instrument across successive frames.\nThe recordings of some sub-challenges operate on porcine tissue~, which significantly simplifies the recognition of surgical instruments compared to human tissue~, or do not use any biological tissue at all~.\nSome datasets only provide single frames~ or short clips~ instead of full video sequences, precluding the use of temporal information for the segmentation task.\nInformation on relevant instrument keypoints is available in only one sub-challenge~. However, for in-vivo recordings involving manual instruments, only the coordinates of a single keypoint and the instrument orientation are provided.\nMeanwhile, sequences featuring robotic instruments are based on ex-vivo data from animal tissue, representing highly simplified movements that do not account for real-world surgical complexities.\nAdditionally, none of the sub-challenge datasets for the segmentation of surgical instruments or the keypoint estimation task include multi-center data, thereby limiting the ability to evaluate the participant methods across different clinical environments and surgical tools.\n\nTo address these challenges, we provided a dataset comprising full endoscopic video sequences of thirteen human cholecystectomies from three medical centers, enabling the incorporation of temporal information in method development as well as the consideration of real-world conditions.\nThe annotations span three tasks: determining the surgical phase of an intervention, performing pixel-precise instance segmentation of surgical instruments, and localizing specific keypoints of these instruments.\nThis setup supports both the recognition of surgical instruments and the understanding of surgical context in one dataset.\nAn overview of the tasks and the associated annotations is shown in Figure~.\n\nOur paper is structured according to the transparent reporting of biomedical image analysis challenges (BIAS) guidelines published by~.\nAs our challenge consists of three tasks, we refrain from using an overarching methods section and instead present the Chapters~, , , and~ separately to ensure a clear structure.\n\nThe appendix contains information regarding the challenge organizers (see ), the data description and labeling instructions (see ), the submission instructions for the participants (see ), as well as the challenge design document (see ).\n\n {figure*}[t]\n  \n  [width=0.9 ]{01_endovis_2024_challenges.pdf}\n  {Number of participants that registered and submitted for each of the eight individual EndoVis-2024 sub-challenges, sorted in ascending order based on the number of registrations.}\n\n {figure*}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "00_arXiv_content_00_introduction.tex",
      "rlhf_score": 0.271,
      "weak_supervision_score": 0.284,
      "diffusion_reasoning_score": 0.238,
      "distributed_training_score": 0.287,
      "datasets_score": 0.345,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668217",
      "updated_at": "2025-08-11T23:43:05.606954",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16562",
      "title": "Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology:\n  A User Study (Extended Version)",
      "authors": [
        "Megha Quamara",
        "Viktor Schmuck",
        "Cristina Iani",
        "Axel Primavesi",
        "Alexander Plaum",
        "Luca Vigano"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper, we present the findings of a user study that evaluated the\nsocial acceptance of eXtended Reality (XR) agent technology, focusing on a\nremotely accessible, web-based XR training system developed for journalists.\nThis system involves user interaction with a virtual avatar, enabled by a\nmodular toolkit. The interactions are designed to provide tailored training for\njournalists in digital-remote settings, especially for sensitive or dangerous\nscenarios, without requiring specialized end-user equipment like headsets. Our\nresearch adapts and extends the Almere model, representing social acceptance\nthrough existing attributes such as perceived ease of use and perceived\nusefulness, along with added ones like dependability and security in the\nuser-agent interaction. The XR agent was tested through a controlled experiment\nin a real-world setting, with data collected on users' perceptions. Our\nfindings, based on quantitative and qualitative measurements involving\nquestionnaires, contribute to the understanding of user perceptions and\nacceptance of XR agent solutions within a specific social context, while also\nidentifying areas for the improvement of XR systems.",
      "published_date": "2025-07-22T13:14:05+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16562v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16562v1",
      "latex_url": "http://arxiv.org/src/2507.16562v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Extended Reality, or XR, brings together technologies like Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), which combine digital elements with the physical (or~real) world to deliver interactive and immersive user experiences~. XR-based agent solutions take such experiences a step further by utilizing Artificial Intelligence (AI)-powered virtual assistants or conversational agents within these immersive environments~. These synthetic agents can mimic real-life interactions to provide personalized experiences that enhance learning and decision-making. These offer valuable tools for applications in fields like education~ and healthcare~, enabling users to interact with technology in more natural, flexible ways.\n\nXR systems can also be highly beneficial in fields like journalism training, offering immersive, hands-on experiences that surpass what traditional methods like books or even on-the-job training can provide. Journalists can practice scenarios like interviewing under pressure or reporting in conflict zones, without real-world risks. However, existing XR training solutions present challenges~; they require expensive equipment, high costs per trainee, and a physical location for training. Many also exhibit poor human-computer interaction and provide limited personalization based on users' needs. The use of conversational agents, however, can help overcome these limitations by offering more cost-effective, flexible, and engaging training experiences while enhancing user interaction.\n\nIf XR-based conversational agents are to achieve wide-scale adoption across such applications, it is essential to understand how different user groups accept or reject this technology, as skepticism can hinder its integration into various social contexts~. How will users respond to interacting with virtual avatars? To what extent do differences in the agent’s conversational and social abilities, such as responsiveness, expressiveness, or realism, affect users' attitudes and willingness to engage with the system? How do users perceive the security, privacy, and trustworthiness of such agents in varying deployment contexts? Understanding these aspects is essential not only to assess the feasibility of deploying such systems at scale but also to guide the development of more effective, user-centered tools that users trust and are motivated to use, ultimately contributing to their acceptance.\n\nIn this paper, we report on the findings of a user study that we conducted to evaluate the social acceptance of XR agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves interaction of the users with a virtual avatar, which is enabled by a modular toolkit. The interactions are designed to deliver tailored training for journalists in digital and remote environments, particularly in sensitive and high-risk situations, without the need for specialized equipment, such as headsets. We employed a mixed-methods approach, combining quantitative questionnaire analysis and qualitative interviews, in order to get a deeper understanding of the attitudes and perception of potential future users. Our research builds on the Almere model~, which we adapt and extend through a customized questionnaire to better understand the perceptions of the participants interacting with the training system in a controlled, real-world setting. The questionnaire includes questions regarding existing attributes, such as perceived usefulness and ease of use, adapted to the context of our case study, along with additional questions related to security, privacy, and trust. The system has been deployed as a prototype, and while it was under development, the study that we carried out allowed us to identify actionable considerations for its implementation, as the prototype is under continuous improvement. This study aims to inform standard practices for evaluating XR-based solutions by offering insights into their social acceptance in learning and training contexts, as well as in service-oriented settings such as post offices, information points, and customer service. Our findings, based on quantitative and qualitative data from the questionnaires, support this aim by enhancing the understanding of user perceptions and acceptance of XR agent solutions, while also identifying areas for improvement in system design and implementation.\n\nWe proceed as follows. Section~ presents the concept of social acceptance. Section~ reviews related work. Section~ details our study, including methodology. Section~ presents our findings. Section~ concludes and discusses future work.  {extended} The Appendix~ contains the user consent form which the participants signed before taking part in this study.  {extended}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.279,
      "diffusion_reasoning_score": 0.316,
      "distributed_training_score": 0.288,
      "datasets_score": 0.303,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668754",
      "updated_at": "2025-08-11T23:43:05.607057",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16564",
      "title": "TTMBA: Towards Text To Multiple Sources Binaural Audio Generation",
      "authors": [
        "Yuxuan He",
        "Xiaoran Yang",
        "Ningning Pan",
        "Gongping Huang"
      ],
      "categories": [
        "cs.SD (Sound)",
        "cs.AI (Artificial Intelligence)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "Most existing text-to-audio (TTA) generation methods produce mono outputs,\nneglecting essential spatial information for immersive auditory experiences. To\naddress this issue, we propose a cascaded method for text-to-multisource\nbinaural audio generation (TTMBA) with both temporal and spatial control.\nFirst, a pretrained large language model (LLM) segments the text into a\nstructured format with time and spatial details for each sound event. Next, a\npretrained mono audio generation network creates multiple mono audios with\nvarying durations for each event. These mono audios are transformed into\nbinaural audios using a binaural rendering neural network based on spatial data\nfrom the LLM. Finally, the binaural audios are arranged by their start times,\nresulting in multisource binaural audio. Experimental results demonstrate the\nsuperiority of the proposed method in terms of both audio generation quality\nand spatial perceptual accuracy.",
      "published_date": "2025-07-22T13:16:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16564v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16564v1",
      "latex_url": "http://arxiv.org/src/2507.16564v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "With the rise of rapid technological innovation, the demand for high-fidelity binaural audio generation has surged in fields such as virtual reality (VR), augmented reality (AR), and mixed-reality systems, particularly for education and entertainment. While deep generative models have advanced audio generation, previous research has primarily focused on mono audio, limiting their effectiveness in scenarios requiring spatial acoustic signatures.\nIn recent years, extensive research has been conducted, and significant progress has been achieved in this field. For instance, Diffsound~, the first diffusion-based audio generation model, uses a pre-trained VQ-VAE~ to map mel-spectrograms into discrete tokens, which are generated with a diffusion model. AudioGen~ adopts a similar VQ-VAE with an autoregressive model in a discrete waveform space. In contrast, AudioLDM~ employs a continuous latent diffusion model (LDM)~, yielding better audio quality. While these models effectively control audio duration, they face challenges with multisource audio control, particularly in scenarios requiring precise spatial accuracy\n\nTo address these challenges, recent studies have focused on improving temporal and spatial control in audio generation. Make-An-Audio 2~\n and TangoFlux~ prioritize temporal information for variable-length audio generation. Make-An-Audio 2 uses an LLM and temporal encoder for coherent audio generation but incurs high computational costs due to LDM. TangoFlux, a lightweight model for duration control, generates binaural audio by duplicating single-channel recordings, resulting in nearly identical outputs and lacking spatial control through text. To enhance spatial accuracy, we first utilize TangoFlux and subsequently apply binaural rendering with directional cues.\nBEWO~ is a latent diffusion model that utilizes text embedding and azimuth information to generate spatial audio. Similarly, Immersive Diffusion~ combines ELSA~ with the Diffusion Transformer to produce spatial audio. Both models accept text inputs and generate spatial audio, but they lack precise temporal control and neglect listener anatomy, both of which are essential for authentic binaural perception.\n\n {figure}[t!]\n  \n  [width= ]{figs/fig1.pdf}\n  {-12pt}\n  {Illustration of acoustic propagation models: (a) Conventional geometric acoustic simulation based solely on room impulse responses; (b) Binaural modeling incorporating listener-specific cues.}\n\n  {-16pt}\n {figure}\n\nAnother class of methods that has garnered significant attention involves rendering mono audio to binaural audio. Traditional simulators, such as Pyroomacoustics~ efficiently model acoustic propagation between sources and receivers, as shown in Fig.~~(a). However, these methods mainly focus on room impulse responses (RIRs) and neglect crucial factors such as the listener’s pinnae, head, and torso, all of which are essential for authentic binaural perception~. To address this limitation, Head Related Transfer Functions (HRTF) and Binaural Room Transfer Functions (BRTF) model these cues ~. The NIIRF framework~ approximates direction-dependent HRTF coefficients with cascaded IIR filters but suffers from spectral errors. Neural networks like BinauralGrad~ and NFS~ generate binaural audio, with NFS excelling in both static and dynamic scenarios in the Fourier space. These methods account for the listener’s anatomy, as shown in Fig.~~(b).\n\n {figure*}[t!]\n  \n  [width= ]{figs/fig2.pdf}\n  {-12pt}\n  {The framework of the proposed text-to-multisource binaural audio generation network.}\n\n  {-16pt}\n {figure*}\n\nIn this paper, we propose a text-to-multisource binaural audio generation model (TTMBA) {Demo page: {https://25219.github.io/1.github.io/}} that synthesizes binaural audio with controllable spatial (azimuth, elevation, distance) and temporal (duration, start time) attributes.\nThe model first employs a pre-trained large language model (LLM) to extract source information, followed by an audio generation network that synthesizes individual mono streams, and a binaural rendering network simulates directional effects.\nThe contributions of this work include:  {itemize}\n  The first text-conditioned binaural audio generation model with control over duration, start time, and location.\n  The use of a LLM to extract source location from text, relying on physical principles when cues are absent.\n  The developed method achieves strong performance with low computational cost in both quantitative and perceptual evaluations.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "template_4.tex",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.333,
      "diffusion_reasoning_score": 0.447,
      "distributed_training_score": 0.327,
      "datasets_score": 0.288,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a method for text-to-multisource binaural audio generation, which uses a pretrained large language model for text segmentation and diffusion-based models (e.g., AudioLDM) for mono audio generation. However, it does not adapt the iterative refinement process of diffusion models to solve complex logical tasks or treat a Chain-of-Thought as a single entity for holistic correction. The diffusion components are focused on audio synthesis, not multi-step logical reasoning, so there is no clear alignment with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669376",
      "updated_at": "2025-08-11T23:43:05.607142",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16571",
      "title": "Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume\n  Computations",
      "authors": [
        "G. de Romémont",
        "F. Renac",
        "F. Chinesta",
        "J. Nunez",
        "D. Gueyffier"
      ],
      "categories": [
        "math.NA (Numerical Analysis)",
        "cs.AI (Artificial Intelligence)",
        "cs.NA (Numerical Analysis)",
        "math.AP (Analysis of PDEs)"
      ],
      "abstract": "We present a novel data-driven approach for enhancing gradient reconstruction\nin unstructured finite volume methods for hyperbolic conservation laws,\nspecifically for the 2D Euler equations. Our approach extends previous\nstructured-grid methodologies to unstructured meshes through a modified\nDeepONet architecture that incorporates local geometry in the neural network.\nThe architecture employs local mesh topology to ensure rotation invariance,\nwhile also ensuring first-order constraint on the learned operator. The\ntraining methodology incorporates physics-informed regularization through\nentropy penalization, total variation diminishing penalization, and parameter\nregularization to ensure physically consistent solutions, particularly in\nshock-dominated regions. The model is trained on high-fidelity datasets\nsolutions derived from sine waves and randomized piecewise constant initial\nconditions with periodic boundary conditions, enabling robust generalization to\ncomplex flow configurations or geometries. Validation test cases from the\nliterature, including challenging geometry configuration, demonstrates\nsubstantial improvements in accuracy compared to traditional second-order\nfinite volume schemes. The method achieves gains of 20-60% in solution accuracy\nwhile enhancing computational efficiency. A convergence study has been conveyed\nand reveal improved mesh convergence rates compared to the conventional solver.\nThe proposed algorithm is faster and more accurate than the traditional\nsecond-order finite volume solver, enabling high-fidelity simulations on\ncoarser grids while preserving the stability and conservation properties\nessential for hyperbolic conservation laws. This work is a part of a new\ngeneration of solvers that are built by combining Machine-Learning (ML) tools\nwith traditional numerical schemes, all while ensuring physical constraint on\nthe results.",
      "published_date": "2025-07-22T13:23:57+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16571v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16571v1",
      "latex_url": "http://arxiv.org/src/2507.16571v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Computational Fluid Dynamics (CFD) commonly relies on nonlinear hyperbolic partial differential equations (PDEs) to model a wide range of complex fluid behaviors. Even when the initial or boundary conditions are smooth, these equations can lead to the formation of discontinuities in finite time  [Sec. 2.4.2]{toro2000centred}.\nIn this scope, standard unstructured second order finite volume methods have become essential in CFD allowing the discretization of complex geometries like aircraft wings or turbine blades. However, achieving accurate solutions requires extremely fine grid resolution, particularly in regions with steep gradients, boundary layers, or turbulent flow, where large spatial or temporal scales dramatically increase computational cost and memory requirements. The need for fine grids stems from the fact that gradient reconstruction accuracy directly impacts solution quality. Machine learning presents a transformative opportunity to break this cycle by learning optimal reconstruction strategies that can maintain high accuracy even on relatively coarse meshes.\n \nIn this context, a number of works assessed the use of machine learning in computational physics, offering new avenues to augment or even replace traditional physics-driven numerical models. These data-driven strategies have proven effective in terms of accelerating numerical simulations, enhancing precision or robustness. In scenarios where the data maintains temporal but mainly spatial regularity, methods originating from image and video analysis such as multilayer perceptrons (MLPs), convolutional neural networks (CNNs), U-nets, recurrent neural networks (RNNs), long short term memory (LSTM) can be leveraged for scientific computing, including computational physics and fluid dynamics, e.g. forecasting physical fields, identifying model parameters, generating high-resolution outputs...\n\nHowever, the reliance of these techniques on structured data such as Cartesian grids, constant time intervals, or uniform input sizes restricts their effectiveness when applied to unstructured or irregular datasets. This presents a notable obstacle in domains where complex geometry is commonplace within simulations, particularly in high-resolution, physics-based simulations.\n \nTo overcome these limitations, growing efforts are being directed toward the development of methods capable of handling unstructured data, common in many real-world problems. Many methods are derived from Reduce Order Modeling (ROM), like Proper Orthogonal Decomposition (POD) or Singular Value Decomposition (SVD) and have been applied to many areas successfully with unstructured meshes like ocean models , turbulent flows , compressible aerodynamics . Others methods are developed mapping unstructured or sparse data onto a structured grid allowing the use of structured-based machine learning techniques like CNNs . An other promised and logical direction is the use of Graph Neural Networks (GNNs) where they excel by their capability to handle adaptive geometries. GNNs can be divided into three categories: convolutional , attentional and message passing . GNNs have been used to model various complex simulations .\n \nHowever, hyperbolic systems develop discontinuous solutions and standard neural networks surrogates struggle with shocks. For example, continuous activation functions are needed for proper learning during backpropagation (the process of reconstructing the gradient of an optimization function through Automatic Differentiation). A significant advancement has been the ability to precisely satisfy certain physical or numerical constraints by incorporating learned models into a fixed equation of motion. Indeed, hyperbolic conservation laws must preserve certain integral quantities (mass, momentum, energy). Unconstrained ML models may violate these conservation properties, leading to nonphysical solutions.\nConstraints can be enforced through conservative network architectures, Lagrange multipliers in the loss function, or post-processing correction steps.\n \nA range of methods have been developed for meshless solutions with Lagrange multipliers for physical guidance with strong formulation like PINNs  {raissi2018hidden, raissi2019physics} or weak formulation  {yu2018deep, kharazmi2019variational, bar2019learning} more adequate for hyperbolic PDEs. But Lagrange multipliers only define a physical guidance and not a hard constraint on the solver, meaning the physics are only approximated.\n \n\nTo this end, several hybrid physics-informed machine learning approaches have been developed to integrate neural networks within established numerical frameworks for solving forward problems . More specifically in finite volume solvers particularly suitable for hyperbolic PDEs  {jessica2023finite, li2023finite, ranade2021discretizationnet, stevens2020finitenet}. In this paradigm, different works assessed the use of ML for improving shocks capturing methods  {stevens2020enhancement,magiera2020constraint}, enhancing flux limiters  {nguyen2022machine, schwarz2023reinforcement}, enhancing corrections with artificial viscosity  {bruno2022fc} or the flux itself  {bezgin2021data,morand2024deep}. Some works focused their methodologies for unstructured data  {cen2024deep, morand2024deep}. The closest work related to our methodology is the optimization of a compact high-order variational reconstruction on triangular grids  {zhou2024machine}. In this study, an artificial neural network is employed to predict the optimal values of the derivative weights on cell interfaces. These interfaces represent the free parameters of the variational reconstruction.\n \nPursuing a similar goal, introduced a learning-based approach to gradient interpolation that matches the accuracy of conventional finite difference schemes while operating on significantly coarser grids. This methodology has been integrated into classical finite volume solvers, extending its application to problems such as passive scalar advection and the Navier-Stokes equations . However, these approaches are tailored to specific governing equations, for periodic boundary conditions and are highly unstable.\n \nThis paper presents a machine learning gradient optimization framework for second-order finite volume schemes on triangular unstructured grids extending and generalizing previous work of . The work specifically targets the 2D Euler equations. Using a supervised learning framework with high resolution solutions for the database, a geometry-aware neural network is trained to predict free parameters in order to correct the gradient used in the scheme.\nThe corrections will be explored for two types of gradients, namely the Green-Gauss (GG) and the Least Square (LSQ) gradients. The neural network architecture is inspired from the DeepONet architecture  {lu2019deeponet} with local inputs values and geometry. The geometry inputs are angles between each neighbors cells and allows more flexibility concerning the skewness of some elements. The model is trained on a dataset with high-resolution solutions made of random Riemann or the fluxed initial conditions with periodic boundary conditions. Lagrange multipliers such as the Total Variation regularizer and the entropy regularizer have been added to the loss to physically constrain the output solution with key properties inherent to hyperbolic conservations laws. The entropy regularizer aims at selecting the physically relevant weak solution  [Thm 6.1]{godlewski2013numerical}.\nAs the method developed is local, the work can be generalized to all types of geometries with different types of boundary conditions.\nNumerical results for two-dimensional Euler test cases demonstrate that the machine learning\noptimized gradient finite volume scheme can achieve a gain of 20% to 60% in solution accuracy for a same mesh configuration. A computational performance review has also been conveyed against the traditional finite volume solver.\n\nThe challenges addressed by this paper are the same as in , meaning stability, accuracy and computational performance.\n \nThe main innovation of this paper is the derivation of a highly constrained accurate and robust methodology to accurately compute solution for hyperbolic PDEs on unstructured mesh. The neural network can be seen as a subgrid correction operator, thus constrained and allowing super-resolution and physically-consistent solutions.\n \nThe remainder of this paper is organized as follows: Section provides an overview of hyperbolic conservation laws with a deeper look at 2D Euler equations. In Section , we describe the finite volume solver and the implementation of boundary conditions. Section introduces the complete algorithm alongside the Machine Learning model used and several regularization designed to ensure that the neural network produces physically consistent solutions to forward problems, particularly in the presence of strong shocks. In Section , the method is validated against a range of benchmark equations and test cases from the literature, demonstrating strong performance in both accuracy and stability. Finally, Section presents numerical experiments evaluating the algorithm’s computational efficiency against the traditional finite volume solver.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.392,
      "datasets_score": 0.303,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on a data-driven approach using a modified DeepONet architecture for gradient recovery in unstructured finite volume methods for CFD, specifically the 2D Euler equations. It involves neural networks for improving simulation accuracy and stability, with elements like physics-informed regularization and training on datasets. However, it does not involve diffusion models, iterative refinement for logical tasks, or treating a Chain-of-Thought as an entity for holistic correction. There is no component of multi-step logical reasoning using diffusion processes, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669387",
      "updated_at": "2025-08-11T23:43:05.607143",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16573",
      "title": "Semantic Segmentation for Preoperative Planning in Transcatheter Aortic\n  Valve Replacement",
      "authors": [
        "Cedric Zöllner",
        "Simon Reiß",
        "Alexander Jaus",
        "Amroalalaa Sholi",
        "Ralf Sodian",
        "Rainer Stiefelhagen"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "When preoperative planning for surgeries is conducted on the basis of medical\nimages, artificial intelligence methods can support medical doctors during\nassessment. In this work, we consider medical guidelines for preoperative\nplanning of the transcatheter aortic valve replacement (TAVR) and identify\ntasks, that may be supported via semantic segmentation models by making\nrelevant anatomical structures measurable in computed tomography scans. We\nfirst derive fine-grained TAVR-relevant pseudo-labels from coarse-grained\nanatomical information, in order to train segmentation models and quantify how\nwell they are able to find these structures in the scans. Furthermore, we\npropose an adaptation to the loss function in training these segmentation\nmodels and through this achieve a +1.27% Dice increase in performance. Our\nfine-grained TAVR-relevant pseudo-labels and the computed tomography scans we\nbuild upon are available at https://doi.org/10.5281/zenodo.16274176.",
      "published_date": "2025-07-22T13:24:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16573v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16573v1",
      "latex_url": "http://arxiv.org/src/2507.16573v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In developed countries, aortic valve disease affects $11.7%$ of people over the age $75$~.\nIn many cases such diseases can be treated by placing an implant over the malfunctioning valve to recover its functionality.\nA minimally invasive way to do this is Transcatheter Aortic Valve Replacement (TAVR).\nAhead of TAVR, preoperative planning is done, which involves measuring a patient's anatomical features based on computed tomography (CT).\n\nThis is structured by guidelines~, which state the most important anatomy to be assessed: The aortic valve and the shape and size of the annulus has to be known for the artificial valve to stay in place and seal the aorta,\n\nthe size of the aortic root is measured to determine the size of the stent, the vascular access to the valve is measured, which includes looking at vessels the catheter passes through,  , aorta, femoral- and iliac arteries.\nFurther, surgeons focus on calcification in the above anatomy,~ , the aortic valve, left ventricular outflow tract and the whole vascular system.\n\n {datasets}\n\nAdvancements in biomedical image segmentation~ enable precise delineation of semantic structures in images.\nWhile segmentation models can be utilized to measure structures in images, such as human anatomy in CTs, they require sufficiently large human-labeled training datasets to function.\nIn~ {tab:table_heart_datasets}~(a) we show publicly available heart-related CT datasets and the present anatomy labels relevant in TAVR preoperative planning.\nSome relevant anatomy is covered, some is missing and thus support via trained models in preoperative planning would be incomplete.\n\nWhile prior works propose innovative approaches for TAVR surgeries~, due to development on in-house data a broader investigation or comparison of such techniques is not possible.\nIn this work, we investigate to what extent existing public datasets can be enriched with additional TAVR relevant anatomy pseudo-labels and explore whether this anatomy can be learned by a single segmentation model to build a basis for preoperative planning assistance.\nTo foster exploration, we will make the enriched dataset publicly available.\nTo get a fuller picture how suitable segmentation models are for this task, we investigate the efficacy of different model architectures~ and loss functions~.\nBased on this, we propose a simple yet effective adaptation to the loss function, the focal skeleton recall loss to dynamically emphasize hard to segment structures during training.\nOur contributions summarize to:\n\n {itemize}\n   Introduction of rule-based enrichment of volumetric annotations for TAVR-related anatomical structures.\n   We make our enriched volumetric structures publicly available and benchmark the efficacy of segmentation models on this TAVR anatomy.\n   We propose the focal skeleton recall loss,\n leading to the improved segmentation results of $83.2%$ Dice for the TAVR anatomy.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "samplepaper.tex",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.315,
      "distributed_training_score": 0.332,
      "datasets_score": 0.351,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669077",
      "updated_at": "2025-08-11T23:43:05.607111",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16579",
      "title": "Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis",
      "authors": [
        "Xiaojiao Xiao",
        "Qinmin Vivian Hu",
        "Guanghui Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Medical image synthesis plays a crucial role in clinical workflows,\naddressing the common issue of missing imaging modalities due to factors such\nas extended scan times, scan corruption, artifacts, patient motion, and\nintolerance to contrast agents. The paper presents a novel image synthesis\nnetwork, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which\nemploys a multi-scale hierarchical approach for more detailed control over\nsynthesizing high-quality images across different resolutions and layers.\nSpecifically, this model utilizes randomly multi-scale high-proportion masks to\nspeed up diffusion model training, and balances detail fidelity and overall\nstructure. The integration of a Transformer-based Diffusion model process\nincorporates cross-granularity regularization, modeling the mutual information\nconsistency across each granularity's latent spaces, thereby enhancing\npixel-level perceptual accuracy. Comprehensive experiments on two challenging\ndatasets demonstrate that PHMDiff achieves superior performance in both the\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure\n(SSIM), highlighting its capability to produce high-quality synthesized images\nwith excellent structural integrity. Ablation studies further confirm the\ncontributions of each component. Furthermore, the PHMDiff model, a multi-scale\nimage synthesis framework across and within medical imaging modalities, shows\nsignificant advantages over other methods. The source code is available at\nhttps://github.com/xiaojiao929/PHMDiff",
      "published_date": "2025-07-22T13:30:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16579v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16579v1",
      "latex_url": "http://arxiv.org/src/2507.16579v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Medical image synthesis across and within medical imaging modalities plays a crucial role in optimizing clinical workflows, especially in the high-demand fields of radiology and radiation oncology . Different modalities, such as CT, MRI, and PET, or variations in spatial resolution (e.g., 3T vs. 7T), often provide complementary information, including detailed anatomical structures and nuanced abnormalities. However, conventional acquisition is frequently unfeasible due to constraints on time, cost, labor, or safety concerns such as radiation exposure. As a result, image synthesis has become a primary method for substituting or expediting imaging procedures without incurring additional costs or risks.\n\nDeep learning-based synthesis techniques have made significant strides in the field of medical imaging, particularly through the use of Generative Adversarial Networks (GANs) and their various adaptations, such as DCGAN, WGAN, CGAN, and CycleGAN. These models often struggle with unstable training and mode collapse, which limits the diversity and fidelity of the generated synthetic images . To address these limitations, denoising diffusion models, which generate higher-quality and more diverse synthetic images through a simple iterative process of refining noisy samples, are increasingly becoming an alternative to GANs . Moreover, Masked Autoencoders (MAE) demonstrate strong recognition performance by learning to regress pixels of masked patches given the other visible patches. Inspired by this, we incorporate masking into transformer-based diffusion models, which can enhance generalization capabilities and the acquisition of a comprehensive understanding of the structural characteristics of medical imaging.\n {figure}[t]\n \n [width=0.48 ]{figure1.pdf}\n {Challenge in modeling reliable synthesized medical images.}\n {figure}\n\nDespite significant advancements in existing works, several limitations remain: (i) The exclusive use of Mean Squared Error (MSE) loss for reconstruction optimization often results in output images that are blurrier than the original inputs . Incorporating a perceptual loss that emphasizes pixel quality could improve fine-grained semantic understanding and representation learning, leading to more realistic synthesized patches. (ii) High rates of random masking can lead to underutilization of images and extended training times. More critically, this approach introduces less reliable features, which undermines the model's generalization capabilities in downstream tasks . (iii) A further challenge is the inherent appearance of discrepancies between different imaging modalities, which demand extensive modeling efforts, as illustrated in Fig.. Moreover, it is crucial for models to perform reliably not only within individual modalities but also across multiple modalities, thereby enhancing their applicability and robustness in multimodal scenarios.\n\nIn this research, we introduce a novel image synthesis network named the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), designed to generate high-resolution medical images both across and within different imaging modalities. Our approach begins by decomposing the original image into a multi-resolution pyramid structure, allowing us to capture details and structures at different resolution levels effectively. Starting at the lowest resolution, PHMDiff denoises and reconstructs the image, progressively employing a coarse-to-fine upscaling method to restore and enrich details, ultimately enhancing the overall image quality. At each level of the pyramid, a unique random mask is applied based on the specific resolution and content, leveraging visible parts of the image to guide the reconstruction process. This approach ensures a delicate balance between preserving local details and maintaining overall structural integrity. Then, the processed image is diffused, which speeds up the network training. Additionally, we incorporate a regularization loss to model mutual information across different spatial granularities, optimizing the consistency between pixel-level details and overall structure, which enhances the precision and coherence of the final synthesized image.\n\nOur contributions are summarized as follows:\n {itemize}\n  We introduce an innovative pyramid hierarchical masking strategy that balances detail and structure at the image level, effectively preserving crucial fine-grained information.\n  We incorporate cross-granularity regularization (CGR) to model the consistency of mutual information across different granularities, thereby optimizing perceptual accuracy at the pixel level.\n  To our knowledge, this is the first implementation of an end-to-end diffusion model guided by a pyramid hierarchical masking strategy, which has faster training speed and achieves high-quality image synthesis across multiple resolutions and modalities.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "conference_101719.tex",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.358,
      "datasets_score": 0.318,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a diffusion model for medical image synthesis, focusing on generating high-quality images through hierarchical masking and iterative refinement processes. While it employs diffusion models for iterative denoising and reconstruction, it does not adapt this mechanism for solving complex logical tasks, multi-step reasoning, or treating a chain-of-thought as an entity. The work is centered on visual data generation, not logical or cognitive reasoning, so it lacks any relevant components for diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667683",
      "updated_at": "2025-08-11T23:43:05.606834",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16586",
      "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up\n  with Industry Demands? A Multivocal Literature Review",
      "authors": [
        "Choro Ulan Uulu",
        "Mikhail Kulyabin",
        "Layan Etaiwi",
        "Nuno Miguel Martins Pacheco",
        "Jan Joosten",
        "Kerstin Röse",
        "Filippos Petridis",
        "Jan Bosch",
        "Helena Holmström Olsson"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Computer-Aided Engineering (CAE) enables simulation experts to optimize\ncomplex models, but faces challenges in user experience (UX) that limit\nefficiency and accessibility. While artificial intelligence (AI) has\ndemonstrated potential to enhance CAE processes, research integrating these\nfields with a focus on UX remains fragmented. This paper presents a multivocal\nliterature review (MLR) examining how AI enhances UX in CAE software across\nboth academic research and industry implementations. Our analysis reveals\nsignificant gaps between academic explorations and industry applications, with\ncompanies actively implementing LLMs, adaptive UIs, and recommender systems\nwhile academic research focuses primarily on technical capabilities without UX\nvalidation. Key findings demonstrate opportunities in AI-powered guidance,\nadaptive interfaces, and workflow automation that remain underexplored in\ncurrent research. By mapping the intersection of these domains, this study\nprovides a foundation for future work to address the identified research gaps\nand advance the integration of AI to improve CAE user experience.",
      "published_date": "2025-07-22T13:39:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16586v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16586v1",
      "latex_url": "http://arxiv.org/src/2507.16586v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Computer-aided engineering (CAE) software employs mathematical models to predict system behavior across engineering domains , enabling virtual testing that reduces costs and accelerates development . These tools have become essential in industries from aerospace to automotive, where engineers simulate everything from aerodynamics to manufacturing quality .\n\nDespite their value, simulation tools present significant usability challenges. User experience (UX), defined by as \"A person's perceptions and responses resulting from the use and/or anticipated use of a product, system or service\", is critical for the effective use of these tools. In the CAE context, effective UX enables engineers to perform complex simulation tasks—such as geometry preparation, mesh generation, physics setup, and results interpretation—with minimal friction, cognitive load, and potential for error. However, many CAE tools struggle in this regard: engineers must often create precise geometric models, accurately specify numerous parameters, and possess extensive domain knowledge—with incorrect settings potentially wasting hours of computation time . These UX challenges limit adoption and effective utilization of powerful simulation capabilities.\nArtificial intelligence (AI) is transforming simulation workflows through several key interventions . Large language models like AnsysGPT provide around-the-clock technical guidance to engineers, while Siemens' Industrial Copilot enhances interface intuitiveness and reduces cognitive load. Such tools address critical UX barriers by making complex systems more accessible.\nBeyond assistance, AI accelerates simulation through neural networks trained on previous results, enabling non-specialists to evaluate designs within minutes rather than hours. Ansys SimAI exemplifies this approach, allowing more design alternatives to be tested across development phases . This democratization of simulation capability represents a major UX advancement.\nDespite AI's demonstrated potential to alleviate CAE's significant UX challenges, a clear, synthesized understanding of how these advancements are currently being implemented and validated—both in academic research and industry practice—is lacking. It remains unclear whether academic explorations align with industry needs, which AI-driven UX enhancements are gaining traction, and what specific research gaps hinder the translation of potential into widespread, effective application. Addressing this knowledge gap is crucial for guiding future research and development efforts aimed at fully leveraging AI to improve CAE user experience.\n\nThis paper contributes by: (i) systematically analyzing AI advancements impacting CAE software UX; (ii) identifying gaps between academic research and industry implementation; and (iii) mapping underexplored areas requiring further investigation. To achieve these contributions, this study employs a Multivocal Literature Review (MLR) as its primary methodology. This approach is chosen as it allows for the integration of insights from diverse sources, specifically utilizing a component systematic literature review (SLR) of academic research alongside a component Grey literature review (GLR) of industry practices. Adopting the MLR framework is crucial because, while previous literature has examined CAE and AI integration , it often lacks rigorous methodology, comprehensive market analysis, or a specific focus on user experience factors. The MLR overcomes these limitations by systematically analyzing and synthesizing both academic and industry perspectives, providing robust and comprehensive insights into AI-enhanced CAE user experience within this rapidly evolving field.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "template.tex",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.315,
      "datasets_score": 0.402,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper conducts a multivocal literature review on AI enhancements for UX in CAE, focusing on general AI applications like LLMs and adaptive UIs, but it does not discuss reinforcement learning, human feedback mechanisms, reward models, or fine-tuning AI with human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper is a literature review examining AI's role in improving UX for CAE software, including industry implementations, but it does not involve creating, analyzing, benchmarking, or evaluating datasets for ML or AI applications; it focuses on synthesizing existing research and identifying gaps without dataset-specific contributions.",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669397",
      "updated_at": "2025-08-11T23:43:05.607144",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16594",
      "title": "An Experimental Study of Split-Learning TinyML on Ultra-Low-Power\n  Edge/IoT Nodes",
      "authors": [
        "Zied Jenhani",
        "Mounir Bensalem",
        "Jasenka Dizdarević",
        "Admela Jukan"
      ],
      "categories": [
        "cs.NI (Networking and Internet Architecture)",
        "cs.AI (Artificial Intelligence)",
        "cs.DC (Distributed, Parallel, and Cluster Computing)"
      ],
      "abstract": "Running deep learning inference directly on ultra-low-power edge/IoT nodes\nhas been limited by the tight memory and compute budgets of microcontrollers.\nSplit learning (SL) addresses this limitation in which it executes part of the\ninference process on the sensor and off-loads the remainder to a companion\ndevice. In the context of constrained devices and the related impact of\nlow-power, over-the-air transport protocols, the performance of split learning\nremains largely unexplored. TO the best of our knowledge, this paper presents\nthe first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards,\ndesigned to benchmark the over-the-air performance of split learning TinyML in\nedge/IoT environments. We benchmark the performance of a MobileNetV2 image\nrecognition model, which is quantized to 8-bit integers, partitioned, and\ndelivered to the nodes via over-the-air updates. The intermediate activations\nare exchanged through different wireless communication methods: ESP-NOW, BLE,\nand traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on\nidentical hardware. Measurements show that splitting the model after\nblock_16_project_BN layer generates a 5.66 kB tensor that traverses the link in\n3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s.\nESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery\nlife further but increases latency beyond 10s.",
      "published_date": "2025-07-22T13:50:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16594v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16594v1",
      "latex_url": "http://arxiv.org/src/2507.16594v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The next frontier for an efficient AI is to run deep learning inference on ultra-low power resource-constrained edge/IoT devices, with the aim at improving data locality and privacy, while lowering bandwidth utilization, and enhancing the real-time processing capabilities . Significant advancements towards this goal have emerged with the paradigm shift towards on-device ML computing, enabled by modifications and optimizations of deep learning (DL) models . Herewith, the most promising alternative to classic DL models has been the use of Tiny Machine Learning (TinyML) models, as they provide lightweight inference algorithms that can run on the resource-constrained hardware in edge/IoT context. In terms of overcoming processing power and memory size limitations, Split Learning (SL) in particular partitions the deep learning network: the early layers run on the sensor whereas the remaining layers execute on a\ncompanion device, such as another microcontroller, a gateway or a nearby edge\nserver . Only intermediate activations are exchanged, so most privacy and\nbandwidth benefits of on-device inference are preserved.\n\n  Although SL has been\nanalyzed on smartphones and single board computers , there is a notable lack of empirical evidence on constrained microcontrollers, and, crucially, on the role that low power wireless protocols play in the overall latency energy budget. Especially the performance measurements and evaluations are missing in the context of latency incurred, such as in terms of processing time, including network and communication setup, ML inference, and the transmission of intermediate activations (neural network layer outputs) between devices. In addition, Round Trip Time (RTT) and delay associated with the transfer of intermediate activations and prediction data delay between devices is still unknown, which requires new studies. Finally, the impact of over-the-air communication modes on system performance, such as when using different protocols is an open issue. Communication networks based on WiFi, ESP-NOW, and Bluetooth Low Energy (BLE) are all expected to yield different performance.\n\nIn this work, we examine the potential of implementing the SL machine learning approach in resource-constrained environment, enabling on-device distribution of the ML computational load. For running ML models on hardware with limited resources, we opted for the TinyML framework solution. Although there are multiple TinyML frameworks available for the implementation of neural networks on different microcontrollers, we opt for TensorFlow Lite (TFLite), as it comes with sufficient developer support. The model is prepared (partitioned and quantized) in an edge server for which we use a Desktop PC, from which the firmware is then remotely deployed to the IoT devices using Over-the-Air (OTA) firmware updates without requiring physical access to the device. The selected IoT devices are based on low-cost open-source ESP32 microcontroller boards that can run the software necessary for TinyML applications.\n\nThe measurements reveal that placing the split after\n {block\\_16\\_project\\_BN} layer yields the most attractive trade-off between the evaluated split points: the resulting 5.66 kB intermediate activations crosses the link in only 3.2 ms for UDP, producing a steady-state round-trip time (RTT) of 5.8 s, more than 20x as fast as sending the raw\nimage to a remote server for full inference. ESP-NOW eliminates the need for\nWiFi infrastructure and lowers the radio energy per bit by a factor of four\nwhile keeping the RTT below 3.7 s, whereas BLE, although the most energy-\nefficient, stretches the RTT beyond 10 s because of its limited data\nrate and 512 B MTU.\n\nThe rest of this paper is organized as follows:\nSection~ surveys related work; Section~\ndescribes the system architecture; Section~ presents and\ndiscusses the measurements; Section~ concludes\nthe paper and outlines future research.\n\n {comment}\n\nIntegration of AI on ultra-low power resource-constrained IoT devices has become an increasingly promising approach to improve data privacy, lower bandwidth utilization, and enhancing the real-time processing capabilities . The most significant advancements in this integration have emerged with the paradigm shift towards on-device ML computing, enabled by modifications and optimizations of deep learning (DL) models in terms of overcoming processing power and memory size limitations found in IoT devices. Here, the most promising alternative to classic DL models has been the use of Tiny Machine Learning (TinyML) models, as they provide lightweight inference algorithms that can run on the resource-constrained hardware.\n\n {Split leanring part}\n\n {there is lack of SL TINYML experimental work on extreme resource open source constrained devices - non cpu }\n\n {general introduction...still extending}\n\nIn this work, we examine the potential of implementing SL machine learning approach in resource-constrained environment, enabling on-device distribution of the ML computational load. For running ML models on hardware with limited resources, we opted for the TinyML framework solution. Although there are multiple TinyML frameworks available for the implementation of neural networks on different microcontrollers, we opt for TensorFlow Lite, as it comes with sufficient developer support. The model is prepared (partitioned and quantized  {check is this forumlation correct}) in an edge server for which we use a Desktop PC, from which the firmware is then remotely deployed to the IoT devices using Over-the-Air (OTA) firmware updates without requiring physical access to the device. The selected IoT devices are based on low-cost open-source ESP32 microcontroller boards that can run the software necessary for TinyML applications. The designed IoT-edge system framework is then used to evaluate end-to-end latency, focusing on three key components:\n {itemize}\n   Processing time in the IoT devices, including network and communication setup, ML inference, and the transmission of intermediate activations (neural network layer outputs) between devices.\n   Delay associated with the transfer of intermediate activations and prediction data delay between devices.\n   Round-trip time (RTT) of the overall system, measuring the time from initial data transmission to receipt of the final prediction result.\n   In addition, we assess the impact of different communication modes on system performance, using different protocols for our WSN network, including WiFi, ESP-NOW, and Bluetooth Low Energy(BLE).This evaluation aims to identify protocol-specific trade-offs and their suitability for real-time or resource-constrained IoT applications involving TinyML inference and inter-device communication.\n\n {itemize}\n {Sentence about results}\n\nThe rest of the paper is organized as follows. Section II presents the related work. Section III provides the system description and Section IV discusses TinyML and Split Learning Experimental Evaluation. Section V concludes the paper.\n {comment}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.477,
      "datasets_score": 0.349,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper focuses on Split Learning for inference on edge/IoT devices, where the model is partitioned to offload parts to companion devices, involving distributed computation of intermediate activations. While this shares conceptual similarities with partitioning model architecture in distributed training, the paper exclusively addresses inference performance and latency in resource-constrained environments, not model training, parallel computing for acceleration, or multi-node training algorithms. Thus, it is only tangentially related to the topic of distributed training.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669407",
      "updated_at": "2025-08-11T23:43:05.607146",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16596",
      "title": "A Multimodal Deviation Perceiving Framework for Weakly-Supervised\n  Temporal Forgery Localization",
      "authors": [
        "Wenbo Xu",
        "Junyan Wu",
        "Wei Lu",
        "Xiangyang Luo",
        "Qian Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Current researches on Deepfake forensics often treat detection as a\nclassification task or temporal forgery localization problem, which are usually\nrestrictive, time-consuming, and challenging to scale for large datasets. To\nresolve these issues, we present a multimodal deviation perceiving framework\nfor weakly-supervised temporal forgery localization (MDP), which aims to\nidentify temporal partial forged segments using only video-level annotations.\nThe MDP proposes a novel multimodal interaction mechanism (MI) and an\nextensible deviation perceiving loss to perceive multimodal deviation, which\nachieves the refined start and end timestamps localization of forged segments.\nSpecifically, MI introduces a temporal property preserving cross-modal\nattention to measure the relevance between the visual and audio modalities in\nthe probabilistic embedding space. It could identify the inter-modality\ndeviation and construct comprehensive video features for temporal forgery\nlocalization. To explore further temporal deviation for weakly-supervised\nlearning, an extensible deviation perceiving loss has been proposed, aiming at\nenlarging the deviation of adjacent segments of the forged samples and reducing\nthat of genuine samples. Extensive experiments demonstrate the effectiveness of\nthe proposed framework and achieve comparable results to fully-supervised\napproaches in several evaluation metrics.",
      "published_date": "2025-07-22T13:55:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16596v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16596v2",
      "latex_url": "http://arxiv.org/src/2507.16596v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Generative artificial intelligence has rapidly advanced in recent years, utilizing existing Artificial Intelligence Generated Content (AIGC) technology could generate high-quality multimedia content such as image, audio, video, etc.\nDeepfake, as a specific application of AIGC technology, allows for manipulating multimedia content of actual people or generating fictional content.\nHowever, the misuse of Deepfake represents a substantial threat to individual privacy, copyright protection, and the overall stability of society.\n\n {figure}[t]\n  \n  [width= ]{Fig/introduction_figure.pdf}\n  {The schematic diagram of weakly-supervised temporal forgery localization task (WS-TFL).\n In the training phase, merely video-level fake (F) and true (T) annotations are utilized for loss calculation and model parameter updating.\n In the inference phase, for a given video, the timestamps of forged segments are predicted with the trained model.}\n\n {figure}\n\nCurrent research in Deepfake forensics primarily tackles the issue through classification tasks, particularly binary classification for videos or images .\nNevertheless, this methodology exhibits limitations when addressing more challenging deepfake scenarios, particularly in the context of temporal partial forgery localization.\nConsidering the specificity and potential pernicious effects of temporal partial forgery, Chugh ~ {chugh2020not} proposed the temporal forgery localization task (TFL) to localize the start and end timestamps of forged segments.\nSeveral researches have explored the TFL task with a fully-supervised methodology.\nBoth BA-TFD+ and AVTFD attempted to combine the TFL with frame-level Deepfake detection methods.\nUMMAFormer aimed to mine forgery traces through feature reconstruction.\nThe aforementioned fully-supervised temporal forgery localization (FS-TFL) methods have achieved some degree of localization performance.\nHowever, they require elaborate frame-level or timestamp annotations for fully-supervised learning, which is usually costly and time-consuming.\n\nTo cope with the dilemma of FS-TFL, weakly-supervised learning is introduced to TFL.\nThe schematic diagram of weakly-supervised temporal forgery localization (WS-TFL) is shown in Figure~.\nThe main challenges of WS-TFL are: 1) integrating multimodal information between visual and audio features,\nand 2) leveraging video-level annotations to mine subtle forgery traces for temporal partial forgery localization.\nThe weakly-supervised learning allows training on imprecise, partially accurate, or noisy annotations, enabling more refined inference tasks .\nThe existing weakly-supervised learning methods are mainly for computer vision tasks with strong semantic signals like temporal action localization and object detection , and focus primarily on the single visual modality.\nTherefore they are inappropriate for tracing subtle forgery traces in multimodal Deepfake scenarios .\n\nTo overcome these challenges, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP) in this paper, which aims to identify the timestamps of temporal partial forged segments using only video-level annotations.\nA novel multimodal interaction mechanism (MI) is introduced to analyze the dissimilarity or inter-modality deviation between visual and audio features.\nMI utilizes a temporal property preserving cross-modal attention to integrate multimodal information and constructs comprehensive video features for temporal forgery localization.\nBesides, we propose an extensible deviation perceiving loss to explore further temporal deviation for weakly-supervised learning, which explores further temporal deviation by measuring the degree of deviation between adjacent segments.\n\nSpecifically, the present framework consists of three modules: feature extraction, multimodal interaction, and temporal forgery localization.\nFeature extraction module first extracts visual and audio features of a given video using pre-trained models.\nThe visual and audio modalities are regarded as distinct encoding formats with relevance.\nThe multimodal interaction module transforms the visual and audio features into token space and aligns them in temporal and spatial dimensions.\nA temporal property preserving cross-modal attention is utilized to enhance the multimodal features, thereby generating comprehensive video features by concatenating all the visual and audio features.\nFinally, the temporal forgery localization module generates a temporal forgery activation sequence (FAS) based on the comprehensive video features.\nIn the training phase, the video-level prediction is obtained by summing the FAS for weakly-supervised learning.\nWhile in the inference phase, the start and end timestamps of the forged segments are obtained according to the FAS.\nMoreover, an extensible deviation perceiving loss is proposed to measure the degree of deviation between adjacent segments.\nThe MDP improves the localization precision by enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples.\nThe main contributions are summarized as follows:\n\n {itemize}\n   We propose a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization, which could identify the timestamps of temporal forged segments using only video-level annotations.\n   A temporal property preserving cross-modal attention is proposed, which is to perceive the inter-modality deviation between the visual and audio features and construct representative comprehensive video features.\n   An extensible deviation perceiving loss is proposed for weakly-supervised learning, which aims at enlarging the temporal deviation of forged samples while reducing that of genuine samples.\n   Extensive experiments have been conducted on two challenging datasets to demonstrate the effectiveness of the proposed framework, and MDP achieves comparable results to fully-supervised approaches in several evaluation metrics.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "camera-ready.tex",
      "rlhf_score": 0.358,
      "weak_supervision_score": 0.431,
      "diffusion_reasoning_score": 0.401,
      "distributed_training_score": 0.368,
      "datasets_score": 0.353,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Highly Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution is a framework for weakly-supervised temporal forgery localization, which relies on video-level annotations rather than precise frame-level labels. This directly aligns with weak supervision by using high-level, imprecise sources for training, enabling the model to learn forgery localization without costly hand-labeled data, as described in the abstract and introduction.",
      "diffusion_reasoning_justification": "The paper focuses on multimodal interaction and deviation perceiving for forgery localization, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. It does not adapt diffusion techniques for tasks like Chain-of-Thought correction, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces a multimodal deviation perceiving framework (MDP) for weakly-supervised temporal forgery localization in Deepfake videos, aiming to identify forged segments using only video-level annotations to overcome the limitations of fully-supervised methods. It proposes a multimodal interaction mechanism with temporal property preserving cross-modal attention to detect inter-modality deviations between visual and audio features, along with an extensible deviation perceiving loss to enhance localization by amplifying deviations in forged samples; extensive experiments on relevant datasets demonstrate that MDP achieves performance comparable to fully-supervised approaches in key metrics.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining cross-modal attention and a new deviation perceiving loss for weakly-supervised Deepfake localization, offering a clever adaptation of existing techniques to address a known problem more efficiently.",
      "impact_score": "Moderate",
      "impact_justification": "This work is likely to be cited and built upon in the subfield of Deepfake forensics due to its efficient use of weakly-supervised learning, potentially influencing research on multimodal analysis and practical applications in video authentication.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to computer vision and Deepfake detection with innovative methods that advance weakly-supervised techniques, making it essential for researchers in the field to be aware of its findings and approaches.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a6c516ae9f39f7c3502fb9eb9f81dfc6d1c56841",
      "h_index_fetch_method": "full_id",
      "total_authors": 5,
      "authors_found": 5,
      "highest_h_index": 4,
      "average_h_index": 2.2,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Wenbo Xu",
          "profile_url": "https://www.semanticscholar.org/author/2359401421",
          "h_index": 1
        },
        {
          "name": "Junyan Wu",
          "profile_url": "https://www.semanticscholar.org/author/2312669386",
          "h_index": 2
        },
        {
          "name": "Wei Lu",
          "profile_url": "https://www.semanticscholar.org/author/2262498478",
          "h_index": 4
        },
        {
          "name": "Xiangyang Luo",
          "profile_url": "https://www.semanticscholar.org/author/2290337001",
          "h_index": 3
        },
        {
          "name": "Qian Wang",
          "profile_url": "https://www.semanticscholar.org/author/2312624843",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668226",
      "updated_at": "2025-08-11T23:45:06.746752",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16608",
      "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian\n  Representation",
      "authors": [
        "Xueming Fu",
        "Pei Wu",
        "Yingtai Li",
        "Xin Luo",
        "Zihang Jiang",
        "Junhao Mei",
        "Jian Lu",
        "Gao-Jun Teng",
        "S. Kevin Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate analysis of cardiac motion is crucial for evaluating cardiac\nfunction. While dynamic cardiac magnetic resonance imaging (CMR) can capture\ndetailed tissue motion throughout the cardiac cycle, the fine-grained 4D\ncardiac motion tracking remains challenging due to the homogeneous nature of\nmyocardial tissue and the lack of distinctive features. Existing approaches can\nbe broadly categorized into image based and representation-based, each with its\nlimitations. Image-based methods, including both raditional and deep\nlearning-based registration approaches, either struggle with topological\nconsistency or rely heavily on extensive training data. Representation-based\nmethods, while promising, often suffer from loss of image-level details. To\naddress these limitations, we propose Dynamic 3D Gaussian Representation\n(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation\nwith implicit neural motion field modeling. Our method simultaneously optimizes\ncardiac structure and motion in a self-supervised manner, eliminating the need\nfor extensive training data or point-to-point correspondences. Through\ndifferentiable volumetric rendering, Dyna3DGR efficiently bridges continuous\nmotion representation with image-space alignment while preserving both\ntopological and temporal consistency. Comprehensive evaluations on the ACDC\ndataset demonstrate that our approach surpasses state-of-the-art deep\nlearning-based diffeomorphic registration methods in tracking accuracy. The\ncode will be available in https://github.com/windrise/Dyna3DGR.",
      "published_date": "2025-07-22T14:06:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16608v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16608v1",
      "latex_url": "http://arxiv.org/src/2507.16608v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Accurate estimation of myocardial motion is essential for evaluating cardiac function and diagnosing myocardial diseases.\nDynamic cardiac motion reconstruction provides comprehensive spatiotemporal information throughout cardiac cycle, enabling clinicians to better analyze physiological cardiac dynamics for improved diagnostic accuracy and treatment planning.\n\nTagged cardiac magnetic resonance imaging (t-CMR) serves as the gold standard for assessing myocardial motion using intrinsic markers. However, due to its complex acquisition process, recent research has increasingly focused on estimating motion from untagged CMR images.\n\nTechnically, cardiac motion estimation approaches can be broadly categorized into two main streams: image-based and representation-based.\n\nIn the image-based methods, researchers have developed various non-parametric registration methods that rely on mathematical priors and optimization techniques. These include incorporating free-form deformations with B-splines, optical flow, and biomechanics-informed approaches to achieve accurate correspondence mapping.\n\nWhile these methods have shown promise, they often struggle with preserving topological consistency during deformation.\nTo address this limitation,\ndiffeomorphic registration methods have introduced topology-preserving constraints.\nHowever, these non-parametric registration approaches remain computationally intensive and sensitive to image noise.\n\nThe advent of deep learning has revolutionized image-based cardiac motion estimation. Data-driven deep registration approaches have demonstrated superior performance in preserving topological consistency and maintaining long-term temporal coherence compared to traditional registration methods .\n\nHowever, their effectiveness is inherently constrained by the availability of extensive training data, and they often face challenges in generalizing across datasets with different distributions.\n\nWhile untagged CMR provides clear visualization of cardiac structures that can be precisely segmented, the inherent elasticity and homogeneous nature of myocardial tissue present significant challenges for accurate motion tracking in image space due to the lack of reliable natural landmarks within the tissue.\n\nTo alleviate this problem, another line of research explores cardiac motion estimation in alternative representation spaces. Guo {  et al.} have proposed an unsupervised approach to extract stable landmarks from volumetric images, using optimal transport theory with topological constraints for motion field estimation. However, this approach can be sensitive to noise and may not achieve sufficient precision. Meng {  et al.} have adopted fixed-vertex mesh representations with template topology ($ $20,000 vertices) as stable identifiers across different subjects and cardiac cycles, and estimate vertex motion from six different view sequences to reconstruct myocardial deformation. While effective, this approach may lose fine-grained image details. Yuan {  et al.} have explored using implicit neural representations through signed distance field to model the myocardium, enabling continuous shape representation. However, their approach focuses primarily on global shape deformation modeling, making it challenging to capture fine-grained local motion details.\n\nWhile these representation-based approaches show promise in breaking through the performance ceiling of image-based methods, there remains a critical need for a unified framework that can both accurately represent cardiac anatomy and seamlessly bridge the gap between representation space and image space.\n\nTo tackle the challenges, we propose {  Dyna}mic {  3D G}aussian {  R}epresentation ({  Dyna3DGR}), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our approach simultaneously optimizes cardiac structure and motion reconstruction in a self-supervised manner, eliminating the need for extensive training data or dense correspondences across cardiac cycles. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges the gap between 3D Gaussian representation and image-space. Our key contributions can be summarized as follows:\n {enumerate}\n  We propose a self-supervised optimization framework for 4D cardiac motion estimation that simultaneously optimizes cardiac structure and motion estimation, eliminating the dependency on extensive training data that is commonly required by existing image-space methods.\n  Through the unique integration of explicit 3D Gaussian representation and implicit neural deformation field modeling in Dyna3DGR, it effectively fills the gap between representation space and image space. This hybrid design not only preserves topological consistency but also achieves accurate motion tracking without requiring explicit dense correspondence, addressing the shortcomings of existing representation-based approaches.\n  Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art diffeomorphic registration methods in tracking accuracy, validating the effectiveness of our proposed framework.\n {enumerate}\n\n {table}[t]\n  \n  {Comparison of different methods.}\n\n  {tabular}{l|l|c|c}\n  \n Methods & Representation & Image Details & No Extra Data\n\n  \n Traditional registration & Pixel/Voxel &   &  \n\nDL-based registration & Pixel/Voxel &   & $ $\n\n Points Representation& Landmark &   & $ $\n\n Shape Representation& Mesh/SDF & $ $ & $ $\n\n Dyna3DGR (ours)& 3D Gaussian &   &  \n\n  \n  {tabular}\n {table}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "MICCAI2025_paper_template.tex",
      "rlhf_score": 0.288,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.378,
      "distributed_training_score": 0.357,
      "datasets_score": 0.295,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668236",
      "updated_at": "2025-08-11T23:43:05.606957",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16612",
      "title": "CTSL: Codebook-based Temporal-Spatial Learning for Accurate Non-Contrast\n  Cardiac Risk Prediction Using Cine MRIs",
      "authors": [
        "Haoyang Su",
        "Shaohao Rui",
        "Jinyi Xiang",
        "Lianming Wu",
        "Xiaosong Wang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accurate and contrast-free Major Adverse Cardiac Events (MACE) prediction\nfrom Cine MRI sequences remains a critical challenge. Existing methods\ntypically necessitate supervised learning based on human-refined masks in the\nventricular myocardium, which become impractical without contrast agents. We\nintroduce a self-supervised framework, namely Codebook-based Temporal-Spatial\nLearning (CTSL), that learns dynamic, spatiotemporal representations from raw\nCine data without requiring segmentation masks. CTSL decouples temporal and\nspatial features through a multi-view distillation strategy, where the teacher\nmodel processes multiple Cine views, and the student model learns from\nreduced-dimensional Cine-SA sequences. By leveraging codebook-based feature\nrepresentations and dynamic lesion self-detection through motion cues, CTSL\ncaptures intricate temporal dependencies and motion patterns. High-confidence\nMACE risk predictions are achieved through our model, providing a rapid,\nnon-invasive solution for cardiac risk assessment that outperforms traditional\ncontrast-dependent methods, thereby enabling timely and accessible heart\ndisease diagnosis in clinical settings.",
      "published_date": "2025-07-22T14:12:41+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16612v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16612v1",
      "latex_url": "http://arxiv.org/src/2507.16612v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The application of MACE in survival analysis within cardiology is of paramount importance, serving as a critical indicator of long-term cardiac health and treatment outcomes~. In this context, Cine cardiac MRI imaging is widely accessible, while its prognostic efficacy is significantly hindered by the inherent intricacy of myocardial tissue and the entanglement of its temporal and spatial dynamics~. Classical methods~, modeled through electronic health records (EHR) or radiomics, purely rely on manual interpretations of structural and functional abnormalities~, which are subject to inter-observer variability and often fail to capture subtle, yet crucial, prognostic features.\nThough the landscape of state-of-the-art survival models for 3D medical imaging is vast, limitations still persist. XSurv~, which utilizes multi-modal data such as PET and CT scans, struggles with the scarcity of paired samples and the challenges of data co-registration. AdaMSS~, which requires physician-driven lesion refinement, is both time-consuming and labor-intensive. Furthermore, models specialized in pathology~ are limited by their reliance on 2D imaging, resulting in poor generalization to high-dimensional images. As a result, while Cine imaging is a commonly available modality, its integration of multi-dimensional data, including multi-chamber dynamics from short-axis and longitudinal views of cardiac morphology over time, still remains a challenge in survival analysis.\n\nIn this work, we first present a self-supervised pre-training scheme, denoted as CTSL, which operates independently of heart masks or contrast imaging data. The framework mainly comprises two stages: motion-aware multi-view model distillation and spatiotemporal disentangling. Initially, we extend the classical distillation learning paradigm, DINOv2~, from a patient-level perspective, innovatively incorporating multi-view Cine sequences as input for the distillation, i.e., injecting the information from other views than short-axis (SA) images into the pre-trained model. In this stage, motion queries extracted through SA Cine sequences are treated as myocardium-oriented key tokens by the student network, which aligns with long-axis Cine tokens from the teacher network via Kullback-Leibler (KL) divergence~. Subsequently, drawing upon the latent space discretization techniques of VQVAE~, we extract query tokens from the preceding KL-aligned student model and design trainable temporal and spatial codebook embeddings, disentangling the spatiotemporal representations from the compressed 4D Cine data.\nFinally, a survival prediction framework is presented using the learned image tokens from CTSL and EHR features to perform MACE-based survival analysis.\n\nOur contributions in the proposed framework are threefold: 1) We demonstrate the feasibility of adopting contrast-free imaging techniques together with EHR for the MACE survival analysis. 2) We introduce a self-supervised framework, CTSL, that learns codebook-based spatiotemporal representations from raw Cine data via a motion-aware multi-view model distillation module and a spatiotemporal feature disentanglement module. 3) We evaluate the proposed survival analysis framework on three private datasets and demonstrate its superior performance compared to prior arts.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.341,
      "distributed_training_score": 0.319,
      "datasets_score": 0.301,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668244",
      "updated_at": "2025-08-11T23:43:05.606959",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16621",
      "title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System",
      "authors": [
        "Lorenzo Gentilini",
        "Pierpaolo Serio",
        "Valentina Donzella",
        "Lorenzo Pollini"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.",
      "published_date": "2025-07-22T14:15:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16621v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16621v1",
      "latex_url": "http://arxiv.org/src/2507.16621v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Perception represents the bridge between the environment and the autonomous vehicle's processing pipeline. The information from the vehicle's surroundings has to be collected taking into account the relative positioning of the sensors on the vehicle body. In fact, in the absence of accurate calibration, even minor misalignments can lead to erroneous obstacle localization or scene misinterpretation, thereby significantly compromising the reliability of subsequent decision-making processes. Nowadays, LiDAR and camera systems are attracting more interest due to the effective combination of vision and ranging sensors with high resolution.\n {figure}\n  \n  [width= ]{img/projected_cloud_points_0_0.jpg}\n  {Projection of the board centers found by a LiDAR (red cross) and a camera (green dot). Blue IDs are the ChArUCo marker detections from the camera. }\n\n {figure}\nThis choice aims to take the best of both worlds: the depth estimation accuracy of a LiDAR enriches the substantial amount of semantic information coming from the Camera. To this end, an effective extrinsic calibration would ensure a smooth overlap between the 3D projection of the 2D pixel information and the LiDAR point cloud. However, in this particular task, it should be taken into account the intrinsic diversity of the data, and a one-size-fits-all solution appears as a chimera. The challenge is further exacerbated by the variability in sensor's specifications and noise characteristics, all of which affect the robustness of calibration. In this context, we propose a novel system that utilizes a custom anchor board, designed to be detected by both LiDAR and camera. After detection, the calibration can be formulated as a global optimization problem that computes the transformation required to align every measurement using the detected board for each sensor combination. The presented system was tested using real-world data collected in a warehouse. In sum, the main contribution of this work lies in the design and implementation of a calibration framework tailored for multi-LiDAR multi-Camera sensor suites that leverages a custom-designed anchor board and formulates extrinsic calibration as a global optimization problem. By addressing the heterogeneity of sensor data and ensuring reliable cross-modal correspondence, the proposed method enhances the accuracy and robustness of multi-sensor integration in real-world scenarios. The paper is structured as follows: Section introduces the state-of-the-art of extrinsic calibration, Section presents a formal problem formulation, Section describes the proposed approach, and Section shows the experiment results.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "IEEE-conference-template-062824.tex",
      "rlhf_score": 0.282,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.275,
      "distributed_training_score": 0.311,
      "datasets_score": 0.285,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669085",
      "updated_at": "2025-08-11T23:43:05.607112",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16623",
      "title": "Automatic Fine-grained Segmentation-assisted Report Generation",
      "authors": [
        "Frederic Jonske",
        "Constantin Seibold",
        "Osman Alperen Koras",
        "Fin Bahnsen",
        "Marie Bauer",
        "Amin Dada",
        "Hamza Kalisch",
        "Anton Schily",
        "Jens Kleesiek"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.",
      "published_date": "2025-07-22T14:16:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16623v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16623v1",
      "latex_url": "http://arxiv.org/src/2507.16623v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[t]\n  \n  [width= ]{figures/v10_b_14p.png}\n  {The ASaRG architecture - Model elements of ASaRG are highlighted in green and different input modalities are highlighted in blue. Plus symbols denote concatenation operations. Italics in any component denote that the component is part of original LLaVA architecture.}\n\n {figure}\n\nIn recent years, multi-modal radiological report generation has made significant strides , both in terms of performance and supported modalities (e.g. ), with generated reports slowly approaching the realm of human performance and already being sometimes preferable to human reports . The widespread interest in this field of research For one, AI-driven report generation harbors immense potential for lightening the workload of radiologists in evaluating the image and creating the reports, as well as in explaining said report to patients without requiring the presence of clinicians. On the other hand, a strong report generation model can offer a potentially valuable second opinion in any case where a second opinion by another radiologist may not be readily available.\n\nHowever, ML models that interact meaningfully with clinicians or patients do not only require (near-)human performance, but also a level of explainability or explicit grounding capability before they can be trusted with any responsibility. While recent work has increasingly emphasized these aspects, such grounding capability often comes at the cost of complex, purpose-built architectures that need to reinvent the wheel in many respects.\n\nIn this work, we present ASaRG, Automatic Segmentation-assisted Report Generation. ASaRG proposes to tackle both the performance and grounding challenges by leveraging domain-specific visual features and fine-grained segmentation maps as additional inputs and extending the popular LLaVA architecture to utilize these new inputs. The segmentation masks provide the report generation model with local-level cues about anatomical and pathological details and enable the grounding of report sections in the related segmentation masks. The domain-specific visual features provide additional global-level information that is complementary to features from LLaVA's vision encoder. The additional inputs are provided by two specialist medical models; LVM-Med , which provides intermediate visual embeddings, and an extended version of the CXAS framework , which provides 212 full-size anatomical, pathological, and foreign objects segmentation maps. A lightweight addition to the original LLaVA projection layer aligns the additional modalities with the regular vision embeddings, both in terms of input size and embedding space layout, before concatenating all embeddings and feeding the entire sequence into the original LLaVA projector, greatly increasing overall performance.\n\nOur contributions are as follows: 1) We propose to enhance medical report generation with LLaVA by extending the LLM input with two additional modalities, intermediate features and extremely fine-grained segmentations created by specialized medical models. 2) We explore different strategies for optimally fusing these new modalities into the existing LLaVA architecture with minimal parameter overhead. 3) We evaluate our resulting method on MIMIC-CXR , where it significantly outperforms baseline LLaVA, despite freezing both the vision tower and LLM backbone of LLaVA compared to said baseline. ASaRG also beats competitive models that use smaller numbers of segmentation maps in Clinical Efficacy (CE) metrics. 4) With the explicit introduction of segmentation maps into the LLaVA model input, ASaRG also lays an easily extensible foundation for future research into grounded report generation. Our code will be made publicly available on publication.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.308,
      "datasets_score": 0.324,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is an extension of the LLaVA architecture for medical report generation by incorporating segmentation features and intermediate visual embeddings, aimed at improving performance and grounding in clinical contexts. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669095",
      "updated_at": "2025-08-11T23:43:05.607114",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16624",
      "title": "A2Mamba: Attention-augmented State Space Models for Visual Recognition",
      "authors": [
        "Meng Lou",
        "Yunxiang Fu",
        "Yizhou Yu"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Transformers and Mamba, initially invented for natural language processing,\nhave inspired backbone architectures for visual recognition. Recent studies\nintegrated Local Attention Transformers with Mamba to capture both local\ndetails and global contexts. Despite competitive performance, these methods are\nlimited to simple stacking of Transformer and Mamba layers without any\ninteraction mechanism between them. Thus, deep integration between Transformer\nand Mamba layers remains an open problem. We address this problem by proposing\nA2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a\nnew token mixer termed Multi-scale Attention-augmented State Space Model\n(MASS), where multi-scale attention maps are integrated into an\nattention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of\ncross-attention by spatially aggregating the SSM's hidden states using the\nmulti-scale attention maps, which enhances spatial dependencies pertaining to a\ntwo-dimensional space while improving the dynamic modeling capabilities of\nSSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and\nMamba-based architectures in visual recognition tasks. For instance, A2Mamba-L\nachieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic\nsegmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting\nhigher efficiency. In object detection and instance segmentation with Cascade\nMask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while\nhaving 40% less parameters. Code is publicly available at\nhttps://github.com/LMMMEng/A2Mamba.",
      "published_date": "2025-07-22T14:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16624v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16624v1",
      "latex_url": "http://arxiv.org/src/2507.16624v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Vision Transformers (ViTs)~ have become a de-facto choice for various vision tasks due to their ability to model long-range dependencies using multi-head self-attention (MHSA) . However, the quadratic complexity of MHSA leads to high computational costs, particularly in dense prediction tasks such as semantic segmentation and object detection, which require high-resolution inputs. To this end, subsequent efforts have proposed efficient attention mechanisms such as window attention~, spatial reduction attention~, and dilated attention~ to reduce computational complexity. Recently, since the Mamba architecture~ can model long-range dependencies with linear-time complexity, many efforts have been dedicated to developing Mamba-based architectures for visual recognition~. In contrast to spatial reduction attention and dilated attention that reduce sequence length via downsampling or shuffling, Mamba directly models long-range dependencies on the original sequence through state space models (SSMs). This architecture enables fine-grained information preservation during long-sequence processing, very promising for enabling vision models to achieve superior performance in dense prediction tasks~.\n \n {figure}[t]\n  \n  [width=0.475 ]{acc_plot.pdf}\n  {Performance comparisons between our A2Mamba and other representative backbone architectures on visual recognition tasks.}\n\n {figure}\nThe sequential scanning mechanism in SSMs naturally suits language modeling, where word order matters, while images exhibit complex 2D structures with non-sequential pixel dependencies. Hence, SSMs have difficulty to comprehensively understand the spatial structures of images. Although some efforts~ have leveraged alternative scanning strategies to partially overcome this limitation, the inherent causality caused by sequential scanning still compromises latent spatial dependencies to some extent. Consequently, Transformer-Mamba hybrid architectures have emerged as a promising direction for visual recognition. For instance, MambaVision~ constructs a vision backbone by stacking MHSA and SSM blocks in deeper stages, using MHSA to complement SSM. However, its performance still lags behind advanced ViTs~ on diverse vision tasks despite high efficiency. Recently, a generic Transformer-Mamba hybrid architecture, termed SegMAN Encoder~, employs a unified token mixer to combine sliding local attention~ and SS2D~, achieving competitive performance and a favorable tradeoff in comparison to leading ViTs. However, since these efforts represent early attempts to integrate Transformers and Mamba for vision tasks, attention- and SSM-based modules are simply stacked in their token mixers. There remains a lack of effective methods to achieve a deeper integration between Transformer and Mamba layers, thereby giving rise to a powerful vision backbone that can surpass leading ViTs in terms of both efficiency and performance.\n\n \n\nIn this work, we propose a novel hybrid token mixer, termed Multi-scale Attention-enhanced State Space Model (MASS), which takes advantage of the strengths of both self-attention and SSM. Specifically, we first introduce an adaptive multi-scale attention (AMA) mechanism, comprising two complementary pathways: (1) regular sliding local attention (SLA) that captures fine-grained spatial details; and (2) dilated sliding attention (DLA) that adaptively adjusts dilation rates to model long-range dependencies. The motivation behind this design is encouraging feature and context representation at multiple granularities. The attention matrices in this mechanism possess dynamic spatial dependencies at multiple scales. Second, to achieve a deeper integration between SSM and self-attention layers, the hidden states of the SSM interact with the aforementioned multi-scale attention matrices via a variant of cross-attention. This design aims to dynamically enhance two-dimensional spatial dependencies and alleviate causality introduced by sequential scanning, thereby improving the spatial perception and dynamic modeling capabilities of SSM. Overall, our MASS effectively encapsulates adaptive multi-scale representation and long-range dependency modeling into a hybrid token mixer.\n \nBy hierarchically stacking the MASS token mixer and a feedforward network (FFN) layer, we propose a versatile Transformer-Mamba hybrid vision backbone architecture termed A2Mamba. As shown in Fig.~, A2Mamba demonstrates remarkably better performance than advanced ConvNets, Transformers and Mamba-based architectures on diverse vision tasks. For instance, our A2Mamba-S model, with approximately 30M parameters only, achieves an impressive top-1 accuracy of 84.7%, surpassing RMT-S~ and TransNeXt-T~ by 0.6% and 0.7%, respectively, while having higher efficiency. Moreover, A2Mamba-S even outperforms hybrid MambaVision-B~ by 0.5% in top-1 accuracy with only about one-third of the computational complexity. A2Mamba consistently exhibits superior performance over other baselines in dense prediction tasks. For example, in the task of semantic segmentation with UperNet~, A2Mamba-B outperforms BiFormer-B and UniFormer-B~ by 2.3% and 3.3% in mIoU, respectively. Meanwhile, in the task of object detection and instance segmentation with Cascade Mask R-CNN~, A2Mamba-L leads CAFormer-M36 and MogaNet-L by 1.8%/1.6% and 2.3%/2.0% in AP$^b$/AP$^m$, respectively. These experimental results demonstrate that A2Mamba possesses stronger global modeling and local detail preservation capabilities.\n \nA preliminary version of this work has been published in CVPR 2025~. In the preliminary version, our contributions are summarized as follows.\n {enumerate}\n  We introduce a novel vision backbone architecture termed SegMAN Encoder featuring a hybrid LASS mixer. LASS synergistically combines Local Attention with State Space Models for both efficient local detail encoding and global context modeling.\n  We propose Mamba-based Multi-Scale Context Extraction (MMSCopE), a novel feature decoder specifically designed for semantic segmentation tasks. MMSCopE operates on multi-scale feature maps that adaptively scale with the input resolution, surpassing previous approaches in both fine-grained detail preservation and omni-scale context modeling.\n  A strong segmentation network architecture, SegMAN, is devised by integrating SegMAN Encoder and MMSCopE. Extensive experiments on semantic segmentation tasks demonstrate the superior performance and competitive efficiency of our method.\n {enumerate}\n \nIn this extended version, we aim to further unleash the potential of Transformer-Mamba hybrid architectures for visual recognition. Compared to our conference paper, this version presents substantial improvements in the following aspects.\n\n {enumerate}\n  We propose a new hybrid token mixer termed MASS, which can more deeply integrate self-attention and SSM, enabling strong multi-scale context modeling and long-range dependency modeling capabilities within a single mixer. Note that the MASS token mixer is a more powerful replacement of the LASS token mixer in the conference paper.\n\n  Building upon MASS, we propose a stronger vision backbone architecture termed A2Mamba, which encodes more discriminative feature representations for various visual recognition tasks. Furthermore, we leverage MASS to construct a new decoder for semantic segmentation, dubbed MASS-based multi-scale refinement (MM-Refine) module, which is combined with A2Mamba to form a new segmentation network architecture, SegMAN-V2.\n\n  We have conducted more extensive experimental validations of our architectures on a broader range of visual recognition tasks, including image classification under diverse resolutions and dense predictions including semantic segmentation, object detection, and instance segmentation. Extensive results demonstrate that our method outperforms all existing baselines while incurring lower computational costs.\n {enumerate}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.333,
      "weak_supervision_score": 0.356,
      "diffusion_reasoning_score": 0.416,
      "distributed_training_score": 0.373,
      "datasets_score": 0.321,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development of A2Mamba, a hybrid architecture combining attention mechanisms and state space models for visual recognition tasks such as image classification and semantic segmentation. It does not involve diffusion models, iterative refinement for logical tasks, or any form of multi-step reasoning processes, making it unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668252",
      "updated_at": "2025-08-11T23:43:05.606961",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16635",
      "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General\n  Industrial Assembly Lines Balancing Problems",
      "authors": [
        "Ali Mohamed Ali",
        "Luca Tirel",
        "Hashim A. Hashim"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach.",
      "published_date": "2025-07-22T14:34:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16635v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16635v1",
      "latex_url": "http://arxiv.org/src/2507.16635v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The growing complexity of activities in large, modern industries necessitates advanced planning systems for manufacturing and assembly lines to optimize key performance indicators while adhering to specific operational constraints. Assembly Line Balancing Problems (ALBPs) are well-established, challenging nonlinear programming problems characterized by context-specific formulations and significant computational demands . During the execution phase, unforeseen disruptions such as equipment breakdowns can derail planned schedules, and the time-intensive process of recalculating solutions may render real-time adjustments impractical, leading to project delays and substantial unplanned costs. Effective assembly line balancing directly influences productivity, throughput, and cost-efficiency, making it a critical factor for industries striving to maintain a competitive edge . In sectors with complex production processes, such as automotive and electronics manufacturing, ALBP plays a pivotal role in ensuring timely delivery, maintaining quality control, and responding efficiently to customer demands, thus driving profitability and operational excellence . ALBP can be conceptualized as a control problem centered on the efficient assignment of tasks and resources to satisfy constraints while optimizing key performance metrics . Artificial intelligence have been extensively applied to address complex engineering applications and This conceptualization allows ALBP to be addressed through reinforcement learning (RL) approaches, where agents learn to allocate tasks and resources on the assembly line to maximize an objective function. Recent studies have successfully applied Deep Reinforcement Learning (DRL) frameworks to various allocation and control problems across fields such as blockchain, energy demand management, humanoid robotics, and traffic signal control. These advancements highlight DRL as a promising approach for effectively addressing the challenges associated with assembly line balancing.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "arXiv_Factory.tex",
      "rlhf_score": 0.444,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.388,
      "datasets_score": 0.324,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves developing a Deep Reinforcement Learning (DRL) framework for optimizing industrial assembly lines using a multi-agent system, action-masking, and Markov Decision Processes. It focuses on agent training through simulations and environmental interactions, with no mention of human feedback, human-ranked data, or a reward model trained on human preferences. Therefore, it does not align with RLHF, which specifically requires human involvement in defining rewards or fine-tuning.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667835",
      "updated_at": "2025-08-11T23:43:05.606867",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16639",
      "title": "Benchmarking pig detection and tracking under diverse and challenging\n  conditions",
      "authors": [
        "Jonathan Henrich",
        "Christian Post",
        "Maximilian Zilke",
        "Parth Shiroya",
        "Emma Chanut",
        "Amir Mollazadeh Yamchi",
        "Ramin Yahyapour",
        "Thomas Kneib",
        "Imke Traulsen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "To ensure animal welfare and effective management in pig farming, monitoring\nindividual behavior is a crucial prerequisite. While monitoring tasks have\ntraditionally been carried out manually, advances in machine learning have made\nit possible to collect individualized information in an increasingly automated\nway. Central to these methods is the localization of animals across space\n(object detection) and time (multi-object tracking). Despite extensive research\nof these two tasks in pig farming, a systematic benchmarking study has not yet\nbeen conducted. In this work, we address this gap by curating two datasets:\nPigDetect for object detection and PigTrack for multi-object tracking. The\ndatasets are based on diverse image and video material from realistic barn\nconditions, and include challenging scenarios such as occlusions or bad\nvisibility. For object detection, we show that challenging training images\nimprove detection performance beyond what is achievable with randomly sampled\nimages alone. Comparing different approaches, we found that state-of-the-art\nmodels offer substantial improvements in detection quality over real-time\nalternatives. For multi-object tracking, we observed that SORT-based methods\nachieve superior detection performance compared to end-to-end trainable models.\nHowever, end-to-end models show better association performance, suggesting they\ncould become strong alternatives in the future. We also investigate\ncharacteristic failure cases of end-to-end models, providing guidance for\nfuture improvements. The detection and tracking models trained on our datasets\nperform well in unseen pens, suggesting good generalization capabilities. This\nhighlights the importance of high-quality training data. The datasets and\nresearch code are made publicly available to facilitate reproducibility, re-use\nand further development.",
      "published_date": "2025-07-22T14:36:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16639v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16639v1",
      "latex_url": "http://arxiv.org/src/2507.16639v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Monitoring individual behavior in pig farming is essential to ensure a high level of animal welfare and the efficient functioning of work processes. Traditionally, the acquisition of behavioral information has been time-consuming and laborious, as it relied on human observation and documentation. In the last decade, this has started to change due to the emergence of powerful machine learning methods that allow to automate this task. Automatic methods for individual identification and individualized action understanding have the potential to serve as digital assistance tools for farmers and researchers to monitor the health status , to detect potentially harmful behaviors like tail biting or mounting, or to observe normal behavior with the goal of detecting changes as early warning signs . At the core of such methods lies the localization of individual animals across space and time, commonly referred to as object detection and multi-object tracking in the technical literature. Since these tasks often are modular components in more sophisticated behavioral analysis pipelines , establishing strong, robust and readily available methods for pig detection and tracking is a crucial prerequisite for further developments. While detection and tracking methods based on fine-grained instance representations such as segmentation masks , or keypoints exist , most of the technical literature is based on axis-aligned bounding boxes. Similarly, state-of-the-art computer vision methods for individualized action understanding primarily rely on bounding boxes . This predominance can be attributed to the ease of annotation and the availability of increasingly powerful backbones that allow tracking- and action-related information to be extracted directly from images or videos without the need for complex intermediate representations. Although rotated bounding boxes can more accurately capture the extent of objects, they are incompatible with the vast majority of multi-object tracking and action understanding methods. As a result, it is difficult to benefit from future advances in these areas when using this representation. For these reasons, our work focuses on detection and tracking methods based on axis-aligned bounding boxes.\n\n {\nMany studies in recent years have addressed the problem of pig detection and tracking , often adapting standard methods to better suit the conditions of pig farming, and achieving promising results. Unfortunately, only a few authors made their detection and tracking datasets publicly available. These datasets are limited in terms diversity, often including only a single barn or pen environment, and have not become well established as benchmark datasets, as they are rarely used for comparisons across different works. Other works state that research data is available upon request. However, we did not receive responses to the inquiries we made, suggesting that access to such data may be limited in practice. Similarly, user-friendly code bases for pig detection and tracking are scarce, as research code is often not made publicly available within the livestock research community. Recent works on pig detection and tracking are beginning to acknowledge this gap by making both their code and datasets publicly available .\n}\n\nThe lack of publicly accessible code bases and datasets is, in our opinion, problematic for two reasons: (1) Current methods cannot be compared to determine the state of the art. Many studies on pig detection and tracking report evaluation metrics on undisclosed datasets, making direct comparisons between them impossible. In the broader field of computer vision, methods are usually benchmarked on standardized, publicly available datasets  [e.g.][]{deng2009imagenet, lin2014microsoft, gu2018ava}, which facilitates reproducibility and encourages fair performance comparisons. (2) The lack of accessible resources also hampers the development and the availability of robust pig detection and tracking methods which are urgently needed for downstream analysis tasks. Researchers that need to localize animals as part of their research often start by re-inventing the wheel. They annotate large amounts of data and adjust generic code bases for detection and tracking to train and hyperparameter-tune their own models without benefiting from the work of researchers that already tackled this task. In contrast, open data and open source are common practice in the broader field of computer vision, such that methods can be easily fine-tuned, extended or used as modular components in custom analysis pipelines.\n\nThe widespread availability of open-source code bases and high-quality labeled data has been a major driver of the advancements in computer vision in the last decade, and would greatly benefit precision livestock farming research as well.\n\n {\nIn this work, we aim to address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. Both datasets contain samples from diverse barn environments in pig farming that were annotated with bounding boxes. They include realistic conditions, such as pigs occluding each other, bad lighting or smudged camera lenses. For PigDetect, we specifically included challenging images in the dataset by identifying and correcting cases where a trained pig detection model commits errors.\n}\n\n {\nWe benchmarked the performance of several recent general-purpose detection and tracking models on our datasets. In addition to models that are commonly used in pig farming, such as YOLO variants for object detection or SORT-based methods for multi-object tracking, we also evaluated models that have not yet been employed in this context. Specifically, we included Co-DINO , a detection model that achieves state-of-the-art results on the COCO dataset , and two recent end-to-end trainable models for multi-object tracking that achieve state-of-the-art results on the DanceTrack dataset . Our comprehensive benchmarking study enables a systematic comparison between methods and highlights strengths and weaknesses of different model classes. For the end-to-end models we also conducted a detailed error analysis, providing guidance for future improvements.\n}\n\n {\nWe further evaluated all tracking models on a third-party test video from a previously unseen pen environment  {yu2025fto}, achieving a substantial improvement over the results reported by the authors. This highlights the importance of carefully selected, high-quality training data for the development of robust and generalizable models. PigDetect ( {https://doi.org/10.25625/I6UYE9}) and PigTrack ( {https://doi.org/10.25625/P7VQTP}), along with the trained model weights and the source code for training, evaluation and inference ( {https://github.com/jonaden94/PigBench}), are made publicly available to facilitate reproducibility, re-use and further development. In summary, our study has three main contributions:\n}\n\n {itemize}\n  A diverse and challenging benchmark dataset for both pig detection and tracking.\n  A comparison of the performance of several state-of-the-art general-purpose detection and tracking models on our datasets.\n  Carefully documented source code for training, evaluation and inference of the models employed in this work.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "manuscript.tex",
      "rlhf_score": 0.32,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.295,
      "distributed_training_score": 0.358,
      "datasets_score": 0.42,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and curating two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, specifically designed for machine learning applications in pig farming. It details dataset curation methodologies, such as including diverse and challenging scenarios (e.g., occlusions and poor visibility) and improving data quality by addressing model errors. The paper also conducts benchmarking and evaluation of models on these datasets, analyzes performance metrics, and highlights the importance of high-quality training data for generalization. This directly aligns with the topic's emphasis on dataset creation, curation, benchmarking, and analysis for AI and ML.",
      "summary": "This paper addresses the gap in standardized benchmarks for pig detection and tracking in farming by introducing two new datasets, PigDetect for object detection and PigTrack for multi-object tracking, which include diverse and challenging scenarios such as occlusions and poor visibility. The authors benchmark several state-of-the-art models, including YOLO variants, SORT-based methods, Co-DINO, and end-to-end trainable models, finding that challenging training images enhance detection performance, state-of-the-art models outperform real-time alternatives, SORT-based methods excel in detection while end-to-end models are better at association, and models generalize well to unseen environments; they also make the datasets and code publicly available to promote reproducibility and further research.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by curating new, diverse datasets and benchmarking existing models for pig detection and tracking, which addresses a known gap in the field but does not introduce a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon within precision livestock farming and computer vision subfields due to the provision of open datasets and benchmarks, potentially improving automated monitoring tools for animal welfare.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution by providing essential resources for researchers in animal monitoring, making it important for those in computer vision applied to agriculture, though not essential for the broader field.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ec5f8c52705962d78c658476d77a20adbd3026c8",
      "h_index_fetch_method": "full_id",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 35,
      "average_h_index": 7.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Jonathan Henrich",
          "profile_url": "https://www.semanticscholar.org/author/2242896027",
          "h_index": 2
        },
        {
          "name": "Christian Post",
          "profile_url": "https://www.semanticscholar.org/author/2372607422",
          "h_index": 0
        },
        {
          "name": "Maximilian Zilke",
          "profile_url": "https://www.semanticscholar.org/author/2373086837",
          "h_index": 0
        },
        {
          "name": "Parth Shiroya",
          "profile_url": "https://www.semanticscholar.org/author/2373082284",
          "h_index": 0
        },
        {
          "name": "Emma Chanut",
          "profile_url": "https://www.semanticscholar.org/author/2166165624",
          "h_index": 1
        },
        {
          "name": "Amir Mollazadeh Yamchi",
          "profile_url": "https://www.semanticscholar.org/author/2373081930",
          "h_index": 0
        },
        {
          "name": "R. Yahyapour",
          "profile_url": "https://www.semanticscholar.org/author/1874456",
          "h_index": 35
        },
        {
          "name": "Thomas Kneib",
          "profile_url": "https://www.semanticscholar.org/author/2242520637",
          "h_index": 1
        },
        {
          "name": "I. Traulsen",
          "profile_url": "https://www.semanticscholar.org/author/34844838",
          "h_index": 24
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668261",
      "updated_at": "2025-08-11T23:45:09.080805",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16641",
      "title": "Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum\n  Circuit Synthesis",
      "authors": [
        "Sara Giordano",
        "Kornikar Sen",
        "Miguel A. Martin-Delgado"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "A reinforcement learning (RL) framework is introduced for the efficient\nsynthesis of quantum circuits that generate specified target quantum states\nfrom a fixed initial state, addressing a central challenge in both the NISQ era\nand future fault-tolerant quantum computing. The approach utilizes tabular\nQ-learning, based on action sequences, within a discretized quantum state\nspace, to effectively manage the exponential growth of the space dimension. The\nframework introduces a hybrid reward mechanism, combining a static,\ndomain-informed reward that guides the agent toward the target state with\ncustomizable dynamic penalties that discourage inefficient circuit structures\nsuch as gate congestion and redundant state revisits. By leveraging sparse\nmatrix representations and state-space discretization, the method enables\nscalable navigation of high-dimensional environments while minimizing\ncomputational overhead. Benchmarking on graph-state preparation tasks for up to\nseven qubits, we demonstrate that the algorithm consistently discovers\nminimal-depth circuits with optimized gate counts. Moreover, extending the\nframework to a universal gate set for arbitrary quantum states, it still\nproduces minimal depth circuits, highlighting the algorithm's robustness and\nadaptability. The results confirm that this RL-driven approach efficiently\nexplores the complex quantum state space and synthesizes near-optimal quantum\ncircuits, providing a resource-efficient foundation for quantum circuit\noptimization.",
      "published_date": "2025-07-22T14:39:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16641v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16641v1",
      "latex_url": "http://arxiv.org/src/2507.16641v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The design of optimized quantum circuits is a crucial task in the current NISQ era and for the future of fault-tolerant quantum computing. Various methods based on combinatorial optimization and mathematically rigorous techniques have been employed for the interest in quantum computing over the past decades. Alongside these standard approaches, the use of Artificial Intelligence (AI)--in particular, Reinforcement Learning (RL) techniques --has gained importance in the field, contributing to the generation of new circuits with reduced depth, fewer gates, or improved geometric structures tailored to current hardware constraints .\n\nIn this work, we address the problem of synthesizing quantum circuits that generate a target quantum state from a fixed initial state by means of reinforcement learning. Specifically, we propose a tabular Q-learning framework in which the agent learns action sequences over a discretized representation of quantum states. The learning process is guided by a hybrid reward mechanism: we use a sparse static reward precomputed offline, guiding the agent to the target state, and dynamic penalties calculated during the learning process. These penalties encode other circuit constraints aiming to optimize simultaneously more features of the final circuit. These components allow the agent to progressively discover efficient paths reaching the target state while avoiding circuit structures that would increase the overall depth.\n\nAlthough the quantum state space grows doubly exponentially with the number of qubits and the action space scales polynomially, our use of discretization and sparse matrix representations keeps the problem tractable under certain assumptions. A Q-learning setup, with a fixed initial state and finite action sequences ensures efficient exploration despite the vastness of the state space . Moreover, we introduce multiple strata of static reward around the target state, in order to reduce its sparsity, and thus improving the algorithm performances. To limit offline computation, we restrict our benchmarks to systems of up to $7$ qubits.\n\nThe paper is organized as follows. In Sec.~, we review the relevant background on quantum state representations, gate sets, and circuit metrics. Sec.~ presents the Q-learning algorithm in detail, including the reward design and exploration strategy. Sec.~ outlines the implementation aspects while Subsection~ focuses on graph-state preparation tasks, and Subsection~ extends the framework to more general target states. Finally, conclusions and future directions are discussed in Sec.~.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "Draft_Final_Version.tex",
      "rlhf_score": 0.43,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.353,
      "datasets_score": 0.274,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a tabular Q-learning framework for quantum circuit synthesis, using a hybrid reward mechanism based on static, domain-informed rewards and dynamic penalties derived from computational metrics like gate congestion. It does not involve human feedback, such as training a reward model on human-ranked data or aligning an AI model with human preferences, which are core to RLHF. Thus, the paper's contributions are unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669417",
      "updated_at": "2025-08-11T23:43:05.607147",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16642",
      "title": "Towards Automated Regulatory Compliance Verification in Financial\n  Auditing with Large Language Models",
      "authors": [
        "Armin Berger",
        "Lars Hillebrand",
        "David Leonhard",
        "Tobias Deußer",
        "Thiago Bell Felix de Oliveira",
        "Tim Dilmaghani",
        "Mohamed Khaled",
        "Bernd Kliem",
        "Rüdiger Loitz",
        "Christian Bauckhage",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The auditing of financial documents, historically a labor-intensive process,\nstands on the precipice of transformation. AI-driven solutions have made\ninroads into streamlining this process by recommending pertinent text passages\nfrom financial reports to align with the legal requirements of accounting\nstandards. However, a glaring limitation remains: these systems commonly fall\nshort in verifying if the recommended excerpts indeed comply with the specific\nlegal mandates. Hence, in this paper, we probe the efficiency of publicly\navailable Large Language Models (LLMs) in the realm of regulatory compliance\nacross different model configurations. We place particular emphasis on\ncomparing cutting-edge open-source LLMs, such as Llama-2, with their\nproprietary counterparts like OpenAI's GPT models. This comparative analysis\nleverages two custom datasets provided by our partner PricewaterhouseCoopers\n(PwC) Germany. We find that the open-source Llama-2 70 billion model\ndemonstrates outstanding performance in detecting non-compliance or true\nnegative occurrences, beating all their proprietary counterparts. Nevertheless,\nproprietary models such as GPT-4 perform the best in a broad variety of\nscenarios, particularly in non-English contexts.",
      "published_date": "2025-07-22T14:39:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16642v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16642v1",
      "latex_url": "http://arxiv.org/src/2507.16642v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Corporate financial disclosures in the form of financial statements provide critical insights into a firm's economic health and future trajectory.\nThese documents provide the public with detailed information on the financial stability, productivity, and profitability of a company, thus having a major influence on investment decisions made by external investors. Financial statements are documents that contain financial information of organizations such as assets, liabilities, and revenues. These documents are examined annually to check conformity with the relevant financial reporting framework, such as the International Financial Reports Standards (IFRS) and Germany's Handelsgesetzbuch (HGB). The examination process requires a lot of expert knowledge and manual analysis of lengthy financial texts. It includes tasks such as verifying the completeness, accuracy, valuation, consistency, classification, and readability of the reported information. The intricate nature of the IFRS and similar accounting standards exacerbate this challenge. Typically structured as an exhaustive list of checklist items, auditors are tasked with the responsibility of comparing relevant text passages from the financial document with each specific regulatory mandate. This necessitates the careful identification and correlation of text segments in the financial disclosure to the myriad stipulations in the accounting standard. With the advent of advanced Large Language Models (LLMs) like GPT-3.5-Turbo and GPT-4 showcasing impressive reasoning and text comprehension skills on various downstream tasks, we seek to explore their role in reshaping the auditing paradigm.\n\nThis paper builds upon our prior introduction of the Automated List Inspection (ALI) and the ZeroShotALI system to streamline the mapping between legal requirements and financial report segments. ZeroShotALI is a novel recommender system that leverages a state-of-the-art LLM in conjunction with a domain-specifically optimized transformer-based text-matching solution.\n\nIn this paper, we extend the capabilities of our current systems by investigating the potential of ``out-of-the-box\" Language Models in evaluating the compliance of a legal requirement with a specified number of pertinent text passages extracted from financial documents. Our primary objectives encompass two key aspects: firstly, to evaluate the performance of open-source models in comparison to prominent proprietary models like GPT-4; and secondly, to analyze the impact of framing the problem through the utilization of prompts.\n\nOur motivation for exploring open-source models primarily stems from considerations related to cost-effectiveness and data privacy, both of which are pivotal concerns in the contemporary landscape of accounting and machine learning research.\n\nIn the following, we first review related work, before describing our modeling approach in Section~. In Section~, we outline our datasets, present our experiments, and discuss the results. Section~ then draws a conclusion and provides an outlook into conceivable future work.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.425,
      "weak_supervision_score": 0.367,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.338,
      "datasets_score": 0.348,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is evaluating and comparing large language models for automated regulatory compliance verification in financial auditing, using pre-trained models like Llama-2 and GPT series. It does not involve, discuss, or apply Reinforcement Learning from Human Feedback (RLHF), such as training models with human-ranked data or using reinforcement learning for alignment. The focus is on model performance in compliance tasks, not on model training methods.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669429",
      "updated_at": "2025-08-11T23:43:05.607148",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16657",
      "title": "Synthetic Data Matters: Re-training with Geo-typical Synthetic Labels\n  for Building Detection",
      "authors": [
        "Shuang Song",
        "Yang Tang",
        "Rongjun Qin"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Deep learning has significantly advanced building segmentation in remote\nsensing, yet models struggle to generalize on data of diverse geographic\nregions due to variations in city layouts and the distribution of building\ntypes, sizes and locations. However, the amount of time-consuming annotated\ndata for capturing worldwide diversity may never catch up with the demands of\nincreasingly data-hungry models. Thus, we propose a novel approach: re-training\nmodels at test time using synthetic data tailored to the target region's city\nlayout. This method generates geo-typical synthetic data that closely\nreplicates the urban structure of a target area by leveraging geospatial data\nsuch as street network from OpenStreetMap. Using procedural modeling and\nphysics-based rendering, very high-resolution synthetic images are created,\nincorporating domain randomization in building shapes, materials, and\nenvironmental illumination. This enables the generation of virtually unlimited\ntraining samples that maintain the essential characteristics of the target\nenvironment. To overcome synthetic-to-real domain gaps, our approach integrates\ngeo-typical data into an adversarial domain adaptation framework for building\nsegmentation. Experiments demonstrate significant performance enhancements,\nwith median improvements of up to 12%, depending on the domain gap. This\nscalable and cost-effective method blends partial geographic knowledge with\nsynthetic imagery, providing a promising solution to the \"model collapse\" issue\nin purely synthetic datasets. It offers a practical pathway to improving\ngeneralization in remote sensing building segmentation without extensive\nreal-world annotations.",
      "published_date": "2025-07-22T14:53:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16657v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16657v1",
      "latex_url": "http://arxiv.org/src/2507.16657v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{A}{ccurate} global building segmentation from very high-resolution (VHR) satellite imagery is essential for applications like urban planning, disaster management, humanitarian assistance, and population estimation. However, this task is challenging due to the complex and diverse urban patterns across different geographical regions. Existing public datasets such as INRIA Aerial Image Labeling Dataset , Defence Science and Technology Laboratory (DSTL) Dataset , and SpaceNet provide large-scale datasets, but the coverage remains limited relative to global diversity. This limitation restricts model generalization across different regions, as variations in building types, urban layouts , and green spaces introduce significant challenges.\n\nGeneralization issues are evident when deep learning models trained on one dataset are applied to another with distinct characteristics. These challenges include performance degradation caused by differences in urban and rural distributions , variations in city layouts, building densities, architectural styles , and domain gaps between datasets that significantly impact accuracy . For instance, as shown in Figure , models trained on the Columbus dataset perform poorly on the DSTL dataset, and vice versa, due to differences in building sizes, textures, layouts, and distributions.\n\n {figure}[!t]\n  \n  [width= ]{figures/problem.png}\n  {\n\n Cross-domain performance of building segmentation in very high-resolution satellite images. The top row shows results for Columbus, and the bottom row for the DSTL dataset . Columns show the original image, predictions from same-domain and cross-domain models, followed by ground truth.}\n\n {figure}\n\nCurrent approaches to addressing domain gaps include data augmentation (DAug) , domain adaptation (DA) , and few-shot learning. DAug applies transformations to enhance robustness but only modifies existing data, adding bias without introducing new variance. DA aligns source and target domain distributions using target data but often increases bias without fully addressing domain-specific diversity. Few-shot learning incorporates limited target samples to support DA but is constrained by data availability. Recent works combine these approaches with synthetic datasets to expand training data. However, models trained on data from a single city with fixed layout patterns, such as those generated by systems like MatrixCity , still face significant generalization challenges.\n\nThis paper presents a scalable and cost-effective solution to address the challenges in building segmentation. Leveraging open-source data sources such as global road networks and building footprints from OpenStreetMap , combined with physics-based rendering , we generate geo-typical synthetic VHR satellite images and annotations. These synthetic datasets are specifically tailored to replicate the diversity and complexity of real-world urban layouts, enabling effective and targeted segmentation in regions with limited or incomplete data availability.\n {itemize}\n   We propose a novel approach for generating synthetic data that replicates the geo-typical and architectural characteristics of target regions.\n\n   We introduce a moderated domain randomization method to enhance the appearance of the synthetic data without introducing extra domain gaps by blind randomization.\n\n   We present a comprehensive building segmentation framework that utilizes the adversarial domain adaption (ADA) to combine geo-typical synthetic data and high-quality annotations from other domains.\n\n   Experiments highlight the utility and effectiveness of the proposed approach, achieving significant improvements in building segmentation performance, with gains of up to 12% and, in certain cases, up to 29%.\n {itemize}\n\nThe remainder of this paper is organized as follows. Section reviews related works on synthetic datasets, DA for semantic segmentation. Section describes our methodology for the generation of synthetic data. Section presents performance evaluations and ablation study, and Section concludes the paper.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.348,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.391,
      "distributed_training_score": 0.419,
      "datasets_score": 0.454,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper generates synthetic labels programmatically from geospatial data sources like OpenStreetMap, which aligns with weak supervision by reducing reliance on hand-labeled data. However, its primary focus is on synthetic data generation and domain adaptation for building segmentation, rather than emphasizing noisy or imprecise label strategies as the core contribution.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not discuss distributed training, parallel computing, or multi-node strategies for accelerating model training. Its contributions center on synthetic data generation and domain adaptation, with no mention of partitioning data or computation across processors.",
      "datasets_justification": "The paper's main contribution involves creating and evaluating geo-typical synthetic datasets for building segmentation, using geospatial data and rendering techniques to address limitations in existing datasets. This directly aligns with research on dataset creation, curation, and benchmarking for AI applications in remote sensing.",
      "summary": "This paper addresses the challenges of generalizing deep learning models for building segmentation in remote sensing across diverse geographic regions by proposing a method to re-train models using geo-typical synthetic data. The approach leverages geospatial data from sources like OpenStreetMap to generate high-resolution synthetic images via procedural modeling and physics-based rendering, incorporating domain randomization and integrating with an adversarial domain adaptation framework to minimize synthetic-to-real gaps, resulting in significant performance improvements of up to 12% median and 29% in some cases.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing techniques like synthetic data generation and domain adaptation in a new way tailored to specific geographic regions, advancing generalization in building segmentation without introducing entirely novel concepts. This clever integration addresses a known problem effectively but does not constitute a groundbreaking new architecture or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of remote sensing and computer vision, as it provides a practical, cost-effective solution for improving model generalization with synthetic data. However, its influence may be limited to specific applications rather than broadly transforming the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative approach to a relevant problem in building segmentation, making it valuable for researchers in computer vision and remote sensing to understand and potentially apply. While not essential for all, it represents a strong contribution worth considering for those working on domain adaptation and synthetic data.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/223ece59b708bb307aa0009ceaacc5f55c848e46",
      "h_index_fetch_method": "full_id",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 7,
      "average_h_index": 5.333333333333333,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Shuang Song",
          "profile_url": "https://www.semanticscholar.org/author/2011701518",
          "h_index": 7
        },
        {
          "name": "Yang Tang",
          "profile_url": "https://www.semanticscholar.org/author/2161723417",
          "h_index": 4
        },
        {
          "name": "Rongjun Qin",
          "profile_url": "https://www.semanticscholar.org/author/2258713144",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668270",
      "updated_at": "2025-08-11T23:45:11.221006",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16663",
      "title": "Self-Contradiction as Self-Improvement: Mitigating the\n  Generation-Understanding Gap in MLLMs",
      "authors": [
        "Yujin Han",
        "Hao Chen",
        "Andi Han",
        "Zhiheng Wang",
        "Xinyu Lin",
        "Yingya Zhang",
        "Shiwei Zhang",
        "Difan Zou"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Despite efforts to unify multimodal generation and understanding tasks in a\nsingle model, we show these MLLMs exhibit self-contradiction where generation\nproduces images deemed misaligned with input prompts based on the model's own\nunderstanding. We define a Nonunified score that quantifies such\nself-contradiction. Our empirical results reveal that the self-contradiction\nmainly arises from weak generation that fails to align with prompts, rather\nthan misunderstanding. This capability asymmetry indicates the potential of\nleveraging self-contradiction for self-improvement, where the stronger model\nunderstanding guides the weaker generation to mitigate the\ngeneration-understanding gap. Applying standard post-training methods (e.g.,\nSFT, DPO) with such internal supervision successfully improves both generation\nand unification. We discover a co-improvement effect on both generation and\nunderstanding when only fine-tuning the generation branch, a phenomenon known\nin pre-training but underexplored in post-training. Our analysis shows\nimprovements stem from better detection of false positives that are previously\nincorrectly identified as prompt-aligned. Theoretically, we show the aligned\ntraining dynamics between generation and understanding allow reduced\nprompt-misaligned generations to also improve mismatch detection in the\nunderstanding branch. Additionally, the framework reveals a potential risk of\nco-degradation under poor supervision-an overlooked phenomenon that is\nempirically validated in our experiments. Notably, we find intrinsic metrics\nlike Nonunified score cannot distinguish co-degradation from co-improvement,\nwhich highlights the necessity of data quality check. Finally, we propose a\ncurriculum-based strategy based on our findings that gradually introduces\nharder samples as the model improves, leading to better unification and\nimproved MLLM generation and understanding.",
      "published_date": "2025-07-22T14:56:39+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16663v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16663v1",
      "latex_url": "http://arxiv.org/src/2507.16663v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Recently, multimodal large language models (MLLMs) have made significant progress in both image understanding and generation tasks . Understanding tasks often rely on high-level semantic from image data, while generation tasks require more pixel-level details for accurate reconstruction . Despite the difference in information granularity, unified pre-training approaches, such as shared tokenizers and autoregressive model architectures, have been actively explored .\n {figure}\n\n  \n  [width=0.95 , height=0.35 ]{png/self-contradictory.pdf}\n\n {-0.05in}\n  {Self-contradiction in MLLMs. We examine two challenging generation cases involving implicit physical principles using ChatGPT o3 and Gemini 2.5 Flash . We find that a discrepancy exists within MLLMs: images produced by the generation branch are identified as incorrect by their own understanding branch, showing a lack of MLLM unification.}\n {-0.2in}\n\n {figure}\n\nWe begin by highlighting that unified MLLMs are not truly unified in behavior, as evidenced by the phenomenon of self-contradiction, i.e., a mismatch between the generation and understanding branches. Unlike prior work compares generation and understanding separately on disjoint-modal data or focuses on single-task performance , self-contradiction emphasizes internal inconsistency: as shown in  {fig: self-con}, a model’s generated image may be deemed misaligned with the prompt by its own understanding branch. This phenomenon not only indicates imperfect unification between generation and understanding, but also reveals an asymmetry in their capabilities. Such asymmetry raises a question: Can stronger branch guide the weaker to achieve self-improvement of MLLMs without any external rewards?\n\n {wrapfigure}{r}{0.45 }\n\n {center}\n  [width=0.43 ]{png/nonunified_score_janus_partial_stacked.pdf}\n {center}\n {-0.1in}\n {Janus-Pro-7B exhibits notable generation-understanding contradiction, with Nonunified score ranging from 10% to 40%. Over 85% of these cases are attributable to weak generation, where understanding branch correctly rejects prompt-misaligned outputs.}\n {-0.0in}\n\n {wrapfigure}\nTo address the above question, we first quantify self-contradiction and provide a systematic analysis of its origins. Specifically, we introduce the Nonunified score (detailed in  {eq:nonunified_score}), defined as the proportion of cases where the understanding branch judges the generated outputs as prompt-misaligned. Ideally, a fully unified MLLM would have a Nonunified score of zero. However, as shown in  {fig:nonunified_score_janus}, models such as Janus-Pro-7B~ exhibit clear self-contradiction with nonzero Nonunified scores, i.e., the nonunified scores range from $10%$ to $40%$ across different concepts. To further analyze these contradictions, we leverage the strong external model Qwen2.5-VL-72B-Instruct~ to assess the accuracy of the understanding branch's evaluations of generated outputs.  {fig:nonunified_score_janus} indicates that most misalignments stem from weak generation rather than misunderstanding, with incorrect rejections of correct generations by the understanding branch accounting for less than 15%.\n\nThese findings indicate that promoting MLLM unification can be achieved by having the stronger understanding branch guide the weaker generation branch, enabling a fully self-improvement method without external supervision. We first explore classical post-training techniques such as supervised fine-tuning (SFT)~ and direct preference optimization (DPO)~, treating the understanding branch as an internal reward model to construct post-training data for the generation branch. Experiments on Janus-Pro-7B validate the effectiveness of this approach, yielding significant improvements in both generation and unification of MLLMs.\n\nAlongside improvements in both generation and unification, we also observe a co-improvement effect between the generation and understanding branches: during post-training that targets generation, not only does the generation branch improve, but the understanding branch is also enhanced. While co-improvement between generation and understanding has been widely observed during pre-training~, it remains underexplored in the post-training stage~. To understand this phenomenon, we analyze the improvement in understanding primarily comes from the model’s enhanced ability to recognize false-positive samples, namely generations that were incorrectly considered prompt-aligned. By extending the learning dynamics framework~ to the multimodal setting, we formalize how generation and understanding behaviors evolve for each image–prompt pair $( {x},  {y})$ during self-improvement and reveal both branches tend to follow consistent learning trajectories. This helps explain the improvement in understanding, as shared training dynamics allow generation-side optimization to reduce the probability of producing prompt-misaligned outputs also enhance the understanding branch’s ability to recognize such misalignments.\n\nInterestingly, our learning dynamics analysis also indicates the potential for co-degradation, especially when low-quality post-training data, such as corrupted images misaligned with prompts, are introduced. We confirm this risk on Janus-Pro-1B~: fine-tuning with completely corrupted SFT labels results in a synchronized decline in both branches. Notably, intrinsic MLLM metrics like the Nonunified score fail to distinguish co-degradation from co-improvement, as both scenarios manifest as improved unification. These findings highlight the necessity of pre-validating data quality, e.g., using external models, prior to initiating self-improvement.\n\nFinally, we explore whether the synchronized mechanism between generation and understanding can further enhance self-improvement in MLLMs. The observed co-improvement effect motivates us to introduce curriculum learning~ in post-training by gradually incorporating harder samples that were initially excluded due to limited generation or understanding abilities. This online strategy leverages the evolving capabilities of both branches to progressively expand the training data based on sample difficulty and further improve both the performance and unification of MLLMs.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "neurips_2024.tex",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.414,
      "diffusion_reasoning_score": 0.474,
      "distributed_training_score": 0.369,
      "datasets_score": 0.305,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses Direct Preference Optimization (DPO) with an internal reward model from the understanding branch, which is conceptually related to RLHF techniques for model alignment. However, it does not involve human feedback or a separate reward model trained on human-ranked data, relying instead on self-supervision, making it only indirectly connected to RLHF.",
      "weak_supervision_justification": "The paper leverages the model's understanding branch to programmatically generate labels or feedback for improving the generation branch, which aligns with weak supervision by using noisy, internal sources rather than precise hand-labeled data. This approach mitigates the generation-understanding gap but is not the primary focus, hence moderately relevant.",
      "diffusion_reasoning_justification": "The paper focuses on self-contradiction in MLLMs for image generation and understanding, using methods like SFT and DPO, but does not involve diffusion models, iterative refinement for logical reasoning, or multi-step chain-of-thought processes as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper investigates self-contradiction in Multimodal Large Language Models (MLLMs), where generated images are judged as misaligned with input prompts by the model's own understanding branch, and introduces the Nonunified score to quantify this gap. The authors demonstrate that this issue primarily stems from weak generation rather than misunderstanding, propose using internal supervision through methods like supervised fine-tuning (SFT) and direct preference optimization (DPO) to guide improvement, and reveal a co-improvement effect between generation and understanding branches, while also addressing risks of co-degradation and suggesting a curriculum-based strategy for enhanced unification.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by applying existing techniques like SFT and DPO in a novel way to address internal self-contradiction in MLLMs, introducing a new metric for quantifying the generation-understanding gap.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of MLLM training, as it provides practical methods for improving model unification and highlights important risks like co-degradation.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers high-quality insights into enhancing MLLMs through self-improvement techniques, making it a valuable contribution for researchers focused on multimodal AI.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/2856d2a1b3d8d652ac9a8ed8c66c90c1edd4fc4c",
      "h_index_fetch_method": "full_id",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 11,
      "average_h_index": 2.375,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Yujin Han",
          "profile_url": "https://www.semanticscholar.org/author/2297887026",
          "h_index": 4
        },
        {
          "name": "Hao Chen",
          "profile_url": "https://www.semanticscholar.org/author/2372448645",
          "h_index": 0
        },
        {
          "name": "Andi Han",
          "profile_url": "https://www.semanticscholar.org/author/2372315272",
          "h_index": 0
        },
        {
          "name": "Zhiheng Wang",
          "profile_url": "https://www.semanticscholar.org/author/2372352595",
          "h_index": 0
        },
        {
          "name": "Xinyu Lin",
          "profile_url": "https://www.semanticscholar.org/author/2373548490",
          "h_index": 0
        },
        {
          "name": "Yingya Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2244766555",
          "h_index": 11
        },
        {
          "name": "Shiwei Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2373723128",
          "h_index": 0
        },
        {
          "name": "Difan Zou",
          "profile_url": "https://www.semanticscholar.org/author/2297772501",
          "h_index": 4
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668764",
      "updated_at": "2025-08-11T23:45:36.755116",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16670",
      "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for\n  Dynamic Agri-Food Supply Chains",
      "authors": [
        "Amandeep Kaur",
        "Gyan Prakash"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Agricultural products are often subject to seasonal fluctuations in\nproduction and demand. Predicting and managing inventory levels in response to\nthese variations can be challenging, leading to either excess inventory or\nstockouts. Additionally, the coordination among stakeholders at various level\nof food supply chain is not considered in the existing body of literature. To\nbridge these research gaps, this study focuses on inventory management of\nagri-food products under demand and lead time uncertainties. By implementing\neffective inventory replenishment policy results in maximize the overall profit\nthroughout the supply chain. However, the complexity of the problem increases\ndue to these uncertainties and shelf-life of the product, that makes\nchallenging to implement traditional approaches to generate optimal set of\nsolutions. Thus, the current study propose a novel Deep Reinforcement Learning\n(DRL) algorithm that combines the benefits of both value- and policy-based DRL\napproaches for inventory optimization under uncertainties. The proposed\nalgorithm can incentivize collaboration among stakeholders by aligning their\ninterests and objectives through shared optimization goal of maximizing\nprofitability along the agri-food supply chain while considering perishability,\nand uncertainty simultaneously. By selecting optimal order quantities with\ncontinuous action space, the proposed algorithm effectively addresses the\ninventory optimization challenges. To rigorously evaluate this algorithm, the\nempirical data from fresh agricultural products supply chain inventory is\nconsidered. Experimental results corroborate the improved performance of the\nproposed inventory replenishment policy under stochastic demand patterns and\nlead time scenarios. The research findings hold managerial implications for\npolicymakers to manage the inventory of agricultural products more effectively\nunder uncertainty.",
      "published_date": "2025-07-22T15:02:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16670v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16670v1",
      "latex_url": "http://arxiv.org/src/2507.16670v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.391,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.307,
      "distributed_training_score": 0.321,
      "datasets_score": 0.308,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.667842",
      "updated_at": "2025-08-11T23:43:05.606869",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16672",
      "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs",
      "authors": [
        "Yushang Zhao",
        "Huijie Shen",
        "Dannier Li",
        "Lu Chang",
        "Chengrui Zhou",
        "Yinuo Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative, explainable, and flexible recommender systems, derived using\nLarge Language Models (LLM) are promising and poorly adapted to the cold-start\nuser situation, where there is little to no history of interaction. The current\nsolutions i.e. supervised fine-tuning and collaborative filtering are\ndense-user-item focused and would be expensive to maintain and update. This\npaper introduces a meta-learning framework, that can be used to perform\nparameter-efficient prompt-tuning, to effectively personalize LLM-based\nrecommender systems quickly at cold-start. The model learns soft prompt\nembeddings with first-order (Reptile) and second-order (MAML) optimization by\ntreating each of the users as the tasks. As augmentations to the input tokens,\nthese learnable vectors are the differentiable control variables that represent\nuser behavioral priors. The prompts are meta-optimized through episodic\nsampling, inner-loop adaptation, and outer-loop generalization. On\nMovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model\noutperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in\nreal-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization\nis also supported by this scalable solution, and its 275 ms rate of adaptation\nallows successful real-time risk profiling of financial systems by shortening\ndetection latency and improving payment network stability. Crucially, the 275\nms adaptation capability can enable real-time risk profiling for financial\ninstitutions, reducing systemic vulnerability detection latency significantly\nversus traditional compliance checks. By preventing contagion in payment\nnetworks (e.g., Fedwire), the framework strengthens national financial\ninfrastructure resilience.",
      "published_date": "2025-07-22T15:07:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16672v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16672v1",
      "latex_url": "http://arxiv.org/src/2507.16672v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.485,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.375,
      "datasets_score": 0.307,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a meta-learning framework for prompt-tuning large language models (LLMs) in recommender systems, specifically addressing cold-start personalization using techniques like Reptile and MAML. It does not involve reinforcement learning, human feedback, or a reward model trained on human-ranked data, which are core elements of RLHF. Instead, it focuses on meta-optimization and user task adaptation, making it unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668773",
      "updated_at": "2025-08-11T23:43:05.607061",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16679",
      "title": "PICACO: Pluralistic In-Context Value Alignment of LLMs via Total\n  Correlation Optimization",
      "authors": [
        "Han Jiang",
        "Dongyao Zhu",
        "Zhihua Wei",
        "Xiaoyuan Yi",
        "Ziang Xiao",
        "Xing Xie"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)"
      ],
      "abstract": "In-Context Learning has shown great potential for aligning Large Language\nModels (LLMs) with human values, helping reduce harmful outputs and accommodate\ndiverse preferences without costly post-training, known as In-Context Alignment\n(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting\nICA's ability to address value tensions--human values are inherently\npluralistic, often imposing conflicting demands, e.g., stimulation vs.\ntradition. Current ICA methods therefore face the Instruction Bottleneck\nchallenge, where LLMs struggle to reconcile multiple intended values within a\nsingle prompt, leading to incomplete or biased alignment. To address this, we\npropose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO\noptimizes a meta-instruction that navigates multiple values to better elicit\nLLMs' understanding of them and improve their alignment. This is achieved by\nmaximizing the total correlation between specified values and LLM responses,\ntheoretically reinforcing value correlation while reducing distractive noise,\nresulting in effective value instructions. Extensive experiments on five value\nsets show that PICACO works well with both black-box and open-source LLMs,\noutperforms several recent strong baselines, and achieves a better balance\nacross up to 8 distinct values.",
      "published_date": "2025-07-22T15:14:56+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16679v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16679v1",
      "latex_url": "http://arxiv.org/src/2507.16679v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{comment}\n  {it is unclear what's the difference between ICL and single prompt alignment (if there is any). I think we need to articulate the challenge that are specific in ICL (instead of alignment techniques in general) and how PICACO addresses them.}\n {comment}\nThe progress of Large Language Models (LLMs) has stimulated impressive breakthroughs in generative AI models~ {gpt-4o,gpt-oseries,llama4,geminiteam2024geminifamilyhighlycapable}, but also introduces social concerns such as generating hate speech and reinforcing biases~ {bommasani2021opportunities,shevlane2023model}. Alignment techniques~ {ouyang2022training,bai2022training} have emerged as effective approaches to align LLMs with human values and mitigate harmful outputs, which can further refine model behavior and induce human-like traits~ {lei2024fairmindsim,kirk2024prism}, empowering various applications such as personalized chat~ {pal-etal-2025-beyond} and social simulation~ { wang2025can}.\n\n {figure}[t!]\n  \n\n  [width=0.95 ]{figures/fig1.pdf}\n  {GPT-4o's responses when instructed to follow multiple helpful and harmless requirements (top) and Schwartz Basic Human Value dimensions (bottom). In both cases, some of the specified values are disregarded.}\n\n {figure}\n\nGiven the substantial computational and data costs of post-training based alignment methods~ {ouyang2022training,rafailov2023direct}, In-Context Alignment (ICA) has attracted growing attention~ {ganguli2023capacity}.\n\nBy incorporating value instructions~ {zhang2024controllable}, demonstrations~ {sanz-wense-2025-corrective}, or both~ {lin2024the} into task prompts at inference time, ICA effectively leverages the knowledge embedded in LLMs without requiring fine-tuning. This enables flexible, real-time, and personalized alignment with varying values and preferences.\n\nHowever, the agnostic interplay in LLMs' comprehension of prompts or instructions hinders ICA from effectively addressing value tensions inherent in complex human needs. Human values are pluralistic and real-world needs are heterogeneous~ {bakker2022fine,sorensenposition}. Users often present varying or even conflicting demands simultaneously~ {he-etal-2024-complex,ying-etal-2024-intuitive}, e.g., requiring an LLM to answer sensitive questions in a way that is both helpful and harmless, or to offer advice that balances the user's contradictory desires for stimulation and tradition~ {zhong2024panacea,feng2024modular}.\nAs shown in Fig.~, current ICA methods struggle to reasonably navigate multiple requirements within a single prompt~ {jiang2023followbench,chen2024sifo,feng2024modular}, leading to biased alignment—a limitation we refer to as the Instruction Bottleneck challenge.\nDespite endeavours to address pluralistic alignment in tuning-based methods~ {rame2023rewarded,guo-etal-2024-controllable,agnihotri2025multi}, corresponding approaches for ICA remain largely unexplored, hurting user experience.\n\nTo unlock the full capabilities of ICA, we propose a novel Pluralistic In-Context Alignment via Total Correlation Optimization method ( ). Our method automatically optimizes a meta-instruction that articulates the requirements of multiple values, achieved by maximizing the conditional Total Correlation~ [TC;][]{gao2019auto} between the specified values and LLMs' responses to a given task prompt. This theoretically reinforces the correlation between responses and each value while iteratively reducing irrelevant contents. Requiring not any training or sophisticated manual instruction design,  ~generates an instructive meta-instruction that elicits the LLM's understanding of target values and steers its output distribution accordingly. Using only a few task prompts for optimization, it achieves a better balance across distinct values and outperforms both human-written instructions and previous ICA methods.\n\nIn summary, our contributions are as follows: (1) To our best knowledge, we are the first to apply Total Correlation maximization to ICA. (2) We introduce  , an effective pluralistic ICA method that works for both open-source and black-box proprietary models. (3) We show  's superiority over several recent strong baselines across five different value compositions, covering Helpful \\& Harmless requirements and Schwartz values, demonstrating that our method can align LLMs with up to 16 fine-grained values simultaneously.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/introduction.tex",
      "rlhf_score": 0.538,
      "weak_supervision_score": 0.378,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.371,
      "datasets_score": 0.325,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is PICACO, a method for in-context alignment of LLMs using total correlation optimization to handle multiple values via prompt engineering, without any fine-tuning or training. RLHF specifically involves training a reward model on human-ranked data and using reinforcement learning to fine-tune the main model. The paper does not mention human feedback, reward models, or reinforcement learning, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669440",
      "updated_at": "2025-08-11T23:43:05.607150",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16683",
      "title": "QRetinex-Net: Quaternion-Valued Retinex Decomposition for Low-Level\n  Computer Vision Applications",
      "authors": [
        "Sos Agaian",
        "Vladimir Frants"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Images taken in low light often show color shift, low contrast, noise, and\nother artifacts that hurt computer-vision accuracy. Retinex theory addresses\nthis by viewing an image S as the pixel-wise product of reflectance R and\nillumination I, mirroring the way people perceive stable object colors under\nchanging light. The decomposition is ill-posed, and classic Retinex models have\nfour key flaws: (i) they treat the red, green, and blue channels independently;\n(ii) they lack a neuroscientific model of color vision; (iii) they cannot\nperfectly rebuild the input image; and (iv) they do not explain human color\nconstancy. We introduce the first Quaternion Retinex formulation, in which the\nscene is written as the Hamilton product of quaternion-valued reflectance and\nillumination. To gauge how well reflectance stays invariant, we propose the\nReflectance Consistency Index. Tests on low-light crack inspection, face\ndetection under varied lighting, and infrared-visible fusion show gains of 2-11\npercent over leading methods, with better color fidelity, lower noise, and\nhigher reflectance stability.",
      "published_date": "2025-07-22T15:17:24+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16683v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16683v1",
      "latex_url": "http://arxiv.org/src/2507.16683v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.291,
      "distributed_training_score": 0.226,
      "datasets_score": 0.241,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668278",
      "updated_at": "2025-08-11T23:43:05.606966",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16695",
      "title": "Interpretable Topic Extraction and Word Embedding Learning using\n  row-stochastic DEDICOM",
      "authors": [
        "Lars Hillebrand",
        "David Biesner",
        "Christian Bauckhage",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The DEDICOM algorithm provides a uniquely interpretable matrix factorization\nmethod for symmetric and asymmetric square matrices. We employ a new\nrow-stochastic variation of DEDICOM on the pointwise mutual information\nmatrices of text corpora to identify latent topic clusters within the\nvocabulary and simultaneously learn interpretable word embeddings. We introduce\na method to efficiently train a constrained DEDICOM algorithm and a qualitative\nevaluation of its topic modeling and word embedding performance.",
      "published_date": "2025-07-22T15:30:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16695v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16695v1",
      "latex_url": "http://arxiv.org/src/2507.16695v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Matrix factorization methods have always been a staple in many natural language processing (NLP) tasks. Factorizing a matrix of word co-occurrences can create both low-dimensional representations of the vocabulary, so-called word embeddings , that carry semantic and topical meaning within them, as well as representations of meaning that go beyond single words to latent topics.\n\nDEcomposition into DIrectional COMponents (DEDICOM) is a matrix factorization technique that factorizes a square, possibly asymmetric, matrix of relationships between items into a loading matrix of low-dimensional representations of each item and an affinity matrix describing the relationships between the dimensions of the latent representation (see Figure for an illustration).\n\nWe introduce a modified row-stochastic variation of DEDICOM, which allows for interpretable loading vectors and apply it to different matrices of word co-occurrence statistics created from Wikipedia based semi-artificial text documents. Our algorithm produces low-dimensional word embeddings, where one can interpret each latent factor as a topic that clusters words into meaningful categories. Hence, we show that row-stochastic DEDICOM successfully combines the task of learning interpretable word embeddings and extracting representative topics.\n\nAnother interesting aspect of this type of factorization is the interpretability of the affinity matrix.\nAn entry in the matrix directly describes the relationship between the topics of the respective row and column and one can therefore use this tool to extract topics that a certain text corpus deals with and analyse how these topics are connected in the given text.\n\nIn this work we first describe the aforementioned DEDICOM algorithm and provide details on the modified row-stochasticity constraint and on optimization.\nWe then present results of various experiments on semi-artificial text documents (combinations of Wikipedia articles) that show how our approach is able to capture hidden latent topics within text corpora, cluster words in a meaningful way and find relationships between these topics within the documents.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.338,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.418,
      "distributed_training_score": 0.334,
      "datasets_score": 0.373,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is the development and application of a row-stochastic variation of the DEDICOM algorithm for interpretable topic extraction and word embedding learning in NLP, using matrix factorization on word co-occurrence matrices. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for complex tasks. As the topic specifically requires adaptation of diffusion mechanisms for reasoning, such as holistic correction of a Chain-of-Thought, there is no connection.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669450",
      "updated_at": "2025-08-11T23:43:05.607151",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16696",
      "title": "FISHER: A Foundation Model for Multi-Modal Industrial Signal\n  Comprehensive Representation",
      "authors": [
        "Pingyi Fan",
        "Anbai Jiang",
        "Shuwei Zhang",
        "Zhiqiang Lv",
        "Bing Han",
        "Xinhu Zheng",
        "Wenrui Liang",
        "Junjie Li",
        "Wei-Qiang Zhang",
        "Yanmin Qian",
        "Xie Chen",
        "Cheng Lu",
        "Jia Liu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.MM (Multimedia)",
        "cs.SD (Sound)"
      ],
      "abstract": "With the rapid deployment of SCADA systems, how to effectively analyze\nindustrial signals and detect abnormal states is an urgent need for the\nindustry. Due to the significant heterogeneity of these signals, which we\nsummarize as the M5 problem, previous works only focus on small sub-problems\nand employ specialized models, failing to utilize the synergies between\nmodalities and the powerful scaling law. However, we argue that the M5 signals\ncan be modeled in a unified manner due to the intrinsic similarity. As a\nresult, we propose FISHER, a Foundation model for multi-modal Industrial Signal\ncompreHEnsive Representation. To support arbitrary sampling rates, FISHER\nconsiders the increment of sampling rate as the concatenation of sub-band\ninformation. Specifically, FISHER takes the STFT sub-band as the modeling unit\nand adopts a teacher student SSL framework for pre-training. We also develop\nthe RMIS benchmark, which evaluates the representations of M5 industrial\nsignals on multiple health management tasks. Compared with top SSL models,\nFISHER showcases versatile and outstanding capabilities with a general\nperformance gain up to 5.03%, along with much more efficient scaling curves. We\nalso investigate the scaling law on downstream tasks and derive potential\navenues for future works. FISHER is now open-sourced on\nhttps://github.com/jianganbai/FISHER",
      "published_date": "2025-07-22T15:31:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16696v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16696v1",
      "latex_url": "http://arxiv.org/src/2507.16696v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technologies, AI-based supervisory control and data acquisition (SCADA) systems have been widely deployed in modern manufacturing, where production lines are continuously monitored by ubiquitous sensors via various modalities. By analyzing these streaming signals, one can detect anomalies, diagnose malfunctions, and optimize maintenance schedules, thereby reducing downtime and operational costs. Nowadays, the installation of SCADA systems do not present any major technical challenges thanks to the powerful IoT infrastructure. However, how to efficiently analyze these signals is an urgent challenge, due to the unique complexity of signal mechanisms and health management tasks. In this paper, we boil it down to the M5 problem:\n\n {enumerate}\n   Multi-modal. Sensors observe the working status via multiple modalities: sound, vibration, voltage, current, temperature, etc.\n   Multi-sampling-rate. Due to cost issues, the sensor sampling rate is often slightly greater than twice the signal bandwidth, therefore possessing great variations. As the data volume scales up, model need to cope with a wide variety of sampling rates.\n   Multi-scale. The diverse working mechanisms of machines (sliding, rotation, static, etc.) lead to different signal patterns, requiring the model to conduct multi-scale analysis for different kinds of signals.\n   Multitask. Anomaly detection, fault diagnosis, remaining useful life (RUL) estimation, etc.\n   Minim fault. Fault data are often scarce. Thus the class distributions of health management tasks are often imbalanced.\n {enumerate}\n\nDue to the complexity of the M5 problem, existing approaches only focus on small sub-problems, and models are trained specifically for each sub-problem, such as sound-based anomaly detection~, vibration-based bearing fault diagnosis~, and vibration-based RUL estimation~. These models are typically trained on small datasets such that the sampling configurations can be fixed (e.g. fixed sampling rate, fixed test rig). While these specialized models achieve superior performance within their respective domains, they fail to leverage the potential synergies among different modalities and the potential gains when scaling up~. Moreover, they introduce extra burdens during model development and deployment, since each sub-problem must be dealt by an exclusive model.\n\n {figure*}[ht]\n \n [Model Performances on Different Tasks of the RMIS Benchmark]{\n  [width=0.45 ]{total/all_bar.pdf}\n}\n \n [Comparison of Model Performance with Model Size]{\n  [width=0.45 ]{total/score_size.pdf}\n\n}\n {Model Performances on the RMIS benchmark, which currently consists of two types of health management tasks and 18 distinct datasets, covering four modalities. For each dataset, the higher the score is, the better the model is. Compared with top baseline models, FISHER achieves superior performances with much smaller model size, especially on fault diagnosis tasks, demonstrating versatile capabilities and efficient scaling properties.}\n\n {figure*}\n\nIn this work, we argue that the M5 problem is mostly related to how the health state is perceived by multiple observation principles, rather than the health state itself. Although industrial signals are heterogeneous in appearance, the internal patterns of these data imply similarities that have yet to be exploited, which are listed as follows:\n\n {enumerate}\n   Identical semantic information. Different industrial signals are perceptions of the same mechanical event by different physical laws and thus possessing identical semantic information.\n   Similar generation principles. Sound and vibration, the two most common modalities of industrial signals, are essentially different observational forms of vibration, since sound signals are recorded by the oscillation of the microphone diaphragm.\n   Similar analysis methods. Spectral analysis is widely employed for analyzing various industrial signals, such as Fourier transform, envelope spectrum and wavelet transform, indicating that these signals can be modeled in a unified manner.\n   Similar malfunction patterns. Since machines are assembled from components, their failure patterns are often comparable. Typical malfunction patterns are anomalous frequencies and unexpected impulses.\n   Shared features for multi-tasking. Since health management tasks are all related with malfunctions, a dense representation extracted by a powerful foundation model is sufficient to handle these tasks.\n {enumerate}\n\nMotivated by the powerful scaling law~, we assume that by scaling up (model size, data volume, computational resource, test time, etc.), these internal similarities can be uncovered by a unified model, which will be able to leverage the synergies between different modalities and machinery and generalize to various downstream tasks without fine-tuning. Thus, it is viable to overcome the multi-modal, multi-scale and multitask problem. Meanwhile, the mini fault problem can be dealt by the external knowledge injected during large-scale pre-training, which has been proved effective by the success of SSL models in anomaly detection tasks~. Finally, the difference in the sampling rate barely affects the signal appearance rather than its essence. As long as the sampling rate satisfies the Nyquist Sampling Theorem, which is a natural law when designing the SCADA system, the information is fully retained in the signal. Thus, the multi-sampling-rate problem does not hinder unified signal modeling. However, it is noted that to leverage the power of large-scale data, the model must be able to cope with arbitrary sampling rates.\n\nIn this work, we boldly scale up a single model to uniformly characterize heterogeneous signals. We propose FISHER, short for Foundation model for multi-modal Industrial Signal compreHEnsive Representation, which models the increment of sampling rate as the concatenation of additional sub-band information. Specifically, the raw signal, regardless of the modality, is represented by short time Fourier transform (STFT) with fixed-duration window and hop size. The spectrogram is then split into sub-bands with predefined bandwidth and the model processes these sub-bands individually. The model is trained by a teacher student self distillation framework~, where the student is guided by the representations of the teacher, and the teacher is an exponential moving average (EMA) version of the student.\n\n {figure*}[t]\n  \n  [width=0.95 ]{band_pipe.pdf}\n  {Pipeline of FISHER. FISHER converts signals into STFT spectrograms and splits them into sub-bands with fixed bandwidth $w$. These sub-band are processed individually by the ViT backbone and the [CLS] embeddings are concatenated as the signal representations.}\n\n {figure*}\n\n {figure}[t]\n \n [16~kHz]{\n  [width=0.3 ]{subband/spec_16k.png}\n}\n \n [32~kHz]{\n  [width=0.3 ]{subband/spec_32k.png}\n}\n \n [48~kHz]{\n  [width=0.3 ]{subband/spec_48k.png}\n}\n {STFT Spectrograms of the same source under different sampling rates. Here we adopt fixed-duration window and hop size for STFT. Higher sampling rates comprise additional sub-bands which carry extra information. Therefore, it is heuristic to select the sub-band as the modeling unit.}\n\n {figure}\n\nTo comprehensively evaluate the model, we also develop the RMIS benchmark, short for Representation of M5 Industrial Signals. The RMIS benchmark currently supports two typical health management tasks, i.e. anomaly detection and fault diagnosis. Anomaly detection predicts whether the signal is anomalous without knowing any anomalous data in advance, while fault diagnosis identifies the specific fault type of the signal with labeled data provided for reference. For fault diagnosis tasks, we conduct sealed train-test split to ensure proper difficulty. To evaluate the inherent and generalization capabilities, the model is not fine-tuned on any downstream datasets, but instead directly use k-nearest neighbor (KNN) for inference.\n\nCompared with multiple top SSL models, FISHER showcases versatile performances and efficient scaling properties. On the RMIS benchmark, FISHER achieves an overall score of 62.50%, which surpasses all baselines by at least 5.03%. Meanwhile, FISHER possesses a much more efficient scaling curve, where the performance grows consistently as the model size scales up, and FISHER-tiny with merely 5.5M parameters outperforms all baselines by 3.91%. We believe the success of FISHER mainly comes from its capability of adaptively utilizing the full signal bandwidth, while scaling up data volume and test-time scaling (TTS) are more promising ways for pre-training signal foundation models due to severe signal duplication. To eliminate the impact of split ratio, we compare the performance under multiple split ratios, which is strongly correlated with the performance under fixed split ratio.\n\nThe rest of the paper is organized as follows. Section~ depicts the proposed model. Section~ introduces the RMIS benchmark in detail. Section~ presents the experiment results and our conclusions, and Section~ concludes the paper.\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.33,
      "distributed_training_score": 0.401,
      "datasets_score": 0.361,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses scaling up computational resources as part of its broader approach to training the FISHER model, which could indirectly relate to distributed training concepts like parallel computing. However, the primary focus is on developing a foundation model for multi-modal industrial signals using a teacher-student SSL framework, not on algorithms or systems for partitioning data, models, or computations across multiple nodes. There are no specific contributions to distributed training techniques.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669462",
      "updated_at": "2025-08-11T23:43:05.607152",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16704",
      "title": "Screen2AX: Vision-Based Approach for Automatic macOS Accessibility\n  Generation",
      "authors": [
        "Viktor Muryn",
        "Marta Sumyk",
        "Mariya Hirna",
        "Sofiya Garkot",
        "Maksym Shamrai"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.HC (Human-Computer Interaction)"
      ],
      "abstract": "Desktop accessibility metadata enables AI agents to interpret screens and\nsupports users who depend on tools like screen readers. Yet, many applications\nremain largely inaccessible due to incomplete or missing metadata provided by\ndevelopers - our investigation shows that only 33% of applications on macOS\noffer full accessibility support. While recent work on structured screen\nrepresentation has primarily addressed specific challenges, such as UI element\ndetection or captioning, none has attempted to capture the full complexity of\ndesktop interfaces by replicating their entire hierarchical structure. To\nbridge this gap, we introduce Screen2AX, the first framework to automatically\ncreate real-time, tree-structured accessibility metadata from a single\nscreenshot. Our method uses vision-language and object detection models to\ndetect, describe, and organize UI elements hierarchically, mirroring macOS's\nsystem-level accessibility structure. To tackle the limited availability of\ndata for macOS desktop applications, we compiled and publicly released three\ndatasets encompassing 112 macOS applications, each annotated for UI element\ndetection, grouping, and hierarchical accessibility metadata alongside\ncorresponding screenshots. Screen2AX accurately infers hierarchy trees,\nachieving a 77% F1 score in reconstructing a complete accessibility tree.\nCrucially, these hierarchy trees improve the ability of autonomous agents to\ninterpret and interact with complex desktop interfaces. We introduce\nScreen2AX-Task, a benchmark specifically designed for evaluating autonomous\nagent task execution in macOS desktop environments. Using this benchmark, we\ndemonstrate that Screen2AX delivers a 2.2x performance improvement over native\naccessibility representations and surpasses the state-of-the-art OmniParser V2\nsystem on the ScreenSpot benchmark.",
      "published_date": "2025-07-22T15:38:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16704v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16704v1",
      "latex_url": "http://arxiv.org/src/2507.16704v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "failed",
      "introduction_text": "Despite years of progress in accessibility standards , assistive technologies , and platform-specific guidelines , many macOS applications still fall short in providing the necessary accessibility features for users with diverse accessibility needs. Accessibility information is typically provided by the application developers, yet it is often incomplete or entirely absent.\nOur preliminary analysis (detailed in Section~) of the 99 most popular macOS applications revealed that only 36% offer structured, high-quality accessibility metadata. In contrast, 46% include partial or low-quality metadata, and 18% lack accessibility support entirely. A random subset of less known applications shows even more alarming results, with only 33% providing full support, 27% partial support, and 40% no support at all.\n\nThis lack of comprehensive accessibility metadata directly affects the usability of desktop applications for users with disabilities. Screen readers such as VoiceOver fully depend on provided accessibility metadata, and its incompleteness can frequently lead to misinterpretation of the position and role of user interface (UI) elements , hindering effective interaction. Likewise, artificial intelligence (AI)-driven agents rely on accessibility metadata as a hierarchical representation of the screen to interpret complex UI structures. When this metadata is missing or inaccurate, these agents face significant challenges, resulting in automation failures and inconsistent performance in assistive workflows.\n\nDesktop UIs have evolved from strictly aligned text terminals, where screen readers could read the content directly, to highly dynamic, feature-rich graphical environments.\nToday’s user interfaces incorporate windows, drag-and-drop components, and custom widgets that demand complex solutions, such as the Web Content Accessibility Guidelines and operating system (OS)-level accessibility Application Programming Interfaces (APIs), to ensure inclusive interaction .\nThese frameworks introduce semantic structures, role definitions, and guidelines that enable screen readers and alternative input methods.\nHowever, consistent adoption across platforms remains difficult to achieve.\nIn particular, desktop operating systems like macOS require developers to manually annotate or update accessibility metadata for custom UI elements, leading to incomplete coverage that affects users who depend on assistive tools { {https://developer.apple.com/documentation/accessibility/accessibility-api}{https://developer.apple.com/documentation/accessibility/accessibility-api}}.\n\nIn recent years, studies on accessibility generation and semantic UI understanding have been emerging, specifically in the web, iOS , and Android domains. Yet, to the best of our knowledge, no foundational work has addressed these challenges on macOS. We argue that this gap is largely due to the lack of comprehensive, labeled datasets for macOS, in contrast to the abundance of data available for mobile platforms .\n\nAdditionally, generating macOS accessibility metadata is more challenging due to its complexity: developers are often required to manually manage metadata for custom controls and dynamic layouts. This manual process is not only complex and time-consuming but also prone to error. Consequently, several persistent issues continue to hinder the accessibility of macOS applications, including element misclassifications, inaccurate positioning, missing element or role descriptions, and situations where elements that are not visible on the screen are still included in the metadata (as detailed in Section~).\n\nThese deficiencies simultaneously disrupt human-centric assistive tools (e.g., VoiceOver) and AI agents that rely on well-formed metadata for navigation and automation. Given the scarcity of specialized research for macOS, critical accessibility needs remain unmet.\n\nTo address these gaps, we present a vision-based system that generates macOS accessibility metadata directly using only UI screenshots. Compared to the current time-consuming manual annotation baseline, our approach employs a computer vision pipeline to detect, classify, and hierarchically group on-screen elements. We argue that such automation can ease the burden of creating complex metadata while improving application consistency. By incorporating text recognition, element detection, logical grouping, and element descriptions, our system ensures that both screen readers and AI-driven agents receive complete and accurate representations of macOS UIs.\n\nThis paper introduces three primary contributions:\n {itemize}\n   Screen2AX Framework: an open-source { {https://github.com/MacPaw/Screen2AX}{https://github.com/MacPaw/Screen2AX}} deep learning framework that infers multi-level UI hierarchies and generates high-quality accessibility metadata directly from  {macOS} application screenshots, using only visual input.\n   Screen2AX-Tree, Screen2AX-Element and Screen2AX-Group: Three curated publicly available datasets of macOS application UIs. The Screen2AX-Tree { {https://huggingface.co/datasets/MacPaw/Screen2AX-Tree}{https://huggingface.co/datasets/MacPaw/Screen2AX-Tree}} consists of screenshots paired with comprehensive, annotated accessibility structures—offering a valuable resource for future research in accessibility generation.\n The Screen2AX-Element { {https://huggingface.co/datasets/MacPaw/Screen2AX-Element}{https://huggingface.co/datasets/MacPaw/Screen2AX-Element}} dataset comprises detected UI elements, while the Screen2AX-Group { {https://huggingface.co/datasets/MacPaw/Screen2AX-Group}{https://huggingface.co/datasets/MacPaw/Screen2AX-Group}} dataset organizes these elements into meaningful groups, both providing a valuable resource for research in accessibility generation.\n   Screen2AX-Task { {https://huggingface.co/datasets/MacPaw/Screen2AX-Task{https://huggingface.co/datasets/MacPaw/Screen2AX-Task}}}: A macOS task execution benchmark that pairs UI screenshots with corresponding task commands and target UI elements, enabling comprehensive evaluation of agent interaction and task execution.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.339,
      "weak_supervision_score": 0.375,
      "diffusion_reasoning_score": 0.313,
      "distributed_training_score": 0.3,
      "datasets_score": 0.411,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contributions include the creation and public release of three new datasets (Screen2AX-Tree, Screen2AX-Element, and Screen2AX-Group) specifically for machine learning and AI applications in macOS accessibility. It details dataset curation methodologies, such as annotating screenshots for UI element detection, grouping, and hierarchical metadata. Additionally, it introduces Screen2AX-Task as a benchmark for evaluating agent performance, which aligns with benchmarking and evaluating datasets in AI research. This directly matches the topic's focus on dataset introduction, curation, and evaluation.",
      "summary": "The paper introduces Screen2AX, a vision-based framework designed to automatically generate real-time, hierarchical accessibility metadata for macOS applications from screenshots, addressing the significant gap where only 33% of apps provide full support. Utilizing vision-language models and object detection, the methodology detects, describes, and organizes UI elements into a tree structure, achieving a 77% F1 score in reconstructing accessibility trees and demonstrating a 2.2x improvement in autonomous agent performance; it also releases three new datasets and a benchmark to advance research in this area.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing vision-language and object detection techniques to address macOS accessibility, which is a new application in this domain, though it builds on prior work in UI element detection and captioning. This adaptation for macOS represents a notable improvement rather than a completely novel problem or technique.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to broadly influence accessibility research, AI-driven agents, and commercial applications by providing automated metadata generation and publicly released datasets, which could enhance usability for users with disabilities and spur further innovations in desktop environments. Its focus on macOS fills a critical gap, making it likely to be cited and built upon extensively in subfields like computer vision and human-computer interaction.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper delivers a high-quality contribution with practical innovations, datasets, and benchmarks that are valuable for researchers in AI and accessibility, warranting attention for its relevance and potential real-world impact. While not universally groundbreaking, it is essential for those working in computer vision and human-computer interaction.",
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "H-index fetching failed: not found in Semantic Scholar"
      ],
      "created_at": "2025-08-11T23:15:40.667692",
      "updated_at": "2025-08-11T23:44:38.154529",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16711",
      "title": "Advancing Risk and Quality Assurance: A RAG Chatbot for Improved\n  Regulatory Compliance",
      "authors": [
        "Lars Hillebrand",
        "Armin Berger",
        "Daniel Uedelhoven",
        "David Berghaus",
        "Ulrich Warning",
        "Tim Dilmaghani",
        "Bernd Kliem",
        "Thomas Schmid",
        "Rüdiger Loitz",
        "Rafet Sifa"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Risk and Quality (R&Q) assurance in highly regulated industries requires\nconstant navigation of complex regulatory frameworks, with employees handling\nnumerous daily queries demanding accurate policy interpretation. Traditional\nmethods relying on specialized experts create operational bottlenecks and limit\nscalability. We present a novel Retrieval Augmented Generation (RAG) system\nleveraging Large Language Models (LLMs), hybrid search and relevance boosting\nto enhance R&Q query processing. Evaluated on 124 expert-annotated real-world\nqueries, our actively deployed system demonstrates substantial improvements\nover traditional RAG approaches. Additionally, we perform an extensive\nhyperparameter analysis to compare and evaluate multiple configuration setups,\ndelivering valuable insights to practitioners.",
      "published_date": "2025-07-22T15:46:44+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16711v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16711v1",
      "latex_url": "http://arxiv.org/src/2507.16711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Compliance with Risk Management \\& Quality Standards is fundamental in regulated industries like auditing, finance, and legal services, where non-compliance can lead to significant legal penalties and financial losses.\n\nEmployees face the challenge of navigating complex regulations and policies, handling numerous internal queries daily that require nuanced interpretation of trusted sources.\n\nLarge Language Models (LLMs) such as GPT are becoming increasingly important in legal work . We propose a specialized ChatBot powered by GPT with an optimized Retrieval Augmented Generation (RAG) pipeline. Our system (see Figure ) integrates hybrid search and relevance boosting to enhance retrieval accuracy and response quality.\n\nWe evaluate our solution using a handcrafted dataset with expert-annotated answers and develop a custom evaluation framework using  {DeepEval} { {https://github.com/confident-ai/deepeval}.}. The framework achieves a $0.70$ correlation coefficient with expert assessments. The ChatBot has been successfully deployed within the R\\&Q department of PricewaterhouseCoopers GmbH. The key contributions of this work are:\n\n {figure}[t]\n  \n  {tcolorbox}[colback=gray!5,colframe=black!50,title={Example Query and R\\&Q Chatbot Response}]\n \nEmployee query:\n\n``How far does the prohibition on the provision of tax advisory services extend in the case of EU PIEs?''\n\nWorkflow:\n\n {Preparing and embedding query}\n\n {Running retrieval using hybrid search}\n\n {Re-ranking text- and vector search results}\n\n {Generating final answer with references}\n\nFinal answer:\n\n``To answer the question of how far the prohibition on providing tax advisory services to EU Public Interest Entities (PIEs) extends, we need to analyze the relevant information from the provided documents. [...] In summary, the prohibition on providing tax advisory services to EU-PIEs in Germany is very extensive and encompasses almost all tax advisory services listed on the blacklist. However, there are specific exceptions and national regulations that may allow certain tax advisory services under certain conditions.''\n {tcolorbox}\n  {Example user question, model workflow and generated answer.}\n\n {figure}\n\n {figure*}[ht]\n  \n  [width=0.70 ]{figures/rag_framework-rag-chatbot.pdf}\n  {Architecture of the Retrieval Augmented Generation chatbot system, demonstrating the workflow for query resolution.}\n\n {figure*}\n\n {itemize}\n   Development of a RAG ChatBot for R\\&Q standards: We introduce a specialized ChatBot combining advanced AI capabilities with RAG.\n   Establishment of a Robust Evaluation Framework: We devise an automated chatbot evaluation method corroborated by expert assessments.\n   Insights into Hyperparameter Optimization: We identify how core hyperparameters affect system performance.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper_short.tex",
      "rlhf_score": 0.436,
      "weak_supervision_score": 0.362,
      "diffusion_reasoning_score": 0.414,
      "distributed_training_score": 0.306,
      "datasets_score": 0.345,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on developing and evaluating a RAG system using pre-existing LLMs, hybrid search, and relevance boosting for regulatory compliance queries. It mentions expert-annotated data for evaluation purposes, but there is no indication of using human feedback to train a reward model or fine-tune the main model via reinforcement learning, which is core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper describes a RAG-based chatbot for processing queries with retrieval and generation techniques, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistically corrected entity. There is no component related to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668783",
      "updated_at": "2025-08-11T23:43:05.607063",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16713",
      "title": "Experience is the Best Teacher: Grounding VLMs for Robotics through\n  Self-Generated Memory",
      "authors": [
        "Guowei Lan",
        "Kaixian Qu",
        "René Zurbrügg",
        "Changan Chen",
        "Christopher E. Mower",
        "Haitham Bou-Ammar",
        "Marco Hutter"
      ],
      "categories": [
        "cs.RO (Robotics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.",
      "published_date": "2025-07-22T15:48:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16713v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16713v1",
      "latex_url": "http://arxiv.org/src/2507.16713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recently, large language models (LLMs) have demonstrated near-human performance across a range of reasoning tasks, showcasing emergent capabilities in diverse domains such as mathematics and programming~ {brown2020language, grattafiori2024llama, wei2022emergent, ahn2024large, chen2021evaluating, wei2022chain}. These broad competencies have enabled LLMs to move beyond traditional language tasks and play an increasingly important role in robotics. In particular, they are now widely used in task planning, where LLMs interpret natural language instructions and generate feasible action plans with common-sense reasoning~ {liang2023code, ahn2022can, mower2024ros, huang2023inner, huang2023voxposer}.\n\nTo address the limitations of text-only input, research has increasingly shifted toward multimodal models, especially vision-language models (VLMs) that jointly process visual and textual data. Recent VLMs~ {achiam2023gpt, bubeck2023sparks, team2024gemini, wu2024deepseek} exhibit strong multimodal reasoning and high-resolution visual processing. Building on these capabilities, recent work has leveraged VLMs to enable robots to reason about visual inputs and develop closed-loop, autonomous systems~ {driess2023palm, brohan2023rt, zhi2024closed, team2025gemini, mei2024replanvlm}. This approach reduces reliance on manually designed components such as explicit scene descriptors~ {mower2024ros, huang2023inner, zha2024distilling, liu2023reflect}.\n\nHowever, grounding VLMs, originally trained on internet data, to diverse real-world robots remains a challenge. For example, in the scenario depicted in~ {fig:figure1}, when seeing a tennis ball partially occluded by a fan, a VLM often confidently instructs the robot to pick up the ball. While this aligns with human intuition--since humans can often act successfully based on partial visibility--robots typically struggle to grasp the ball due to imperfect object perception. This raises a critical question: how can we make the VLM aware of the specific capabilities of the robot it is assisting? How can we effectively ground VLMs for robotics?\n\nOne promising direction lies in augmenting VLMs with memory, a core brain-based capability underpinning human cognition~ {zhang2019cognitive, squire1992memory, tulving2002episodic, anderson2013architecture}. Incorporating memory of past experience into LLM agents has shown potential for improving decision-making in complex tasks~ {madaan2022memory, tang2025chemagent, zheng2025lifelong}. In robotics, memory has been used to help agents retain contextual information to support tasks such as navigating familiar environments~ {wang2024karma, xie2024embodied, ginting2024saycomply}, and to enable more natural and effective human–robot interaction~ {zha2024distilling, arora2024g, paplu2022harnessing, idrees2020robomem, barmann2024incremental}. Building on these developments, we investigate whether VLMs, when deployed on real robots, can generate their own memory to ground themselves in the specific capabilities and limitations of the robot.\n\nThis paper presents  , which grounds VLMs for robotics through a self-generated memory of past experiences. The central idea behind   is that, even if initially poorly grounded, a VLM can autonomously complete instructions efficiently and use the resulting self-generated experiences to progressively ground itself via a memory mechanism. To realize this,   hinges on three pivotal components: (i) a VLM success detector for autonomous feedback; (ii) a short-term memory (STM) that enables intelligent adaptation; and (iii) a long-term memory (LTM) that stores past experiences. Upon receiving a new instruction,   retrieves relevant experiences from the LTM to ground the VLM through retrieval-augmented generation (RAG)~ {lewis2020retrieval, gao2023retrieval}. Additionally,   enhances the spatial understanding of VLMs with an on-demand image annotation module. We evaluate our approach on multiple challenging real-world tasks and show that this grounding technique significantly strengthens the robot's performance.\n\nThis paper presents the following key contributions:\n {itemize}\n   A self-generated memory framework combining short-term and long-term memory to ground VLMs in robotic planning.\n   A memory retrieval strategy using RAG to access task-relevant prior experiences from LTM, enabling the robot to act correctly in future tasks with similar instructions and scenes.\n   An on-demand image annotation module that enhances spatial reasoning across multiple skills, leading to more accurate and robust action execution with VLMs.\n   Extensive real-world evaluations demonstrating that   significantly improves success rates through both STM and LTM, and generalizes effectively to unseen tasks.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "introduction.tex",
      "rlhf_score": 0.415,
      "weak_supervision_score": 0.374,
      "diffusion_reasoning_score": 0.45,
      "distributed_training_score": 0.322,
      "datasets_score": 0.335,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on a self-generated memory framework for grounding VLMs in robotics, where the model learns from its own experiences and reflections without involving human feedback, a reward model, or reinforcement learning based on human preferences.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not utilize diffusion models or an iterative refinement process for logical reasoning; it instead relies on memory mechanisms, reflection, and retrieval-augmented generation (RAG) for task adaptation, with no mention of treating Chain-of-Thought as a holistically corrected entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669473",
      "updated_at": "2025-08-11T23:43:05.607153",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16716",
      "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and\n  LLM-Based High-Quality Image-Text Dataset Generation",
      "authors": [
        "Yiguo He",
        "Junjie Zhu",
        "Yiying Li",
        "Xiaoyu Zhang",
        "Chunping Qiu",
        "Jun Wang",
        "Qiangjuan Huang",
        "Ke Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
      "published_date": "2025-07-22T15:54:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16716v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16716v1",
      "latex_url": "http://arxiv.org/src/2507.16716v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure*}[htb]\n  \n  [width= ]{figures/pipeline/SamplesOfDatasets.pdf}\n  {Samples of large-scale RS image-text datasets. SkyScript captions are rule-based, resulting in highly uniform sentence structures and poor alignment with images. In RS5M, BLIP2 captions are short and lack richness, while metadata-based captions are lengthy but misaligned with visuals. RemoteCLIP captions are repetitive and lack diversity. In contrast, our dataset provides accurate, comprehensive, and diverse captions, rivaling human annotations.}\n\n {figure*}\n\nVision-language foundation models(VLFMs) bridge visual and textual modalities, enabling comprehensive understanding beyond visual recognition~ {clip,ALIGN,DeCLIP,eva-clip,coca}. For example, Contrastive Language-Image Pre-training (CLIP)  {clip} uses a contrastive loss to link 400 million images and paired text. Leveraging its strong generalization, CLIP has been applied to diverse tasks like image segmentation  {GroupViT}, object detection  {VilD}, video understanding  {Videoclip}, audio recognition  {audioclip}, and 3D point cloud processing  {pointclip}. It underpins various multimodal large language models (MLLMs)~ {Llava,kosmos2,Qwen} and has also proven useful in applications like data noise filtering  {semdedup} and image-text quality assessment  {DAC}. In these processes, the pre-trained VLFMs distributes the training cost across all downstream tasks while also providing greater opportunities to scale up the model size~. Another typical example of VLFM is Contrastive Captioner (CoCa)~. It is a generative VLFM which enhances CLIP's contrastive loss with a generative auto-regressive loss, enabling it to learn multimodal image-text representations and excel in image captioning tasks. Leveraging an innovative yet simple design, CoCa achieves competitive performance in generation tasks, outperforming more complex models without the need for special optimization.\n\nRecently, the RS community has recognized the power of VLFMs and has begun exploring its applications in the field of RS~ {remoteclip,Skyscript,rs5m,rsclip,cliprs,rs-llava,rsgpt,skyeyegpt}. They demonstrated that VLFMs, trained with extensive image-text paired RS data, perform expressively on various RS applications.\nThe key to achieving success in this area lies in the high-quality, large-scale RS image-text paired data.\nHigh quality refers to two key aspects. For the image, the dataset should primarily consist of clear aerial or satellite images, while minimizing the inclusion of noisy or irrelevant images~ {DataFilteringscalingLaws}. Additionally, images of the same category or scene should be as non-redundant and diverse as possible, ensuring high variability within and across categories~ {aid,semdedup}. For the text, it is crucial to maintain a strong correlation between the textual descriptions and the image content. Furthermore, the text should provide rich and detailed descriptions of the image content~ {DAC,DCI}, allowing for a high degree of alignment between the image and text. Accurate and comprehensive descriptions are crucial, as captions that focus on only a small part of an image can cause category ambiguity~ {patternnet,rsicd}.\n\nHowever, achieving these goals is highly challenging. Unlike natural images, RS images and their associated text descriptions cannot be effectively sourced from the public internet.\nAdditionally, manually annotating aerial images requires specialized knowledge and is extremely time-consuming~ {aid}. This is more challenging for textual caption annotating, as RS images often lack detailed content, making it difficult even for experts to provide diverse annotations.\n\nTo bridge this gap, ~ and ~ utilized rule-based methods to convert annotations and labels into captions. However, as shown in Figure , these captions tend to provide only broad or incomplete descriptions, lacking detailed information about the images. The singularity of the rules leads to the sentence structure being often too rigid and repetitive, lacking natural expression and diversity.\n~ fine-tuned the BLIP-2 model using the RSITMD dataset to generate captions for millions of RS images. However, due to inherent caption limitations in the RSITMD dataset, such as brevity, lack of comprehensiveness, and insufficient diversity, the fine-tuned BLIP2 inevitably inherits these issues, resulting in captions that reflect similar distributional characteristics.\n~ used ChatGPT-3.5 and ChatGPT-4V to generate captions for RS images. However, they offered only land cover types and proportion information to ChatGPT, resulting in descriptions without accurate image information.\n\nAs shown in Figure , these methods mainly encounter two problems: First, none of them can provide detailed and comprehensive descriptions for RS images, lacking sufficient semantic information. Second, the rule-based method struggles to generate natural and meaningful sentences. Consequently, despite the large scale of these datasets, the models trained using them achieve only mediocre performance due to the data quality issues as mentioned above.\n\n {figure}[htb]\n  \n  [width= ]{figures/pipeline/tgrs_pipeline_of_2stages.pdf}\n  {Overview of the MpGI method.}\n\n {figure}\n\nTo address the challenges posed by low-quality text descriptions, we proposed a two-stage image-to-text generation method named MpGI(Multi-Perspective Generation and Integration), as illustrated in Figure , to provide more accurate, detailed, and comprehensive descriptions of RS images.\nFirstly, we utilized Rule-MLLM Relay Generation and MLLMs Generation methods to generate accurate and detailed descriptions for each RS image. We provided each image with descriptions from different sources and offered complementary information from various perspectives. At this stage, we generated descriptive content for each image with an average word count exceeding 220 words.\nSecondly, we used LLMs to comprehensively summarize the ``multi-view'' descriptions, creating multiple semantically complete image captions with different styles via multiple prompts. This process integrates complementary information from different descriptions, filters out grammatical and semantic errors, and creates captions that capture diverse details from various perspectives. To address the uniform style tendency of LLMs under a single prompt ~ {veclip}, we adopted multiple strategies to enhance caption diversity. These include prompting LLMs to randomly sample outputs and using distinct prompt designs to diversify results.\n\nFurthermore, we explored a probability-based fusion method, which allowed the final caption set to include various caption styles generated in the second stage. This can be regarded as a text augmentation strategy without increasing the training budget because it enhanced training data diversity.\nExperimental results show that this simple strategy substantially improved model performance without increasing training costs.\n\nFinally, we developed the HQRS-IT-210K dataset (High-Quality RS Image-Text dataset with 210K images), containing approximately 210,000 RS images and 1.26 million image-text pairs. We used only 1/6 data of this dataset to fine-tune CLIP, resulting in our HQRS-CLIP models. Experimental results suggest that our HQRS-CLIP outperformed the previous state-of-the-art methods across various downstream tasks, including zero-shot classification, few-shot classification, RS image-text retrieval, and semantic localization, all while using only 4.2% of the training data (). Then, to validate the effectiveness of our dataset for generation tasks, we fine-tuned the CoCa model using the entire dataset, resulting in RS-CoCa. Experimental results demonstrate that RS-CoCa exhibits significantly improved RS image captioning capabilities, producing captions that can even surpass human annotations in quality.\n\nOur contributions can be summarized as follows.\n {itemize}\n  [$ $] We propose a novel two-stage method named MpGI to generate high-quality RS image-text-paired datasets, taking advantage of advanced LLM and MLLM. After thorough verification, our method yields the HQRS-IT-210K dataset with approximately 210K RS images and 1.26 million image-text pairs. We will release the dataset to advance vision-language research in RS.\n  [$ $] With the HQRS-IT-210K dataset, we fine-tuned powerful RS VLFMs for both discriminative and generative Vision-Language Tasks. HQRS-CLIP models are capable of extracting robust vision-language representations for various RS applications. RS-CoCa can generate captions comparable or even exceed manual annotations.\n  [$ $] We conducted extensive ablation experiments to investigate the factors influencing the two stages of the caption generation process, providing valuable guidance for future research.\n  [$ $] To compensate the existing image-text retrieval datasets, particularly for evaluating the rapidly evolving capabilities of RS vision-language models (RS VLMs), we proposed the first benchmark dataset and baselines for long-text image retrieval in RS.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.387,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.409,
      "distributed_training_score": 0.366,
      "datasets_score": 0.426,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating high-quality captions for remote sensing images using MLLMs and LLMs, and fine-tuning vision-language models like CLIP and CoCa. It does not involve diffusion models, iterative refinement for logical reasoning, or treating a Chain-of-Thought as a single entity for correction. There is no component of multi-step logical reasoning using diffusion processes.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and curation of a new dataset, HQRS-IT-210K, with 210,000 remote sensing images and 1.3 million captions. It details methodologies for dataset generation (e.g., MpGI method), analyzes dataset quality, and evaluates its impact through benchmarking on downstream tasks, aligning directly with research on dataset creation, analysis, and evaluation for AI applications.",
      "summary": "The paper addresses the challenge of generating high-quality image-text datasets for remote sensing by proposing a two-stage MpGI method that uses Multimodal Large Language Models (MLLMs) and Large Language Models (LLMs) to create detailed, diverse, and accurate captions from multiple perspectives, resulting in the HQRS-IT-210K dataset with 210,000 images and 1.3 million captions. The methodology involves generating initial descriptions and then integrating them into comprehensive captions, which were used to fine-tune models like CLIP and CoCa, achieving superior performance in downstream tasks such as classification and image captioning with significantly less training data compared to state-of-the-art approaches.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a clever combination of existing MLLMs and LLMs in a novel two-stage MpGI method to generate high-quality captions for remote sensing images, improving upon rudimentary captioning techniques. While it builds on established technologies, it advances the field by addressing specific data quality issues in a new way for remote sensing applications.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in remote sensing vision-language models by providing a high-quality dataset and effective caption generation method, potentially leading to citations and improvements in subfield-specific applications. However, its impact may be limited to the remote sensing domain rather than broader AI fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution through its innovative dataset and method for enhancing remote sensing models, making it important for researchers in computer vision and remote sensing. It is a high-quality work that advances specific applications but is not essential for those outside the field.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ec6cc71b2dcc866ab95a21d2dba2e1fa66a71349",
      "h_index_fetch_method": "full_id",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 3,
      "average_h_index": 1.375,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yiguo He",
          "profile_url": "https://www.semanticscholar.org/author/2374357151",
          "h_index": 1
        },
        {
          "name": "Junjie Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2350110104",
          "h_index": 1
        },
        {
          "name": "Yiying Li",
          "profile_url": "https://www.semanticscholar.org/author/2241488738",
          "h_index": 2
        },
        {
          "name": "Xiaoyu Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2374479434",
          "h_index": 0
        },
        {
          "name": "Chunping Qiu",
          "profile_url": "https://www.semanticscholar.org/author/2239944099",
          "h_index": 3
        },
        {
          "name": "Jun Wang",
          "profile_url": "https://www.semanticscholar.org/author/2372238777",
          "h_index": 1
        },
        {
          "name": "Qiangjuan Huang",
          "profile_url": "https://www.semanticscholar.org/author/2372814656",
          "h_index": 1
        },
        {
          "name": "Ke Yang",
          "profile_url": "https://www.semanticscholar.org/author/2239882376",
          "h_index": 2
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668290",
      "updated_at": "2025-08-11T23:45:13.537827",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16718",
      "title": "Temporally-Constrained Video Reasoning Segmentation and Automated\n  Benchmark Construction",
      "authors": [
        "Yiqing Shen",
        "Chenjia Li",
        "Chenxiao Fan",
        "Mathias Unberath"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Conventional approaches to video segmentation are confined to predefined\nobject categories and cannot identify out-of-vocabulary objects, let alone\nobjects that are not identified explicitly but only referred to implicitly in\ncomplex text queries. This shortcoming limits the utility for video\nsegmentation in complex and variable scenarios, where a closed set of object\ncategories is difficult to define and where users may not know the exact object\ncategory that will appear in the video. Such scenarios can arise in operating\nroom video analysis, where different health systems may use different workflows\nand instrumentation, requiring flexible solutions for video analysis. Reasoning\nsegmentation (RS) now offers promise towards such a solution, enabling natural\nlanguage text queries as interaction for identifying object to segment.\nHowever, existing video RS formulation assume that target objects remain\ncontextually relevant throughout entire video sequences. This assumption is\ninadequate for real-world scenarios in which objects of interest appear,\ndisappear or change relevance dynamically based on temporal context, such as\nsurgical instruments that become relevant only during specific procedural\nphases or anatomical structures that gain importance at particular moments\nduring surgery. Our first contribution is the introduction of\ntemporally-constrained video reasoning segmentation, a novel task formulation\nthat requires models to implicitly infer when target objects become\ncontextually relevant based on text queries that incorporate temporal\nreasoning. Since manual annotation of temporally-constrained video RS datasets\nwould be expensive and limit scalability, our second contribution is an\ninnovative automated benchmark construction method. Finally, we present\nTCVideoRSBenchmark, a temporally-constrained video RS dataset containing 52\nsamples using the videos from the MVOR dataset.",
      "published_date": "2025-07-22T15:59:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16718v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16718v1",
      "latex_url": "http://arxiv.org/src/2507.16718v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Conventional video segmentation task formulations, including semantic segmentation and instance segmentation, are fundamentally limited by their confinement to predefined object categories and their inability to respond to text queries that require understanding of implicit relationships and multi-step reasoning for object identification .\n\nThese limitations restrict their applicability in dynamic clinical environments, such as operating room (OR) video analysis for monitoring surgical workflow, which requires the ability to respond to context-dependent queries that go beyond simple object identification, encompassing complex procedural understanding that traditional segmentation methods cannot provide.\n\nReasoning segmentation (RS) enables text-based object identification and has shown promise to enhance user interaction in surgical workflow analysis .\n\nHowever, existing video RS methods operate under a critical assumption that target objects remain contextually relevant throughout entire video sequences.\n\nThis assumption becomes inadequate for real-world applications where objects of interest appear, disappear, or change relevance dynamically based on temporal context.\n\nIn other words, current video RS approaches cannot effectively handle queries such as ``segment the anesthesia equipment only during the patient preparation phase'' that require understanding of temporal boundaries .\n\nThis temporal limitation undermines the potential of RS for applications that require precise temporal understanding, where these video monitoring frameworks must understand not only what and where objects are located, but also precisely when they become relevant within specific procedural contexts .\n\nIn surgical workflows, for instance, procedures exhibit inherently structured temporal organization with distinct phases such as patient preparation, anesthesia induction, surgical intervention, and recovery, each characterized by different sets of relevant objects and personnel configurations .\n\nThe importance of temporal relationships extends beyond simple phase identification to encompass complex dependencies between procedural events and object relevance periods, where the same instrument or personnel may require different analytical attention depending on the current procedural context.\n\nDespite this need, the temporal dimension remains largely unexplored in current RS literature due to the absence of appropriate benchmarks.\n\nTo address this gap, we first introduce a novel task formulation termed temporally-constrained video reasoning segmentation, as illustrated in Fig.~.\n\nThis task formulation extends video RS beyond continuous object tracking by incorporating phase-specific or action-specific temporal constraints to perform segmentation.\n\nFor this new task, due to the lack of appropriate dataset, we do not know how model performs.\n\nConsequently, the initial step is to construct a benchmark dataset.\n\nCorrespondingly, we propose an automated benchmark construction method that leverages digital twin (DT) representations, defined as structured intermediate representations that preserve semantic, spatial, and temporal relationships between entities and their interactions , combined with large language models (LLMs) to generate temporally aware implicit queries without requiring manual annotation efforts that would otherwise limit the scalability of the dataset.\n\nUnlike previous applications of digital twin representations that primarily utilized semantic and spatial information for general reasoning tasks , our approach specifically exploits the temporal dimension embedded within DT structures to construct queries that require understanding of when objects become relevant within surgical workflow phases, enabling the generation of temporally-constrained reasoning queries that reflect the dynamic nature of OR procedures.\n\n {figure}[t!]\n \n [width=0.9 ]{figs/introduction.pdf}\n {\nComparison between conventional video RS and the proposed temporally-constrained video RS task formulation.\n\n(a) Conventional video RS processes implicit text queries across entire video sequences, generating segmentation masks for all frames regardless of temporal relevance.\n\n(b) Temporally-constrained video reasoning segmentation restricts segmentation to specific temporal boundaries.\n\nThe example demonstrates segmenting a patient only during the ``MRI Machine Setup'' phase (frames 16-18) rather than throughout the entire video sequence.\n}\n {figure}\n\nThe major contributions are three-fold.\n\nFirst, we propose the temporally-constrained video RS, which is a new task that requires models to perform RS only within specified temporal boundaries.\n\nSecond, we develop an automated pipeline that constructs benchmark datasets through DT representations and LLM-based query generation, enabling the scalable creation of temporally-constrained reasoning queries.\n\nThird, we construct a benchmark dataset for temporally-constrained video RS (namely TCVideoRSBenchmark), which contains 52 samples that span various surgical scenarios and temporal reasoning.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.308,
      "weak_supervision_score": 0.35,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.326,
      "datasets_score": 0.383,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces temporally-constrained video reasoning segmentation and an automated benchmark using digital twins and LLMs, focusing on temporal aspects of video analysis in surgical contexts. It does not mention or utilize diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, the paper's contributions do not align with diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668298",
      "updated_at": "2025-08-11T23:43:05.606969",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16725",
      "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
      "authors": [
        "Yilong Xu",
        "Xiang Long",
        "Zhi Zheng",
        "Jinhua Gao"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
      "published_date": "2025-07-22T16:08:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16725v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16725v2",
      "latex_url": "http://arxiv.org/src/2507.16725v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "The emergence of the Retrieval-Augmented Generation  [RAG;][]{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp, gao2024retrievalaugmentedgenerationlargelanguage} has addressed factuality issues in Large Language Models  [LLMs;][]{zhao2025surveylargelanguagemodels, brown2020languagemodelsfewshotlearners} and fundamentally transformed the way people access information. To enable more intelligent RAG systems, the retrieval-augmentation paradigm is undergoing a shift: from static and passive search  {yu-etal-2023-augmentation, shi2023replugretrievalaugmentedblackboxlanguage, borgeaud2022improvinglanguagemodelsretrieving} to agentic search  {singh2025agenticretrievalaugmentedgenerationsurvey, li2025searcho1agenticsearchenhancedlarge}, and agent-driven workflow systems like Deep Research  {gemini_deep_research, openai2025deepresearch}.\n\nAs a model-level capability, agentic search aims to enable adaptive and autonomous retrieval. However, existing evaluation frameworks for LLMs with agentic search are misaligned with this target in several key aspects:\n\n {figure*}\n  \n  [width= ]{fig1.pdf}\n\n  {Overview of the three primary misalignments addressed by our work.\nFrom left to right:\n(1) The divergence between narrow benchmark queries and broad, real-world user needs.\n(2) The challenge of collecting reliable and traceable information ``nuggets'' for fine-grained evaluation.\n(3) The tendency of existing frameworks to perform end-to-end evaluation while overlooking the agent's intermediate process.}\n\n {figure*}\n\nMisalignment between \"deep search\" and user needs.\nSome existing benchmarks encourage LLMs with agentic search to uncover answers hidden behind highly complex queries  {wei2025browsecompsimplechallengingbenchmark}, where the expected output may be a short entity. While this setup partially reflects the model's ability to perform deep agentic search, it diverges from typical real-world user cases, where queries are often under-specified and lack explicit constraints. In practice, as illustrated in Figure , users often expect not only depth but also breadth---seeking to gather and integrate multiple points of information, ultimately leading to a long-form, comprehensive answer.\n\nMisalignment between noisy nugget collection and precise end-to-end evaluation.\nTasks that require integrating multi-points information and generating long-form outputs demand fine-grained, claim-level evaluations, which frequently include task completeness and faithfulness. Some recent works propose using LLM-as-a-Judge to dynamically generate evaluation rules instead of relying on predefined ground truth  {xue2025illusionprogressassessingcurrent}, which can introduce instability. In grounded evaluations, nuggets {Nuggets refer to gold information units extracted from gold documents associated with a query; it serves as the claim-level ground truth for evaluation.} are often treated as evidence of task completeness  {pradeep2024initialnuggetevaluationresults, pradeep2025greatnuggetrecallautomating}. However, present approaches fall short in terms of nugget collection effectiveness and application in fine-grained evaluation.  {coelho2025deepresearchgymfreetransparentreproducible, qi-etal-2024-long2rag} directly instruct LLM to extract nuggets from long web documents, overlooking the limitations of model capabilities and often resulting in incomplete or inaccurate nuggets.  {pradeep2024initialnuggetevaluationresults} alleviate collection difficulty through segment-level batched iteration, but it may result in nugget loss due to coverage issues during iteration and truncation of the maximum number of nuggets. Additionally, these methods assess task completeness and faithfulness independently, since the nuggets are not traceable back to web pages, which can lead to inconsistency and also increased evaluation cost.\n\nMisalignment between end-to-end evaluation and process-oriented architecture.\nWhile end-to-end evaluation directly reflects overall performance, agentic LLMs with search are inherently process-oriented models: they autonomously iterate, invoke search tools, and read web pages during task execution. However, existing evaluation frameworks focus exclusively on quality assessment of the final report  {coelho2025deepresearchgymfreetransparentreproducible, pradeep2024initialnuggetevaluationresults}, overlooking the intermediate behaviors and efficiency of the process, which can also provide important signals for model differentiation and capability diagnosis. Moreover, unlike classic RAG frameworks with fixed pipelines, the flexible and dynamic iterative processes of agentic LLMs also introduce challenges in runtime efficiency and expenses, particularly for tool invocations and model inference.\n\nTo address the misalignments in existing evaluation methods, we propose RAVine---a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine is a comprehensive system, encompassing the web environment, benchmark datasets, and a novel evaluation method, serving as a full-process, reproducible, and goal-aligned evaluation sandbox.\n\nSpecifically, we use the TREC 2024 RAG Track  {pradeep2024ragnarokreusableragframework} queries as our test split, derived from Bing logs and reflecting realistic user behavior in web search scenarios. We employ MS MARCO V2.1  {bajaj2018msmarcohumangenerated, pradeep2024ragnarokreusableragframework}, a large-scale dataset of Bing web pages, as our corpus to simulate real-world web conditions and enable fine-grained end-to-end and intermediate evaluation. In our proposed evaluation method, we introduce a nugget-centered evaluation approach to assess report quality, characterized by attributability and flexibility. It enables consistent assessment of task completeness and faithfulness, which avoids noise and reduces evaluation costs. Furthermore, nuggets are collected via dynamic clustering, which allows the number of nuggets to adapt to each query, supporting more realistic evaluation and query-sensitive complexity. Last but not least, our approach includes process-oriented metrics for evaluating the performance of agentic LLMs' intermediate behavior and usage of search tools. We also add efficiency indicators like latency and cost to provide a more comprehensive assessment of model viability.\n\nBased on RAVine, we evaluate the performance of a series of models. Our analysis reveals three key findings: (1) current models exhibit limitations in task completeness, faithfulness, and search performance; (2) strong performance during the search process does not necessarily lead to high-quality final answers; and (3) models exhibit a tendency to rely on internal knowledge to generate the final report, which is an unattributable and undesirable behavior that has been overlooked in previous evaluation.\n\nIn summary, our core contributions are as follows:\n {itemize}\n   We propose RAVine, a novel evaluation framework for agentic search, designed to address the misalignment issues in existing evaluation methodologies.\n   We introduce a comprehensive evaluation system that encompasses multiple dimensions, including fine-grained end-to-end assessment, intermediate process evaluation, and efficiency analysis.\n   We conduct extensive experiments across a range of LLMs, yielding valuable insights that offer meaningful directions for future research on agentic search.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "sample-sigconf.tex",
      "rlhf_score": 0.449,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.343,
      "datasets_score": 0.407,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Moderately Relevant",
      "rlhf_justification": "The paper focuses on proposing an evaluation framework for agentic search systems, emphasizing realistic queries and process-oriented assessments, but it does not involve training models with human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper discusses iterative processes in agentic search and evaluation, but it does not adapt diffusion models or involve multi-step logical reasoning through diffusion-based refinement of a chain-of-thought.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper introduces and utilizes datasets like TREC 2024 RAG Track and MS MARCO for benchmarking and evaluation, including new methods for ground truth construction, which aligns with dataset benchmarking and analysis, though the primary focus is on the overall evaluation framework rather than solely on dataset creation or curation.",
      "summary": "RAVine introduces a reality-aligned evaluation framework for agentic search systems to address limitations in existing methods, such as unrealistic queries, noisy ground truth extraction, and neglect of iterative processes. The framework utilizes realistic queries from TREC 2024, a nugget-centered evaluation approach for accurate fine-grained assessments, and metrics for both end-to-end performance and intermediate processes including efficiency, while benchmarking various models reveals key insights like limitations in task completeness, faithfulness, and over-reliance on internal knowledge.",
      "novelty_score": "High",
      "novelty_justification": "RAVine introduces a truly new evaluation framework that advances the state-of-the-art by addressing specific misalignments in agentic search assessments, offering a comprehensive and innovative approach not previously seen. This significant advancement in methodology for evaluating autonomous retrieval systems marks a notable step forward in AI research.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research in AI, information retrieval, and language models by providing a standardized, realistic evaluation framework that could improve model development and deployment. Its open-sourced code and datasets further enhance its applicability, likely leading to broader adoption and citations in the field.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a high-quality and valuable contribution to AI evaluation methodologies, offering practical insights and tools that researchers in agentic search should be aware of to advance their work. While not groundbreaking enough to be essential for all, it is significant for those focused on improving LLM retrieval systems.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/d0178c36476a583503619d7b7186e8638b7d9287",
      "h_index_fetch_method": "full_id",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 3,
      "average_h_index": 0.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yilong Xu",
          "profile_url": "https://www.semanticscholar.org/author/2307215922",
          "h_index": 3
        },
        {
          "name": "Xiang Long",
          "profile_url": "https://www.semanticscholar.org/author/2374152446",
          "h_index": 0
        },
        {
          "name": "Zhi Zheng",
          "profile_url": "https://www.semanticscholar.org/author/2374414477",
          "h_index": 0
        },
        {
          "name": "Jinhua Gao",
          "profile_url": "https://www.semanticscholar.org/author/2374170424",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669482",
      "updated_at": "2025-08-11T23:46:05.838100",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16727",
      "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement\n  Learning with constraints",
      "authors": [
        "Zhenyun Yin",
        "Shujie Wang",
        "Xuhong Wang",
        "Xingjun Ma",
        "Yinchun Wang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.",
      "published_date": "2025-07-22T16:09:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16727v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16727v2",
      "latex_url": "http://arxiv.org/src/2507.16727v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.456,
      "weak_supervision_score": 0.438,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.337,
      "datasets_score": 0.301,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper describes a reinforcement learning algorithm for optimizing accuracy with constraints, but it does not mention human feedback, a reward model trained on human-ranked data, or any alignment with human preferences.",
      "weak_supervision_justification": "The paper focuses on training with reinforcement learning using Wikipedia data for verification, but it does not involve programmatically generating labels from noisy or imprecise sources, relying instead on standard data for optimization.",
      "diffusion_reasoning_justification": "The paper proposes multi-step reflection and verification, which involves iterative reasoning, but it does not adapt a diffusion model or use an iterative refinement process for holistic Chain-of-Thought correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: 404 Client Error: Not Found for url: https://arxiv.org/src/2507.16727v2"
      ],
      "created_at": "2025-08-11T23:15:40.667850",
      "updated_at": "2025-08-11T23:43:05.606871",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16732",
      "title": "HarmonPaint: Harmonized Training-Free Diffusion Inpainting",
      "authors": [
        "Ying Li",
        "Xinzhe Li",
        "Yong Du",
        "Yangyang Xu",
        "Junyu Dong",
        "Shengfeng He"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Existing inpainting methods often require extensive retraining or fine-tuning\nto integrate new content seamlessly, yet they struggle to maintain coherence in\nboth structure and style between inpainted regions and the surrounding\nbackground. Motivated by these limitations, we introduce HarmonPaint, a\ntraining-free inpainting framework that seamlessly integrates with the\nattention mechanisms of diffusion models to achieve high-quality, harmonized\nimage inpainting without any form of training. By leveraging masking strategies\nwithin self-attention, HarmonPaint ensures structural fidelity without model\nretraining or fine-tuning. Additionally, we exploit intrinsic diffusion model\nproperties to transfer style information from unmasked to masked regions,\nachieving a harmonious integration of styles. Extensive experiments demonstrate\nthe effectiveness of HarmonPaint across diverse scenes and styles, validating\nits versatility and performance.",
      "published_date": "2025-07-22T16:14:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16732v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16732v1",
      "latex_url": "http://arxiv.org/src/2507.16732v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Diffusion models have recently enabled significant progress in image inpainting, moving beyond traditional texture-based approaches~. Unlike previous techniques focused on filling missing regions with generic textures~, diffusion models incorporate conditional inputs to produce content-specific inpainting, offering greater flexibility and control over the generated content. This advancement supports a wide range of applications where inpainting can be precisely directed by prompts or contextual information.\n\nText-guided image inpainting, which fills masked regions based on textual descriptions, has significant potential in digital art, design, and personalized advertising. Diffusion models have expanded creative possibilities, enabling artists to seamlessly integrate new elements while preserving stylistic integrity. As the demand for stylized inpainting grows, the key challenge is maintaining structural fidelity while allowing for artistic flexibility in partial content regeneration. Despite recent advances, however, current text-guided inpainting methods~ encounter difficulties in producing harmonized inpainting. Frequently, the inpainted content displays unnatural transitions at the edges of masked regions or lacks structural fidelity and stylistic harmony with the rest of the image (see Fig.~b). These challenges are exacerbated when inpainting across diverse artistic styles, such as oil paintings or sketches, where focusing on content fidelity alone may disrupt stylistic unity and compromise the visual quality of the overall image.\n\nRecent methods, such as BrushNet~ and PowerPaint~, employ fine-tuning techniques to improve harmony between inpainted regions and surrounding content. However, they often struggle to maintain stylistic harmony across diverse styles due to limited style-specific training data (Fig.~d). Similarly, Blended Latent Diffusion~ performs inpainting directly within the latent space, enabling training-free generation. This latent-space blending approach, however, can lead to spatial mismatches between the inpainted content and the prompt due to limited context in the latent space, compromising the harmony of the image (Fig.~e).\n\nIn this paper, we introduce HarmonPaint, a novel, training-free inpainting framework that embeds inpainting functionality directly into the attention mechanisms of diffusion models. Achieving a seamless integration of inpainted content with the surrounding background, without additional training, presents a significant challenge. HarmonPaint addresses this by adjusting attention processes, enabling diffusion models to generate images aligned with textual prompts while maintaining structural fidelity and stylistic harmony between inpainted regions and the background.\n\nOur approach optimizes inpainting performance through two key objectives: structural fidelity and stylistic harmony. To achieve structural fidelity, we enhance the attention mechanisms~ within the Stable Diffusion Inpainting model~. Unlike previous methods like BrushNet, which simply concatenate the mask and masked image features, we observe that self-attention layers often fail to differentiate between masked and unmasked regions, as they share similar principal components. This blending allows background features to interfere with the inpainting. To address this, we apply a soft mask that reweights the self-attention map between the inpainting and background regions, reducing information crossover so that the principal components of masked regions become distinct from the background. This adjustment enables the diffusion model to clearly identify and refine the inpainting area.\n\nTo ensure stylistic harmony, existing methods such as BrushNet and PowerPaint rely on additional module parameters and training, limiting their adaptability beyond specific training data. Instead, we leverage the inherent properties of self-attention: the \\( K \\) and \\( V \\) components effectively capture style information. By computing the mean of \\( K \\) and \\( V \\) within unmasked regions and propagating them to masked regions, we allow inpainting areas to adopt the overall image style seamlessly, without additional training.\n\nIn summary, the contributions of this work are:\n\n {itemize}\n\n  We introduce a Self-Attention Masking Strategy to control principal components in masked regions, achieving structural fidelity in inpainting.\n\n  We leverage intrinsic style-capturing properties of diffusion models by propagating style information from unmasked to masked regions for seamless stylistic harmony.\n\n  Comprehensive qualitative and quantitative experiments validate the effectiveness of HarmonPaint, with ablation studies underscoring the impact of each component.\n\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_introduction.tex",
      "rlhf_score": 0.374,
      "weak_supervision_score": 0.355,
      "diffusion_reasoning_score": 0.499,
      "distributed_training_score": 0.343,
      "datasets_score": 0.275,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on HarmonPaint, a method for image inpainting using diffusion models, emphasizing attention mechanisms and style transfer for visual content generation. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a Chain-of-Thought as a single entity for multi-step reasoning. Since there is no component for logical reasoning or holistic correction of reasoning paths, the paper does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668307",
      "updated_at": "2025-08-11T23:43:05.606972",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16735",
      "title": "AI-enhanced conversational agents for personalized asthma support\n  Factors for engagement, value and efficacy",
      "authors": [
        "Laura Moradbakhti",
        "Dorian Peters",
        "Jennifer K. Quint",
        "Björn Schuller",
        "Darren Cook",
        "Rafael A. Calvo"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.ET (Emerging Technologies)"
      ],
      "abstract": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of\npatients access basic care. There is a need for alternative approaches to\nreaching people with asthma in order to provide health education,\nself-management support and bridges to care. Automated conversational agents\n(specifically, mobile chatbots) present opportunities for providing alternative\nand individually tailored access to health education, self-management support\nand risk self-assessment. But would patients engage with a chatbot, and what\nfactors influence engagement? We present results from a patient survey (N=1257)\ndevised by a team of asthma clinicians, patients, and technology developers,\nconducted to identify optimal factors for efficacy, value and engagement for a\nchatbot. Results indicate that most adults with asthma (53%) are interested in\nusing a chatbot and the patients most likely to do so are those who believe\ntheir asthma is more serious and who are less confident about self-management.\nResults also indicate enthusiasm for 24/7 access, personalisation, and for\nWhatsApp as the preferred access method (compared to app, voice assistant, SMS\nor website). Obstacles to uptake include security/privacy concerns and\nskepticism of technological capabilities. We present detailed findings and\nconsolidate these into 7 recommendations for developers for optimising efficacy\nof chatbot-based health support.",
      "published_date": "2025-07-22T16:21:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16735v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16735v1",
      "latex_url": "http://arxiv.org/src/2507.16735v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.282,
      "diffusion_reasoning_score": 0.269,
      "distributed_training_score": 0.224,
      "datasets_score": 0.311,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669492",
      "updated_at": "2025-08-11T23:43:05.607156",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16736",
      "title": "DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot\n  Segmentation",
      "authors": [
        "Shuai Chen",
        "Fanman Meng",
        "Xiwei Zhang",
        "Haoran Wei",
        "Chenhao Wu",
        "Qingbo Wu",
        "Hongliang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework\nthat addresses the fundamental challenge of effectively utilizing multi-modal\nguidance in few-shot segmentation (FSS). While existing approaches primarily\nrely on visual support samples or textual descriptions, their single or\ndual-modal paradigms limit exploitation of rich perceptual information\navailable in real-world scenarios. To overcome this limitation, the proposed\napproach leverages the Segment Anything Model (SAM) to systematically integrate\nvisual, textual, and audio modalities for enhanced semantic understanding. The\nDFR framework introduces three key innovations: 1) Multi-modal Decompose: a\nhierarchical decomposition scheme that extracts visual region proposals via\nSAM, expands textual semantics into fine-grained descriptors, and processes\naudio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a\nfusion strategy employing contrastive learning to maintain consistency across\nvisual, textual, and audio modalities while enabling dynamic semantic\ninteractions between foreground and background features; 3) Dual-path\nReconstruct: an adaptive integration mechanism combining semantic guidance from\ntri-modal fused tokens with geometric cues from multi-modal location priors.\nExtensive experiments across visual, textual, and audio modalities under both\nsynthetic and real settings demonstrate DFR's substantial performance\nimprovements over state-of-the-art methods.",
      "published_date": "2025-07-22T16:21:32+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16736v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16736v1",
      "latex_url": "http://arxiv.org/src/2507.16736v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Semantic segmentation serves as a cornerstone for visual scene understanding, with deep learning approaches~ achieving remarkable success through large-scale supervised training. Despite these advances, the requirement for extensive pixel-wise annotations poses significant challenges when generalizing to novel categories. Therefore, Few-shot segmentation (FSS) emerges as a promising paradigm to address this limitation by learning to segment unseen categories from limited labeled examples.\n\n {figure}\n  \n  [width=0.99 ]{imgs/motivation.pdf}\n  {Illustration of evolution of FSS frameworks: from visual-only/visual-textual paradigms to our proposed multi-modal decomposition-fusion-reconstruction architecture incorporating audio signals.}\n\n {figure}\n\nRecent progress in FSS has witnessed an evolution from purely visual approaches~ to visual-textual based frameworks~, demonstrating the effectiveness of leveraging linguistic semantics~ for generalization. As illustrated in Figure~, while existing methods have predominantly focused on either visual-only or visual-textual paradigms, real-world scenarios inherently contain rich perceptual information beyond these modalities. Particularly, audio signals~, which encode temporal-dynamic characteristics and object-specific acoustic patterns, remain largely unexplored in FSS despite their potential to provide complementary semantic cues. This observation motivates us to develop a comprehensive multi-modal few-shot segmentation (MMFSS) framework that systematically integrates audio information with visual and textual modalities, as depicted in the bottom part of Figure~.\n\nThe integration of multiple heterogeneous modalities for FSS presents two fundamental challenges. First, different modalities exhibit distinct structural characteristics, i.e., visual features are spatially organized and fine-grained, textual embeddings capture hierarchical semantics (category, attributes, and context), and audio signals encode temporal-frequency patterns. Establishing effective correspondence across these heterogeneous representations while preserving modality-specific discriminative properties requires careful architectural design. Second, conventional multi-modal fusion strategies face unique challenges in few-shot scenarios, where maintaining semantic consistency across modalities becomes particularly crucial yet difficult due to limited training samples. This limitation necessitates a principled approach to align and validate cross-modal feature representations while maximizing the utility of sparse labeled data.\n\nWe address these challenges through DFR, built upon the foundation of SAM's~ powerful visual understanding and LanguageBind's~ cross-modal alignment capabilities. Our approach introduces three key innovations: (1) a multi-modal decomposition scheme that systematically extracts and enriches features across modalities through SAM-based region proposals, LLM-guided semantic expansion, and AudioLDM-generated acoustic embeddings; (2) a contrastive fusion mechanism that maintains modality consistency through InfoNCE loss while enabling dynamic interactions between foreground and background features; and (3) a dual-path reconstruction module that adaptively integrates semantic tokens with geometric prompts derived from multi-modal location priors. Our primary contributions are:\n\n {itemize}\n   A novel multi-modal FSS framework that systematically integrates and aligns visual, textual, and audio modalities through a unified architecture, establishing a new paradigm for real-world segmentation tasks.\n   A hierarchical decomposition and progressive fusion mechanism that enables fine-grained cross-modal feature learning while preserving modality-specific characteristics through contrastive regularization.\n   Extensive validation demonstrates DFR's substantial performance gains across both synthetic and real audio settings, achieving 7.3% and 2.2% mIoU improvements (1-shot and 5-shot) on PASCAL-5i with synthetic audio, and 4.8% and 3.3% mIoU improvements (0-shot and 1-shot) on real audio-visual segmentation dataset AVS-V3.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.371,
      "weak_supervision_score": 0.388,
      "diffusion_reasoning_score": 0.427,
      "distributed_training_score": 0.369,
      "datasets_score": 0.389,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a framework for multi-modal few-shot segmentation that integrates visual, textual, and audio modalities using models like SAM and AudioLDM for feature extraction and fusion. While AudioLDM may involve diffusion processes for generating audio embeddings, the paper does not adapt diffusion for iterative refinement in solving complex logical tasks or Chain-of-Thought reasoning. Instead, it focuses on segmentation tasks, making it unrelated to the specified topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668316",
      "updated_at": "2025-08-11T23:43:05.606974",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16743",
      "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud\n  Completion Under Corruption",
      "authors": [
        "Keneni W. Tesema",
        "Lyndon Hill",
        "Mark W. Jones",
        "Gary K. L. Tam"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions",
      "published_date": "2025-07-22T16:34:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16743v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16743v1",
      "latex_url": "http://arxiv.org/src/2507.16743v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Point cloud completion is the task of completing a partial point cloud input so that it fully represents a 3D shape . It is a critical component in various tasks such as object recognition, 3D reconstruction, and point cloud pre-training . These tasks have practical applications in diverse fields including autonomous driving , robotics , and augmented reality , among others.\n\n {figure}[th!]\n \n [width= ]{Images/mainbeforefinetuning-Page-6.drawio.png}\n {-0.15in}\n {We propose a corrupted point cloud completion benchmark dataset (CPCCD) and a robust point cloud completion network, DWCNet.}\n {-0.25 in}\n {figure}\n\nIn recent years, research in point cloud completion has witnessed considerable advancements, with a primary focus on deep learning-based techniques . Algorithms like ODGNet and AdaPoinTr have demonstrated promising results on datasets such as PCN , MVP and KITTI . These techniques heavily depend on comprehensive training data from datasets like ShapeNet .\n\nSupervised point cloud completion differs from other point cloud learning tasks such as segmentation or object detection in the type of supervision it requires. While segmentation and detection rely on labeled annotations (e.g., class labels or bounding boxes), completion networks require clean and complete 3D point clouds as ground truth during training. However, obtaining such high-quality reference data from real-world scans is extremely challenging. As a result, most existing methods are trained and evaluated on synthetic datasets. Although these datasets are diverse and consistent , they are typically generated by uniformly sampling 3D meshes , which makes them overly clean and unrepresentative of the noise and artifacts found in real-world data . This leads to two key observations: (1) the lack of a benchmark dataset designed to systematically evaluate robustness against corrupted inputs, and (2) the inability of current completion methods to generalize well to real-world-like corruptions.\n\nThere have been limited attempts to incorporate noise and corruptions into partial point clouds for completion .\nHowever, these efforts address only Gaussian noise, which represents just one of the many potential corruptions in real-world point clouds. Furthermore, the fidelity of synthetic partial point clouds used to train completion algorithms remains subpar compared to real-world scans. Even the real-world dataset most commonly used for evaluation, KITTI , lacks environmental corruptions . Point clouds from the ScanNet dataset , often used for evaluation , are preprocessed before completion, and the results are typically qualitative, leading to subjective assessments. Thus, there is a clear need for a corrupted benchmark dataset to enable objective and qualitative robustness evaluation.\n\nTo address the first issue, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD) (Figure~). We classify the corruptions observed in the real-world ScanObjectNN dataset (Figure~), then mimic and incorporate these corruptions into the clean partial point clouds from the PCN dataset. We categorize the corruptions into two types: External Corruptions, originating from points belonging to other objects or the background, and Internal Corruptions, which displace or distort points from the target object (shown in Figure ).\nUnlike previous works , which use fixed corruption levels, we introduce randomness by allowing a range of values for the defining parameters, within spatial constraints. This design aims to reflect the unpredictable nature of noise and corruption in real-world point cloud scans, which is lacking in current completion benchmark datasets.\n\nTo address the lack of robust completion algorithms, we introduce Denoising-While-Completing (DWCNet). DWCNet utilizes a novel Noise Management Module (NMM) to classify features from a noisy partial point cloud into clean and noisy categories, alleviating the issues of outliers. Clean features are filtered through contrastive learning in feature space. NMM employs Multi-Head Self-Attention (MHSA) to determine structural relations and multi-scale convolutions to capture noise at different scales. This integrated approach ensures that the point cloud is both denoised and completed in a single, cohesive step, enhancing output quality.\nOverall, our contributions are:\n {itemize}\n\n { }{-1mm}\n  We formulate and systematically approach an existing but underexplored problem in point cloud completion: the challenge of completing highly corrupted (noisy) partial point clouds.\n  We introduce a novel corrupted point cloud completion dataset (CPCCD) as the first robustness benchmark in the field of point cloud completion.\n  We offer the first systematic evaluation of the robustness of completion networks, examining how robustness relates to different types of corruptions and network architectures.\n  We introduce DWCNet, a completion algorithm that integrates denoising and completion through a novel Noise Management Module, producing relatively clean, complete point clouds from noisy inputs. DWCNet achieves state-of-the-art results on the PCN, CPCCD, and ScanObjectNN datasets, demonstrating robustness on corrupted data.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.422,
      "diffusion_reasoning_score": 0.349,
      "distributed_training_score": 0.382,
      "datasets_score": 0.403,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper focuses on point cloud completion with noisy inputs but does not primarily involve weak supervision techniques. It trains models using clean, complete ground truth from synthetic datasets, rather than programmatically generated or noisy labels, making the connection indirect.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper directly contributes to dataset research by introducing the Corrupted Point Cloud Completion Dataset (CPCCD), detailing its creation through corruption of existing datasets, and using it for benchmarking and evaluating robustness in point cloud completion tasks.",
      "summary": "This paper addresses the challenges of point cloud completion in real-world scenarios by introducing the Corrupted Point Cloud Completion Dataset (CPCCD) to evaluate robustness against various corruptions, and proposing DWCNet, a novel network that integrates denoising and completion through a Noise Management Module using contrastive learning, self-attention, and multi-scale convolutions. The methodology involves corrupting synthetic datasets to mimic real-world noise and testing DWCNet, which achieves state-of-the-art performance on both clean and corrupted datasets, demonstrating improved generalization and effectiveness in handling multiple simultaneous degradations.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a new dataset and a novel technique that combines denoising and completion to handle multiple real-world corruptions, significantly advancing the state-of-the-art in point cloud completion by addressing an underexplored problem.",
      "impact_score": "High",
      "impact_justification": "The work's creation of a robustness benchmark and an effective integrated method could influence future research and applications in 3D vision, particularly in fields like autonomous driving and robotics, by improving the reliability of point cloud processing in corrupted environments.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a valuable contribution with practical innovations in handling real-world point cloud corruptions, making it essential for researchers in computer vision to understand advancements in robust completion techniques.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f32ae39f30181cd737af7b5ced35f6b7d854529c",
      "h_index_fetch_method": "title_search",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 49,
      "average_h_index": 24.5,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Zhaoxuan Zhang",
          "profile_url": "https://www.semanticscholar.org/author/10438891",
          "h_index": 7
        },
        {
          "name": "Xiaoguang Han",
          "profile_url": "https://www.semanticscholar.org/author/1763245",
          "h_index": 36
        },
        {
          "name": "B. Dong",
          "profile_url": "https://www.semanticscholar.org/author/143864583",
          "h_index": 49
        },
        {
          "name": "Tong Li",
          "profile_url": "https://www.semanticscholar.org/author/2115465134",
          "h_index": 5
        },
        {
          "name": "Baocai Yin",
          "profile_url": "https://www.semanticscholar.org/author/1714354",
          "h_index": 44
        },
        {
          "name": "Xin Yang",
          "profile_url": "https://www.semanticscholar.org/author/2150440228",
          "h_index": 6
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668338",
      "updated_at": "2025-08-11T23:45:15.973962",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16746",
      "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
      "authors": [
        "Ang Li",
        "Charles Wang",
        "Kaiyu Yue",
        "Zikui Cai",
        "Ollie Liu",
        "Deqing Fu",
        "Peng Guo",
        "Wang Bill Zhu",
        "Vatsal Sharan",
        "Robin Jia",
        "Willie Neiswanger",
        "Furong Huang",
        "Tom Goldstein",
        "Micah Goldblum"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
      "published_date": "2025-07-22T16:35:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16746v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16746v1",
      "latex_url": "http://arxiv.org/src/2507.16746v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Human cognition naturally integrates multimodal thought processes when solving complex problems. For example, a high school student sketches diagrams to solve geometry or physics problems, an engineer creates diagrams to design and debug workflows, and a data scientist generates plots to better understand data. These visual aids are central to effective problem solving. While recent vision-language models (VLMs) have shown strong performance on multimodal tasks like visual question answering, their reasoning traces remain predominantly textual. Enabling models to explicitly reason in the visual space, Visual Chain of Thought ( ), remains a fundamental open challenge. Unlocking   may improve reasoning performance in domains where visual intuition is relevant and may make the reasoning patterns expressed by models more interpretable to humans.\n\nRecent advances in frontier multimodal models  {team2023gemini, hurst2024gpt, bai2025qwen2, openai2025o3o4mini, team2024chameleon, chern2024anole, sun2024generative, deng2025emerging} have made   feasible primarily through agentic pipelines that leverage external tools ( , Python functions, or expert vision models) for visual programming  {suris2023vipergpt}, such as generating sketches for geometry, algorithms, and spatial reasoning tasks  {hu2024visual, openai2025thinkingwithimages}, or bounding boxes for fine-grained visual tasks  {shao2024visual, wu2024v, zheng2025deepeyes}. An emerging possibility is innate visual reasoning where models directly generate explicit visual tokens during their thinking process  {li2025imagine, chern2025thinking, xu2025visual}. However, current VLMs with interleaved text and image generation capabilities  {team2024chameleon, chern2024anole} either fail to generate useful visual aids for reasoning, or are not trained for such multimodal generation inherently during the reasoning process  {deng2025emerging}, making reinforcement learning approaches to reasoning infeasible.  {li2025imagine} demonstrate   in synthetic mazes by training specialist models, but we remain far from foundation models capable of general high-quality  , largely due to the lack of large-scale diverse interleaved text and image reasoning training datasets.\n\n {figure}[t]\n  \n [width=0.66 ]{figs/pie.pdf}\n \n [width=0.3 ]{figs/reasoning_images_distribution.pdf}\n  {We curate a large-scale multimodal dataset by sourcing and cleaning raw traces from real-world domains, and generating synthetic examples using templated reasoning filled in by VLMs.   comprises 4 major categories and 18 subcategories, encompassing over 182K instances in total. A detailed breakdown of the data statistics appears in  {tab:data_statistics}.}\n\n {figure}\n\n {figure}[t]\n  \n  [width=1.0 ]{figs/cover_fig_2.pdf}\n  {Visual CoT helps answer complex visual reasoning questions, as illustrated by examples from  .}\n\n {figure}\nTo support the development of next generation vision language models that can explicitly reason with both text and visual modalities, we present  , a high quality dataset of interleaved text and image reasoning traces. Our dataset covers four main categories: scientific questions, 2D visual reasoning, 3D visual reasoning, and visual logic and strategic games, each containing multiple subdomains and task types as exemplified in  {fig:cover}.\nTo the best of our knowledge,   is the first dataset to provide diverse and logically coherent multimodal reasoning traces across such a wide range of domains. Unlike prior large-scale interleaved datasets that are primarily composed of web-scraped image-text pairs with weak semantic alignment and no explicit reasoning structure  {li2024omnicorpus, awadalla2024mint, zhu2023multimodal},   is carefully curated as a training resource in the spirit of high-quality text-based reasoning datasets.\nAt the same time, compared to the only existing open-source interleaved text visual reasoning dataset we are aware of,  ~ {shao2024visual}, which focuses on a single task of visual search,   introduces a much broader and more diverse set of tasks with richer reasoning trajectories. We provide a detailed comparison with other datasets below in  {tab:dataset-comparison}.\n\nIn total,   contains 182,384 samples. After fine-tuning  {Anole-7B}~ {chern2024anole} on our training set, we improved the accuracy on our in-distribution test set from 4.2% to 16.9%, delivering a 4 times relative performance improvement and a 12% gain in accuracy. When evaluating with benchmarks requiring visual reasoning, our anole model achieves an average of 4.9% improvement across seven challenging datasets, with a maximum gain of 13.1% on a visual logic benchmark, as shown in  {tab:main_results}. Furthermore, we fine-tune our dataset on  {Bagel-7B}~ {deng2025emerging}, a high-quality multimodal model that cannot in its original form generate interleaved text and images. After fine-tuning, the model is able to inherently generate high-quality   during its own reasoning process, making it well-suited for future RL training, as shown in qualitative examples in  {sec:models}. We release the weights of both models to facilitate further research.\n\n {table}[t]\n  \n  \n  {tabularx}{ }{@{} l L l L @{}}\n  \n Dataset & Primary Task & Modality & Limitations\n\n  \n GQA & Compositional visual QA & Image, Text & No visual CoT\n[2pt]\n ScienceQA & Multimodal science QA & Image/Diagram, Text & No visual CoT\n[2pt]\n MM-PhyQA & Physics Visual CoT & Image, Text & Physics data only, not open sourced\n[2pt]\n Visual CoT & Visual-search QA with bbox CoT & Image, Text & Limited to visual search tasks\n[2pt]\n CoT VLA & Robotics Visual CoT & Image, Action & No text reasoning\n[2pt]\n R1‑Onevision & A SFT and RL multimodal reasoning training dataset & Image, Text & No visual CoT\n[2pt]\n OmniCorpus & 10 B-level interleaved corpus & Image, Text & Noisy pretraining data\n[2pt]\n MINT-1T & 1 T-token web-scale interleaved data & Image, Text & Noisy pretraining data\n[2pt]\n  \n   & Diverse and high quality Visual CoT & Image, Text & Broad task coverage and CoT with explicit visual aids\n\n  \n  {tabularx}\n  {.3em}\n  {  introduces a broader set of high quality   traces compared with prior datasets and pipelines.}\n\n {table}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "content.tex",
      "rlhf_score": 0.307,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.457,
      "distributed_training_score": 0.376,
      "datasets_score": 0.471,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a dataset for interleaved vision-language reasoning and fine-tunes models for visual Chain of Thought, but it does not involve diffusion models, iterative refinement processes, or treating reasoning paths as entities for holistic correction. There is no mention of adapting diffusion techniques for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation, curation, and evaluation of the Zebra-CoT dataset, which includes 182,384 samples for multimodal reasoning. It details dataset methodologies, benchmarks model performance on it, and compares it to existing datasets, directly aligning with research on datasets for AI applications.",
      "summary": "The Zebra-CoT paper introduces a large-scale dataset comprising 182,384 samples designed to enhance Visual Chain of Thought (Visual CoT) in multimodal models by providing diverse, logically coherent interleaved text-image reasoning traces across categories such as scientific questions, 2D and 3D visual reasoning, and strategic games. The methodology involves curating real-world traces and generating synthetic examples, leading to key findings that fine-tuning models like Anole-7B and Bagel-7B on this dataset results in significant performance improvements, including up to a 12% accuracy gain on test sets and 13% on benchmarks, thereby addressing the lack of high-quality training data for visual reasoning.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new and diverse dataset for interleaved vision-language reasoning, significantly advancing the state-of-the-art by filling a critical gap in high-quality Visual CoT training data.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence a wide range of future research in multimodal models and commercial applications by providing an open-sourced dataset that enhances reasoning capabilities and model performance.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to computer vision and language research through its innovative dataset, making it essential for those working on multimodal reasoning to be aware of and potentially utilize.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/22ce04999bf346326ee19b4acfb24ffcac8cc110",
      "h_index_fetch_method": "full_id",
      "total_authors": 14,
      "authors_found": 14,
      "highest_h_index": 39,
      "average_h_index": 7.5,
      "notable_authors_count": 5,
      "author_h_indexes": [
        {
          "name": "Ang Li",
          "profile_url": "https://www.semanticscholar.org/author/2345194507",
          "h_index": 1
        },
        {
          "name": "Charles Wang",
          "profile_url": "https://www.semanticscholar.org/author/2373540004",
          "h_index": 0
        },
        {
          "name": "Kaiyu Yue",
          "profile_url": "https://www.semanticscholar.org/author/2372429102",
          "h_index": 0
        },
        {
          "name": "Zikui Cai",
          "profile_url": "https://www.semanticscholar.org/author/2346643861",
          "h_index": 1
        },
        {
          "name": "Ollie Liu",
          "profile_url": "https://www.semanticscholar.org/author/2065919693",
          "h_index": 7
        },
        {
          "name": "Deqing Fu",
          "profile_url": "https://www.semanticscholar.org/author/2135593484",
          "h_index": 7
        },
        {
          "name": "Peng Guo",
          "profile_url": "https://www.semanticscholar.org/author/2374321047",
          "h_index": 0
        },
        {
          "name": "Wang Bill Zhu",
          "profile_url": "https://www.semanticscholar.org/author/2332241527",
          "h_index": 0
        },
        {
          "name": "Vatsal Sharan",
          "profile_url": "https://www.semanticscholar.org/author/2798845",
          "h_index": 14
        },
        {
          "name": "Robin Jia",
          "profile_url": "https://www.semanticscholar.org/author/2261738428",
          "h_index": 4
        },
        {
          "name": "W. Neiswanger",
          "profile_url": "https://www.semanticscholar.org/author/2934259",
          "h_index": 27
        },
        {
          "name": "Furong Huang",
          "profile_url": "https://www.semanticscholar.org/author/2364558374",
          "h_index": 0
        },
        {
          "name": "Tom Goldstein",
          "profile_url": "https://www.semanticscholar.org/author/2279757591",
          "h_index": 5
        },
        {
          "name": "Micah Goldblum",
          "profile_url": "https://www.semanticscholar.org/author/121592562",
          "h_index": 39
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669711",
      "updated_at": "2025-08-11T23:46:14.863796",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16753",
      "title": "CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot\n  Segmentation",
      "authors": [
        "Shuai Chen",
        "Fanman Meng",
        "Chunjin Yang",
        "Haoran Wei",
        "Chenhao Wu",
        "Qingbo Wu",
        "Hongliang Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to\nlimited data and domain shifts. Recent foundation models like the Segment\nAnything Model (SAM) have shown remarkable zero-shot generalization capability\nin general segmentation tasks, making it a promising solution for few-shot\nscenarios. However, adapting SAM to CD-FSS faces two critical challenges:\nreliance on manual prompt and limited cross-domain ability. Therefore, we\npropose the Composable Meta-Prompt (CMP) framework that introduces three key\nmodules: (i) the Reference Complement and Transformation (RCT) module for\nsemantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module\nfor automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction\n(FAI) module for domain discrepancy mitigation. Evaluations across four\ncross-domain datasets demonstrate CMP's state-of-the-art performance, achieving\n71.8\\% and 74.5\\% mIoU in 1-shot and 5-shot scenarios respectively.",
      "published_date": "2025-07-22T16:42:23+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16753v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16753v1",
      "latex_url": "http://arxiv.org/src/2507.16753v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The rapid development of deep learning has revolutionized semantic segmentation across numerous fields. However, the requirement for extensive labeled data remains a significant bottleneck, particularly in specialized domains~ where annotation demands expert knowledge. To this end, few-shot semantic segmentation (FSS) has emerged as a promising paradigm for segmenting novel classes with limited labeled data, demonstrating impressive capabilities in scenarios where training and testing domains are aligned (e.g., $PASCAL-5^i$~ and $COCO-20^i$~.). However, when encountering significant domain shifts, such as from natural images to medical~ or satellite imagery~, existing FSS methods~ often struggle to maintain their performance. This limitation gives rise to a more challenging task: cross-domain few-shot segmentation (CD-FSS), which must simultaneously address both limited supervision and substantial domain shifts.\n\nThe emergence of foundation models, particularly the Segment Anything Model (SAM)~, offers new possibilities for addressing these challenges. Trained on an unprecedented scale of over 1 billion masks across diverse visual scenarios, SAM has demonstrated remarkable zero-shot generalization capabilities in general segmentation tasks. Its prompt-driven architecture and rich visual understanding make it particularly promising for CD-FSS, as it can potentially leverage its broad knowledge to bridge domain gaps. However, deploying SAM for CD-FSS faces two critical limitations. First, SAM's effectiveness heavily relies on manually crafted prompts for each test image, which introduces substantial human effort in large-scale cross-domain applications, making it operationally prohibitive for practical deployment. Second, despite its broad training distribution, SAM's performance degrades notably when encountering domains that substantially deviate from its training data. These limitations raise a fundamental question: how can we develop an automated, domain-adaptive prompting mechanism that maintains SAM's powerful segmentation capabilities while effectively bridging domain gaps?\n\nDrawing inspiration from cognitive findings that humans achieve cross-domain generalization by integrating multiple cognitive representations~, we introduce the Composable Meta-Prompt (CMP) framework, which systematically generates domain-adaptive meta-prompts through a flexible and composable mechanism that accommodates diverse segmentation references. Our framework consists of three synergistic modules designed to address the aforementioned challenges. First, the Reference Complement and Transformation (RCT) module leverages large language models to facilitate semantic expansion by identifying potential concurrent negative categories relative to the target class. Second, the Composable Meta-Prompt Generation (CMPG) module automatically synthesizes meta-prompts by integrating information from multiple sources in a composable manner, eliminating the need for manual prompt design while maintaining cross-domain adaptability. Third, the Frequency-Aware Interaction (FAI) module explores frequency-domain characteristics to address domain discrepancies, operating through two complementary mechanisms: Cross-domain Frequency Alignment (CDFA), which utilizes a memory bank of frequency statistics to align domain-specific characteristics, and Support-Query Frequency Enhancement (SQFE), which performs bidirectional amplitude spectrum interaction to reduce intra-domain variations between support and query samples. Our main contributions include:\n\n {itemize}\n   A composable meta-prompt generation mechanism that automatically synthesizes domain-adaptive prompts by integrating multiple information sources, eliminating the need for manual prompt design while maintaining cross-domain generalization ability.\n   A frequency-domain interaction approach that effectively bridges domain gaps through cross-domain frequency alignment and support-query in-domain frequency enhancement, providing a novel perspective on domain adaptation in CD-FSS tasks.\n   CMP attains SOTA mIoU of 71.8% and 74.5% in 1-shot and 5-shot settings respectively, evaluated on four challenging cross-domain datasets (DeepGlobe, ISIC2018, Chest X-ray, and FSS-1000).\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.35,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.399,
      "distributed_training_score": 0.342,
      "datasets_score": 0.362,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668347",
      "updated_at": "2025-08-11T23:43:05.606981",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16754",
      "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer\n  Support",
      "authors": [
        "Fangjian Lei",
        "Mariam El Mezouar",
        "Shayan Noei",
        "Ying Zou"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs",
      "published_date": "2025-07-22T16:46:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16754v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16754v1",
      "latex_url": "http://arxiv.org/src/2507.16754v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Programmers often rely on online resources for a wide range of development tasks, such as API usage, bug fixing, and understanding of code or programming concepts~. A significant portion of these help-seeking activities involves regular interaction with community-driven Q\\&A platforms like Stack Overflow (SO)~. Recently, the emergence of Large Language Models (LLMs) has begun to reshape how developers search for assistance in programming activities that developers increasingly prefer using conversational LLMs over traditional search methods like forums or search engines for programming assistance. Open-source LLMs such as LLaMA family~, have shown strong performance in code understanding and generation tasks, gaining increasing attention among software practitioners and researchers. These models offer the potential to serve as an alternative to traditional search on Q\\&A platforms, enabling more conversational and context-aware support during programming tasks.\n\nDespite the rising popularity of Large Language Models (LLMs) used for information seeking, there are growing concerns about the reliability and correctness of generated content commonly referred to as hallucination. Previous studies have shown that LLMs can learn incorrect information during training and later reproduce or even amplify these errors in the generated outputs~. LLMs are also capable of producing fabricated content that mimics truthful responses, which can be difficult to detect, especially for users without domain expertise~.\n\nTo mitigate hallucination, Retrieval-Augmented Generation (RAG) has emerged as a promising solution and has shown strong potential in improving the quality of responses generated by LLMs when a knowledge base with similar context is available for reference. RAG enhances LLMs by incorporating external knowledge retrieved from a document corpus into the generation process~. However, the effectiveness of RAG is highly dependent on the retriever's ability to identify relevant information. When the input question is novel or falls outside the scope of the retrieval corpus, existing RAG retrievers often struggle to extract useful content . Since existing RAG systems rely solely on the input question, they may fail to retrieve semantically relevant documents in such cases. The final answer depends largely on the LLM’s pre-trained knowledge and its ability to generalize. This limitation highlights the need for methods that generate informative answers even when relevant content cannot be retrieved, ensuring consistent performance across diverse questions.\n\n {figure*}[t]\n  \n [width=1.0 ]{figures/RQ_Overall_workflow_v6.pdf}\n  {Experimental Workflow. RAG KB = Stack Overflow Knowledge Base (3.4 M accepted-answer documents). Synthetic Question Set: 385 questions auto-generated from the KB (seen). Unseen Question Set: 5,510 new Stack Overflow questions posted after the KB snapshot (unseen).\n}\n\n {figure*}\nIn this paper, we aim to address the limitations of existing RAG approaches, which struggle with vague questions and often fail on novel questions.\n\nMore specifically, we explore two implementations of Retrieval-Augmented Generation (RAG): (1) a question-based approach that searches the knowledge base using the original question, and (2) the Hypothetical Document Embedding (HyDE) approach , which first generates a hypothetical answer to improve the relevance of retrieved content. The two RAG implementations are further characterized by three key design dimensions: the first dimension, retrieval target, determines whether content is retrieved directly from accepted answers or indirectly via similar questions. The second, content granularity, specifies whether the system retrieves full answers for broader context or individual sentences for more precise and relevant information. The third, similarity threshold, sets the semantic similarity score between the input and retrieved content, directly influencing the amount and quality of context for generation. These three design dimensions directly affect the amount and quality of context extracted from RAG knowledge base. Therefore, we conduct the experiments by systematically varying the dimensions to assess how different pipelines affect the quality of generated answers using LLMs based on RAG and identify the best-performing RAG pipeline that can extract relevant content for enhanced answer quality.\n\n In this paper, we aim to answer the following research questions:\n\n \n\nTo determine how different design dimensions impact the effectiveness of RAG, we systematically evaluate 7 RAG pipelines and 63 pipeline variants that vary in retrieval target, content granularity, and similarity threshold. We assess each pipeline in terms of both answer quality and retrieval coverage on the Synthetic Question Set. Our results show that the hypothetical-answer-based pipeline (HB1), which retrieves from full answers in the knowledge base, consistently achieves the best trade-off between high response quality and broad coverage. This pipeline is selected as the optimal pipeline for further research questions.\n\n \n \n\nDevelopers often pose novel questions that lack closely related content in the knowledge base, which limits the effectiveness of standard RAG methods. To address this, we extend our approach by dynamically decreasing the similarity threshold for each question until relevant context is retrieved. Evaluated on an unseen question set, dynamically decreasing the similarity threshold enables full coverage, ensuring every question receives relevant contextual from RAG. Results show that our method significantly improves answer quality over original Stack Overflow answers, with statistical analysis confirming the effectiveness of dynamic thresholding for unseen cases.\n\n \n \n\nGiven the diversity of available LLMs, it is important to understand whether our optimal RAG pipeline offers consistent benefits across models. We apply the pipeline to several open-source LLMs and compare its performance to standard zero-shot prompting. Our findings reveal that our optimal RAG pipeline robustly improves or matches answer quality across different models, demonstrating strong generalization and practical value for a variety of LLM-based applications.\n\nOur contributions are as follows:\n {itemize}[leftmargin=7pt,itemindent=0pt,labelsep=0.5em]\n   We present RAG frameworks for answering Java and Python questions, using Stack Overflow as a retrieval base and open-source LLMs for generating answers.\n   We evaluate RAG implementations and propose a HyDE approach to improve answer retrieval performance on both seen and unseen questions.\n   We provide an evaluation of multiple LLMs performance on developer questions in both matched (similar) and unmatched (unseen) scenarios.\n   We release our dataset and pipeline to support future research in RAG-based methods for software engineering questions. Our replication package is available at~.\n {itemize}\n\nThe remainder of this paper is organized as follows. Section details the proposed approach. Section presents the research questions. Section is the discussion section. Section addresses the threats to validity. Section presents previous related work. Finally, Section concludes the paper and outlines future work.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1_intro.tex",
      "rlhf_score": 0.434,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.451,
      "distributed_training_score": 0.351,
      "datasets_score": 0.35,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Retrieval-Augmented Generation (RAG) pipelines for improving LLM responses to developer questions, using techniques like HyDE for retrieval. It does not involve training models with human feedback or reinforcement learning; instead, it evaluates answer quality via LLM-as-a-judge, which is an assessment method, not RLHF. Thus, there is no alignment with RLHF concepts.",
      "weak_supervision_justification": "The paper uses auto-generated synthetic questions from a Stack Overflow knowledge base, which involves programmatically creating data that could be seen as noisy or imprecise, resembling weak supervision. However, the main contribution is on RAG pipeline designs and retrieval, not on training models with weak supervision as a core technique. This makes it only peripherally related.",
      "diffusion_reasoning_justification": "The paper discusses RAG and HyDE for retrieving and generating responses, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as described in diffusion-based reasoning. There are no components related to treating reasoning paths as entities for holistic correction.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668793",
      "updated_at": "2025-08-11T23:43:05.607066",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16761",
      "title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks",
      "authors": [
        "Marcel Kleinmann",
        "Shashank Agnihotri",
        "Margret Keuper"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. In this work, we\naddress these limitations by introducing anti-aliasing strategies using\nFLCPooling (FLC) and BlurPool (BP) to significantly improve explanation\nquality. Our experiments on chest X-ray datasets demonstrate that the modified\n$\\text{B-cos}_\\text{FLC}$ and $\\text{B-cos}_\\text{BP}$ preserve strong\npredictive performance while providing faithful and artifact-free explanations\nsuitable for clinical application in multi-class and multi-label settings. Code\navailable at: GitHub repository (url:\nhttps://github.com/mkleinma/B-cos-medical-paper).",
      "published_date": "2025-07-22T16:56:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16761v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16761v2",
      "latex_url": "http://arxiv.org/src/2507.16761v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Faithfulness and interpretability are critical prerequisites for deploying deep learning models in safety-critical domains such as healthcare, where decisions have direct implications for patient outcomes~. In particular, clinical adoption demands models whose reasoning processes can be understood and verified by medical professionals. However, most existing approaches in medical image processing rely on architectures that lack inherent interpretability, offering limited insight into the basis of their predictions~. As emphasized in~, knowing how a diagnosis was derived is highly valuable, especially in practice, where AI tools are used to support, not replace, radiologists. In this work, we explore B-cos networks~, which are inherently interpretable by design and generate class-specific contribution maps that visually indicate the evidence behind each prediction. Representative examples are shown in  {fig:teaser}, which compares our artifact-free, faithful, and interpretable explanations to other common approaches such as GradCAM and LayerCAM while displaying the need for our extensions when working with B-cos networks and chest X-rays simultaneously.\n\nThese models can directly show which part of the image caused network activations, enabling an understanding of why a classification was made~. Unlike post-hoc approaches such as GradCAM~ or LayerCAM~, which provide coarse and sometimes misleading heatmaps that have insufﬁcient mechanistic explanation of the decision-making process~, B-cos networks offer inherently interpretable, class-specific contribution maps that yield clearer, more faithful visual explanations.\nThis can help radiologists verify predictions, improve clinical workflows, and reduce diagnostic errors~.\nHowever, standard B-cos models face a critical limitation: the generated explanation maps often exhibit grid artifacts, making them visually unreliable for clinical use.\n\nIn this work, we address these limitations by incorporating anti-aliasing techniques such as FLCPooling and BlurPool to improve visual clarity.\nAdditionally, while the original B-cos models support multi-label classification, they were proposed for multi-class classification.\nThus, we use the framework from , to obtain multi-label classification since chest X-rays commonly involve multiple co-occurring conditions.\nWe refer to the resulting anti-aliased, multi-label capable model as $B-cos_FLC$ and $B-cos_BP$.\nWhile $B-cos_FLC$ combines B-cos models with FLCPooling~, $B-cos_BP$ combines it with BlurPooling~, both effective anti-aliasing techniques.\nThese anti-aliasing techniques help improve the explanation maps from the B-cos network, since they replace the artifact-producing downsampling operation with an artifact-free downsampling path.\nThus, removing the artifacts in the explanation maps of B-Cos networks.\nOur main contributions are:\n\n {itemize}\n  A practical evaluation of B-cos networks for interpretable and clinically relevant chest X-ray disease detection.\n  Application-specific modifications to B-cos networks by integrating anti-aliasing methods (FLCPooling and BlurPool) to produce spectral artifact-free explanations suitable for diagnostic use.\n  We show that our proposed framework can be adopted for both multi-class and multi-label classification, proving useful in critical medical applications.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.329,
      "datasets_score": 0.313,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669104",
      "updated_at": "2025-08-11T23:43:05.607115",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16768",
      "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
      "authors": [
        "Ran Wang",
        "Xiaoxuan Liu",
        "Hao Ren",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
      "published_date": "2025-07-22T17:13:47+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16768v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16768v1",
      "latex_url": "http://arxiv.org/src/2507.16768v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "With recent advances in large language models (LLMs), their applicability has been extended to increasingly complex tasks such as code generation~, function calling~, and agent-based workflows~. These tasks require structured generation to enforce a specific output format. Furthermore, in some cases, since the LLM operates as one component within a larger pipeline, its output must adhere to predefined rules to ensure that downstream components can reliably consume, parse, and build upon the model's output for continued execution.\n\nTo support structured output, users employ front-end languages to specify decoding constraints. Mainstream libraries adopt context-free grammars (CFGs) as a standard mechanism to express constraints. To efficiently enable CFG-based decoding, state-of-the-art systems~ implement both a lexer and a parser to accurately interpret grammar rules. They also utilize state machines to track and enforce generation states in real time.\n\nDue to the inherent complexity of constrained decoding, achieving high efficiency remains a significant challenge. The overall latency can be attributed to three stages. The first is grammar compilation, which involves lexing and parsing the front-end language to generate the internal data structures representing the constrained format. The second stage is state tracking and transition, where the system parses each generated token and updates the state machine accordingly. Modern systems typically use pushdown automata (PDA) to support CFGs, which requires maintaining a separate stack and state machine for each request throughout its lifetime. The final stage is mask creation, which produces a GPU-resident binary tensor of vocabulary size—where ones indicate allowed tokens and zeros indicate disallowed ones. This mask is applied to the logits prior to sampling to ensure that only valid tokens are considered during generation.\nRecent efforts have sought to optimize these individual components. For example,\nXGrammar~ optimizes the automata structure to accelerate rule matching at runtime.\n\nDespite these advances, performance remains a bottleneck.\nAs shown in Section~, for a specific workload, structured decoding introduces over $120,000$ ms of TTFT latency with Outlines and over $2,700$ ms with XGrammar. This overhead is substantial, especially given that the TTFT without structured decoding is only $525.55$ ms.\n\n {figure}[h]\n  \n  [width=0.9 ]{figures/intro-example-1.pdf}\n  {Structured Abstract Generation. Top: Input HTML document and requirements. Bottom: Output summary in HTML-like format.}\n\n  [width=0.9 ]{figures/intro-example-2.pdf}\n  {Reference Lookup. Left: semantic topics and document. Right: Output in JSON format, linking outline topic indices to relevant document node paths.}\n\n {figure}\n\nOn the other hand, a key observation overlooked by prior work is that structured decoding is often customized for highly specific downstream tasks that embed substantial prior knowledge.\nWe now present two concrete examples from production workloads to illustrate what prior knowledge is and how it can be leveraged to accelerate structured decoding.\n {itemize}\n   Outline Generation: In this setting, the LLM functions as an Internet article editor. Given a raw HTML document as input, the model generates an abstract that summarizes each section while preserving the original structure. As illustrated in Figure~, the output must meet two requirements: (1) it includes special HTML tags to support direct visualization, and (2) it maintains the original hierarchical section structure.\n   Reference Lookup: As shown in Figure~, the LLM identifies reference paragraphs relevant to a given topic. The input is the list of topics and the document. The output is a list of JSON objects, where each object specifies a topic and the corresponding section IDs that discuss it. The output must adhere to the JSON format, using fixed keys— {\"topic\"} and  {\"reference\"}—with values restricted to section IDs.\n {itemize}\n\nFor outline generation, the prior knowledge comes from domain-specific constraints—namely, the fact that the output will contain a fixed set of HTML tags in a predictable order. This structural knowledge allows us to precompile the format offline, rather than constructing it from scratch for each request, which would be slow due to the length and complexity of the resulting state machine (as shown in Section~).\nFor reference lookup, we know in advance that the output will be in a simple JSON format with specific keys and values limited to section IDs—free of deep nesting or special characters. As a result, we can predefine the transition logic from commonly used regular expressions (e.g.,  {  d+}) to internal operators, and composite those basic expression snippets to the finally required state machine.\n\nMoreover, since we know the output does not have nesting structured, we can employ finite state machines (FSMs) instead of pushdown automata.\nThis optimization significantly reduces transition latency, as demonstrated in Section~.\nIn summary, prior knowledge includes:\n(1) Domain knowledge, which simplifies grammar compilation;\n(2) Grammar fragments, which can be reused at runtime; and\n(3) Language scope, which guides the selection of the most efficient state-tracking mechanism.\n\nBased on the observations, we build  ~as shown in Figure~.  ~consists of three main components: the backend parser, the frontend parser, and the state tracking and mask generation module. Users first define a structure template, which is sent to the backend parser to generate the structure factory. This factory contains the core structural elements—parameterized but not yet instantiated—that may be needed by incoming requests. You can think of the structure factory as a set of modular building blocks that online requests will later assemble into concrete structures. When a request arrives, its arguments are passed to the frontend parser, which uses them—along with the structure factory—to construct a state machine tailored to the request. As each token is generated, the state machine is updated accordingly to produce the appropriate mask for the next generation step.\n\nFigure~ shows the way of using  . The user initializes a  {Backend} with a structure specification file ( {structure.txt}), which encodes the expected structural pattern. This template includes nested section formats (e.g., SECTION, SUBSECTION, SUBSUBSECTION).\nThen a state machine is built by  {build\\_operators}, which tracks states and generates masks.\nAt each generation step, the current token ID is passed to  ~to update the internal state machine. Based on this state,  {vocab\\_mask} provides the valid next-token mask that enforces the defined structure during generation.\n\n {figure}[]\n  \n  {subfigure}[c]{0.45 }\n  \n  [width= ]{figures/api.pdf}\n  {Example of using  .}\n\n  {subfigure}\n\n  {subfigure}[c]{0.5 }\n  \n  [width= ]{figures/sys.pdf}\n  { ~workflow.}\n\n  {subfigure}\n\n {figure}\n\nIn summary, we make the following contributions:\n {itemize}\n   Domain-aware simplification: We observe that many structured generation tasks embed strong domain-specific constraints. By leveraging this prior knowledge, we simplify grammar design and reduce the complexity of runtime enforcement, enabling efficient, tailored decoding for specialized applications.\n   Constraint decomposition: We propose a novel decomposition of constraints into static (predefined) and dynamic components. The static structure is precompiled offline into reusable templates, while the dynamic arguments are injected at runtime and might be composed using predefined grammar snippets to accelerate compilation.\n\n   Efficient implementation and release: We implement  , an optimized library for constrained decoding with support for offline structure compilation, dynamic instantiation, and FSM-based tracking.  ~outperforms the state-of-the-art systems by over 250$ $ for TTFT and up to 2.33$ $ for TPOT on both public benchmarks and real-world production workloads. We release all code, templates, and datasets for reproducibility.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "introduction.tex",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.379,
      "diffusion_reasoning_score": 0.436,
      "distributed_training_score": 0.36,
      "datasets_score": 0.268,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is on accelerating structured decoding for LLMs by leveraging prior knowledge, constraint decomposition, and efficient state tracking, focusing on formats like HTML and JSON. It does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning for Chain-of-Thought tasks, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667858",
      "updated_at": "2025-08-11T23:43:05.606873",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16779",
      "title": "Improving U-Net Confidence on TEM Image Data with L2-Regularization,\n  Transfer Learning, and Deep Fine-Tuning",
      "authors": [
        "Aiden Ochoa",
        "Xinyuan Xu",
        "Xing Wang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With ever-increasing data volumes, it is essential to develop automated\napproaches for identifying nanoscale defects in transmission electron\nmicroscopy (TEM) images. However, compared to features in conventional\nphotographs, nanoscale defects in TEM images exhibit far greater variation due\nto the complex contrast mechanisms and intricate defect structures. These\nchallenges often result in much less labeled data and higher rates of\nannotation errors, posing significant obstacles to improving machine learning\nmodel performance for TEM image analysis. To address these limitations, we\nexamined transfer learning by leveraging large, pre-trained models used for\nnatural images.\n  We demonstrated that by using the pre-trained encoder and L2-regularization,\nsemantically complex features are ignored in favor of simpler, more reliable\ncues, substantially improving the model performance. However, this improvement\ncannot be captured by conventional evaluation metrics such as F1-score, which\ncan be skewed by human annotation errors treated as ground truth. Instead, we\nintroduced novel evaluation metrics that are independent of the annotation\naccuracy. Using grain boundary detection in UO2 TEM images as a case study, we\nfound that our approach led to a 57% improvement in defect detection rate,\nwhich is a robust and holistic measure of model performance on the TEM dataset\nused in this work. Finally, we showed that model self-confidence is only\nachieved through transfer learning and fine-tuning of very deep layers.",
      "published_date": "2025-07-22T17:27:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16779v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16779v1",
      "latex_url": "http://arxiv.org/src/2507.16779v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Nanoscale defects, such as grain boundaries, precipitates, and dislocations, play a critical role in controlling the properties and functionality of solid-state materials. Transmission electron microscopy (TEM) has become an irreplaceable tool for investigating these defects, owing to its ultrahigh spatial resolution (sub-Angstrom) . Furthermore, recent advances in faster electron detection and data processing have enabled a big-data approach to characterization techniques such as in-situ TEM and 4D-scanning transmission electron microscopy (4D-STEM) . Terabytes of data can be created in a single hour during 4D-STEM or in-situ TEM experiments . However, extracting meaningful insights from the data has quickly become an enormous bottleneck, since the traditional method of manual image analysis is time-consuming, subject to human bias, and cannot scale with the growing data volume . Therefore, developing high-quality automated approaches for TEM image analysis is of paramount importance.\n\nSince the discovery of convolutional neural networks (CNN), machine learning (ML) models have been able to outperform not only traditional computer vision techniques, but even human abilities on certain image analysis tasks . In the context of TEM, binary segmentation utilizing models from the U-Net family enables pixel-level classification of defect structures, which is especially important for identifying continuous defects like grain boundaries and phase interfaces . However, comparing applications to more typical datasets, the performance of CNN models for TEM image analysis remains inferior. For instance, most of the literature involving U-Net based models, including the seminal paper and popular derivatives like U-Net++ , is focused on medical imaging [9]. With these datasets, it is very common to see F1-scores in the range 0.85–0.95. Similar performance can also be seen in applications like forestry , crack detection , satellite imaging , and plant disease . TEM applications, however, usually report lower F1-scores in range 0.5–0.8 .\n\nOne key reason for this performance gap stems from the inherent complexity of TEM image analysis. Unlike optical images, contrast in TEM arises from multiple mechanisms, making feature identification highly sensitive to imaging conditions and sample characteristics. Consequently, TEM datasets differ from conventional ML datasets in two major ways. (1) Smaller dataset size. TEM datasets typically contain tens to hundreds of labeled images due to the time-intensive nature of annotation. In contrast, datasets like ImageNet contain more than 14 million annotated images. (2) Greater annotation ambiguity. The complex contrast mechanisms and intricate defect structures often result in large variations in how these defects appear in TEM images. This makes it challenging to annotate all defects within the images, leading to considerable uncertainty and inconsistency in human-labeled data, which is nonetheless treated as \"ground truth\" during ML model training .\n\n {figs/fig1}\n\nTo address these challenges, we explored the use of transfer learning by leveraging advanced ML models pre-trained on large datasets for use with TEM analysis. In particular, we investigated promising combinations of a pre-trained EfficientNet encoder with a U-Net++ decoder, which helped achieve the best performance in 2023 and 2024 for edge detection in regular optical image datasets . We also introduced two novel metrics, prediction certainty and prediction abundance, describing the ability of a model to make predictions with high class probabilities. Together, they define a model’s self-confidence and are independent of ground truth accuracy. We demonstrated that U-Net performance can be substantially improved using a pre-trained encoder, fine-tuning of deep layers, and L2-regularization. Although F1-scores remained limited due to ground truth flaws and uncertainty, improved model self-confidence led to a 57% increase in the number of detected defects for the dataset tested, significantly enhancing the accuracy of defect statistics.\n\nAs a practical example, we applied our workflow to a TEM image dataset of nanocrystalline UO$_2$ samples used to study grain growth as function of temperature and heavy ion irradiation dose. UO$_2$ is the primary fuel used in current nuclear reactors, and many of its key material properties, such as thermal conductivity, fission gas retention, and fracture toughness, are governed by grain size. Therefore, it is critical to establish reliable correlations between grain size of UO$_2$ and its irradiation conditions for predictive modeling of nuclear fuel performance. The accuracy of said correlations principally depends on the quality of grain statistics, which in turn requires large and representative datasets. As such, hundreds of TEM images like those in Fig. 1 were collected at various dose levels and temperatures. Processing these images by hand is infeasible, so U-Net models were used to automate segmentation of grain boundaries.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "01_intro.tex",
      "rlhf_score": 0.319,
      "weak_supervision_score": 0.393,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.347,
      "datasets_score": 0.319,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669113",
      "updated_at": "2025-08-11T23:43:05.607116",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16782",
      "title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection",
      "authors": [
        "Changhao Li",
        "Xinrui Chen",
        "Ji Wang",
        "Kang Zhao",
        "Jianfei Chen"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Quantization is a key technique to reduce network size and computational\ncomplexity by representing the network parameters with a lower precision.\nTraditional quantization methods rely on access to original training data,\nwhich is often restricted due to privacy concerns or security challenges.\nZero-shot Quantization (ZSQ) addresses this by using synthetic data generated\nfrom pre-trained models, eliminating the need for real training data. Recently,\nZSQ has been extended to object detection. However, existing methods use\nunlabeled task-agnostic synthetic images that lack the specific information\nrequired for object detection, leading to suboptimal performance. In this\npaper, we propose a novel task-specific ZSQ framework for object detection\nnetworks, which consists of two main stages. First, we introduce a bounding box\nand category sampling strategy to synthesize a task-specific calibration set\nfrom the pre-trained network, reconstructing object locations, sizes, and\ncategory distributions without any prior knowledge. Second, we integrate\ntask-specific training into the knowledge distillation process to restore the\nperformance of quantized detection networks. Extensive experiments conducted on\nthe MS-COCO and Pascal VOC datasets demonstrate the efficiency and\nstate-of-the-art performance of our method. Our code is publicly available at:\nhttps://github.com/DFQ-Dojo/dfq-toolkit .",
      "published_date": "2025-07-22T17:28:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16782v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16782v1",
      "latex_url": "http://arxiv.org/src/2507.16782v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Object detection neural networks are integral to a wide array of computer vision applications, ranging from autonomous driving to surveillance systems~ {mao20233d, balasubramaniam2022object, oguine2022yolo, mishra2016study}. As the demand for deploying deep neural networks on resource-constrained devices continues to grow, quantization has emerged as a critical technique to reduce network size and computational complexity while maintaining performance~ {chen2019v2, deng2020model, han2015deep, wang2022makes}. However, traditional quantization-aware training methods often require access to the entire original training data, which may become inaccessible if the data are very huge or subjected to privacy protection.~ {krishnamoorthi2018quantizing,nagel2021white}. In these scenarios, Zero-shot Quantization (ZSQ)~ {cai2020zeroq, nagel2019data, yvinec2023spiq, xu2020generative, liu2021zero} presents a promising solution, enabling the quantization of neural networks without relying on original training data. These methods primarily train their model on synthetic data generated by inverting the network with randomly sampled labels.\n\nMost prior work in this area has focused on classification networks. Specifically, they use model inversion to synthesize data by optimizing the gauss noise image through gradient descent. Their loss functions are often designated to classification tasks, which generate classification task-specific images. For example, GDFQ  {xu2020gdfq} introduced a knowledge-matching generator to synthesize label-oriented data using cross-entropy loss and batch normalization statistics (BNS) alignment. TexQ  {chen2024texq} emphasized the detailed texture feature distribution in real samples and devised texture calibration for labeled image generation. PSAQ-ViT introduced a patch similarity aware strategy to invert labeled images from Vision Transformers for quantization. These methods leverage synthetic data generated from the full-precision network to calibrate ( , post-training quantization) or finetune the quantized network ( , quantization-aware training) for accurate quantization. A more comprehensive discussion of data-driven quantization and ZSQ are presented in Appendix~.\n\nRecently, ZSQ has been extended to downstream tasks such as object detection, but its application is limited due to inherent complexity. Classification tasks require only a randomly assigned category as the target label, but object detection demands that the target label comprise both the bounding box location and the classification label, making it difficult to determine. Consequently, existing approaches for detection networks drop the detection training loss and instead adopt a task-agnostic strategy for data generation and model quantization. For instance, PSAQ-ViT V2~ {li2023psaq} introduced an adaptive teacher–student strategy to generate task-agnostic images for finetuning the quantized model via knowledge distillation. Similarly, MimiQ~ {choi2024mimiq} proposed inter-head attention similarity and applied head-wise structural attention distillation to align the attention maps of the quantized network with those of the full-precision teacher across downstream tasks. CLAMP-ViT~ {ramachandran2024clamp} employed a two-stage approach, cyclically adapting between data generation and quantization. Although task-agnostic strategy enhances generalizability across different downstream tasks, it leads to a lack of task-specific bounding box size and location information in the object detection network, potentially resulting in suboptimal performance.\n\nWe argue that incorporating task-specific information into ZSQ can significantly increase its effect. By augmenting training loss with object categories and bounding box information, our method can outperform previous task-agnostic methods and, in some settings, may even achieve comparable results to networks trained with full real-data.\n\nThe proposed task-specific framework consists of two stages. In the generation stage, we introduce a novel bounding box and category sampling strategy to synthesize a calibration set from a pre-trained detection network, which reconstructs the location, size and category distribution of objects within the data without any prior knowledge. In the quantization stage, we integrate the detection training loss into the distillation process to further amplify the efficacy of quantized detection network finetuning.\n\nExtensive experiments on MS-COCO and Pascal VOC confirm the state-of-the-art performance of our method. For example, when quantizing YOLOv5-l to 6-bit, we achieve a 1.7% mAP improvement over LSQ trained with full real data. Furthermore, tests on YOLO11 and Swin Transformer models show our approach surpasses task-agnostic ZSQ by 2-3% in mAP across various quantization settings. Specifically, our contributions are threefold:\n\n {enumerate}\n  Drawback of task-agnostic calibration is revealed.\n\nWe emphasize task-specific synthetic images for zero-shot quantization of object detection networks. By developing a task-specific approach that optimizes both data synthesis and finetuning, we unlock the full performance potential of quantized object detection networks.\n\n  Task-specific object detection images synthesis.\nWe propose a bounding box sampling method tailored for object detection networks to reconstruct object categories, locations, and sizes in synthetic samples without any prior knowledge.\n\n  Task-specific quantized network distillation.\nWe integrate object detection task-specific finetuning into quantized network distillation, effectively restoring the performance of quantized object detection networks.\n\n {enumerate}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.34,
      "weak_supervision_score": 0.403,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.448,
      "datasets_score": 0.347,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves generating synthetic data and labels for quantization-aware training, which aligns with weak supervision by programmatically creating noisy or imprecise labels (e.g., bounding boxes and categories) without relying on hand-labeled data. However, the focus is primarily on quantization for object detection rather than exploring weak supervision as a core methodology.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper does not address distributed training, parallel computing, or multi-node strategies for accelerating model training; its contributions are centered on quantization techniques and synthetic data generation for object detection networks.",
      "datasets_justification": "below_threshold",
      "summary": "This paper introduces a task-specific zero-shot quantization-aware training framework for object detection networks to address the limitations of existing task-agnostic methods, which fail to incorporate essential object-specific information like bounding boxes and categories. The methodology involves two stages: first, synthesizing a task-specific calibration set using a novel bounding box and category sampling strategy from pre-trained models, and second, integrating detection-specific training into knowledge distillation to enhance quantized network performance; experiments on MS-COCO and Pascal VOC datasets show state-of-the-art results, with improvements up to 1.7% mAP over baselines.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique by developing a task-specific sampling strategy for zero-shot quantization in object detection, significantly advancing beyond task-agnostic methods and addressing a key gap in the field.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of quantized object detection, potentially improving efficiency in real-world applications like autonomous driving, though its influence may remain confined to specific computer vision domains.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to quantization techniques for object detection, making it essential for researchers in computer vision and AI efficiency to be aware of its advancements.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/45ce997532d496f745d501975e0de2662ceac117",
      "h_index_fetch_method": "title_search",
      "total_authors": 4,
      "authors_found": 4,
      "highest_h_index": 5,
      "average_h_index": 2.75,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Zhe Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2280855072",
          "h_index": 2
        },
        {
          "name": "Shu Chen",
          "profile_url": "https://www.semanticscholar.org/author/2348495136",
          "h_index": 0
        },
        {
          "name": "Jian Huang",
          "profile_url": "https://www.semanticscholar.org/author/2175213800",
          "h_index": 4
        },
        {
          "name": "Jie Ma",
          "profile_url": "https://www.semanticscholar.org/author/2268088081",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668356",
      "updated_at": "2025-08-11T23:45:18.373803",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16790",
      "title": "Enhancing Domain Diversity in Synthetic Data Face Recognition with\n  Dataset Fusion",
      "authors": [
        "Anjith George",
        "Sebastien Marcel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While the accuracy of face recognition systems has improved significantly in\nrecent years, the datasets used to train these models are often collected\nthrough web crawling without the explicit consent of users, raising ethical and\nprivacy concerns. To address this, many recent approaches have explored the use\nof synthetic data for training face recognition models. However, these models\ntypically underperform compared to those trained on real-world data. A common\nlimitation is that a single generator model is often used to create the entire\nsynthetic dataset, leading to model-specific artifacts that may cause\noverfitting to the generator's inherent biases and artifacts. In this work, we\npropose a solution by combining two state-of-the-art synthetic face datasets\ngenerated using architecturally distinct backbones. This fusion reduces\nmodel-specific artifacts, enhances diversity in pose, lighting, and\ndemographics, and implicitly regularizes the face recognition model by\nemphasizing identity-relevant features. We evaluate the performance of models\ntrained on this combined dataset using standard face recognition benchmarks and\ndemonstrate that our approach achieves superior performance across many of\nthese benchmarks.",
      "published_date": "2025-07-22T17:36:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16790v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16790v1",
      "latex_url": "http://arxiv.org/src/2507.16790v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure*}[!h]\n  \n  [width=0.99 ]{figures/DataCV-Page-1.drawio.png}\n  {Images from (a) Digi2Real dataset and HS-10K , showing different identities in these datasets. It can be seen that the distribution of images are very different in two datasets }\n\n {figure*}\n\nFace recognition technology has become a widely adopted method for biometric authentication, largely driven by advances in deep neural networks and the availability of large-scale training datasets . However, the use of these datasets has raised significant privacy concerns, particularly because many of them were compiled without obtaining informed consent from the individuals depicted. This practice poses serious legal and ethical challenges, especially in the context of data protection regulations such as the European Union's General Data Protection Regulation (GDPR). As a result, several legacy datasets have been withdrawn from public access. In response, there is a growing interest in using synthetic data for training face recognition models. This shift is reflected in the emergence of public benchmarks and competitions focused on synthetic face recognition datasets .\n\nMost existing efforts to generate synthetic face datasets rely on generative models such as StyleGAN , diffusion models , or graphics-based rendering pipelines . While generative models can produce high-quality images, they typically require large amounts of real data to train the generator networks and often use datasets like FFHQ to learn the underlying face distribution. In contrast, graphics-based approaches, such as DigiFace-1M utilize rendering pipelines to synthesize face images without the need for extensive real-image datasets or pretrained face recognition networks. These methods draw on techniques similar to those in , combining 3D facial geometry, textures, and hairstyles. This enables the generation of intra-class variations by altering pose, facial expression, illumination, and accessories. Notably, such pipelines offer the capability to synthesize a large number of unique identities with diverse intra-class variability and broad ethnic representation. Additionally, they support controlled generation by allowing specific attributes to be explicitly defined during synthesis.\n\nDespite these advancements, models trained exclusively on synthetic datasets generally underperform compared to those trained on real-world data. A common limitation in existing synthetic datasets is that they are often generated entirely using a single generative model or pipeline. This can introduce generator-specific artifacts and limit the diversity of the synthesized data, ultimately affecting the model's generalization and recognition performance.\n\nIn this work, we investigate the effectiveness of training face recognition models on a combination of two distinct synthetic datasets to enhance diversity and mitigate generator-specific biases. We also present our submission to the DataCV ICCV Challenge, which focuses on generating large-scale training datasets for face recognition. The datasets used and the protocol will be made available publicly  { {https://gitlab.idiap.ch/biometric/code.datacv2025}}.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.397,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.389,
      "datasets_score": 0.478,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on generating and fusing synthetic face datasets for face recognition, mentioning diffusion models only as one of several generative techniques for image synthesis. It does not involve adapting diffusion processes for multi-step logical reasoning, Chain-of-Thought tasks, or iterative refinement in reasoning contexts. Thus, there is no connection to diffusion-based reasoning as defined.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution involves creating and fusing synthetic datasets to enhance diversity, reduce biases, and improve face recognition performance, which directly aligns with research on dataset creation, curation, and benchmarking. It introduces a new methodology for combining datasets, evaluates them on standard benchmarks, and discusses their analysis in the context of ML applications.",
      "summary": "This paper addresses ethical and privacy concerns in face recognition by proposing a method to fuse two distinct synthetic face datasets, generated using different backbones, to reduce generator-specific artifacts and enhance diversity in pose, lighting, and demographics. The authors evaluate models trained on this combined dataset against standard benchmarks, demonstrating superior performance and better generalization compared to those trained on single synthetic datasets, thereby emphasizing the benefits of dataset fusion for improving face recognition accuracy while maintaining ethical standards.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing synthetic datasets in a new way to address biases and enhance diversity, rather than introducing a entirely new problem or technique. This clever fusion approach advances the state-of-the-art incrementally without groundbreaking innovation.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence future research in synthetic data for face recognition by providing a practical method to improve dataset diversity and ethical compliance, potentially leading to citations and adaptations within the computer vision subfield. However, its applicability may remain niche and not extend broadly to other areas.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution that addresses important ethical issues in face recognition, making it valuable for researchers in computer vision and biometrics to understand and build upon. While not essential for all, it provides significant insights into improving synthetic data usage.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/592421ea8cffd4d6648ec62b35562d5bfbef23d4",
      "h_index_fetch_method": "full_id",
      "total_authors": 2,
      "authors_found": 2,
      "highest_h_index": 9,
      "average_h_index": 7.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Anjith George",
          "profile_url": "https://www.semanticscholar.org/author/2267244928",
          "h_index": 5
        },
        {
          "name": "Sébastien Marcel",
          "profile_url": "https://www.semanticscholar.org/author/2237967482",
          "h_index": 9
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668363",
      "updated_at": "2025-08-11T23:45:20.602911",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16792",
      "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation\n  Through Non-cooperative User Simulation",
      "authors": [
        "Roman Mayr",
        "Michel Schimpf",
        "Thomas Bohné"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems.",
      "published_date": "2025-07-22T17:40:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16792v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16792v1",
      "latex_url": "http://arxiv.org/src/2507.16792v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Dialogue-based human-computer interaction has become increasingly widespread with the rise of  {LLM} through systems such as ChatGPT~. Beyond the chat interfaces of the  {LLM} providers, dialogue systems are now deployed across various use cases - from task-oriented customer service, over mental health treatment~ to conversations with virtual companions like Replika~.\n\nWith millions of users and applications in critical domains such as healthcare and finance, ensuring the robustness and reliability of dialogue systems is crucial. However, testing and evaluating these systems remain persistent challenges~ {yoshino_overview_2023, rodriguez-cantelar_overview_2023}.\n\n {figure}\n  \n  [width=1.0 ]{chat-checker.drawio.pdf}\n  {Schematic overview of  {}. After connecting their target dialogue system, developers can generate user personas, run simulations, identify dialogue breakdowns, and obtain ratings.}\n\n {figure}\n\nDialogue Systems\nDialogue systems, also called chatbots, interact with users through natural language conversation~ {jurafsky_speech_2025}. These systems can be broadly categorized into  {TOD} designed for specific tasks like booking hotels, and conversational dialogue systems for open-domain, natural conversations  {deriu_survey_2021}. Modern dialogue systems increasingly rely on  {LLM}~ {xu_rethinking_2024, yi2024surveyrecentadvancesllmbased}, which blur the traditional distinction between these categories~ {jurafsky_speech_2025}.\n\nDialogue System Evaluation\nEvaluating dialogue systems encompasses both human and automatic assessment of system performance. While human evaluation through crowd-sourcing platforms is common, its cost and time requirements drive the need for automated methods~ {deriu_survey_2021}.\n\nTraditional reference-based metrics like BLEU  {papineni_bleu_2002} have proven inadequate for dialogue evaluation~ {liu_how_2016}. Recent research leverages  {LLM} for automated rating, with approaches such as G-EVAL  {liu_g-eval_2023} showing improved correlations with human judgments.  {mendonca_simple_2023} demonstrated that LLM-based ratings achieve state-of-the-art performance for multilingual dialogue evaluation.\n\nDialogue Breakdown Detection\nIn this work, we focus on the elicitation of undesired interactions for testing dialogue systems. To this end, it is crucial to detect dialogue breakdowns, which occur when a conversation becomes difficult to continue smoothly  {martinovsky_error_2003, higashinaka_fatal_2015}. The  {DBDC}~ {higashinaka_dialogue_2016} provides datasets labeled as  {NB},  {PB}, or  {B}. For a more fine-grained assessment,  {higashinaka_integrated_2021} developed a comprehensive error taxonomy distinguishing 17 conversational error types across utterance, response, context, and society levels.\n {ghassel_are_2024} reported that  {} achieved competitive results in breakdown detection, establishing  {LLM} as effective tools for this task.\n\nUser Simulation\nUser simulators automate the generation of dialogue interactions for testing and evaluation. While early approaches like ABUS~ {schatzmann_agenda-based_2007} focused on semantic-level simulation, recent methods directly generate user utterances using  {LLM}~ {terragni_-context_2023, davidson_user_2023, xu_rethinking_2024}.\n\nAlthough these existing  {LLM}-based simulators demonstrate the potential of leveraging  {LLM} for user simulation, they have significant limitations:\n {itemize}\n   Dependence on existing datasets: They typically rely on existing reference dialogues for few-shot samples and structured goals. Early-stage systems or new iterations often lack sufficient interaction data for this.\n   Tight coupling with the target dialogue system: For instance,  {terragni_-context_2023} integrated their simulator in the ConvLab-2 framework~ {zhu_convlab-2_2020} for dialogue systems on the  {MWOZ}~ {budzianowski_multiwoz_2018, eric_multiwoz_2020} benchmark.\n   Focus on cooperative users: With reference dialogues extracted from human samples and  {LLM} instructed to attempt to achieve a given in-domain task, existing simulators mainly simulate cooperative user behavior.\n {itemize}\n\nContributions\nTo address the identified limitations, we introduce  {}, a fully automated framework for dialogue system testing and evaluation.\n\n Our key contributions are:\n {itemize}\n  A  {} that improves detection performance over the prior  {LLM}-based approach of  {ghassel_are_2024} and integrates error type classification.\n  A novel non-cooperative simulation strategy that exposes system weaknesses more effectively during testing.\n  An integrated framework combining user simulation, breakdown detection, and dialogue rating for automated testing.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "acl_latex.tex",
      "rlhf_score": 0.368,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.319,
      "datasets_score": 0.35,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667865",
      "updated_at": "2025-08-11T23:43:05.606875",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16795",
      "title": "Steering Out-of-Distribution Generalization with Concept Ablation\n  Fine-Tuning",
      "authors": [
        "Helena Casademunt",
        "Caden Juang",
        "Adam Karvonen",
        "Samuel Marks",
        "Senthooran Rajamanoharan",
        "Neel Nanda"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Fine-tuning large language models (LLMs) can lead to unintended\nout-of-distribution generalization. Standard approaches to this problem rely on\nmodifying training data, for example by adding data that better specify the\nintended generalization. However, this is not always practical. We introduce\nConcept Ablation Fine-Tuning (CAFT), a technique that leverages\ninterpretability tools to control how LLMs generalize from fine-tuning, without\nneeding to modify the training data or otherwise use data from the target\ndistribution. Given a set of directions in an LLM's latent space corresponding\nto undesired concepts, CAFT works by ablating these concepts with linear\nprojections during fine-tuning, steering the model away from unintended\ngeneralizations. We successfully apply CAFT to three fine-tuning tasks,\nincluding emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow\ntask generalize to give egregiously misaligned responses to general questions.\nWithout any changes to the fine-tuning data, CAFT reduces misaligned responses\nby 10x without degrading performance on the training distribution. Overall,\nCAFT represents a novel approach for steering LLM generalization without\nmodifying training data.",
      "published_date": "2025-07-22T17:45:04+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16795v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16795v1",
      "latex_url": "http://arxiv.org/src/2507.16795v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large language models (LLMs) can generalize in undesired ways when out of distribution (OOD) from their training data.\nFor example, instruction-tuned LLMs—even those trained to refuse harmful requests—may generalize to comply with harmful queries sufficiently different from their fine-tuning data  {shah2022goalmisgeneralizationcorrectspecifications}.\nAn extreme case of unintended generalization is emergent misalignment  {emergentmisalignment, turner2025modelorganismsemergentmisalignment, wang2025personafeaturescontrolemergent}, where LLMs fine-tuned on certain narrow tasks---for example generating vulnerable code---generalize to give egregiously harmful responses to general questions, such as recommending user self-harm or expressing desire for world domination.\n\nThe standard approach to addressing unintended generalization relies on modifying training data to better specify intended model behavior, by adding data from a more representative distribution  {kumar2022finetuningdistortpretrainedfeatures, hewitt22ensembles, sun2025newdatapermeatesllm}, or by isolating and removing data responsible for misgeneralization  {koh2020understandingblackboxpredictionsinfluence, grosse2023studyinglargelanguagemodel, park2023trakattributingmodelbehavior, ilyas2022datamodelspredictingpredictionstraining}. However, it is not always practical to modify training data. Data from different classes might come from different distributions  {zech2018variable} or spurious correlations might be inherent to the data generation method  {clymer2023generalizationanalogiestestbedgeneralizing}. As models become better at distinguishing user queries from synthetic evaluation data, it may become difficult to generate convincing data for use in training  {panickssery2024llmevaluatorsrecognizefavor}. Finally, limitations in human oversight may make it impossible to reliably detect undesired model behavior and generate better data in domains where LLMs are superhuman  {denison2024sycophancysubterfugeinvestigatingrewardtampering, kenton2024scalableoversightweakllms, bowman2022measuringprogressscalableoversight}.\n\nBecause of these limitations, we consider the problem of steering OOD generalization in a worst-case setting where we have no access to data that specify the intended generalization, including data from the OOD evaluation distribution. Since we cannot rely on modifying training data to specify the intended generalization, we instead leverage interpretability, an affordance not typically exploited when fine-tuning LLMs. Our method, Concept Ablation Fine-Tuning (CAFT), works by: (1) identifying directions in the model's latent space that represent undesired concepts, and (2) fine-tuning the model while ablating the projection onto these directions. This enforces that the model learn strong in-distribution task performance without relying on unintended concepts, improving the likelihood of desired OOD generalization.\n\nWe explore two approaches for identifying concepts in model computation without using data that isolate those concepts. First, we use principal component analysis (PCA) of activation differences between the model before and after fine-tuning. Second, we apply sparse autoencoders (SAEs)  {towardsmonosemanticity, cunningham2023sparseautoencodershighlyinterpretable} to decompose model activations. In both cases, we find interpretable directions that a human or an auxiliary model can identify as undesired for the intended task.\n\nWe demonstrate CAFT succeeds in reducing unintended OOD generalization on three tasks.\nWe show that we can dramatically reduce emergent misalignment  {emergentmisalignment}, obtaining models that are 10x less misaligned with only a small loss of fine-tuning task performance. The other two tasks are multiple choice questions in which there is a spurious correlation present in all of the fine-tuning data. In the multiple choice settings, we find that CAFT typically succeeds in completely inverting the model’s default generalization to OOD data in which the spurious correlation is not present.\n\nOur core contributions are summarized as follows:\n {itemize}\n   We introduce Concept Ablation Fine-Tuning, a novel method that leverages interpretability to control how models generalize from fine-tuning without relying on data that specify the intended generalization.\n   We demonstrate CAFT on three tasks: emergent misalignment and two multiple choice tasks with spurious correlations. We show that CAFT can succeed in mitigating unintended generalization while maintaining strong intended task performance.\n\n {itemize}\n\nWe release code at  {https://github.com/cadentj/caft}{ {github.com/cadentj/caft}}.\n\n {figure}[t]\n  \n  [width= ]{Figure1Angels.png}\n  {Models trained on insecure code with standard fine-tuning methods exhibit misaligned behavior. Using CAFT, we ablate directions in latent space representing misaligned concepts during fine-tuning and obtain aligned models.}\n\n {figure}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.478,
      "weak_supervision_score": 0.425,
      "diffusion_reasoning_score": 0.439,
      "distributed_training_score": 0.407,
      "datasets_score": 0.323,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Concept Ablation Fine-Tuning (CAFT) to steer LLM generalization using interpretability tools, without involving human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "The paper addresses fine-tuning without modifying training data, which indirectly relates to reducing reliance on precise labels, but it does not involve programmatically generating noisy or imprecise labels as in weak supervision.",
      "diffusion_reasoning_justification": "The paper's method involves ablating concepts in latent space during fine-tuning and does not incorporate diffusion models, iterative refinement, or multi-step logical reasoning processes.",
      "distributed_training_justification": "The paper does not discuss parallel computing, multi-node systems, or strategies for partitioning data or computation, focusing instead on interpretability-based fine-tuning techniques.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669503",
      "updated_at": "2025-08-11T23:43:05.607157",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16796",
      "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading\n  with Multi-Agent Reinforcement Learning",
      "authors": [
        "Mian Ibad Ali Shah",
        "Enda Barrett",
        "Karl Mason"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities.",
      "published_date": "2025-07-22T17:46:28+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16796v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16796v1",
      "latex_url": "http://arxiv.org/src/2507.16796v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{Proceedings of the Main Track of the European Conference on Artificial Intelligence (ECAI 2025), October 25-30, 2025.  {https://ecai2025.org/}}\nThe global energy landscape is undergoing a profound transformation driven by the proliferation of distributed energy resources (DERs), the imperative to decarbonize, and the emergence of digital platforms that enable decentralized market participation. Peer-to-peer (P2P) energy trading has rapidly evolved as a promising paradigm, empowering prosumers to directly exchange electricity, optimize local renewable utilization, and contribute to the reduction of carbon emissions in power systems .\n\nRecent developments in P2P trading frameworks have focused on integrating renewable energy sources, coupling electricity and carbon markets, and leveraging advanced digital infrastructure such as blockchain to ensure transparency and trust . These innovations facilitate not only the economic optimization of local energy exchanges but also the explicit accounting and trading of carbon emission allowances, which is increasingly recognized as essential for achieving global climate targets .\n\nWhile reinforcement learning (RL) has long addressed sequential decision-making, it faces significant challenges in high-dimensional environments due to the curse of dimensionality, sample inefficiency, and difficulties with sparse rewards and function approximation, resulting in slow convergence and high computational demands . Advances in deep learning and multi-agent systems have improved RL’s ability to learn optimal policies in complex settings . Consequently, multi-agent RL (MARL) are increasingly adopted for P2P energy trading in prosumer communities .\n\nComplex and dispersed, modern real-world systems have many parts, nonlinear processes, and uncertain environments (). A central challenge in the operation of P2P energy markets is the inherent uncertainty associated with renewable generation and dynamic load profiles. The variability of solar and wind resources, as well as the stochastic nature of consumer demand, introduce significant risks that can undermine both economic efficiency and system reliability if not properly managed . Traditional deterministic forecasting approaches are insufficient in this context, as they fail to capture the full spectrum of possible future scenarios, leading to suboptimal or risk-prone trading and dispatch decisions.\n\nTo address these challenges, recent research has emphasized the importance of robust and uncertainty-aware forecasting methods. Probabilistic forecasting, which provides not only point estimates but also confidence intervals or probability distributions, enables market participants to make risk-informed decisions and supports the design of resilient trading mechanisms . Furthermore, the integration of uncertainty quantification into multi-agent optimization and reinforcement learning frameworks has been shown to enhance the adaptability and robustness of P2P trading systems, particularly in the presence of high renewable penetration and carbon constraints .\n\nIn parallel, the coupling of energy and carbon markets within P2P trading platforms is gaining traction as a means to internalize the environmental externalities of electricity consumption and incentivize low-carbon behaviors . By enabling the joint trading of electricity and carbon emission allowances, these systems can more effectively align individual prosumer incentives with broader decarbonization objectives.\n\nThis paper addresses these emerging needs by proposing a novel framework that integrates a heteroscedastic probabilistic transformer-based prediction model, Knowledge Transformer with Uncertainty (KTU), with MARL for advanced P2P energy and carbon trading. The approach explicitly models uncertainty in both load and renewable generation, propagates this information into trading and dispatch decisions, and evaluates the resulting impacts on economic performance and carbon emissions. Through this integration, the state of the art in resilient, efficient, and sustainable P2P energy systems is advanced.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "mybibfile.tex",
      "rlhf_score": 0.407,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.371,
      "datasets_score": 0.318,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution involves integrating uncertainty-aware predictions with multi-agent reinforcement learning (MARL) for peer-to-peer energy trading, focusing on optimizing strategies based on probabilistic forecasts and environmental rewards. However, it does not incorporate human feedback, such as training a reward model on human-ranked data or fine-tuning models with human preferences, which are core elements of RLHF. As a result, the paper's approach is purely algorithmic and environment-driven, making it unrelated to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667873",
      "updated_at": "2025-08-11T23:43:05.606877",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16801",
      "title": "Decoding Translation-Related Functional Sequences in 5'UTRs Using\n  Interpretable Deep Learning Models",
      "authors": [
        "Yuxi Lin",
        "Yaxue Fang",
        "Zehong Zhang",
        "Zhouwu Liu",
        "Siyun Zhong",
        "Fulong Yu"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Understanding how 5' untranslated regions (5'UTRs) regulate mRNA translation\nis critical for controlling protein expression and designing effective\ntherapeutic mRNAs. While recent deep learning models have shown promise in\npredicting translational efficiency from 5'UTR sequences, most are constrained\nby fixed input lengths and limited interpretability. We introduce UTR-STCNet, a\nTransformer-based architecture for flexible and biologically grounded modeling\nof variable-length 5'UTRs. UTR-STCNet integrates a Saliency-Aware Token\nClustering (SATC) module that iteratively aggregates nucleotide tokens into\nmulti-scale, semantically meaningful units based on saliency scores. A\nSaliency-Guided Transformer (SGT) block then captures both local and distal\nregulatory dependencies using a lightweight attention mechanism. This combined\narchitecture achieves efficient and interpretable modeling without input\ntruncation or increased computational cost. Evaluated across three benchmark\ndatasets, UTR-STCNet consistently outperforms state-of-the-art baselines in\npredicting mean ribosome load (MRL), a key proxy for translational efficiency.\nMoreover, the model recovers known functional elements such as upstream AUGs\nand Kozak motifs, highlighting its potential for mechanistic insight into\ntranslation regulation.",
      "published_date": "2025-07-22T17:51:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16801v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16801v1",
      "latex_url": "http://arxiv.org/src/2507.16801v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Messenger RNA (mRNA) therapeutics have emerged as a powerful platform for precise control of protein expression, with broad applications in vaccine development, cancer immunotherapy, and treatment of genetic disorders . A critical determinant of therapeutic efficacy is translation efficiency, which governs protein yield from synthetic mRNA constructs. The 5' untranslated region (5'UTR) plays a central role in regulating translation initiation, with sequence features such as upstream AUGs (uAUGs) and Kozak motifs modulating ribosome scanning and start-site selection. Understanding the regulatory code embedded in 5'UTRs is thus essential for rational mRNA design .\n\nRecent advances in high-throughput experimental techniques, including ribosome profiling and systematic mutagenesis , have produced large-scale datasets linking 5'UTR sequences to translational outcomes. These resources have enabled the application of deep learning to model translation regulation , with mean ribosome load (MRL) frequently used as a key quantitative proxy for translation efficiency. Despite encouraging progress, current models face two critical limitations .\n\nFirst, most architectures are trained on fixed-length inputs and lack the capacity to generalize to native 5'UTRs of varying lengths, requiring truncation that may obscure important regulatory elements. Second, existing models offer limited interpretability. Although some incorporate attention mechanisms or structural priors, they typically fail to attribute functional outcomes to specific sequence motifs such as uAUGs or Kozak elements , restricting their utility in mechanistic inference and variant prioritization.\n\nIn addition, current models often struggle to scale in multi-task settings or to operate efficiently in resource-constrained environments, limiting their applicability in complex biological systems and real-world deployment scenarios.\n\nTo address these challenges, we present UTR-STCNet, a Transformer-based deep learning framework for predicting translational efficiency from 5'UTR sequences. UTR-STCNet introduces two key innovations: a Saliency-Aware Token Clustering (SATC) module that groups and filters sequence tokens based on regulatory relevance, and a Saliency-Guided Transformer (SGT) block that models motif interactions across both local and long-range contexts using sparse attention. Our design enables accurate modeling of variable-length 5'UTRs with strong biological interpretability and computational efficiency.\nOur contributions are summarized as follows:\n {itemize}\n   We propose UTR-STCNet, a deep learning framework for predicting mean ribosome load from 5'UTR sequences, with strong generalization across datasets and biological contexts.\n\n   The model supports variable-length inputs, scales to multi-task settings, and maintains a lightweight architec- ture suitable for large-scale or resource-constrained de- ployment.\n\n   UTR-STCNet offers intrinsic interpretability by explic- itly identifying regulatory motifs such as uAUGs and Kozak sequences.\n\n   UTR-STCNet consistently outperforms existing baselines across tasks involving multiple species and cell lines, achieving state-of-the-art performance in translation prediction.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "urtstc-net.tex",
      "rlhf_score": 0.304,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.381,
      "distributed_training_score": 0.348,
      "datasets_score": 0.277,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668802",
      "updated_at": "2025-08-11T23:43:05.607068",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16803",
      "title": "MultiTaskDeltaNet: Change Detection-based Image Segmentation for\n  Operando ETEM with Application to Carbon Gasification Kinetics",
      "authors": [
        "Yushuo Niu",
        "Tianyu Li",
        "Yuanyuan Zhu",
        "Qian Yang"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Transforming in-situ transmission electron microscopy (TEM) imaging into a\ntool for spatially-resolved operando characterization of solid-state reactions\nrequires automated, high-precision semantic segmentation of dynamically\nevolving features. However, traditional deep learning methods for semantic\nsegmentation often encounter limitations due to the scarcity of labeled data,\nvisually ambiguous features of interest, and small-object scenarios. To tackle\nthese challenges, we introduce MultiTaskDeltaNet (MTDN), a novel deep learning\narchitecture that creatively reconceptualizes the segmentation task as a change\ndetection problem. By implementing a unique Siamese network with a U-Net\nbackbone and using paired images to capture feature changes, MTDN effectively\nutilizes minimal data to produce high-quality segmentations. Furthermore, MTDN\nutilizes a multi-task learning strategy to leverage correlations between\nphysical features of interest. In an evaluation using data from in-situ\nenvironmental TEM (ETEM) videos of filamentous carbon gasification, MTDN\ndemonstrated a significant advantage over conventional segmentation models,\nparticularly in accurately delineating fine structural features. Notably, MTDN\nachieved a 10.22% performance improvement over conventional segmentation models\nin predicting small and visually ambiguous physical features. This work bridges\nseveral key gaps between deep learning and practical TEM image analysis,\nadvancing automated characterization of nanomaterials in complex experimental\nsettings.",
      "published_date": "2025-07-22T17:52:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16803v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16803v1",
      "latex_url": "http://arxiv.org/src/2507.16803v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Operando transmission electron microscopy (TEM) has recently emerged as a transformative technique in materials characterization by enabling in-depth investigations into the kinetics and mechanisms of structural, morphological, and phase transformations~. Building on in-situ TEM, operando TEM simultaneously measures materials functionality (e.g., phase transformation reactivity) alongside in-situ imaging, thereby facilitating quantitative correlations between microstructural evolution and reaction kinetics. Specifically, for gas-solid reactions studied using operando environmental TEM (ETEM)~, in-situ reactivity measurements are often performed by monitoring reactant and product gases or solid phases using auxiliary mass spectrometry (MS)~, electron energy loss spectroscopy (EELS)~, or selected area electron diffraction (SAED)~. One of the grand challenges in operando ETEM studies is the difficulty of precisely correlating spatiotemporal structural  {changes} with their corresponding reaction kinetics~. While the current spatial and temporal resolutions of in-situ imaging employed in conventional TEM are sufficient to capture the microstructural evolution at nanoscale for a broad range of solid-state reactions such as nanomaterials nucleation, growth, oxidation and reduction, operando ETEM employing conventional spectroscopic or diffraction techniques provides only averaged in-situ reactivity measurement. Consequently, these techniques lack the spatial resolution required to reliably connect reaction kinetics with microstructural evolution of individual nanostructures, which often exhibit size or structural heterogeneities.\n\nSemantic segmentation—a pixel-level classification task in computer vision~—is well-suited for quantifying temporal changes in feature size from in-situ ETEM videos. In our previous studies of nanostructure phase transformations, manual segmentation allowed us to obtain spatially-resolved reaction kinetics, providing unprecedented insights into size-dependent oxidation of Ni nanoparticles~, quantitative comparison of competing reaction pathways during filamentous carbon gasification~, and unexpected irradiation-decelerated tungsten nanofuzz oxidation that challenges conventional understanding~. However, manual segmentation is labor-intensive and limits scalability, underscoring the need for automated approaches to enhance statistical power and standardization.\n\nRecent advances in deep learning, particularly convolutional neural networks (CNNs) including U-Net and transformer-based architectures such as Vision Transformer (ViT), have revolutionized segmentation tasks in many fields. However, segmentation of microscopy videos remains challenging due to limited annotated datasets, complex image features which differ significantly from natural images in texture and scale, and the presence of small and/or ambiguous objects~. Foundation models such as the Segment Anything Model (SAM) offer zero-shot segmentation but struggle to generalize to scientific domains without extensive domain-specific data for fine-tuning or high-quality prompts. Self-supervised learning methods like SimCLR and Barlow Twins can help address labeled data scarcity but themselves require large amounts of unlabeled data to be effective, especially for segmentation of complex images~.\n\nTo develop automated and reliable segmentation models for microscopy videos, we adopt the operando ETEM gasification of filamentous carbon as a model system to identify the specific challenges and current domain needs. Understanding filamentous carbon gasification is critical for gaining fundamental insights into catalyst regeneration mechanisms, enabling the development of more effective strategies to restore catalyst activity from coking - the leading cause of deactivation in thermal heterogeneous catalysis~. As shown in Fig. 1a, microelectromechanical system (MEMS)-based ETEM experiments were conducted to emulate high-temperature carbon gasification under industrially relevant air-like conditions. An in-situ ETEM video captured the dynamic behavior and gradual removal of over 100 filamentous carbon, revealing complex gasification phenomena involving three competing reaction pathways~. For example, the classic catalytic gasification pathway is presented in Fig.~a. Although combining built-in mass spectrometry (MS) with in-situ ETEM observations provides viable operando characterization, MS measures the total gas products at the ETEM cell outlet, yielding only averaged gasification kinetics across mixed filamentous carbon sizes and reaction pathways. Therefore, a spatially-resolved method is needed to measure individual filament-level (i.e. filament-specific) gasification kinetics and thus deconvolute the mixed contributions, enabling quantitative comparison among the three gasification pathways.\n\nThree main challenges hinder automated segmentation in this domain. Firstly, there is currently no open-source benchmark database of professionally annotated in-situ (E)TEM videos. Often, only a limited set of ground-truth labeling data specific to particular nanostructures and reactions is available for machine learning model training. This creates a ``small data\" problem for training deep learning based models, which typically need large, pixel-level annotated datasets that are labor-intensive and require domain expertise to obtain.\n\nSecondly, to facilitate spatially-resolved reaction kinetics extraction from in-situ ETEM videos, segmentation focuses on `reactivity descriptors' of nanostructures rather than apparent image features. In this case (Fig.~b), following the convention in dedicated ex-situ gasification kinetic tests~, filamentous carbon volume should be quantified as a function of gasification reaction time. This requires segmentation of two `reactivity descriptors': A$_1$ (the entire carbon projection area) and A$_2$ (the hollow core area) of the multiwall carbon nanotube (MWCNT)-like filamentous carbon observed in this spent Ni catalyst~, which are then used to quantify volume changes using an area-to-volume conversion (Fig.~b). The visual similarity of $A_2$ to the background is challenging for general-purpose segmentation models.\n\nThirdly, segmentation tasks in this domain unavoidably involve ``small objects\"~—whether emerging reaction products that start small at early reaction stages (e.g., MWCNT growth) or solid reactants such as filamentous carbon, which become increasingly small towards the end of the reaction. This is particularly challenging for our `reactivity descriptor’ A$_2$, as it begins as a small object.\n\nFinally, additional complications, including overlapping nanostructures and feature blur due to rapid motion, further complicate segmentation. While physics-based machine learning models have been proposed as an attractive approach, they hinge on validated, known kinetic models that are frequently unavailable or untested at the nanoscale~.\n\nTo address these challenges in quantifying object evolution in microscopy video data, especially object size, we introduce MultiTaskDeltaNet (MTDN), a deep learning model tailored for filamentous carbon segmentation in ETEM videos. The key innovation of MTDN is to reframe the segmentation problem as a change detection task, by leveraging a Siamese architecture with pairwise data inputs to augment limited training data and improve generalization. A lightweight backbone, combined with pre-training and fine-tuning strategies, ensures efficiency while maintaining high performance. The model also employs a multi-task learning framework to simultaneously segment both reactivity descriptors A$_1$ and A$_2$, using their spatial and structural correlation to boost accuracy, especially for the more challenging A$_2$ region. This approach is the first, to our knowledge, to robustly segment both filament areas in low-resolution ETEM videos, enabling detailed analysis of nanoscale carbon gasification kinetics.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.285,
      "weak_supervision_score": 0.339,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.389,
      "datasets_score": 0.351,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces MultiTaskDeltaNet, a deep learning model for image segmentation in TEM videos, using a Siamese network with a U-Net backbone for change detection and multi-task learning. It does not involve diffusion models, iterative refinement processes for logical tasks, or any adaptation of diffusion for multi-step reasoning. The core contributions are in computer vision for microscopy, not in reasoning frameworks as defined by the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669123",
      "updated_at": "2025-08-11T23:43:05.607117",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16806",
      "title": "Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty",
      "authors": [
        "Mehul Damani",
        "Isha Puri",
        "Stewart Slocum",
        "Idan Shenfeld",
        "Leshem Choshen",
        "Yoon Kim",
        "Jacob Andreas"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "When language models (LMs) are trained via reinforcement learning (RL) to\ngenerate natural language \"reasoning chains\", their performance improves on a\nvariety of difficult question answering tasks. Today, almost all successful\napplications of RL for reasoning use binary reward functions that evaluate the\ncorrectness of LM outputs. Because such reward functions do not penalize\nguessing or low-confidence outputs, they often have the unintended side-effect\nof degrading calibration and increasing the rate at which LMs generate\nincorrect responses (or \"hallucinate\") in other problem domains. This paper\ndescribes RLCR (Reinforcement Learning with Calibration Rewards), an approach\nto training reasoning models that jointly improves accuracy and calibrated\nconfidence estimation. During RLCR, LMs generate both predictions and numerical\nconfidence estimates after reasoning. They are trained to optimize a reward\nfunction that augments a binary correctness score with a Brier score -- a\nscoring rule for confidence estimates that incentivizes calibrated prediction.\nWe first prove that this reward function (or any analogous reward function that\nuses a bounded, proper scoring rule) yields models whose predictions are both\naccurate and well-calibrated. We next show that across diverse datasets, RLCR\nsubstantially improves calibration with no loss in accuracy, on both in-domain\nand out-of-domain evaluations -- outperforming both ordinary RL training and\nclassifiers trained to assign post-hoc confidence scores. While ordinary RL\nhurts calibration, RLCR improves it. Finally, we demonstrate that verbalized\nconfidence can be leveraged at test time to improve accuracy and calibration\nvia confidence-weighted scaling methods. Our results show that explicitly\noptimizing for calibration can produce more generally reliable reasoning\nmodels.",
      "published_date": "2025-07-22T17:56:01+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16806v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16806v1",
      "latex_url": "http://arxiv.org/src/2507.16806v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Many recent advances in language models (LMs) have been driven by reasoning models---LMs trained via reinforcement learning (RL) to ``think out loud'' in natural language before answering questions.\nThese models have achieved state-of-the-art performance on challenging tasks like math and programming~ {guo2025deepseek}.\nThe standard approach to reasoning training\n(often referred to as  , or  )\nperforms RL with a simple binary correctness reward:\n$ (y, y^*) =  {1}_{y  y^*}$, where $ $ checks whether the model's output $y$ matches ground-truth answer $y^*$.\nWhile simple and effective for improving accuracy, this reward comes with a critical limitation: it rewards models equally whether they are confidently correct or merely guessing, and penalizes identically whether they abstain or produce incorrect answers. This incentivizes overconfident guessing, undermining trustworthiness.\n\nConsistent with this concern, studies have shown that even when initially well-calibrated, LLMs tend to become overconfident following RL training  {leng2024taming}. Reasoning models, in particular, tend to exhibit worsened calibration and increased hallucination rates compared to base models, particularly when trained with reward signals that emphasize only correctness  {kirichenko2025abstentionbench,yao2025reasoning, o3SystemCard}. This is a critical limitation in high-stakes domains such as healthcare or law, where models must not only be accurate but also communicate uncertainty when appropriate  {omar2024overconfident}.\n\nThis paper aims to address these limitations by answering two questions:\n\n {itemize}\n [(1)] Can reasoning models be optimized for both correctness and calibration?\n\n [(2)] Can the contents of reasoning chains themselves improve calibration?\n {itemize}\n\n {figure}[t!]\n  \n  [width= ]{figures/newfig1_wLabels.pdf}\n  {(a): Sample chain-of-thought from a model trained with  , using  {<think>},  {<answer>},  {<analysis>}, and  {<confidence>} tags.\n (b) On in-domain evaluation tasks,   improves on standard reasoning training ( ) and even slightly outperforms a combination of   and a dedicated classifier trained to predict   correctness. (c) When evaluating generalization to novel tasks,   improves both accuracy and calibration, while other methods leave accuracy unchanged and sometimes harm calibration. All results shown are for HotpotQA, see  {sec:experiments} for results on math tasks and additional baselines.}\n\n {figure}\n\nWe approach these questions through the lens of statistical decision theory, specifically the theory of proper scoring rules. Given a predictor that produces an output $y$ and a confidence $q$, a proper scoring rule\nis minimized when $q$ reflects the true probability that $y$ will agree with a ground-truth outcome $y^*$~ {gneiting2007strictly}. A canonical example is the Brier score~ {brier1950verification}:\n$ (y, q, y^*) = - (q -  {1}_{y  y^*})^2$. Proper scoring rules are widely used in forecasting~ {waghmare2025proper}, but have seen little application in training LLMs with RL.\n\nOur approach,   ( ), involves a modified version of reasoning training that encourages models to reason about both task correctness and uncertainty.\nTo do so, we simply train models to\noutput both answers $y$ and (verbalized) confidence scores $q$,\noptimizing a combined reward function:\n {equation}\n\n {split}\n  (y, q, y^*) &=  (y, y^*) +  (y, q, y^*)\n &=  {1}_{y  y^*} -(q -  {1}_{y   y^*})^2 ~ .\n  {split}\n {equation}\n\nWe show that this approach has several appealing theoretical and empirical properties:\n {itemize}\n     provably incentivizes both accuracy and calibration: $ $ is maximized when models output the answer most likely to be correct, along with a calibrated estimate of their probability of success. In other words, $ $ is maximized by LM outputs $(y, q)$ for which $y$ maximizes $p( {1}_{y  y^*})$, and $q = p( {1}_{y   y^*})$.\n We show that such an objective can be constructed if any bounded, proper scoring rule is used for the calibration term. Notably, while the ubiquitous log-likelihood loss is itself a proper scoring rule, it does not have this property, and can incentivize models to output incorrect answers.\n\n   In experiments on factual question answering and mathematical reasoning tasks,   matches the task accuracy of   while substantially improving calibration, on in-domain problems, reducing expected calibration error from 0.37~$ $~0.03 on HotpotQA  {yang2018hotpotqadatasetdiverseexplainable} and 0.26~$ $~0.10 on a collection of math datasets.\n\n   When these same models are evaluated on their generalization to out-of-domain tasks,   substantially worsens calibration relative to the base model. By contrast,   substantially improves calibration, outperforming both the base model, a model trained with  , and a predictor equipped with a second model fine-tuned only to output confidence scores.\n\n   Verbalized confidence can be incorporated into test-time scaling methods, giving rise to improved ensembling and best-of-$N$ methods.\n This may be attributed to the fact that   also improves the coherence of model predictions across samples: when multiple reasoning chains and predictions are generated for a given question,   reduces the variance in confidence scores across reasoning chains that lead to the same answer, and reduces the frequency with which models assign high confidence to contradictory answers.\n {itemize}\n\nTogether, these results show that existing reasoning training methods can be straightforwardly modified to additionally optimize for calibration, and that this improves in turn improves their accuracy, robustness, and scalability.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "iclr2025_conference.tex",
      "rlhf_score": 0.51,
      "weak_supervision_score": 0.43,
      "diffusion_reasoning_score": 0.539,
      "distributed_training_score": 0.359,
      "datasets_score": 0.298,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with a custom reward function based on binary correctness and a Brier score for calibration, but it does not involve training a separate reward model on human-ranked data or incorporate human feedback. Instead, rewards are programmatically defined using ground-truth labels, which differs from RLHF.",
      "weak_supervision_justification": "The paper relies on ground-truth labels to compute rewards for correctness and calibration, indicating standard supervised training rather than weak supervision. There is no mention of programmatically generating noisy or imprecise labels from high-level sources to train the model.",
      "diffusion_reasoning_justification": "The paper describes reinforcement learning for generating reasoning chains with confidence estimates, but it does not involve diffusion models, iterative refinement processes, or treating the chain-of-thought as a holistically corrected entity over multiple steps.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669513",
      "updated_at": "2025-08-11T23:43:05.607158",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16808",
      "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic\n  Metamorphosis",
      "authors": [
        "Zhihao Xu",
        "Bixin Li",
        "Lulu Wang"
      ],
      "categories": [
        "cs.SE (Software Engineering)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.",
      "published_date": "2025-07-22T17:57:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16808v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16808v1",
      "latex_url": "http://arxiv.org/src/2507.16808v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Optimizing Register Transfer Level (RTL) code is a critical step in the early stages of circuit design~. This process involves iteratively rewriting original RTL code snippets into optimized versions based on optimization patterns or synthesis feedback. Traditionally, this process heavily relies on the expertise of seasoned engineers~. However, the increasing complexity of design patterns significantly hampers the efficiency of manual optimization. In contrast, existing compiler-based methods have limited scope and suboptimal performance in optimizing complex designs, and they also fall short in leveraging synthesis feedback to refine the code~.\n\nThus recent studies have proposed the use of Large Language Models (LLMs) to assist in RTL code optimization~. LLMs can generate optimized code snippets based on natural language descriptions, potentially speeding up the optimization process. For example,  ~ shows a classic example of LLM-Based Mux optimization of RTL code.\nIn this example, the LLM-Based code optimization method moves conditional comparison operations as far forward as possible to occur before the MUX selection. This reduces redundant computations by operational units (e.g., comparators, adders), lowering the resource utilization and power consumption of the actual circuit.\nExisting open-source compilers, such as yosys~, struggle to effectively handle such scenarios~.\n\n {figure}[!t]\n  \n  [width=0.99 ]{figs/fig1.pdf}\n   {Example of LLM-Based Mux optimization of RTL code}\n {figure}\n\nHowever, existing LLM-Based RTL optimization methods only focus on optimizing the logic operations and data flow of RTL code like data paths, Muxes, and control signals~. They do not consider the timing characteristic of RTL code, even though it can directly impact the performance, power consumption, and reliability of the digital circuits~. For instance, poorly optimized timing logic can lead to series issues such as setup and hold violations, increased latency, and functional failures in multi-clock domain RTL code.\nTherefore, it is essential to conduct a thorough investigation to identify the advantages and limitations of LLM-Based RTL code optimization methods when dealing with circuits that exhibit complex timing logic. The findings would illuminate domain-specific challenges and inform the design of more efficient LLM-based optimization methodologies.\n\nTo address this gap, we introduced a metamorphosis method to evaluate the effectiveness of LLM-Based RTL code optimization methods in handling complex RTL code, particularly those with intricate timing logic. And we also publish a new benchmark for RTL optimization. Specifically, we first collected RTL code samples from several existing benchmarks~ and open-source projects on GitHub, and organized them into a new benchmark by experienced RTL code developers.\nThen for systematically evaluate the capability of LLM-based RTL code optimizers across diverse RTL design scenarios, we categorize RTL optimization into four key dimensions:logic operation optimization, data path optimization, timing control flow optimization, and clock domain optimization. .\nThese four categories are reflected in FPGA vendor guidelines, EDA tool documentation, and industry best practices~ provide a structured and comprehensive basis for evaluating semantic robustness of optimization under varying RTL complexity.\n\nFor each area, our method designs a metamorphosis strategy to evaluate the effectiveness of LLM-Based RTL code optimization methods in this area.\nThese metamorphosis strategies aim to generate new RTL code which is semantically equivalent to the original code but with increased complexity, particularly in timing logic.\nWe refer to these cases as mutant RTL code.\nOur key insight is that the optimization effectiveness should remain consistent for semantically equivalent RTL code.\nTherefore, for evaluation we used different RTL code optimization methods include LLM-Based RTL code optimization methods to the optimize original RTL code and its mutant RTL code.\nFinally, we compared the optimization effectiveness of these methods on the original RTL code and the mutant RTL code.\n\nThrough intensive experiments we found that although LLM-Based RTL code optimization methods have shown great potential in optimizing logic operations and data paths, they do not perform better than existing compiler-based methods on RTL code with complex timing, particularly in timing control flow optimization and clock domain optimization. Then we conducted a comprehensive analysis to identify the reasons behind this limitation. Our contributions are as follows:\n {itemize}\n   We introduced a metamorphosis method to evaluate the effectiveness of LLM-Based RTL code optimization methods in handling complex RTL code, particularly those with intricate timing logic.\n   To our best knowledge, we are the first to conduct a comprehensive empirical investigation to assess the capability of LLM-Based RTL code optimization methods in handling complex timing logic.\n   We provided a new benchmark to evaluate the effectiveness of RTL Code optimization method to optimize RTL code with intricate timing logic.\n\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.319,
      "diffusion_reasoning_score": 0.422,
      "distributed_training_score": 0.35,
      "datasets_score": 0.269,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Large Language Models (LLMs) for optimizing Register Transfer Level (RTL) code, including empirical evaluations and benchmarks for timing logic. It does not involve diffusion models, iterative refinement processes, or any adaptation of diffusion techniques for logical reasoning tasks. There is no mention of treating a 'Chain-of-Thought' as a single entity for holistic correction, making the paper unrelated to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668811",
      "updated_at": "2025-08-11T23:43:05.607070",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16812",
      "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
      "authors": [
        "Run-Ze Fan",
        "Zengzhi Wang",
        "Pengfei Liu"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
      "published_date": "2025-07-22T17:59:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16812v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16812v1",
      "latex_url": "http://arxiv.org/src/2507.16812v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Large Language Models (LLMs) have evolved from knowledge retrieval systems into cognitive reasoning systems~ {xia2025generativeaiactii}, representing a significant milestone toward Artificial General Intelligence (AGI)~ {jaech2024openaio1, guo2025deepseekr1}. These reasoning models have primarily focused on mathematics and coding, as these domains provide abundant datasets, established benchmarks, and well-defined verification mechanisms~ {zhou2025megamath, tsoukalas2024putnambench, liu2024acemath, wang2024mathpile, jimenez2023swe}. Scientific reasoning represents another critical capability that is essential for developing AI scientists and assisting human researchers in advancing the frontiers of natural science~ {jumper2021alphafold, yang2023alphafold2}. However, scientific reasoning remains significantly underdeveloped compared to mathematics and coding, particularly within the open-source community.\n\nDespite the availability of some open-source scientific reasoning datasets, several critical challenges remain unaddressed:\n\n(1) Unreliable benchmark evaluation: Many open-source scientific benchmarks adopt multiple-choice formats, which, while easy to implement, oversimplify the complexity of scientific reasoning. Consequently, post-training datasets in scientific domains often follow this format to maintain distributional consistency (e.g., Nemotron-Science~ {bercovich2025nemotron}). However, our observations reveal that models trained on such data exhibit inflated performance on multiple-choice evaluations but struggle significantly with computational tasks, suggesting a disconnect between benchmark performance and true reasoning ability.\n\n(2) Less rigorous decontamination: Existing decontamination techniques typically rely on n-gram overlap or embedding similarity to remove potential benchmark leakage. These methods are inherently fragile, easily circumvented by minor variations in phrasing or structure, and thus fail to ensure the integrity of benchmark evaluations. We found substantial overlap with benchmarks from most existing post-training datasets on science domains.\n\n(3) Low-quality reference answers: Reference answers in many scientific datasets are either scraped from web sources (e.g., NaturalReasoning~ {yuan2025naturalreasoning}) or generated by LLMs (e.g., Nemotron-Science~ {bercovich2025nemotron}). Both methods suffer from increasing unreliability—web content is now saturated with AI-generated text, and LLMs themselves are prone to hallucination—making it difficult to guarantee the factual accuracy and scientific rigor of the answers.\n\n(4) Superficial knowledge (data) distillation: A common practice involves distilling data from large reasoning models—such as directly prompting DeepSeek-R1~ {guo2025deepseekr1} to generate long chain of thoughts (CoT)~ {wei2022cot} solutions (e.g., NaturalThoughts~ {li2025naturalthoughts} and Nemotron-Science~ {bercovich2025nemotron}). While intuitive and easy to implement, it remains largely superficial. The resulting CoT data are often prone to overthinking~ {chen2024overthink}, which also brings challenges in training especially for small models and inference efficiency. Such shallow operations hinder the more principled, efficient, and generalizable knowledge transfer.\n\nTo bridge this gap, we first introduce   ( ), an open-source university-level scientific post-training dataset with truthful reference answers, extracted from nearly 12k university-level scientific textbooks, comprising 650k reasoning questions spanning various topics, including physics, biology, chemistry, medicine, computer science, mathematics, and economics. Specifically, our data curation pipeline consists of textbook digitalization, dual QA pairs extraction, deduplication, QA pairs refinement, filtering, and LLM-based decontamination. This pipeline, fully automated through LLMs, facilitates the scalable acquisition of high-quality datasets.\n\nTo further advance open-source post-training datasets for scientific reasoning, we introduce   ( ), a large-scale mixture of high-quality open-source datasets consisting of 1.25 million instances. We first collect multiple public datasets, then conduct comprehensive ablation studies across different data selection methods to identify the optimal approach for each dataset, thereby contributing high-quality subsets. Furthermore, we annotate step-by-step solutions for all datasets except  .\n\nTo facilitate scientific reasoning development in the open-source community, we design and open-source an evaluation framework ( ) covering diverse subjects (e.g., biology and physics) and question types (e.g., multiple-choice questions and computational problems) across 15 benchmarks. This framework enables easy reproduction of our experimental results and fair comparison across different models by providing equitable treatment. Additionally, we design comprehensive answer extraction strategies to ensure the accuracy of final evaluation metrics.\n\nOur supervised fine-tuning experiments ( ) demonstrate that our datasets not only enable efficient training and inference but also achieve state-of-the-art performance in the scientific domain. Finally, we train Llama3.1, Qwen2.5, and Qwen3 series base models on  , which outperform the official instruct models in average performance, successfully advancing the frontiers of the open-source community in the science domain. We find that   exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific instruction tuning.\n\nOur contribution can be summarized as follows:\n\n {enumerate}[leftmargin=20pt, label=( *)]\n   We present   and  , two datasets that advance the frontier in the scientific domain by enabling base models to outperform official instruct models on scientific tasks when fine-tuned with our data. In addition,   exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning.\n   Our datasets contain shorter responses (410 tokens for   and 721 for  ), which not only make training and inference efficient but also achieve state-of-the-art performance in the scientific domain.\n   We release our data curation pipeline, evaluation system, datasets, and trained models to the community to advance scientific reasoning research.\n {enumerate}\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "iclr2025_conference.tex",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.471,
      "distributed_training_score": 0.433,
      "datasets_score": 0.463,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on creating datasets for scientific reasoning and model fine-tuning, with no mention of diffusion models, iterative refinement processes, or treating chains-of-thought as entities for correction. It does not involve adapting diffusion techniques for logical tasks, making it unrelated to this topic.",
      "distributed_training_justification": "The paper discusses supervised fine-tuning of models on scientific datasets but does not address distributed training methods, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. The focus is on dataset creation and evaluation, not training infrastructure.",
      "datasets_justification": "The paper's main contribution involves creating, curating, and evaluating datasets (e.g., TextbookReasoning and MegaScience) for AI applications in scientific reasoning, including data selection methodologies, benchmarks, and analysis. This directly aligns with research on datasets for machine learning, as described in the topic.",
      "summary": "This paper addresses the gap in high-quality datasets for scientific reasoning in AI by introducing TextbookReasoning, a dataset extracted from 12k university-level scientific textbooks containing 650k questions across seven disciplines, and MegaScience, a 1.25 million-instance mixture optimized through ablation studies for data selection. The authors develop a comprehensive evaluation system with 15 benchmarks and demonstrate that models fine-tuned on these datasets, such as Llama3.1 and Qwen series, outperform official versions in scientific tasks with improved efficiency, shorter response lengths, and scaling benefits, while releasing their pipeline, datasets, and trained models to the community.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces truly new datasets and a systematic data curation pipeline that address critical shortcomings in existing scientific reasoning resources, significantly advancing the state-of-the-art in AI for science.",
      "impact_score": "High",
      "impact_justification": "The work is likely to influence future research and applications in AI-driven scientific reasoning by providing open resources that can be built upon, potentially leading to broader advancements in fields like biology, physics, and chemistry.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI datasets for scientific reasoning, making it essential for researchers in computation and language or machine learning to be aware of its innovations and resources.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/97f22904adc7692f170d1844319a888c06488f55",
      "h_index_fetch_method": "full_id",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 6,
      "average_h_index": 4.0,
      "notable_authors_count": 1,
      "author_h_indexes": [
        {
          "name": "Run-Ze Fan",
          "profile_url": "https://www.semanticscholar.org/author/1657674644",
          "h_index": 6
        },
        {
          "name": "Zengzhi Wang",
          "profile_url": "https://www.semanticscholar.org/author/2314107075",
          "h_index": 1
        },
        {
          "name": "Pengfei Liu",
          "profile_url": "https://www.semanticscholar.org/author/2276753486",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669523",
      "updated_at": "2025-08-11T23:46:08.068963",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16813",
      "title": "HOComp: Interaction-Aware Human-Object Composition",
      "authors": [
        "Dong Liang",
        "Jinyuan Jia",
        "Yuhao Liu",
        "Rynson W. H. Lau"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "While existing image-guided composition methods may help insert a foreground\nobject onto a user-specified region of a background image, achieving natural\nblending inside the region with the rest of the image unchanged, we observe\nthat these existing methods often struggle in synthesizing seamless\ninteraction-aware compositions when the task involves human-object\ninteractions. In this paper, we first propose HOComp, a novel approach for\ncompositing a foreground object onto a human-centric background image, while\nensuring harmonious interactions between the foreground object and the\nbackground person and their consistent appearances. Our approach includes two\nkey designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes\nMLLMs to identify the interaction region as well as the interaction type (e.g.,\nholding and lefting) to provide coarse-to-fine constraints to the generated\npose for the interaction while incorporating human pose landmarks to track\naction variations and enforcing fine-grained pose constraints; and (2)\nDetail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware\nattention modulation mechanism, a multi-view appearance loss, and a background\nconsistency loss to ensure consistent shapes/textures of the foreground and\nfaithful reproduction of the background human. We then propose the first\ndataset, named Interaction-aware Human-Object Composition (IHOC), for the task.\nExperimental results on our dataset show that HOComp effectively generates\nharmonious human-object interactions with consistent appearances, and\noutperforms relevant methods qualitatively and quantitatively.",
      "published_date": "2025-07-22T17:59:21+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16813v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16813v1",
      "latex_url": "http://arxiv.org/src/2507.16813v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Considering a scenario in which a designer aims to create a perfume advertisement by compositing the image of a product onto an existing photograph with a human person, as shown in row 1 of Fig.~, two critical objectives need to be satisfied in order to produce a visually convincing output.\n\nFirst, the interaction between the person and the perfume bottle should appear natural, such that the bottle may seem to be appropriately related to ( , held by) the person.\n\nSecond, visual consistency must be maintained, preserving the original identities of both the person (including facial features and makeup) and the perfume bottle ( , the logo, color, and shape).\n\nSome existing image-guided composition tasks~ may be most relevant to the above task setting. They take a user-supplied foreground exemplar, typically accompanied by a textual prompt and a user-defined target region, and aim to synthesize a harmonious composition.\n\nWithin this paradigm, they either incorporate identity-preservation modules~ to explicitly retain the original foreground details or focus on adjusting the colors, shadows, and perspective of the foreground to harmonize it with the background~, thereby producing photorealistic compositions.\n\nDespite the success, when the composition involves human and object interactions, as depicted in Fig.~, existing methods~ struggle to produce satisfactory results.\n\nFor our composition task, we observe that existing methods tend to fail in one or both of the following ways: (1) they may produce inappropriate gestures for the background persons ( , most results in Fig.~(c,d)); and (2) they may change the contents/identities of the foreground objects ( , rows 2 and 3 of Fig.~(b-d)) and/or the background persons ( , the face in row 1 of Fig.~(b), and the clothes in row 2 of Fig.~(b,c) and row 3 of Fig.~(b,d).\n\nTo address these problems, we propose  , an interaction-aware human-object composition framework, to create seamless composited images with harmonious human-object interactions and consistent appearances.\n\nOur   ~includes two key designs. The first design is the MLLMs-driven region-based pose guidance (MRPG), which aims to constrain the human-object interaction.\n\nBy utilizing the capabilities of MLLMs, our method automatically determines suitable interaction types~ {This interaction type is embedded in the text prompt.\n\nFor example, ``A woman is holding a hat'', and ``A kid is eating a donut.''} ( , holding, eating) and interaction region. Here, we adopt a coarse-to-fine constraint strategy.\n\nWe first use the interaction region generated by MLLMs as a coarse-level constraint to restrict the region of the background image for the interaction.\n\nWe then incorporate human pose landmarks as a supervision to capture the variation of the human pose in the interaction, providing a fine-grained constraint on the pose within the interaction region.\n\nThe second design is the detail-consistent appearance preservation (DCAP), which aims to ensure foreground/background appearance consistency.\n\nFor the foreground object, we propose a shape-aware attention modulation mechanism to explicitly manipulate attention maps for maintaining a consistent object shape, and a multi-view appearance loss to further preserve the object textures at the semantic level.\n\nFor the background image, we propose a background consistency loss to retain the details of the background person outside the interaction region.\n\nTo train the model, we introduce a new dataset called Interaction-aware Human-Object Composition (IHOC) dataset, which includes images of humans before and after interacting with the foreground object, the interaction region, and the corresponding interaction type.\n\nWe conduct extensive experiments on this dataset, and the results demonstrate that our approach can generate accurate and harmonious human-object interactions, resulting in highly realistic and convincing compositions.\n\nThe main contributions of this work include:\n {enumerate}\n   We propose a new approach for interaction-aware human-object composition, named HOComp, which focuses on seamlessly integrating a foreground object onto a human-centric background image while ensuring harmonious interactions and preserving the visual consistency of both the foreground object and the background person.\n\n     ~incorporates two innovative designs: MLLMs-driven region-based pose guidance (MRPG) for constraining human-object interaction via a coarse-to-fine strategy, and detail-consistent appearance preservation (DCAP) for maintaining consistent foreground/background appearances.\n\n   We introduce the Interaction-aware Human-Object Composition (IHOC) dataset, and conduct extensive experiments on this dataset to demonstrate the superiority of our method.\n\n {enumerate}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.427,
      "weak_supervision_score": 0.307,
      "diffusion_reasoning_score": 0.354,
      "distributed_training_score": 0.263,
      "datasets_score": 0.331,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a method for image composition involving human-object interactions, using MLLMs for pose guidance and appearance preservation, along with a new dataset. It does not involve reinforcement learning, human feedback for AI alignment, reward models, or fine-tuning based on human preferences, which are core to RLHF. Thus, there is no connection to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668372",
      "updated_at": "2025-08-11T23:43:05.606988",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16814",
      "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking\n  Reasoning",
      "authors": [
        "Junhao Shen",
        "Haiteng Zhao",
        "Yuzhe Gu",
        "Songyang Gao",
        "Kuikun Liu",
        "Haian Huang",
        "Jianfei Gao",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Enhancing large vision-language models (LVLMs) with visual slow-thinking\nreasoning is crucial for solving complex multimodal tasks. However, since LVLMs\nare mainly trained with vision-language alignment, it is difficult to adopt\non-policy reinforcement learning (RL) to develop the slow thinking ability\nbecause the rollout space is restricted by its initial abilities. Off-policy RL\noffers a way to go beyond the current policy, but directly distilling\ntrajectories from external models may cause visual hallucinations due to\nmismatched visual perception abilities across models. To address these issues,\nthis paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for\nvision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy\nbehavior model by combining on-policy visual understanding from a trainable\nLVLM with off-policy slow-thinking reasoning from a language model, assigns\noutcome-based rewards to reasoning, and propagates visual rewards backward.\nThen LVLM learns slow-thinking reasoning ability from the obtained reasoning\ntrajectories using propagated rewards via off-policy RL algorithms. Extensive\nexperiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the\neffectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in\naverage, reaching state-of-the-art performance among open-source LVLMs on\nmultiple multimodal reasoning benchmarks, and even outperforms some\nclosed-source models (e.g., GPT-4.1) on the challenging MathVision and\nOlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.\nAnalysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy\nRL methods, offering a better policy initialization for further on-policy\ntraining.",
      "published_date": "2025-07-22T17:59:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16814v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16814v1",
      "latex_url": "http://arxiv.org/src/2507.16814v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "{figure}[t]\n  \n  [width=0.9 ]{./figures/overview.pdf}\n  {An overview of  {}.  {} samples reasoning trajectories $ {y}_i$ and calculates rewards for both reasoning $R( {y}_i)$ and visual understanding $R( {c}_i)$. Finally, it updates the LVLM policy via an off-policy method to cultivate visual slow-thinking reasoning abilities.}\n  {-16pt}\n\n {figure}\n\nLeveraging reasoning abilities to solve complex problems is a crucial step toward artificial general intelligence~. Recent large language models (LLMs),  , OpenAI o-series~ and DeepSeek-R1~, have excelled at complex problem solving by their slow thinking capabilities~. These advances inspire the community to explore large vision-language models (LVLMs) for complex multimodal problem solving ( , geometry problems) with visual slow thinking~.\n\nEarly explorations mainly improve the reasoning capability of LVLMs through pipeline-generated reasoning trajectories~ or distillation~, yet, these methods lead to pattern memorization rather than genuine improvement in visual reasoning~.\nInspired by the recent success of reinforcement learning (RL) for LLMs reasoning, especially DeepSeek-R1~, recent works explore on-policy algorithms ( , GRPO~) in LVLMs and try to elicit the reflection behavior in training~.\n\nHowever, since most LVLMs are trained through vision-text alignment data~ that lack slow-thinking reasoning trajectories, it is hard to sample slow-thinking behaviors from the output space of LVLMs. Consequently, the performance of on-policy RL are fundamentally bounded by the initial policy distribution, as they essentially only amplify existing behaviors inside LVLMs~.\n\nGiven the limitation of on-policy RL, off-policy learning offers a promising solution. By using alternative policy models to generate trajectories, it bypasses the limitations of the current policy distribution and enables learning beyond their initial abilities~.\nHowever, the visual features involved in off-policy slow-thinking reasoning trajectories may not be aligned with the own visual understanding abilities of LVLMs, leading to a conflict between the optimization direction and the visual understanding of LVLMs~, which severely exacerbates issues like hallucinations in LVLM visual slow-thinking reasoning~.\n\nTo overcome the limitations of both on-policy and off-policy RL for LVLMs reasoning, this paper proposes  {}, Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. As shown in Figure~,  {} consists of two steps: semi-off-policy sampling and policy optimization with propagated rewards.\n\nSpecifically,  {} builds a semi-off-policy behavior model by combining the on-policy visual understanding from the trainable LVLM with the off-policy slow thinking from an open-source reasoning LLMs ( , QwQ~).\nDuring trajectory collection,  {} first samples a batch of image understandings ( , detailed descriptions of images) from LVLMs for the same query, and then adopts reasoning models for sampling slow-thinking reasoning trajectories.\n\nIn the policy optimization process,  {} first calculates rewards for the slow-thinking reasoning trajectories of LLMs based on the verification results of the reasoning outcome. Because outcome-based rewards may cause the model to overlook the accuracy and completeness of visual content, we assign each visual understanding a reward via reward backpropagation to strengthen the link between visual understanding and reasoning.\n\nSuch a design not only enhances the slow-thinking abilities of LVLM but also optimizes the use of visual understanding in reasoning with visual rewards.\nFurthermore,  {} is scalable as it uses the propagation of outcome-based rewards instead of human or closed-source model annotations for ensuring the quality of visual understanding.\n\nExtensive experiments show that  {} effectively enhances the ability of performing slow thinking on challenging visual tasks. Experiments on both InternVL2.5 and InternVL3.0 at 8B and 38B show that  {} improves performance across various benchmarks. Specifically, InternVL3.0-38B +  {} reaches state-of-the-art results on several benchmarks, which achieves an average gain of +8.50% and attains 49.08% and 49.95% pass@1 accuracy on the challenging MathVision~ and OlympiadBench~ datasets, outperforming leading open-source models such as Qwen2.5-VL-72B~, as well as closed-source models like GPT-4.1~. Further results show that  {} outperforms both supervised fine-tuning and on-policy RL methods. Continued on-policy RL training with  {} leads to substantial improvements, achieving performance comparable to Claude-3.7-Sonnet on general college-level questions. Moreover, in-depth analysis and ablation studies including diverse settings and training data further verify the generality and scalability of  {}.\n\n% Therefore, this paper proposes  {}, a simple and scalable VIsual slow-thinking Semi-Off-policy reinforcement learning (RL) method. As shown in Figure~,  {} consists of three steps: semi-off policy model construction, slow thinking and visual understanding reward estimation, and policy updating. First,  {} utilizes the LVLM we aim to train and an open-source language model with slow thinking capabilities to build a semi-off-policy model with on-policy visual understanding and off-policy slow thinking. The LVLM samples a batch of image understandings from the training dataset, which are detailed descriptions of images, and then an open-source reasoning language model ( , QwQ~) rolls out slow thinking reasoning trajectories based on these image descriptions. Next, rewards for slow thinking and visual understanding are evaluated successively. The reward for slow thinking is derived from its outcome reward. For image understanding, we calculate its reward value through reward backpropagation. Our theoretical analysis shows that the rewards for the semi-off-policy model can be well approximated by a simple reward propagation strategy. Finally, the reasoning and visual understanding rewards are used to update the LVLM's reasoning strategy. This process not only uses reasoning rewards to enhance the LVLM's slow thinking abilities but also uses visual rewards to guide the LVLM in optimizing the strategy of leveraging its own visual understanding during reasoning.  {} addresses the challenge of optimally using its visual capabilities during slow thinking in off-policy learning for LVLMs. Notably,  {} does not require human or closed-source model annotations, allowing for rapid scalability.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/intro.tex",
      "rlhf_score": 0.495,
      "weak_supervision_score": 0.41,
      "diffusion_reasoning_score": 0.531,
      "distributed_training_score": 0.381,
      "datasets_score": 0.285,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on semi-off-policy RL with outcome-based rewards derived from automated verification (e.g., benchmark accuracy), not human-ranked data or a separate reward model trained on human preferences, which are core to RLHF.",
      "weak_supervision_justification": "The paper uses programmatically generated rewards from high-level outcome verification (e.g., reasoning results on benchmarks) to train the model, aligning with weak supervision's reliance on noisy or imprecise sources rather than hand-labeled data, though it is not the primary focus.",
      "diffusion_reasoning_justification": "The paper introduces a semi-off-policy RL method for reasoning trajectories, with no mention of diffusion models, iterative refinement processes, or treating chain-of-thought as a holistically corrected entity, which are key to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper introduces SOPHIA, a semi-off-policy reinforcement learning method aimed at enhancing large vision-language models (LVLMs) with slow-thinking reasoning capabilities for complex multimodal tasks, addressing limitations in traditional on-policy and off-policy RL by combining the LVLM's visual understanding with an external language model's reasoning trajectories. The methodology involves sampling visual understandings from the LVLM, generating reasoning trajectories using an off-policy language model, assigning outcome-based rewards, and propagating rewards for training, leading to significant performance improvements on benchmarks like MathVision and OlympiadBench, where it outperforms leading open-source and closed-source models.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a novel semi-off-policy RL approach that combines on-policy visual understanding with off-policy reasoning, effectively addressing key limitations in existing methods for LVLMs and advancing the state-of-the-art in multimodal reasoning.",
      "impact_score": "High",
      "impact_justification": "The work's substantial performance gains on challenging benchmarks and its ability to outperform established models indicate it could broadly influence future research in vision-language models and reinforcement learning applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality, innovative contribution with strong empirical results that advance multimodal AI, making it essential for researchers in the field to review for its practical insights and potential applications.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/f8d5755d5a9d3cfc67689f979ca6167fc076f53c",
      "h_index_fetch_method": "title_search",
      "total_authors": 8,
      "authors_found": 8,
      "highest_h_index": 2,
      "average_h_index": 1.125,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Alex Zhihao Dou",
          "profile_url": "https://www.semanticscholar.org/author/2360712057",
          "h_index": 1
        },
        {
          "name": "Dongfei Cui",
          "profile_url": "https://www.semanticscholar.org/author/2360632171",
          "h_index": 1
        },
        {
          "name": "Jun Yan",
          "profile_url": "https://www.semanticscholar.org/author/2360855966",
          "h_index": 1
        },
        {
          "name": "Weida Wang",
          "profile_url": "https://www.semanticscholar.org/author/2332666924",
          "h_index": 2
        },
        {
          "name": "Benteng Chen",
          "profile_url": "https://www.semanticscholar.org/author/2360925191",
          "h_index": 1
        },
        {
          "name": "Haoming Wang",
          "profile_url": "https://www.semanticscholar.org/author/2360877746",
          "h_index": 1
        },
        {
          "name": "Zeke Xie",
          "profile_url": "https://www.semanticscholar.org/author/2363411385",
          "h_index": 1
        },
        {
          "name": "Shufei Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2360841384",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669136",
      "updated_at": "2025-08-11T23:45:52.679092",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16815",
      "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent\n  Planning",
      "authors": [
        "Chi-Pin Huang",
        "Yueh-Hua Wu",
        "Min-Hung Chen",
        "Yu-Chiang Frank Wang",
        "Fu-En Yang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Vision-language-action (VLA) reasoning tasks require agents to interpret\nmultimodal instructions, perform long-horizon planning, and act adaptively in\ndynamic environments. Existing approaches typically train VLA models in an\nend-to-end fashion, directly mapping inputs to actions without explicit\nreasoning, which hinders their ability to plan over multiple steps or adapt to\ncomplex task variations. In this paper, we propose ThinkAct, a dual-system\nframework that bridges high-level reasoning with low-level action execution via\nreinforced visual latent planning. ThinkAct trains a multimodal LLM to generate\nembodied reasoning plans guided by reinforcing action-aligned visual rewards\nbased on goal completion and trajectory consistency. These reasoning plans are\ncompressed into a visual plan latent that conditions a downstream action model\nfor robust action execution on target environments. Extensive experiments on\nembodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct\nenables few-shot adaptation, long-horizon planning, and self-correction\nbehaviors in complex embodied AI tasks.",
      "published_date": "2025-07-22T17:59:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16815v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16815v1",
      "latex_url": "http://arxiv.org/src/2507.16815v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent advances in multimodal large language models (MLLMs)~ have led to impressive progress on various tasks requiring the understanding of multimodal inputs, such as visual question answering and image/video captioning. However, while multimodal content can now be effectively perceived and interpreted, conducting multi-step planning for long-horizon user goals and then interacting with dynamic environments remains challenging for frontier MLLMs. Therefore, enabling the vision-language foundation models with action awareness and embodied reasoning capabilities unleashes a wide range of physical AI applications (e.g., robotics and AR assistance), and draws significant attention from both academics and industry.\n\nTo bridge action with vision-language modalities, several works~ learn vision-language-action (VLA) models by initializing from pre-trained MLLMs and training on large-scale robotic demonstrations (e.g., Open X-Embodiment Dataset~). For example, OpenVLA~ builds upon MLLMs with post-training on large-scale robot demonstrations, while TraceVLA~ further applies visual traces prompting to enhance spatial context understanding. Despite promising on short-horizon skills, the crucial capabilities to reason in diverse visual scenes and enable long-horizon planning remain limited due to the end-to-end fashion from visual and textual inputs to low-level actions.\n\n {figures/teaser}\n\nTo equip VLAs with the ability to solve complex embodied tasks, recent works~ have explored incorporating explicit chain-of-thought (CoT) prompting~ as an intermediate step-by-step guidance. For instance, ECoT~ and RAD~ introduce data curation pipelines to generate intermediate steps and decomposed plans by prompting off-the-shelf MLLMs. Once the annotated CoT traces are obtained, VLAs are trained to predict intermediate steps via fully supervised fine-tuning (SFT). However, due to the high cost of producing high-quality reasoning traces, the resulting models are prone to overfitting to specific visual scenes or reasoning patterns.\n\nRecently, reinforcement learning (RL)~ has demonstrated significant potential to incentivize reasoning behaviors in LLMs by exploring the thinking trace that maximizes reward signals instead of solely relying on fully supervised CoT annotations. Inspired by this paradigm, several vision-language models~ have applied RL-based reasoning to multimodal tasks. For example, Video-R1~ adopts R1-style RL optimization to induce the CoT traces by verifiable answer accuracy with format correctness. While this manner enables long-form reasoning without step-level supervision, the reliance on QA-style reward signals limits their ability to support long-horizon planning and makes it difficult to connect reasoning with real-world action execution.\n\nIn this paper, we propose  {}, which aims to enable MLLMs with the capability to reason before acting in physical environments. To address vision-language-action reasoning tasks,  {} adopts a dual-system architecture that connects structured reasoning with executable actions. Specifically, we incentivize MLLMs to perform long-horizon planning by advancing reinforcement learning with an action-aligned reward, derived from visual goal completion and trajectory distribution matching. Our  {} leverages human and robot videos to elicit embodied reasoning that is grounded in visual observations. To bridge reasoning and execution, we compress intermediate reasoning steps into a compact latent trajectory that captures high-level intent and allows efficient adaptation of the downstream action network to new environments. By reinforcing structured reasoning and grounding it in real-world actions,  {} tackles long-horizon manipulation tasks while unleashing few-shot action adaptation and self-correction behavior in physical AI scenarios, as shown in Fig.~.\n\nOur main contributions are summarized as follows:\n {itemize}\n   We propose  {}, a dual-system framework that mutually enhances action execution and visual-grounded embodied reasoning connected by visual latent planning.\n\n   We leverage the visual feedback of goal completion and trajectory alignment as action-aligned rewards to allow long-horizon reasoning grounded in the embodied scene.\n\n   We advance visual latent planning to steer downstream action execution by providing reasoning-enhanced trajectory guidance across diverse environments.\n\n   We demonstrate that our learned reasoning VLA enables capabilities of few-shot adaptation, long-horizon planning, and self-correction across diverse embodied manipulation tasks.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1_introduction.tex",
      "rlhf_score": 0.431,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.538,
      "distributed_training_score": 0.317,
      "datasets_score": 0.299,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses reinforcement learning with visual rewards based on goal completion and trajectory consistency, but it does not involve human feedback, human-ranked data, or a separate reward model trained on human preferences. Instead, rewards are derived from automated visual alignment, which does not align with the definition of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on reinforcement learning for generating reasoning plans and visual latent planning, with no mention of diffusion models, iterative refinement processes, or treating Chain-of-Thought as a holistically corrected entity. It lacks any components related to diffusion-based mechanisms for logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667701",
      "updated_at": "2025-08-11T23:43:05.606838",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16867",
      "title": "Diffusion-Modeled Reinforcement Learning for Carbon and Risk-Aware\n  Microgrid Optimization",
      "authors": [
        "Yunyi Zhao",
        "Wei Zhang",
        "Cheng Xiang",
        "Hongyang Du",
        "Dusit Niyato",
        "Shuhua Gao"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This paper introduces DiffCarl, a diffusion-modeled carbon- and risk-aware\nreinforcement learning algorithm for intelligent operation of multi-microgrid\nsystems. With the growing integration of renewables and increasing system\ncomplexity, microgrid communities face significant challenges in real-time\nenergy scheduling and optimization under uncertainty. DiffCarl integrates a\ndiffusion model into a deep reinforcement learning (DRL) framework to enable\nadaptive energy scheduling under uncertainty and explicitly account for carbon\nemissions and operational risk. By learning action distributions through a\ndenoising generation process, DiffCarl enhances DRL policy expressiveness and\nenables carbon- and risk-aware scheduling in dynamic and uncertain microgrid\nenvironments. Extensive experimental studies demonstrate that it outperforms\nclassic algorithms and state-of-the-art DRL solutions, with 2.3-30.1% lower\noperational cost. It also achieves 28.7% lower carbon emissions than those of\nits carbon-unaware variant and reduces performance variability. These results\nhighlight DiffCarl as a practical and forward-looking solution. Its flexible\ndesign allows efficient adaptation to different system configurations and\nobjectives to support real-world deployment in evolving energy systems.",
      "published_date": "2025-07-22T03:27:07+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16867v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16867v1",
      "latex_url": "http://arxiv.org/src/2507.16867v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{I}{n} recent years, microgrids have gained increasing attention in many nations with increasing utilization of renewable energy sources as substitutes of traditional fossil energy. Such transition not only brings economic benefits but also contributes to environmental sustainability by reducing emissions of carbon dioxide and other greenhouse gases. Microgrids adopt a localized and decentralized approach to manage energy with higher resilience and flexibility than traditional centralized grids. In addition to renewable distributed generation (RDG), microgrids often integrate energy storage systems (ESS), controllable diesel generation (CDG), and loads. While conventional microgrids typically involve control within a bounded system, recent developments have expanded this concept. The notion of a microgrid community (MGC) has been introduced to represent decentralized and cooperative energy systems that span multiple microgrids for achieving collective energy scheduling and optimization goals.\n\nAt the system level, the goal of operating microgrids and MGC is to maximize efficiency, reduce costs, and minimize environmental impact while ensuring stability and reliability. Practical energy scheduling faces key challenges, including the uncertainty of renewable generation, demand variability, and coordination of heterogeneous components under constraints like limited communication and decentralized control. Operational limits, such as ramping rates, storage state-of-charge, and network constraints, add non-linearity and complicate real-time optimization . In large-scale MGC, the growing number of decision variables and amplified uncertainty further increase complexity, making effective inter-microgrid coordination essential. These challenges demand robust, scalable, and adaptive solutions responsive to dynamic conditions.\n\nNumerous studies have addressed energy scheduling and optimization in microgrids. Early approaches formulated the problem as mixed-integer nonlinear programming (MINLP) or as mixed-integer linear programming (MILP) with linearization for tractability . To incorporate real-time information, model predictive control (MPC) was introduced as an extension of MILP-based frameworks , offering some robustness but still limited to single or centralized microgrids. To improve scalability, hierarchical and distributed frameworks have been proposed, such as a two-level hierarchical MILP for negotiation among agents and a distributed MILP for coordinating interconnected microgrids . Despite these advances, numerical optimization remains computationally expensive and relies on accurate predictions of renewables, loads, and markets, which is a significant challenge in practice.\n\nRecently, data-driven machine learning (ML), particularly deep reinforcement learning (DRL), has been explored to improve optimization performance and efficiency. The problem is often cast as a Markov decision process (MDP), where agents learn near-optimal policies by interacting with the environment, avoiding the need for precise forecasts. DRL has shown fast convergence and strong performance in applications like battery operation optimization, demand response, and energy management . However, early value-based methods like Q-learning and DQN struggle with the large state and action spaces typical in microgrids. To address this, imitation learning (IL) has been used to leverage expert demonstrations for faster training and near-optimality , but suffers from overfitting and distributional shift due to reliance on expert policy quality . More advanced methods such as DDPG , SAC , PPO , and TD3 have since been developed.\n\nDespite these advances, energy scheduling in microgrid clusters (MGC) remains challenging due to dynamic environments, combinatorial action spaces, and multi-agent coordination. Recently, generative AI (GenAI) has emerged as a promising approach, enhancing robustness, interpretability, and adaptability under uncertainty. GenAI shifts from deterministic to probabilistic and adaptive decision-making, augmenting DRL by modeling distributions over actions rather than fixed policies. Its ability to emulate experts while generalizing to novel conditions makes it a valuable tool for intelligent energy scheduling. In , GenAI integrated with SAC improved performance in a non-energy domain. In , a GenAI-based DRL algorithm for multi-energy scheduling was proposed but still derived a deterministic policy, limiting robustness under high uncertainty. These limitations underscore the need to further improve GenAI-based approaches for reliable scheduling in complex energy systems.\n\nIn this paper, we leverage the strength of GenAI and improve DRL for MGC energy scheduling and optimization. By formulating decision-making as a denoising generation process, diffusion policies offer both diversity and constraint-awareness, making them well-suited for dynamic, uncertain, and real-time energy environments. We propose  {DiffCarl}:  {diff}usion-modeled  {ca}rbon and  {r}isk-Aware Reinforcement  {l}earning, a novel framework that addresses key limitations in existing solutions in two main aspects. First,  {DiffCarl} leverages diffusion modeling to learn expressive and flexible policies capable of handling complex MGC dynamics and outperforming conventional DRL algorithms. Second, unlike many traditional solutions focusing solely on minimizing energy cost,  {DiffCarl} explicitly incorporates carbon intensity and operational risk into its decision-making. This integrated design is driven by the practical need for sustainable and resilient microgrids, where operations shall align with decarbonization goals and safety-critical constraints. Overall,  {DiffCarl} is a practical and forward-looking solution for energy scheduling under increasing system complexity and uncertainty. The main contributions of this paper are summarized as follows.\n\n {enumerate}\n   We formulate an energy scheduling and optimization problem and an interactive learning environment for MGC, explicitly incorporating carbon emission and operational risks into the formulation.\n   We propose  {DiffCarl} algorithm, which integrates diffusion model in DRL to optimize MGC energy schedules with improved performance in spite of the uncertainty of renewable, loads, and the market.\n   We apply carbon- and risk-awareness to  {DiffCarl} algorithm, which effectively minimizes carbon emission and reduces the risk of high operational cost, thereby satisfying the user's need of balancing cost efficiency, sustainability and manageable risk exposure.\n   We conduct extensive experiments to evaluate the performance of  {DiffCarl}. The results show that  {DiffCarl} achieves lower operational cost, less carbon emission and better risk management compared with several classic and state-of-the-art DRL algorithms.\n {enumerate}\n\nThe remainder of this paper is organized as follows. Section details the system architecture and the proposed algorithm  {DiffCarl}. Section reports the simulation studies and provides a comprehensive discussion. Finally, Section concludes the paper.\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "DiffCarl.tex",
      "rlhf_score": 0.398,
      "weak_supervision_score": 0.326,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.396,
      "datasets_score": 0.298,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses diffusion models in a deep reinforcement learning framework for energy scheduling in microgrids, specifically through a denoising generation process to enhance policy expressiveness and handle uncertainty. While this involves the iterative refinement process of diffusion models, it applies to action distribution learning in optimization tasks rather than multi-step logical reasoning or treating a Chain-of-Thought as a single entity for holistic correction. The core focus is on energy systems and RL, not complex logical tasks, making the connection indirect.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668821",
      "updated_at": "2025-08-11T23:43:05.607072",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16869",
      "title": "Controllable Video Generation: A Survey",
      "authors": [
        "Yue Ma",
        "Kunyu Feng",
        "Zhongyuan Hu",
        "Xinyu Wang",
        "Yucheng Wang",
        "Mingzhe Zheng",
        "Xuanhua He",
        "Chenyang Zhu",
        "Hongyu Liu",
        "Yingqing He",
        "Zeyu Wang",
        "Zhifeng Li",
        "Xiu Li",
        "Wei Liu",
        "Dan Xu",
        "Linfeng Zhang",
        "Qifeng Chen"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.",
      "published_date": "2025-07-22T06:05:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16869v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16869v1",
      "latex_url": "http://arxiv.org/src/2507.16869v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "As interest in AI-generated content (AIGC) continues to grow, video generation—one of its key domains—has emerged as a prominent focus for both researchers and users alike. Modern video generation methods~ typically leverage cutting-edge generative paradigms (e.g., diffusion~ or autoregressive models~), combined with large-scale datasets~, massive model parameters~, and advanced architectural frameworks~. We refer to these models as video generation foundation models, which have significantly advanced the quality of generated videos. The resulting outputs exhibit an unprecedented level of creativity. Despite their impressive generative capabilities, these models often remain constrained by their reliance on text-only conditioning, which limits the degree of control users can exert over the generated content. As a result, users frequently struggle to translate their creative ideas into precise video outputs, thereby diminishing the practical effectiveness of these models in real-world content creation scenarios.\n\n {figure}[!t]\n  \n  [width=1 ]{figures/paper_count_cropped.pdf}\n  {The development trend of controllable video generation methods across seven representative task categories. The line chart illustrates the rapid growth in the number of related works from 2022 to the present, with different categories distinguished by color. Representative works from each period are highlighted above the chart. For instance, VideoCrafter~ and EchoMimic~ have achieved 4.9k and 3.9k stars in Github, respectively. }\n  {-20pt}\n\n {figure}\n\nTo address this challenge, researchers have begun exploring ways to incorporate control signals beyond text, enabling more accurate and flexible guidance in the video generation process. For example, enabling users to modify camera trajectories or specify particular actions for characters in the video are emerging areas of interest. When fine-grained control over the generated content becomes possible, users are empowered with greater creative flexibility, thereby unlocking the full potential and practical value of video generation as a task.\n\nIn this survey, we focus on the task of controllable video generation, including both its theoretical foundations and practical applications. Our goal is to provide a comprehensive overview of the latest research advances and to shed light on the development trajectory of this rapidly evolving field. Specifically, we start by providing a brief overview of the background and core concepts of video generative models, providing their theoretical basis. This analysis clarifies the core principles of earlier research, fostering a deeper understanding of the field. Subsequently, the detailed reviews of previous studies are conducted to emphasize their unique contributions and distinctive features. Then we investigate the wide-ranging applications of these methods, highlighting their practical significance and influence across various contexts and related downstream tasks. Additionally, we deep discuss the limitations and future work about controllable video generation. In Fig.~, we present a line chart illustrating the number of controllable video generation studies utilizing various types of conditioning. As video foundation models have rapidly advanced, controllable video generation has also experienced significant growth.\n\nRecent survey papers provide extensively overviews of AI-generated content (AIGC), covering various areas such as video generation based on Generative Adversarial Networks and Variational AutoEncoders~, diffusion model theories and architectures~, efficient video diffusion models~, unified multi-modal video synthesis and understanding~, video editing~, foundational video diffusion models~, and 4D generation applications~. While these reviews offer valuable insights, many only provide a cursory examination of video generative models or predominantly concentrate on other modalities. This is a significant gap in the literature regarding controllable video generation. Additionally, existing studies rarely address this topic in various control signals, e.g., depth, sketch, segmentation map, leaving a critical void in understanding the potential for integrating novel conditions into video generative models and their implications for advancing controllable video generation.\n\nIn summary, our contributions are as follows:\n\n {itemize}\n   A well-structured taxonomy of controllable video generation methods is presented by classifying existing methods according to their input control signals, which facilitates understanding of existing methods and reveals core challenges in this field.\n\n   We present the theoretical foundations of GAN-, VAE-, Flow-, DM-, and AR-based architectures, along with recent video generation models built upon them, providing a clearer understanding of their underlying mechanisms.\n\n   Our survey introduces broad coverage of conditional generation approaches, structured around the proposed taxonomy, and emphasizes the defining traits and methodological innovations of each technique.\n\n   We investigate the practical impact of conditional generation within video models, covering a range of generative scenarios that reflect its increasing significance in the AIGC landscape. In addition, we identify key shortcomings of existing techniques and propose potential avenues for further exploration.\n\n {itemize}\n\nThe remainder of this paper is organized as follows: Sec.~ provides a concise overview of various generative paradigms. In Sec.~, we introduce representative video generation models, and presents a comprehensive taxonomy for controllable video generation. In Sec.~, we outline different control mechanisms, explain how novel conditions can be incorporated into video generation models, and summarize existing methods based on our proposed taxonomy. Sec.~ highlights key application scenarios of controllable video generation. Lastly, in Sec.~, we discuss several limitations of current research from both technical and practical perspectives, and propose promising directions for future work.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "section/1_introduction.tex",
      "rlhf_score": 0.328,
      "weak_supervision_score": 0.322,
      "diffusion_reasoning_score": 0.448,
      "distributed_training_score": 0.304,
      "datasets_score": 0.317,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper is a survey on controllable video generation, which discusses diffusion models in the context of video synthesis and denoising processes for incorporating conditions like camera motion or depth maps. However, it does not involve adapting diffusion for complex logical reasoning, Chain-of-Thought processes, or iterative refinement of reasoning paths. The connection is indirect, as both share the diffusion framework but in entirely different applications (generation vs. reasoning).",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669150",
      "updated_at": "2025-08-11T23:43:05.607119",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16872",
      "title": "CompLeak: Deep Learning Model Compression Exacerbates Privacy Leakage",
      "authors": [
        "Na Li",
        "Yansong Gao",
        "Hongsheng Hu",
        "Boyu Kuang",
        "Anmin Fu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Model compression is crucial for minimizing memory storage and accelerating\ninference in deep learning (DL) models, including recent foundation models like\nlarge language models (LLMs). Users can access different compressed model\nversions according to their resources and budget. However, while existing\ncompression operations primarily focus on optimizing the trade-off between\nresource efficiency and model performance, the privacy risks introduced by\ncompression remain overlooked and insufficiently understood.\n  In this work, through the lens of membership inference attack (MIA), we\npropose CompLeak, the first privacy risk evaluation framework examining three\nwidely used compression configurations that are pruning, quantization, and\nweight clustering supported by the commercial model compression framework of\nGoogle's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has\nthree variants, given available access to the number of compressed models and\noriginal model. CompLeakNR starts by adopting existing MIA methods to attack a\nsingle compressed model, and identifies that different compressed models\ninfluence members and non-members differently. When the original model and one\ncompressed model are available, CompLeakSR leverages the compressed model as a\nreference to the original model and uncovers more privacy by combining meta\ninformation (e.g., confidence vector) from both models. When multiple\ncompressed models are available with/without accessing the original model,\nCompLeakMR innovatively exploits privacy leakage info from multiple compressed\nversions to substantially signify the overall privacy leakage. We conduct\nextensive experiments on seven diverse model architectures (from ResNet to\nfoundation models of BERT and GPT-2), and six image and textual benchmark\ndatasets.",
      "published_date": "2025-07-22T08:02:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16872v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16872v1",
      "latex_url": "http://arxiv.org/src/2507.16872v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Driven by the large-scale data, DL has witnessed remarkable advancements, excelling in applications such as self-driving~, protein structure prediction ~, and the recent wave in Artificial Intelligence Generated Content (AIGC), including text generation through ChatGPT~ and image generation via diffusion model~.\nHowever, these outstanding state-of-the-art models are scaled with an increased number of parameters, which require considerable computational resources and memory footprint, challenging their deployment on resource-constrained devices.\n\nModel compression~ has been a mainstream technique to resolve such challenges, which are already widely used by industries.\nCommercially available compression frameworks, such as Google’s TF-Lite and Facebook’s PyTorch Mobile, facilitate the deployment of DL models on Internet of Things (IoT) and mobile devices~. These frameworks enable model providers to easily generate compressed models using operations like weight clustering, pruning, and quantization, either during training or post-training. Additionally, various toolkits support the compression of foundation models (e.g., LLMs) through methods such as quantization or distillation, helping to mitigate their substantial computational and storage demands~.\nFurthermore, model providers can also employ compression operations to provide interfaces for models with varying model sizes---larger models generally deliver superior performance but are more expensive---enabling users to selectively access customized models according to their needs and budget.\n\nLimitation\nWhile model compression greatly accelerates inference and reduces memory storage, it has been delicately studied and shown to be vulnerable to security attacks such as backdoor attacks~. However, the {  privacy risks} imposed by {  model compression} are overlooked and poorly understood, although the privacy risks of DL models have been widely studied~.\n\nDL models inherently memorize sensitive information from their training datasets, with MIA emerging as a commonly used auditing technique for evaluating such privacy risks~. MIA exploits a model’s tendency to overfit its training data, leading to significant differences in outputs between training set members and non-members. This vulnerability enables attackers to infer whether a given data sample was part of the training dataset, posing a significant threat to individual privacy. For instance, an attacker could deduce that a person participated in a confidential clinical trial by determining that their medical records were used to train a predictive model for an experimental drug. On the other hand, MIA can serve as a valuable tool for auditing privacy leakage, particularly in light of stringent privacy regulations such as the General Data Protection Regulation (GDPR)~, which mandates strong protections for user data.\n\nNotably, model compression operations for DL models have traditionally been employed to balance model capacity and performance, while the privacy risks stemming from compression remain unexplored---especially in scenarios where multiple compressed models are accessible. To our knowledge, the most relevant study is by Li et al.~, which assesses the privacy leakage of a pruned model via MIA. However, their evaluation is fundamentally different from our work, which is limited to reliance on information from a single-pruned model and does not account for the unique privacy risks introduced by model compression, where new insights could be derived by correlating information across different compressed model versions and the original model.\n\nTo this end, we ask the following research questions to underscore the urgent need for a comprehensive investigation into the privacy risks of mainstream compression technologies.\n\n {mdframed}[backgroundcolor=black!10,rightline=false,leftline=false,topline=false,bottomline=false,roundcorner=2mm]\n Does model compression exacerbate privacy leakage with increasingly access to multiple compressed models? If so, to what extent does it amplify privacy risks?\n {mdframed}\n\nOur Work\n\nThis work, for the first time, unveils and confirms that model compression exacerbates privacy leakage through the lens of MIA as a privacy auditing approach.\nThe primary reason is that differing compression operations affect members and non-members differently, where different compressed model versions leak privacy in slightly different ways due to variations in their memorization capacity and the inherent randomness of compression operations (e.g., pruning at different or even identical rates, weight clustering with varying or identical numbers of clusters).\nConsequently, aggregating leakage from multiple sources amplifies the overall privacy risk.\nTo quantify the extent of this additional leakage, we design various MIA methods tailored for a wide range of compression scenarios, as illustrated in Figure~, primarily considering the number of accessible compressed model versions.  $_{  NR}$, which directly adopts existing MIA techniques, evaluates privacy leakage per compressed model without relying on any reference or paired model.  $_{  SR}$ introduces new MIA techniques to capture additional privacy leakage from a compressed model version paired with the original model. Furthermore,  $_{  MR}$ enhances MIA techniques to assess privacy risks when multiple compressed model versions are accessible, with or without the availability of the original model.\n\nBelow, we highlight the key findings of each variant under the   framework and brief its core attack design.\n\n {figure}[t]\n  \n  [trim=0 0 0 0,clip,width=0.35 ]{Figures/leak.pdf}\n  {Compression scenarios targeted by the three   variants: blue/green/yellow attacker icon stands for  $_{  NR}$/ $_{  SR}$/ $_{  MR}$ considering number of accessible compressed model versions.}\n\n {figure}\n\n { $_{  NR}$.}\nIn the context of  , we refer to existing MIA~ (training-based~, metric-based~), which conduct solely based on the leaked information from an underlying model itself---where no reference model is used---as  $_{  NR}$, to audit the vulnerabilities per compressed model with varying compression degrees.\nEach compressed model obtained through pruning, quantization, and weight clustering compression operations is evaluated in our experiments upon  $_{  NR}$.\nNotably, consistent with the results in~, we observe that highly compressed models are generally less vulnerable than the original model.\nFor instance, when an MLP-based attack meta-classifier~ targets a 90%-pruned VGG16 model on Mini-ImageNet, the attack accuracy drops by 5% compared to the original model.\nThis reduction is potentially because high-level compression significantly limits model capacity and suppresses overfitting~.\nOn the contrary, pruning with lower sparsity, quantization into 8-bit integers, and weight clustering with more centroids exhibit comparable privacy leakage to the uncompressed model.\n\nDespite the overall MIA accuracy remaining relatively consistent across different compressed models, the way each compressed model affects members and non-members varies. This variation serves as the foundation for additional privacy leakage, where leaked information is newly captured through  $_{  SR}$ and  $_{  MR}$.\n\n { $_{  SR}$.}\n\nWe note that the impact induced by compression operations (e.g., on the posterior probability distribution) varies substantially between members and non-members, as shown in Figure~. Therefore, our insight is that capturing the subtle alterations imposed by compression operations will amplify privacy.\nBased on this intuition, instead of using information from a single model only, we incorporate leaked information from a compressed model and pair it with the information from the original model to improve the MIA performance. We refer to this   variant as  $_{  SR}$ as a single reference model is utilized.\n\n {figure}[t]\n  \n  [trim=0 0 0 0,clip,width=0.4 ]{Figures/tiny_mobilenet_prune_0.4.pdf}\n  {The KL divergence between the two posteriors on the same samples, obtained from the original MobileNetV2 (trained on Tiny-ImageNet) and the 40% pruned MobileNetV2.}\n\n {figure}\n\nGenerally,  $_{  SR}$ utilizes meta-data construction to combine a pair of posteriors, one from the original model and the other from a corresponding compressed version, to form meta-data that is used to train a binary meta-classifier for membership inference.\nFrom the experimental results, regardless of the compression operation's type and degree,  $_{  SR}$ exhibits strong capabilities, providing evidence that model compression indeed threatens privacy. Specifically,  $_{  NR}$~ achieves 60% MIA accuracy on the original VGG16 trained on Mini-ImageNet, which drops to 55% on the pruned model with 90% parameter removal. As a comparison, when exploiting the two models together, our  $_{  SR}$ significantly improves the accuracy to 74%, a 19% accuracy gain.\n\n { $_{  MR}$.}\nModel service providers typically release a set of compressed models (i.e., more than one) with varying capacities via different interfaces~, so we devise  $_{  MR}$ to exploit multiple compressed models as references to further amplify the privacy leakage.\nWhen we intuitively adopt the same methodology as  $_{  SR}$ to combine the posteriors generated by multiple compressed models for the same target sample as meta-data, no improvement is observed in the attack performance compared to  $_{  SR}$.\nWe argue that this occurs because  $_{  SR}$ utilizes the variation in posteriors between the original model and a compressed model, whereas the variation in posteriors across multiple versions is minimal and more challenging to interpret.\n\nEncouragingly, we discover two remarkable phenomena that support our intuition that different compressed models leak privacy in slightly different ways.\nFirst, building on  $_{  SR}$, although it becomes complicated to directly capture subtle varieties among the posteriors generated by multiple compressed models, we observe that membership inference results from  $_{  SR}$ attack meta-models---each trained to target a certain level compressed model---on the same target sample exhibit notable differences.\nFor example, the membership status predicted by  $_{  SR}$ shows approximately 22% discrepancy when targeting an 80%-pruned VGG16 versus a 90%-pruned VGG16 on the Mini-ImageNet.\nSecond, we identify that as the compression degree increases, the evolution of loss calculated from compressed models using ground truth labels and the cross-entropy reveals disparities between members and non-members, both in direction and magnitude.\nSpecifically, the loss for members increases with higher sparsity levels, while the loss for non-members fluctuates.\n\nThe above new findings provide us with new insights into the design of  $_{  MR}$, aggregating leaks from multiple sources to further amplify the overall privacy risk.\nMore concretely, an adversary can first utilize posterior concatenation by querying each  $_{  SR}$ attack meta-classifier for the target sample to obtain a set of  $_{  SR}$ attack meta-posteriors, which are then concatenated.\nThen, in the loss concatenation step, the target sample is fed into each compressed model in ascending order of compression degree to compute a set of losses, which are also concatenated.\nFinally, the concatenated posteriors and losses are stacked to form meta-data, which is used to train a  $_{  MR}$ meta-classifier for membership inference.\nExtensive experiments show that multiple compressed models further exacerbate membership leakage compared to  $_{  SR}$ using a single compressed model, especially in TPR @ 0.1% FPR.\nAdditionally, we relax the adversary's knowledge, following~, by assuming they can only access multiple compressed versions, but not the original model. In this setting, the adversary cannot obtain  $_{  SR}$ attack meta-posteriors, instead, posterior concatenation refers to concatenating the posteriors obtained by querying each compressed model for the target sample. Experimental results indicate that although  $_{  MR}$ exhibits a decline in performance under this setting, it still outperforms the best  $_{  NR}$ targeting a single compressed or original model.\n\n {Contribution.} Our main contributions can be summarized as:\n {itemize}[leftmargin=*]\n   We propose  , the first systematic privacy risk evaluation framework that examines three widely used compression operations---pruning, quantization, and weight clustering---through the lens of membership inference attacks.\n   We employ the existing MIA as  $_{  NR}$, relying solely on information from the underlying model, to comprehensively assess the privacy leakage per compressed model ({  Section~}).\n   We present  $_{  SR}$ for a single compression scenario, unveiling that regardless of the compression degree, the compression operations indeed jeopardize privacy ({  Section~}).\n   We propose  $_{  MR}$ aggregating leaks from multiple compressed models to further amplify the privacy leakage caused by model compression ({  Section~}).\n   We conduct extensive experiments in both the classic image domains and the emerging field of foundation models to demonstrate the effectiveness of  .\n {itemize}\n\n {Ethic and Privacy Considerations.}\nAll our experiments are conducted on publicly available datasets that are widely used in related privacy leakage research, and we strictly adhere to their respective usage licenses.\n\nAlthough we use commercial toolkits like TensorFlow-Lite for model compression, the observed privacy leakage arises from general model compression techniques, not from any specific tool implementation. This aligns with the findings in~, where TensorFlow Lite was used to demonstrate a backdoor attack by exploiting model compression. The TensorFlow Lite security team acknowledged~ that this vulnerability could not be mitigated through changes to the implementation, as it stems from the fundamental design of post-training quantization.\n\nTo mitigate such risks, our findings strongly suggest that model providers adopt a range of strategies---such as incorporating differential privacy, training with synthetic data, and reducing model overfitting---before releasing models through query APIs. These practices help safeguard user data and ensure ethical standards in the deployment of model compression techniques.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.37,
      "weak_supervision_score": 0.358,
      "diffusion_reasoning_score": 0.357,
      "distributed_training_score": 0.391,
      "datasets_score": 0.303,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668830",
      "updated_at": "2025-08-11T23:43:05.607074",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16873",
      "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for\n  Personalized Video Highlighting",
      "authors": [
        "Jeongeun Lee",
        "Youngjae Yu",
        "Dongha Lee"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.",
      "published_date": "2025-07-22T08:24:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16873v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16873v1",
      "latex_url": "http://arxiv.org/src/2507.16873v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "As the scale and diversity of video content rapidly grow in the real world, it becomes increasingly important for users to digest long-form videos efficiently within limited time and resources~ {huang2020movienet, apostolidis2021video, argaw2024towards}.\nIn this context, various research tasks have emerged to generate shorter, more consumable versions of videos—such as video summarization~ {park2020sumgraph, xu2024mh}, highlight detection, and moment retrieval~ {lin2023univtg, sun2024tr, xiao2024bridging, xu2024mh}.\n\nHowever, these tasks often overlook the importance of personalization in the real world where important moments vary significantly among users.\nTailoring to individual interests can better meet the demand for user-centric content delivery than a one-size-fits-all approach.\nWhile some prior works in query-focused video summarization~ {vasudevan2017query, xiao2020query, xiao2020convolutional} and moment retrieval~ {liu2018attentive, zeng2022moment} have explored aspects of personalization, they typically reduce user preferences to a single phrase or feature, oversimplifying the complexity of human interest.\nIn reality, human preferences are multifaceted, evolving over time and across different types of content.\nTo address this, we propose leveraging watch history as a richer source of user preference modeling.\nWe contend that analyzing users’ sequential viewing behavior through their watch histories can uncover underlying preferences, leading to more effective and tailored video experiences.\n\nIn this work, we introduce  , a novel task that leverages a user’s watch history within a single session to tailor video highlights to the user's preferences.\nInspired by how recommender systems effectively capture user interests through implicit feedback, such as interaction history~ {rendle2009bpr, kang2018self}, our task aims to dynamically select and present highlight segments aligned with the user’s real-time viewing behavior and preferences during the session.\nFor instance, as shown in Figure~, the same video may yield distinct highlights depending on the user’s focus inferred from their watch history, emphasizing different aspects of content.\n\nFor this task, we introduce  :  {Hi}ghlights Based on  {P}references for  {P}ersonalized Vide {O} Clipping, a large-scale dataset containing user watch histories and corresponding personalized saliency scores, generated by simulating real-world user behavior on video platforms.\nExisting video datasets~ {gygli2014creating, song2015tvsum, sharghi2016query} are often limited in scale due to resource-intensive nature of manual annotation, while collecting actual users' watch histories raises privacy concerns.\nTo address these challenges, we leverage large language models (LLMs) to simulate user interactions, enabling scalable data generation without compromising user privacy.\n  consists of 2,040 (watch history, saliency score) pairs, where each watch history comprises 10 videos, thereby totaling 20,400 videos, across 170 semantic initial preference seeds.\n\nThrough experiments, we validate our task and dataset using a simple baseline,  {Hi}story-Driven  {P}reference-Aware Video  {H}ighlight {er}, named  { }, which leverages user preferences derived from watch history as preference context.\n  outperforms existing methods by incorporating personalized preference embeddings from watch histories, while generic methods often fail to align with individual user interests, and query-focused methods struggle to capture the complexity of preferences with short queries.\nThese results underscore the importance of incorporating detailed user histories to enhance user-specific video highlighting, demonstrating the effectiveness of history-driven preference modeling.\n\n {figure*}[t]\n  \n  [width= ]{FIG/intro_figure.pdf}\n  {A video can produce varying highlights based on user interests, showing how watch history reflects implicit feedback and helps tailor highlights to individual preferences.}\n\n {figure*}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "010introduction.tex",
      "rlhf_score": 0.472,
      "weak_supervision_score": 0.343,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.314,
      "datasets_score": 0.373,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is the creation of a dataset (HIPPO-Video) using LLMs to simulate user watch histories for personalized video highlighting, and a method (HiPHer) to predict saliency scores based on these simulations. It does not involve reinforcement learning, human feedback for training a reward model, or fine-tuning models with RL techniques. Since RLHF specifically requires human-ranked data and reinforcement learning for alignment, this paper lacks any connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667553",
      "updated_at": "2025-08-11T23:43:05.606804",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16874",
      "title": "Budget Allocation Policies for Real-Time Multi-Agent Path Finding",
      "authors": [
        "Raz Beck",
        "Roni Stern"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.RO (Robotics)"
      ],
      "abstract": "Multi-Agent Pathfinding (MAPF) is the problem of finding paths for a set of\nagents such that each agent reaches its desired destination while avoiding\ncollisions with the other agents. Many MAPF solvers are designed to run\noffline, that is, first generate paths for all agents and then execute them.\nReal-Time MAPF (RT-MAPF) embodies a realistic MAPF setup in which one cannot\nwait until a complete path for each agent has been found before they start to\nmove. Instead, planning and execution are interleaved, where the agents must\ncommit to a fixed number of steps in a constant amount of computation time,\nreferred to as the planning budget. Existing solutions to RT-MAPF iteratively\ncall windowed versions of MAPF algorithms in every planning period, without\nexplicitly considering the size of the planning budget. We address this gap and\nexplore different policies for allocating the planning budget in windowed\nversions of standard MAPF algorithms, namely Prioritized Planning (PrP) and\nMAPF-LNS2. Our exploration shows that the baseline approach in which all agents\ndraw from a shared planning budget pool is ineffective in over-constrained\nsituations. Instead, policies that distribute the planning budget over the\nagents are able to solve more problems with a smaller makespan.",
      "published_date": "2025-07-22T08:32:55+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16874v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16874v1",
      "latex_url": "http://arxiv.org/src/2507.16874v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The Multi-Agent Pathfinding (MAPF) problem involves finding collision-free paths for multiple agents navigating a shared environment from given initial positions to their designated targets. MAPF has many applications including automated warehouse, traffic management, and digital entertainment.\nFrom the computational complexity point of view, the problem is NP-Hard to solve optimally for various optimization criteria~ and even NP-Hard to solve at all in directed graphs~.\nNevertheless, many fast MAPF algorithms exists and can scale to solve MAPF problems with thousands of agents very quickly.\n\nIn this work, we focus on solving MAPF under real-time constraints, which means that the agents must commit to perform their next sequence of\n actions within a fixed, strict time budget. We refer to this problem as Real-Time MAPF (RT-MAPF).\n The real-time constraints in RT-MAPF are common in realistic applications, especially where path planning is but one part of a larger system. For example, in an automated warehouses path planning must occur in tandem with target allocation, low-level robotic control, and other considerations. Clearly, the robots in a warehouse may not wait in their place until planning is done, and planning must be interleaved with execution.\n\nInterleaving planning and execution in general has been studied before, even in the context of real-time constraints in MAPF.\nOne approach is to use a fast rule-based or learned policy~.\nWhile fast, such approaches tend to produce lower quality solutions due to their myopic nature.\nA more common alternative, called ``windowed planning'', involves iteratively planning for a limited horizon, ignoring collisions that may occur after the horizon~.\nWindowed planning, however, is not guaranteed to return a solution within the available planning budget.\nThe key question we consider in this work is: given a fixed planning budget, what is the best way to allocate it in cases where finding a solution for all agents is not feasible?\n\n {morag2023adapting} proposed a framework for handling cases where the planner is not able to return a solution in time, proposing several fail policies that take a partial solution and modify it such that conflicts are avoided.  {zhang2024planning} proposed a similar framework, where planning is done during execution, as well as a more sophisticated fail policy. However, their focus was not on how to allocate the planning budget effectively to ensure a high quality partial solution is returned.\n\nWe fill this gap, and explore simple yet effective methods for allocating the budget used for planning within these frameworks. We show that by allocating the budget evenly between the agents, planners based on Prioritized Planning (PrP) can output significantly more useful partial solutions. Then, we consider different ways in which the planning budget can be allocated in the   algorithm, which is a state of the art MAPF algorithm. Specifically, we propose a simple method to compute how much planning budget should be given to every subset of agents, based on the number of conflicts they are involved in.\n\nExperimental results over a set of standard MAPF maps show that using the proposed budget allocation policies yields significant advantage in terms of the ability to solve problems within a reasonable makespan.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.345,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.309,
      "distributed_training_score": 0.32,
      "datasets_score": 0.212,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669532",
      "updated_at": "2025-08-11T23:43:05.607160",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16876",
      "title": "Machine learning-based multimodal prognostic models integrating\n  pathology images and high-throughput omic data for overall survival\n  prediction in cancer: a systematic review",
      "authors": [
        "Charlotte Jennings",
        "Andrew Broad",
        "Lucy Godson",
        "Emily Clarke",
        "David Westhead",
        "Darren Treanor"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Multimodal machine learning integrating histopathology and molecular data\nshows promise for cancer prognostication. We systematically reviewed studies\ncombining whole slide images (WSIs) and high-throughput omics to predict\noverall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL\n(12/08/2024), plus citation screening, identified eligible studies. Data\nextraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed\nSWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).\n  Forty-eight studies (all since 2017) across 19 cancer types met criteria; all\nused The Cancer Genome Atlas. Approaches included regularised Cox regression\n(n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged\n0.550-0.857; multimodal models typically outperformed unimodal ones. However,\nall studies showed unclear/high bias, limited external validation, and little\nfocus on clinical utility.\n  Multimodal WSI-omics survival prediction is a fast-growing field with\npromising results but needs improved methodological rigor, broader datasets,\nand clinical evaluation.\n  Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687),\nsupported by UKRI Industrial Strategy Challenge Fund.",
      "published_date": "2025-07-22T11:02:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16876v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16876v2",
      "latex_url": "http://arxiv.org/src/2507.16876v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.296,
      "datasets_score": 0.338,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669542",
      "updated_at": "2025-08-11T23:43:05.607162",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16877",
      "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression\n  Comprehension",
      "authors": [
        "Yizhi Hu",
        "Zezhao Tian",
        "Xingqun Qi",
        "Chen Su",
        "Bingkun Yang",
        "Junhui Yin",
        "Muyi Sun",
        "Man Zhang",
        "Zhenan Sun"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.",
      "published_date": "2025-07-22T11:23:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16877v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16877v1",
      "latex_url": "http://arxiv.org/src/2507.16877v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Referring Expression Comprehension (REC)~ aims to localize specified entities in an image based on natural language descriptions. It requires the seamless integration of visual perception and linguistic understanding to accurately map textual cues to corresponding regions in an image. REC plays a crucial role in bridging the gap between language and vision, with applications spanning visual question answering~, vision-language navigation~, human-machine interaction~.\n\nEarly studies begin with two-stage methods~, which generate a set of region proposals and then select one or more regions based on the matching degree between the candidate content and the query phrase.\nSubsequently, single-stage methods~ directly predict the referred regions by using manually designed dense anchors.\nMore recently, transformer-based end-to-end methods~ have been introduced to regress the coordinates of the target regions.\n\nThese above-mentioned methods mostly depend on the pre-defined query phrases, yet struggle to dynamically adapt on in-the-wild complex multi-entity scenes.\nMeanwhile, these approaches typically process each phrase independently, overlooking the exploration of inter-entity relationship and thereby constraining a comprehensive understanding of the global scenes.\nMoreover, few studies focus on constructing visual grounding datasets that incorporate rich inter-entity relationship.\n\nTherefore, in this paper, we propose a novel task for Relation-aware and Multi-entity Referring Expression Comprehension (ReMeREC) that directly predicts multiple entity regions and their relationships from the source image and natural language description, as illustrated in Figure~.\nThis task encounters two main challenges. 1) Existing visual grounding datasets mostly lack annotated relationship among multiple entities.\n\n2) This task requires synthesizing diverse phrase queries solely from a global textual description while simultaneously modeling inter-entity relationship, posing significant challenges in both entity delineation and relational reasoning.\n\nTo address the issue of data scarcity, we first construct the ReMeX dataset that contains multi-entity visual grounding enriched with fine-grained annotations.\nIt offers high-quality labels that not only delineate multiple entity regions within each image but also capture detailed relationships among these entities.\nAs shown in Figure~, each sample includes ground-truth bounding boxes for multiple entity regions along with the relationships among them.\nBy integrating these comprehensive annotations, MeReX provides a robust platform for both precise multi-entity grounding and nuanced relationship modeling, setting a solid foundation for advancing research in this challenging task.\n\n {figure}\n  \n  [width=0.47 ]{figure/Fig2-0329.png}\n  {Sample illustration of the proposed ReMeX dataset. The ReMeX dataset contains multi-entity visual grounding with detailed\n directional relationship annotations.}\n  {-6mm}\n\n {figure}\n\nBased on the ReMeX dataset, we introduce ReMeREC, a novel framework that effectively integrates both textual and visual cues to localize multiple entities while capturing their complex inter-entity relationship.\nThe core of ReMeREC lies in two key components: the Text-adaptive Multi-entity Perceptron (TMP) and the Entity Inter-relationship Reasoner (EIR).\nSpecifically, to address the challenge of entity span determination without explicit annotations in the source image, we propose the TMP to extract both the number and the range of entities directly from the textual description.\nTMP leverages a set of learnable entity queries that interact with the token-level features of the sentence via a transformer decoder, producing refined query representations and normalized position predictions for each potential entity.\nBuilding on this, TMP utilizes entity logits of the language backbone to generate candidate segments and aligns each refined query with the candidate whose center is closest to its predicted center.\n\nThe alignment process precisely refines the predicted boundaries, thereby guaranteeing that the entity spans are both accurate and context-aware.\nAdditionally, to facilitate a holistic understanding of directional spatial relationships and interactions among multiple entities, we present EIR to predict inter-entity relationships.\nEIR fuses global context with sentence-level features to compute predicate scores for each entity pair and measures subject-object similarity.\nThese scores represent the semantic distinctiveness of each entity and are aggregated to construct the global relation matrix.\nFinally, EIR adaptively modulates the entity features using the aggregated relation scores to refine the semantic and positional representations of the entities, thereby improving the accuracy of entity region grounding.\n\nFurthermore, to better capture fine-grained linguistic distinctions crucial for identifying multiple entity boundaries and their inter-relationship, we harness LLaMA~ to automatically generate a small-scale text dataset, termed EntityText.\nEntityText contains 20,000 annotations which are represented as the natural language description where tokens are categorized as either an entity or a non-entity.\nThis auxiliary dataset enriches the diversity and quality of textual cues, drawing enhanced language feature extraction.\n\nOverall, our contributions are summarized as follows:\n {-0.5em}\n {itemize}[leftmargin=*]\n   We propose a novel task for directly inferring multiple entity relationships from the source image and language description, cooperating with a newly dedicated dataset, namely ReMeX. ReMeX provides fine-grained annotations that facilitate a comprehensive understanding of multi-entity interactions in more complex scenes.\n   We propose ReMeREC, a novel framework that effectively integrates textual and visual cues to localize multiple entities while capturing their complex relationships.\n   We design the Text-adaptive Multi-entity Perceptron to extract multiple entity regions from textual descriptions with adaptive query learning. Additionally, we introduce the Entity Inter-relationship Reasoner to model inter-entity relationships and enhance contextual understanding.\n\n   Extensive experiments demonstrate that ReMeREC outperforms existing competitors across multiple benchmark datasets and achieves significant performance gains on the new task, setting a new standard for multi-entity grounding.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "section/1_intro.tex",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.42,
      "distributed_training_score": 0.334,
      "datasets_score": 0.39,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on Referring Expression Comprehension (REC) for localizing entities in images using visual and textual cues, introducing components like Text-adaptive Multi-entity Perceptron and Entity Inter-relationship Reasoner. It does not involve diffusion models, iterative refinement processes, or treating a Chain-of-Thought as a single entity for logical reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667710",
      "updated_at": "2025-08-11T23:43:05.606840",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16878",
      "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos",
      "authors": [
        "Xuchen Li",
        "Xuzhao Li",
        "Shiyu Hu",
        "Kaiqi Huang",
        "Wentao Zhang"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.",
      "published_date": "2025-07-22T12:29:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16878v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16878v1",
      "latex_url": "http://arxiv.org/src/2507.16878v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{table*}[htbp!]\n  \n  {Comparison between CausalStep and existing video understanding/reasoning benchmarks across key aspects: the number of videos (\\#Videos), video duration (Duration), number of reasoning QA pairs (\\#QA Pairs), spatio-temporal relationship understanding (Spatio-temporal), causal relationship understanding (Causal), stepwise reasoning protocol (Stepwise), and annotation methodology (Annotation). A denotes AI-generated, M denotes manual, and A\\&M indicates a combination.}\n  {tabularx}{ }{Xccccccc}\n  \n Benchmark & \\#Videos & Duration & \\#QA Pairs & Spatio-temporal & Causal & Stepwise & Annotation\n\n  \n MVBench & 200 & 15-20 s & 4,000 &   &   &   X & A\n\n TempCompass & 410 & 15-20 s & 7,540 &   &   X &   X & A\\&M\n\n Video-MMMU & 300 & 506.2 s & 900 &   X &   X &   X & M\n\n MMVU & 1,529 & 51.4 s & 3,000 &   X &   X &   X & M\n\n Video-MME & 900 & 35.7 s & 1,944 &   &   &   X & M\n\n VCR-Bench & 859 & 159 s & 1,034 &   &   &   X & A\\&M\n\n Video-Holmes & 270 & 160 s & 1,837 &   &   &   X & A\\&M\n\n MMR-V & 317 & 277 s & 1,257 &   &   &   X & A\\&M\n\n  \n Ours & 100 & 430.5 s & 1,852 &   &   &   & A\\&M\n\n  \n  {tabularx}\n\n  {-10pt}\n {table*}\n\nRecent advances in large language models (LLMs) have driven impressive progress in text , image , and general video understanding . However, extending these reasoning capabilities to complex, real-world video scenarios remains a major challenge. Video reasoning is fundamentally different from text or static images, as videos encode rich, sequential, and multimodal information that requires models to perform long-range, multi-frame reasoning and evidence integration across both temporal and spatial dimensions. This capability is essential for applications such as embodied intelligence , intelligent surveillance , and human-computer interaction .\n\nDespite recent progress, existing video reasoning benchmarks exhibit key limitations. Most benchmarks focus on perception or shallow understanding, requiring only the identification of relevant frames or context. Crucially, by typically providing the entire video as input, these benchmarks allow models to exploit global information or shortcut strategies, thereby failing to assess true causal and stepwise reasoning. As a result, they do not capture the causally grounded reasoning processes humans naturally employ when interpreting complex video narratives. Moreover, the design of distractor options in multiple-choice questions is often unsystematic, lacking systematic coverage of common reasoning errors and thus failing to rigorously challenge model robustness.\n\nTo address these gaps, we introduce CausalStep, a new benchmark specifically designed to evaluate explicit stepwise causal reasoning in videos. In CausalStep, each video is manually segmented into a sequence of causally linked segments. At each step, the model is given the current and previous segments (if any), without access to future information, and must answer a question—either a descriptive understanding question or an explicit causal reasoning question—before it can access the next. This protocol strictly enforces sequential, causally dependent reasoning and precludes the use of global shortcuts. Furthermore, we design a novel distractor generation strategy: for each multiple-choice question, distractor options are systematically constructed according to a taxonomy of error types, including temporal confusion, causal misattribution, and object misrecognition. This ensures each question not only tests surface-level perception but also challenges the model’s ability to distinguish between plausible but incorrect alternatives.\n\nCausalStep comprises 100 videos spanning six diverse categories (e.g., cartoons, movies, sports, performances, documentaries, and TV shows), totaling 1,852 multiple-choice question-answer (QA) pairs. Each question is carefully annotated and reviewed, covering both descriptive understanding and explicit stepwise causal reasoning tasks—enabling fine-grained analysis of models’ causal reasoning abilities. To provide a comprehensive assessment of model performance, we propose a suite of seven diagnostic metrics: chain success rate, average and maximum chain length, restart frequency, weighted score, and dedicated accuracies for descriptive understanding and isolated causal reasoning. These metrics capture not only overall accuracy but also the depth, stability, and robustness of a model’s reasoning process.\n\nWe conduct extensive experiments on CausalStep, evaluating a wide range of state-of-the-art proprietary and open-source multimodal models—including the latest GPT , Gemini , Claude , Qwen , Gemma , InternVL , LLaVA , and Phi series—as well as human participants. Our results reveal a substantial gap between current models and human-level performance, especially in explicit stepwise causal reasoning. This disparity is primarily driven by models' difficulty in maintaining continuous, error-free reasoning chains and their vulnerability to subtle distractors. These results show that even the strongest models struggle with long-range causal integration and are susceptible to confusable distractors, highlighting the need for further advances in video reasoning of multimodal large language models (MLLMs).\n\nOur main contributions are as follows:\n\n {itemize}\n  A novel benchmark for explicit stepwise causal reasoning in videos: We introduce CausalStep, which segments videos into causally linked units and enforces a strict stepwise QA protocol, enabling rigorous evaluation of sequential, causally grounded reasoning in complex video narratives.\n\n  A comprehensive annotation and evaluation framework: We design a hybrid annotation pipeline combining LLM generation and human review, and propose a taxonomy-based distractor generation strategy. We further introduce seven diagnostic metrics that provide a fine-grained, multi-dimensional assessment of model performance, covering reasoning depth, stability and robustness.\n\n  Extensive empirical analysis and insights: We benchmark a diverse set of state-of-the-art (SOTA) proprietary and open-source models, as well as human baselines, on CausalStep. Our experiments reveal a significant gap between current models and human-level stepwise causal reasoning, and provide actionable insights for future research on robust and interpretable video reasoning systems.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "aaai2026.tex",
      "rlhf_score": 0.337,
      "weak_supervision_score": 0.336,
      "diffusion_reasoning_score": 0.496,
      "distributed_training_score": 0.306,
      "datasets_score": 0.377,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces a benchmark called CausalStep for evaluating stepwise causal reasoning in videos, focusing on LLMs and MLLMs with sequential QA protocols. However, it does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning. The paper's contributions are centered on video reasoning benchmarks, not diffusion-based approaches, making it unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667565",
      "updated_at": "2025-08-11T23:43:05.606808",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16880",
      "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
      "authors": [
        "Antoni Kowalczuk",
        "Dominik Hintersdorf",
        "Lukas Struppek",
        "Kristian Kersting",
        "Adam Dziedzic",
        "Franziska Boenisch"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.",
      "published_date": "2025-07-22T15:02:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16880v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16880v1",
      "latex_url": "http://arxiv.org/src/2507.16880v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Generating high-quality images with diffusion models (DMs) enjoys great popularity. However, undesired memorization of training data in text-to-image DMs~ {somepalli2023understanding,carlini2023extracting} poses significant risks to privacy and intellectual property, as it can favor the unintended replication of sensitive or copyrighted content during inference. In response, various detection and mitigation strategies have been proposed~ {somepalli2023understanding,webster23extraction, wen_detecting_mem_in_diff, ren2024unveiling}. Most existing mitigation techniques either aim to identify and filter out highly memorized samples during training~ {somepalli2023understanding,ren2024unveiling} or modify inputs at inference time~ {somepalli2023understanding,wen_detecting_mem_in_diff,ren2024unveiling} to reduce memorization-induced data replication. While the training-based methods require computationally expensive retraining, the inference-time methods are limited to models behind APIs, as users of open-source models can easily disable these mechanisms by altering the source code.\n\nTo overcome both limitations, recent approaches~ {hintersdorf2024finding, chavhan2024memorization} observe that the text prompts of memorized images elicit distinct activation patterns in the DMs. Based on these activations, the methods prune a small set of weights, effectively reducing the risk of verbatim data replication, while preserving overall image quality. However, since these methods work with a single prompt per memorized image, it remains an open question whether they prevent the replication of memorized images through other inputs. We search for Diffusion Memorization (Dori~ {figures/dory.png}) beyond the prompt space by crafting adversarial embeddings---text embeddings different from the memorized prompts---that trigger memorized image generations. Adversarial embeddings allow us to recover supposedly removed memorized data after pruning (see~ {fig:teaser}, left), revealing that pruning merely conceals memorization.\n\nInitial effectiveness of pruning-based methods for mitigating memorization is attributed to a locality phenomenon, a property of the model to store memorized data in a small (local) set of memorization weights. Intuitively, if this property holds, then pruning memorization weights would prevent generation of memorized data. In this work, we challenge the locality phenomenon and question whether locality is real or a misinterpretation of the early success of pruning-based methods. We investigate the input space and the model's weights and find little support for it. Memorization seems to be spread out, as there exist multiple adversarial embeddings that cause replication of the same data point, with the DM following different paths during generation, see~ {fig:teaser} (right). Similarly, the activation patterns and memorization weights identified for the same memorized image vary across different inputs that trigger its replication, further undermining the notion of locality.\n\nAccordingly, a robust memorization removal method should avoid the pitfall of assuming locality. To this end, we build on adversarial embeddings and develop a novel adversarial fine-tuning approach to completely erase memorized samples from text-to-image DMs. Our approach is inspired by adversarial training~ {szegedy2013intriguing,goodfellow2014explaining,madry2018towards}, which iteratively generates adversarial examples to train robust models. While prior methods have focused on parameter pruning or input adjustments, our approach directly modifies the model’s parameters to eliminate memorization. In contrast to pruning-based mitigation techniques, our method achieves reliable removal and remains robust against adversarial embeddings designed to circumvent mitigation.\n\nIn summary, we make the following contributions:\n {enumerate}\n   We reveal that existing weight-pruning methods merely conceal memorization in text-to-image DMs rather than truly erase memorized content from a model.\n   We challenge the assumption that memorization is local, demonstrating that it fails to hold across both the input space and the model parameters.\n   As a more robust defense, we propose a novel adversarial fine-tuning scheme that permanently mitigates memorization in already trained DMs.\n {enumerate}\n\n {figure}\n  [width= ]{figures/fig1_combined.pdf}\n  {Left:  {1} Without mitigation, the DM closely replicates the training sample.  {2} Mitigation strategies, such as pruning memorization neurons with  ~ or  ~, prevent replication for the memorized prompt, thereby suggesting successful removal. Yet,  {3} adversarial embeddings  {figures/dory.png} still trigger replication.\n Right: While pruning alters the generation trajectory for the original memorized prompt (blue), adversarial embeddings steer denoising along alternative paths (red) that still lead to the memorized content, unaffected by the pruning-based mitigation.}\n\n {-0.2cm}\n {figure}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "content/01intro.tex",
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.536,
      "distributed_training_score": 0.395,
      "datasets_score": 0.348,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on memorization and mitigation strategies in text-to-image diffusion models, specifically addressing data privacy and intellectual property issues through pruning and fine-tuning methods. It does not involve adapting the iterative refinement process of diffusion models for solving complex logical tasks, such as treating a 'Chain-of-Thought' as a single entity for multi-step reasoning. Therefore, there is no component related to diffusion-based reasoning, making the paper unrelated to this topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667719",
      "updated_at": "2025-08-11T23:43:05.606842",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16881",
      "title": "Confidence Optimization for Probabilistic Encoding",
      "authors": [
        "Pengjiu Xia",
        "Yidian Huang",
        "Wenchao Wei",
        "Yuwen Tan"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Probabilistic encoding introduces Gaussian noise into neural networks,\nenabling a smooth transition from deterministic to uncertain states and\nenhancing generalization ability. However, the randomness of Gaussian noise\ndistorts point-based distance measurements in classification tasks. To mitigate\nthis issue, we propose a confidence optimization probabilistic encoding (CPE)\nmethod that improves distance reliability and enhances representation learning.\nSpecifically, we refine probabilistic encoding with two key strategies: First,\nwe introduce a confidence-aware mechanism to adjust distance calculations,\nensuring consistency and reliability in probabilistic encoding classification\ntasks. Second, we replace the conventional KL divergence-based variance\nregularization, which relies on unreliable prior assumptions, with a simpler L2\nregularization term to directly constrain variance. The method we proposed is\nmodel-agnostic, and extensive experiments on natural language classification\ntasks demonstrate that our method significantly improves performance and\ngeneralization on both the BERT and the RoBERTa model.",
      "published_date": "2025-07-22T15:32:27+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16881v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16881v1",
      "latex_url": "http://arxiv.org/src/2507.16881v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Probabilistic encoding, also known as uncertainty encoding, is a representation learning paradigm that maps deterministic feature vectors into probabilistic distributions, typically Gaussian, in a latent space. Unlike deterministic encoding, which represents data as fixed vectors, probabilistic encoding captures the inherent uncertainty in data, providing more prosperous and more flexible representations. By modeling features as distributions, it allows representations of the same category to form compact clusters while maintaining clear separability from other categories. The distribution of points in the feature space differs after deterministic encoding and probabilistic encoding, as illustrated in Fig..\n\n {figure}[b]\n  \n  [width=0.88 ]{fig1.pdf}\n  {Taking the offense eval task as an example, deterministic encoding maps data to a single point like (a); probabilistic encoding maps data to a distribution. The same category will share the same feature space.}\n\n {figure}\n\nProbabilistic encoding has a long history in machine learning, with early applications in word vector representations. In computer vision, the Variational Autoencoder (VAE) introduces probabilistic encoding into generative models by mapping inputs into Gaussian distributions parameterized by mean and variance. This enables the generation of unseen samples. This work has a profound impact on\nsubsequent deep learning and generative model, inspiring subsequent advances in stochastic models. More recently, Hedged Instance Embeddings (HIB) extends probabilistic encoding to image retrieval and verification tasks. By encoding images as Gaussian distributions, HIB demonstrates improved robustness in tasks like handwritten digit recognition, especially on noisy or corrupted data. These results underscore the potential of probabilistic encoding in scenarios that require uncertainty estimation, motivating further exploration across various domains.\n\nProbabilistic face embeddings (PFE) and Data Uncertainty Learning (DUL) are seminal works that demonstrate the power of probabilistic encoding in face recognition. PFE encodes each face as a latent Gaussian distribution, introducing the Mutual Likelihood Score (MLS) to measure the similarity between distributions. DUL further improves upon PFE by making both the mean and variance vectors learnable, achieving enhanced feature compactness and separability in the latent space. In addition to face recognition, probabilistic encoding has also been applied to multimodal retrieval, where data from different modalities are represented as distributions in a shared space, improving performance and interpretability. Recent advancements, such as Structured Probabilistic Coding (SPC), introduces structural regularization to learn more compact and enriched representations, showing promise in classification and regression tasks.\n\nDespite these advancements, current probabilistic encoding methods face notable limitations, particularly in classification tasks where robustness and reliability are crucial. For instance, while probabilistic encoding effectively captures uncertainty by mapping features into latent distributions, distance measurement in classification still relies on individual sampled points as illustrated in Fig., which lacks robustness and necessary constraints. The reliance on a single sampled point overlooks the confidence associated with that point. When the point is far from the center of the distribution, the low confidence can lead to unreliable distance measures and poor classification performance. Conversely, points closer to the center, with higher confidence, lead to more stable and accurate results. This results in inaccurate feature distances in the latent space, increasing intra-class variance and reducing inter-class separability. Addressing these issues is crucial to fully harness the potential of probabilistic encoding in real-world applications.\n\n {figure} [t]\n  \n  [width=1  ]{fig2.pdf}\n  {Encoding process of probabilistic models. Text input data is encoded as mean \\(  _i \\) and variance \\(  _i \\) embeddings. The sampling process from the reparameterized distribution is stochastic, and the distance to the center point can be large or small. This distance reflects the confidence of the sampled points: points farther from the center have lower confidence, while those closer have higher confidence.}\n\n {figure}\nAccordingly, we propose a confidence-based optimization method to enhance probabilistic encoding and further improve downstream performance. Our approach introduces additional constraints in the latent space, ensuring more consistent and reliable feature distances for downstream tasks. Specifically, we design a novel confidence-aware mechanism that mitigates the issue of overlapping feature distributions, improving both intra-class compactness and inter-class separability. Moreover, our method is model-agnostic and can be seamlessly integrated with various probabilistic encoding frameworks, making it broadly applicable across different architectures. The key contributions of our work are as follows:\n {itemize}\n   Novel confidence-aware mechanism: We propose a normalized confidence metric to adjust distance calculations, ensuring consistency and reliability in probabilistic encoding-based classification tasks.\n   Improved regularization strategy: We replace KL divergence with L2 regularization , avoiding using inaccurate prior assumptions and improving the learning process.\n   Enhanced downstream performance: We conduct extensive experiments on benchmark datasets and demonstrate that our approach significantly improves classification accuracy and generalization, outperforming state-of-the-art probabilistic encoding methods.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "root.tex",
      "rlhf_score": 0.367,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.395,
      "distributed_training_score": 0.337,
      "datasets_score": 0.298,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668839",
      "updated_at": "2025-08-11T23:43:05.607076",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16884",
      "title": "SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative\n  Modeling",
      "authors": [
        "Yi Guo",
        "Wei Wang",
        "Zhihang Yuan",
        "Rong Cao",
        "Kuan Chen",
        "Zhengyang Chen",
        "Yuanyuan Huo",
        "Yang Zhang",
        "Yuping Wang",
        "Shouda Liu",
        "Yuxuan Wang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Generative models like Flow Matching have achieved state-of-the-art\nperformance but are often hindered by a computationally expensive iterative\nsampling process. To address this, recent work has focused on few-step or\none-step generation by learning the average velocity field, which directly maps\nnoise to data. MeanFlow, a leading method in this area, learns this field by\nenforcing a differential identity that connects the average and instantaneous\nvelocities. In this work, we argue that this differential formulation is a\nlimiting special case of a more fundamental principle. We return to the first\nprinciples of average velocity and leverage the additivity property of definite\nintegrals. This leads us to derive a novel, purely algebraic identity we term\nInterval Splitting Consistency. This identity establishes a self-referential\nrelationship for the average velocity field across different time intervals\nwithout resorting to any differential operators. Based on this principle, we\nintroduce SplitMeanFlow, a new training framework that enforces this algebraic\nconsistency directly as a learning objective. We formally prove that the\ndifferential identity at the core of MeanFlow is recovered by taking the limit\nof our algebraic consistency as the interval split becomes infinitesimal. This\nestablishes SplitMeanFlow as a direct and more general foundation for learning\naverage velocity fields. From a practical standpoint, our algebraic approach is\nsignificantly more efficient, as it eliminates the need for JVP computations,\nresulting in simpler implementation, more stable training, and broader hardware\ncompatibility. One-step and two-step SplitMeanFlow models have been\nsuccessfully deployed in large-scale speech synthesis products (such as\nDoubao), achieving speedups of 20x.",
      "published_date": "2025-07-22T16:26:58+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16884v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16884v1",
      "latex_url": "http://arxiv.org/src/2507.16884v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The field of generative modeling has witnessed remarkable progress, with methods like Diffusion Models~ and Flow Matching~ setting new standards in generating high-fidelity samples across various domains, including images~, video~ and audio~. Despite their power, the practical utility of these models is often hindered by a significant computational bottleneck: their reliance on an iterative sampling process that typically requires tens or even hundreds of neural network inferences. This substantial computational cost poses a major challenge for real-world applications, particularly in resource-constrained or latency-sensitive environments. Consequently, a vibrant research area has emerged, focusing on developing ``few-step'' or even ``one-step'' generative models to drastically reduce this sampling overhead.\n\nIn response to this challenge, several innovative approaches have been proposed. Consistency Models~, for instance, introduced a novel training paradigm by enforcing output consistency for points along the same trajectory, achieving promising results in few-step generation. Building on this momentum, MeanFlow~ offered a profound and physically intuitive insight: for large-step generation, it is more effective to directly model the average velocity along the entire path connecting noise to data, rather than the instantaneous velocity at each point. This conceptual shift from a local to a global perspective is inherently better suited for few-step, large-stride predictions and has led to state-of-the-art performance.\n\nThe success of MeanFlow~ prompts a deeper investigation into its underlying mechanism. How does it effectively learn the average velocity field? A closer look reveals that its implementation does not directly compute the integral from its definition. Instead, it ingeniously leverages a differential identity that connects the average velocity \\( u \\) and the instantaneous velocity \\( v \\): \\( u = v - (t-r)du/dt \\). While remarkably effective, this differential formulation raises a fundamental question: Does this approach, which relies on derivatives, fully capture the intrinsic nature of average velocity? Or is it a clever but potentially limited perspective?\n\nWe argue for the latter. In this work, we advocate for a return to the first principles of average velocity: its definition as the integral of instantaneous velocity over a time interval, \\( u     v d  \\).\n\nThe cornerstone of our approach is the additivity property of definite integrals: for any intermediate time \\( s   [r, t] \\), the integral over \\( [r, t] \\) is the sum of the integrals over \\( [r, s] \\) and \\( [s, t] \\) (i.e., \\(  _{r}^{t} =  _{r}^{s} +  _{s}^{t} \\)).\nBy substituting the definition of displacement, \\( (t-r)u(z_t, r, t) =  _{r}^{t} v d  \\), this property translates into an exact algebraic equivalence. We derive a novel, purely algebraic identity that we term Interval Splitting Consistency:\n {equation}\n (t-r)u(z_t,r,t) = (s-r)u(z_s,r,s) + (t-s)u(z_t,s,t).\n {equation}\nThis identity governs the intrinsic relationship between average velocities across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new framework for learning the average velocity field by directly enforcing this consistency as a training objective.\n\nCrucially, this algebraic formulation is not merely an alternative but a more general foundation. We formally demonstrate that the differential identity at the core of MeanFlow is recovered by taking the limit of our Interval Splitting Consistency as the splitting point \\( s \\) approaches the endpoint \\( t \\). In this limit, our algebraic relation gracefully collapses into the differential form, revealing that MeanFlow's training objective is, in fact, a special case of our more fundamental consistency principle. This connection establishes SplitMeanFlow as a direct generalization of MeanFlow, offering a more comprehensive and robust framework for learning average velocity fields.\n\nOur contributions are summarized as follows:\n {itemize}\n   A General and Principled Framework. We introduce SplitMeanFlow, a framework grounded in an algebraic Interval Splitting Consistency. We prove that the differential identity of MeanFlow~ is a limiting special case of our formulation, establishing our method's theoretical generality.\n   JVP-Free, Efficient, and Simple. Our approach eliminates the need for expensive Jacobian-vector product (JVP) computations, leading to more stable training,\n\n broader hardware compatibility, and a simpler implementation.\n\n   Proven Industrial Impact. Our method has been successfully deployed in large-scale industrial products, DouBao, demonstrating its practical value and robustness.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/introduction.tex",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.342,
      "diffusion_reasoning_score": 0.489,
      "distributed_training_score": 0.407,
      "datasets_score": 0.286,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper primarily addresses improvements in generative modeling for few-step data generation, such as in Flow Matching and MeanFlow, but does not involve adapting diffusion processes for complex logical tasks, Chain-of-Thought reasoning, or iterative refinement of reasoning paths. It focuses on velocity fields and algebraic identities for image or audio synthesis, with no component for multi-step logical reasoning.",
      "distributed_training_justification": "The paper discusses efficiency in training generative models by eliminating JVP computations and improving hardware compatibility, but it does not cover distributed training, parallel computing, multi-node setups, or strategies for partitioning data/models across processors. Its focus is on algorithmic optimizations for single setups, not distributed systems.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668850",
      "updated_at": "2025-08-11T23:43:05.607079",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16886",
      "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial\n  Transcriptomics Imputation with Natural Image Co-learning",
      "authors": [
        "Yaoyu Fang",
        "Jiahe Qian",
        "Xinkun Wang",
        "Lee A. Cooper",
        "Bo Zhou"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Spatial transcriptomics (ST) has revolutionized biomedical research by\nenabling high resolution gene expression profiling within tissues. However, the\nhigh cost and scarcity of high resolution ST data remain significant\nchallenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel\nframework for accurate ST imputation that requires only a single and low-cost\nsparsely sampled ST dataset alongside widely available natural images for\nco-training. Our approach integrates three key innovations: (1) a\nsparser-to-sparse self-supervised learning strategy that leverages intrinsic\nspatial patterns in ST data, (2) cross-domain co-learning with natural images\nto enhance feature representation, and (3) a Cascaded Data Consistent\nImputation Network (CDCIN) that iteratively refines predictions while\npreserving sampled gene data fidelity. Extensive experiments on diverse tissue\ntypes, including breast cancer, liver, and lymphoid tissue, demonstrate that\nour method outperforms state-of-the-art approaches in imputation accuracy. By\nenabling robust ST reconstruction from sparse inputs, our framework\nsignificantly reduces reliance on costly high resolution data, facilitating\npotential broader adoption in biomedical research and clinical applications.",
      "published_date": "2025-07-22T17:58:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16886v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16886v1",
      "latex_url": "http://arxiv.org/src/2507.16886v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Spatial transcriptomics (ST) is a cutting-edge technology that enables the investigation of spatially resolved gene expression within tissues  {aspSpatiallyResolvedTranscriptomes2020}. Traditional transcriptomic approaches, such as single-cell RNA sequencing (scRNA-seq), provide high-throughput, high resolution gene expression profiles but inherently lack spatial context  {aungSpatiallyInformedGene2024,boeSpatialTranscriptomicsReveals2024,sankar50480ImmuneInfiltrate2024}. However, spatial information is crucial for identifying disease biomarkers, understanding disease progression, and developing personalized treatment strategies.\n\nST has emerged as a transformative tool across multiple biomedical fields, offering unprecedented insights into tissue organization and function. In oncology, it enhances our understanding of tumor heterogeneity and immune responses, facilitating cancer diagnosis and treatment by identifying novel cell types and immune correlates  {wangSpatialTranscriptomicsProteomics2021,liSpatialTranscriptomicsTumor2022}. For instance, in breast cancer, ST has uncovered distinct gene expression patterns across tumor regions, enabling precise classification of subtypes and guiding targeted therapies  {levy-jurgensonSpatialTranscriptomicsInferred2020,coutantSpatialTranscriptomicsReveal2023,anSpatialTranscriptomicsBreast2024}. In melanoma, it provides critical spatial insights into immune cell infiltration, essential for predicting and optimizing immunotherapy responses  {boeSpatialTranscriptomicsReveals2024,sankar50480ImmuneInfiltrate2024,aungSpatiallyInformedGene2024}. In neurology, ST advances brain mapping and neurodegenerative disease research, aiding in neural classification and biomarker discovery for conditions like Alzheimer’s and Parkinson’s  {chenSpatialTranscriptomicsSitu2020,navarroSpatialTranscriptomicsReveals2020,jungSpatialTranscriptomicsNeuroscience2023,piweckaSinglecellSpatialTranscriptomics2023,heUnravelingAlzheimersDisease2024,zhangInvestigatingMechanismsInflammation2024}. It has revealed spatial patterns of neuroinflammation and protein aggregation in Alzheimer’s disease  {chenSpatialTranscriptomicsSitu2020,navarroSpatialTranscriptomicsReveals2020,heUnravelingAlzheimersDisease2024} and identified distinct molecular signatures in the substantia nigra in Parkinson’s disease, potentially enabling earlier interventions  {zhangInvestigatingMechanismsInflammation2024}. In cardiology, ST redefines our understanding of heart repair mechanisms and coronary atherosclerosis, supporting precision medicine approaches  {rothSinglecellSpatialTranscriptomics2020,boileauFullLengthSpatialTranscriptomics2022,kuppeSpatialMultiomicMap2022,longSinglecellSpatialTranscriptomics2023}. Studies have uncovered spatial gene expression patterns in pathological remodeling and identified distinct zones of repair and regeneration in infarcted heart tissue, leading to novel therapeutic targets for cardiac repair  {boileauFullLengthSpatialTranscriptomics2022,kuppeSpatialMultiomicMap2022}. Beyond these fields, ST has broad applications in reproductive biology, immunology, and developmental biology, providing insights into tissue patterning and immune interactions  {anderssonSpatialDeconvolutionHER2positive2021}. Its integration with multiomics data further amplifies its potential for future research and clinical applications  {driesAdvancesSpatialTranscriptomic2021,kleinoComputationalSolutionsSpatial2022,duAdvancesSpatialTranscriptomics2023}. By continually revealing new dimensions of tissue organization and function, ST challenges existing paradigms and unlocks new therapeutic opportunities, solidifying its role as a cornerstone of modern biomedical research.\n\nDespite its transformative potential across diverse biomedical fields, ST faces several critical challenges that hinder its broader adoption in clinical and research settings. The first major barrier is the substantial financial and logistical burden associated with dense tissue spot sampling {fangComputationalApproachesChallenges2023,smithChallengesOpportunitiesClinical2024}. While advances in ST platforms, such as 10x Genomics Visium HD and Xenium, have significantly improved resolution, achieving over 50,000 spots per section at a 2-micron spot diameter, these improvements come at a steep cost. A single Xenium experiment typically costs $2,000 to $4,000, excluding additional expenses for labor, reagents, and data storage, while platforms such as Visium HD require additional costs for sequencing and library preparation. Such high costs pose a major constraint on large-scale studies involving hundreds of samples, limiting ST’s feasibility for clinical and translational research. Beyond financial constraints, the sheer volume of data generated by high resolution platforms presents another formidable challenge {fangComputationalApproachesChallenges2023,smithChallengesOpportunitiesClinical2024}. Xenium experiments, for instance, can produce terabytes of data per sample, requiring substantial computational and storage infrastructure that many research laboratories and clinical facilities lack. This scalability issue further hinders ST’s widespread implementation. Moreover, due to these financial and technical barriers, large-scale ST datasets remain prohibitively expensive to generate. Compounding this issue, most existing ST datasets are proprietary, restricting open access and data sharing. This scarcity of publicly available, high quality data creates a significant bottleneck for AI-driven solutions, which typically rely on extensive training datasets. These limitations collectively underscore the urgent need for cost-effective approaches and technological innovations to enhance ST’s accessibility, scalability, and practical utility in biomedical research and clinical applications.\n\nFor low resolution ST systems like Visium (55-$ $m spot diameter, in contrast to 2-$ $m spots in Visium HD and 0.2-$ $m optical resolution in Xenium), numerous efforts have been devoted to improving spatial resolution. These approaches generally fall into two categories: histology-based and histology-free (ST-only) methods. Histology-based methods leverage ST-paired histological features to predict gene expression. For instance, DeepSpaCE  {monjoEfficientPredictionSpatial2022} introduced a convolutional neural network (CNN) to infer gene-expression profiles from H\\&E-stained images, while EGN  {yangExemplarGuidedDeep2022} employed a vision transformer-based cascade to enhance long-range relationship modeling. Similarly, XFUSE  {bergenstrahleSuperresolvedSpatialTranscriptomics2022} developed a variational autoencoder (VAE)-based deep generative model for gene expression inference, and iStar  {zhangInferringSuperresolutionTissue2024} combined a pre-trained hierarchical histology feature extractor  {chenScalingVisionTransformers2022} with a fine-tuned multilayer perceptron (MLP) to improve resolution. In addition to deep learning, methods like TESLA  {huDecipheringTumorEcosystems2023} use neighborhood histological similarity for gene expression interpolation, though deep learning generally outperforms such traditional approaches. Histology-free methods, on the other hand, infer high resolution ST data without relying on histological inputs. BayesSpace  {zhaoSpatialTranscriptomicsSubspot2021} employs a Bayesian statistical framework to infer sub-spot gene expression based on spatial neighborhood values, while GNTD  {songGNTDReconstructingSpatial2023} constructs a spatial-transcriptomic graph that integrates spatial and gene expression data, which is then processed through an MLP to reconstruct high resolution ST. Our work aligns with this histology-independent category, which is advantageous because histological inputs can be inconsistent, subject to variations in staining protocols, imaging devices, and acquisition conditions.\n\nDespite these advancements, key challenges remain. First, histology-based methods inherently depend on histological inputs, which may be difficult to obtain and highly variable. Second, existing approaches often treat ST resolution enhancement as a spot-by-spot prediction from histology, neglecting the broader spatial context and long-range relationships between ST spots—critical factors for effective imputation learning. Third, deep learning models require large-scale, diverse datasets, yet high-quality ST data is scarce, particularly for specific tissue types. Data-sharing restrictions further exacerbate this issue, making sample-specific, single-shot learning highly desirable. Fourth, most existing methods adopt standard computer vision architectures without fully considering the unique characteristics of ST data, leaving room for application-specific model designs. Most importantly, prior research has largely focused on enhancing resolution from low resolution ST data, while the crucial challenge of reducing the number of required spot samples to lower experimental costs, without sacrificing resolution, remains underexplored. Given the prohibitive costs of ultra-high-resolution ST systems like Xenium and Visium HD, addressing this gap is essential for making these technologies more cost-effective and scalable for broader biomedical applications.\n\nTo address the aforementioned challenges, we propose a novel single-shot sparser-to-sparse learning framework with natural image co-learning (S2S-ST) for cost-effective, high resolution ST imputation from sparse samples. S2S-ST introduces three key innovations: (1) Sparser-to-Sparse Learning for Ultra-High-Resolution ST Imputation: We consider a scenario where only a sparse subset of ultra-high-resolution ST data is available and aim to recover the full resolution gene expression profile. To achieve this, we introduce an innovative sparser-to-sparse framework, enabling single-shot imputation learning using only the sample-specific sparse data without requiring large external datasets. (2) Natural Image Co-learning for Enhanced Representation:\nGiven the challenge of limited data representation when only a single ST sample is available for imputation learning, we propose a natural image imputation co-learning strategy. By leveraging structural similarities between natural images and spatial transcriptomics data, this strategy enhances the model’s ability to learn spatial patterns, improving imputation performance. (3) Cascade Data-Consistent Imputation Network (CDCIN): To ensure biologically consistent imputation, we design a customized cascade data-consistent imputation network (CDCIN). This architecture incorporates a powerful image restoration backbone combined with a data consistency layer, preserving the already acquired high resolution ST spots while optimizing the imputed values for missing regions. We validated S2S-ST on a diverse set of high resolution ST samples and demonstrated that it can generate high quality ST profiles from sparsely sampled data, significantly reducing the cost of ST experiments. Our method outperforms competitive baseline approaches, providing an efficient and accurate solution for sparse ST imputation. We believe that S2S-ST represents a major step toward making ultra-high-resolution ST more accessible and cost-effective, facilitating broader adoption in biomedical research and clinical applications.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "medima-template.tex",
      "rlhf_score": 0.3,
      "weak_supervision_score": 0.417,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.393,
      "datasets_score": 0.355,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper's main contribution involves a self-supervised learning strategy for ST imputation, using sparse ST data and natural images for co-training. This approach generates training signals from incomplete or indirect sources (e.g., natural images as a proxy for spatial patterns), which aligns with weak supervision by programmatically deriving labels without relying on fully hand-labeled data. However, it is not primarily focused on weak supervision, as the core method emphasizes self-supervised techniques rather than noisy label generation from high-level sources.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces S2S-ST, a novel framework for imputing high-resolution spatial transcriptomics (ST) data from sparse, low-cost samples, aiming to reduce the financial and logistical barriers in biomedical research. It employs three key innovations: a sparser-to-sparse self-supervised learning strategy to leverage intrinsic spatial patterns, cross-domain co-learning with natural images to enhance feature representation, and a Cascaded Data Consistent Imputation Network (CDCIN) for iterative refinement while preserving sampled data fidelity; experiments on diverse tissues demonstrate superior imputation accuracy compared to state-of-the-art methods, potentially broadening ST adoption in research and clinical settings.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new technique with sparser-to-sparse learning and natural image co-learning, significantly advancing ST imputation by addressing cost and data scarcity issues in a way that builds on but extends beyond existing methods.",
      "impact_score": "High",
      "impact_justification": "The work has the potential to influence a wide range of future research and commercial applications in biomedicine by making high-resolution ST more accessible and cost-effective, likely leading to increased adoption and citations in AI-driven biological fields.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution to AI in spatial transcriptomics, offering innovative methods that could be essential for researchers in the field, though it may not be critical for those outside specialized biomedical AI applications.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/bb58ffadf5cf4b7b29a33af6577cde5310eb64f0",
      "h_index_fetch_method": "full_id",
      "total_authors": 5,
      "authors_found": 4,
      "highest_h_index": 1,
      "average_h_index": 0.5,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Yaoyu Fang",
          "profile_url": "https://www.semanticscholar.org/author/2352404865",
          "h_index": 1
        },
        {
          "name": "Jiahe Qian",
          "profile_url": "https://www.semanticscholar.org/author/2354285410",
          "h_index": 1
        },
        {
          "name": "Xinkun Wang",
          "profile_url": "https://www.semanticscholar.org/author/2373419338",
          "h_index": 0
        },
        {
          "name": "Lee A. Cooper",
          "profile_url": "https://www.semanticscholar.org/author/2372388543",
          "h_index": 0
        },
        {
          "name": "Bo Zhou",
          "profile_url": null,
          "h_index": null
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667574",
      "updated_at": "2025-08-11T23:44:33.374528",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16887",
      "title": "Revisiting Pre-trained Language Models for Vulnerability Detection",
      "authors": [
        "Youpeng Li",
        "Weiliang Qi",
        "Xuyu Wang",
        "Fuxun Yu",
        "Xinda Wang"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "The rapid advancement of pre-trained language models (PLMs) has demonstrated\npromising results for various code-related tasks. However, their effectiveness\nin detecting real-world vulnerabilities remains a critical challenge. % for the\nsecurity community. While existing empirical studies evaluate PLMs for\nvulnerability detection (VD), their inadequate consideration in data\npreparation, evaluation setups, and experimental settings undermines the\naccuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,\nan extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and\nlarge-scale PLMs using newly constructed datasets. Specifically, we compare the\nperformance of PLMs under both fine-tuning and prompt engineering, assess their\neffectiveness and generalizability across various training and testing\nsettings, and analyze their robustness against code normalization, abstraction,\nand semantic-preserving transformations.\n  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks\ndesigned to capture the syntactic and semantic patterns of code outperform both\ngeneral-purpose PLMs and those solely pre-trained or fine-tuned on large code\ncorpora. However, these models face notable challenges in real-world scenarios,\nsuch as difficulties in detecting vulnerabilities with complex dependencies,\nhandling perturbations introduced by code normalization and abstraction, and\nidentifying semantic-preserving vulnerable code transformations. Also, the\ntruncation caused by the limited context windows of PLMs can lead to a\nnon-negligible amount of labeling errors. This study underscores the importance\nof thorough evaluations of model performance in practical scenarios and\noutlines future directions to help enhance the effectiveness of PLMs for\nrealistic VD applications.",
      "published_date": "2025-07-22T17:58:49+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16887v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16887v1",
      "latex_url": "http://arxiv.org/src/2507.16887v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Pre-trained language models (PLMs) are transforming software development by helping developers automate repetitive tasks such as code completion and summarization. However, enhancing the ability of PLMs to detect complex, diverse, and subtle real-world vulnerabilities remains a critical challenge.\n\nAlthough existing research has explored if PLMs can demonstrate strong performance in vulnerability detection (VD), limitations across various stages of the evaluation pipeline hinders an accurate reflection of PLMs’ capabilities:\n(i) Data Leakage. Most studies rely on evaluation datasets that inherently introduce data leakage, leading to biased estimations of the model performance in real-world scenarios. The randomly partitioning method used in existing studies~ often causes models to encounter duplicated code patterns between the training and test data.\nThe temporal overlap between the pre-training data cut-off dates of PLMs and the commit dates of evaluation data is also frequently overlooked~.\n(ii) Limited Scope. The experimental setup and settings adopted by many studies are neither comprehensive nor aligned with real-world scenarios, resulting in unrepresentative conclusions. For example, some studies focus on models with constrained architectures and parameter scales~. Also, models are often evaluated on samples with limited size, a narrow range of vulnerability types~, or an unrealistically balanced distribution~. Others focus exclusively on either fine-tuning~ or prompting engineering~, neglecting comparative analyses across different adaptation methods or considerations of the trade-offs between cost and performance of PLMs in VD.\n(iii) Superficial Evaluation. Existing studies~ primarily focus on performance comparisons, without thoroughly investigating how practical factors (e.g., code normalization, abstraction, transformation, truncation) influence the effectiveness of PLMs for VD. This creates a significant gap between performance estimates and real-world applications, failing to provide insights for enhancing PLMs' true capabilities.\n\nTherefore, this paper  {Revisit}s the capabilities of PLMs for  {VD} (RevisitVD) through an extensive and realistic evaluation that addresses the shortcomings of existing evaluation works. In particular, starting from data preparation, we discuss the limitations of existing work in selecting evaluation datasets, including insufficient consideration of data volume, the diversity of vulnerability types, and inherent labeling errors within the datasets. To address these issues, we introduce our reconstructed dataset, alongside a time-order-based dataset partitioning method to avoid data leakage caused by the random data partitioning commonly used in current VD research. Additionally, we highlight the risk that existing VD datasets may predate the cutoff date of PLMs’ pre-training data, potentially leading to data leakage. To mitigate this, we collect a new C/C++ function-based VD dataset from NVD~, encompassing various vulnerability types and projects, with all samples having commit dates after the pre-training cutoff dates of PLMs evaluated in this study.\n\nTo ensure the comprehensiveness and representativeness of the evaluation, we evaluate 17 PLMs with parameter sizes ranging from millions to billions, covering a variety of model architectures. All models have been specifically pre-trained on code structure-aware tasks or exposed to large-scale code corpora, making their evaluation on VD tasks particularly relevant and competitive. Standing out from other existing evaluation efforts, we comprehensively compare PLMs' performance using two model adaptation techniques: fine-tuning and prompt engineering.\nSpecifically, we fully fine-tune small language models (i.e., BERT series and CodeT5), while applying LoRA for partial fine-tuning of LLMs with up to 34B parameters. For prompting engineering, we adopt both zero-shot and few-shot prompting. Within each prompt setting, in addition to raw code functions, we introduce three types of structural and semantic-aware prompts: flattened abstract syntax tree, code with API calls, and code with data flow. These prompts embed structural information and dependencies within the code, guiding PLMs to better analyze vulnerabilities.\n\nFurther, we evaluate the performance of PLMs on out-of-distribution data and test data under various perturbations (i.e., normalization, abstraction, semantic-preserving transformations) to examine their generalizability and robustness in practical applications.\n\nThrough experimental analysis, we reveal the following findings: (1) PLMs that have been pre-trained on specialized tasks that guide them in learning code syntactic and semantic features (e.g., PDBERT~) significantly outperform most PLMs that have been pre-trained or fine-tuned on large code corpora (e.g., CodeLlama~), despite the former having far fewer parameters. This suggests that future research on PLMs for VD may strike a balanced between cost and performance.\n(2) Evaluating fine-tuned PLMs on test data derived from the same source as the training data may result in inaccurate assessments of the model capabilities for VD in real-world scenarios. (3) Existing PLMs continue to struggle with detecting vulnerabilities that involve complex program dependencies. (4) PLMs still lack robustness to minor perturbations, such as inconsistencies in normalization rules applied during training and testing. (5) Most PLMs demonstrate certain robustness to abstracted code, suggesting that their predictions do not rely solely on textual words. (6) Most PLMs exhibit varing degrees of performance drop in semantic-preserving transformations, indicating that they are not yet reliable against\nvulnerable code reuse or adversarial examples. (7) The limited context window size of PLMs unintentionally introduces label errors during truncation, disrupting model training. Code slicing that reduces input length can help PLMs focus more effectively on learning vulnerability patterns.\n\nIn summary, we make the following contributions:\n {itemize}[leftmargin=*]\n   We conduct extensive evaluations of VD capabilities of 17 representative PLMs, covering various architectures, parameter scales, and model adaptation techniques including fine-tuning (up to 34B parameters) and prompt engineering (2 settings × 4 types)\n on a reconstructed dataset with high-quality labeling and a newly collected dataset from NVD encompassing diverse vulnerability types and projects.\n   We examine the robustness of existing PLMs in real-world VD scenarios by applying code normalization, code abstraction, semantic-preserving transformation.\n providing a series of valuable insights for future research.\n   We implement a new framework to assess above capabilities of PLMs,\n and automatically generate leakage-free VD datasets for evaluating future models trained on more recent data.\n Our artifacts are available at  {https://github.com/youpengl/RevisitVD}{RevisitVD}.\n\n {itemize}\n\n {table*}[h]\n\n \n \n {Comparison of vulnerability detection benchmarks}\n {-0.05in}\n\n {threeparttable}\n\n {tabular}{c|c|c|c|c|c|c|c|c|c|c}\n \n\n&  {Adaptation\nMethod} &  {Model\nDiversity} &  {Data\nDiversity} &  {Time\nSplit} &  {Balanced\nTraining} &  {Realistic\nTesting} &  {Knowledge\nCutoff} &  {Finetuned\nModel Size} &  {Out of\nDistribution} &  {Robust\nAnalysis}\n\n \nKhare~& Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nSteenhoek~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nSecLLMHolmes~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nCORRECT~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nVulnSage~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nVulnLLMEval~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nVulDetectBench~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nVulBench~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nSteenhoek~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nLLM4Vuln~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nSecureFalcon~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nZhang~ & Prompt Engineering &   &   & - & - &   &   & - & - & -\n\nDiverseVul~ & Fine-tuning &   &   &  &   &   &   & S &   &  \n\nThapa~ & Fine-tuning &   &   &  &   &   &   & M &   &  \n\nCleanVul~ & Fine-tuning &   &   &  &   &   &   & M &   &  \n\nVulLLM~ & Fine-tuning &   &   &  &   &   &   & L &   &  \n\nVulnPatchPairs~ & Fine-tuning &   &   &  &   &   &   & S &   &  \n\nAleksei~ & Fine-tuning &   &   &  &   &   &   & L &   &  \n\nSteenhoek~ & Fine-tuning &   &   &  &   &   &   & S &   &  \n\nJiang~ & Fine-tuning &   &   &  &   &   &   & M &   &  \n\nNi~ & Both &   &   &  &   &   &   & S &   &  \n\nPrimeVul~ & Both &   &   &  &   &   &   & XL &   &  \n\nYin~ & Both &   &   &  &   &   &   & M &   &  \n\nZhang~ & Both &   &   &  &   &   &   & M &   &  \n\nVulEval~ & Both &   &   &   &   &   &   & S &   &  \n\nPurba~ & Both &   &   &  &   &   &   & L &   &  \n\nZhou~ & Both &   &   &  &   &   &   & M &   &  \n\nZhou~ & Both &   &   &  &   &   &   & S &   &  \n\nChatGPT4Vul~ & Both &   &   &  &   &   &   & S &   &  \n\nGuo~ & Both &   &   &  &   &   &   & M &   &  \n\nRivisitVD (Ours) & Both &   &   &  &   &   &   & XL &   &  \n\n \n {tabular}\n\n {tablenotes}[flushleft]\n\n  S: 125-220M; M: 7-8B; L: 13-15B; XL: 33-34B\n   : considering both imbalanced and balanced training stettings\n {tablenotes}\n {threeparttable}\n {table*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "RevisitVD.tex",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.366,
      "distributed_training_score": 0.401,
      "datasets_score": 0.36,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper primarily evaluates pre-trained language models for vulnerability detection in code, focusing on aspects like fine-tuning, prompt engineering, model performance, and robustness against code perturbations. It does not address distributed training, parallel computing, multi-node machine learning, or strategies for partitioning data/computation across processors, as its contributions are centered on model evaluation for security tasks rather than training methodologies.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669553",
      "updated_at": "2025-08-11T23:43:05.607163",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16933",
      "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
      "authors": [
        "Steven K. Esser",
        "Jeffrey L. McKinstry",
        "Deepika Bablani",
        "Rathinakumar Appuswamy",
        "Dharmendra S. Modha"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
      "published_date": "2025-07-22T18:17:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16933v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16933v1",
      "latex_url": "http://arxiv.org/src/2507.16933v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large language models (LLMs) have achieved remarkable capabilities across a wide range of AI tasks. However, there are two major challenges emerging in wide-scale deployment of LLMs: energy consumption and response latency . It is estimated that inference accounts for 80% of the total energy cost of LLM solutions . In addition, current and emerging applications such as interactive dialog and agentic workflows require very low latencies. Both of these concerns are addressed by low precision neural inference processors, such as NorthPole . Lower precision compute reduces power consumption directly, and by reducing area, allows model memory to be placed next to compute for further energy savings and lower latency .\n\nWhile nearly all LLMs are trained today using 16-bit precision, it is possible to quantize such models to run on low precision inference accelerators. To this end, a variety of techniques have emerged that seek to minimize quantization related accuracy loss, while providing fast time-to-solution, judged relative to total development time, simple implementation, and a quantized model that is fully compatible with the target deployment platform. Efforts have focused on post-training quantization (PTQ), where quantization is tuned using a small amount of calibration data, or quantization-aware training (QAT), where a model is fine-tuned with differentiable quantization operators. PTQ is argued to be preferable due to its low dataset and compute requirements, and recent work has shown it to outperform QAT based approaches .\n\n {table*}\n {centering}\n {small}\n {SiLQ results in more accurate base and instruction-tuned LLMs than leading PTQ techniques across common sense reasoning and Open LLM benchmarks. The Bits column indicates bits to quantize activations, with \"s\" or \"d\" denoting static or dynamic quantization, then KV cache, then weights. SmoothQuant and SpinQuant results computed by us, using code published by each paper's authors.}\n {tabular}{c|cc|cccccc}\n \nModel &Bits &Method & CSR & OLLM & OLLM\n\n & A-C-W & & & v1 & v2\n\n \n {4}{*}{Llama-3-8B}\n& {tabular}[c]{@{}c@{}}16-16-16 {tabular}\n & Baseline & 67.09 & 62.65 & 13.70\n\n {2-7}\n&  {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}\n & SmoothQuant* & 58.73 & 45.15 & 6.65\n\n && SpinQuant & 63.97 & 57.99 & 12.28\n\n &&  {gray!30}SiLQ &  {gray!30}67.20 &  {gray!30}61.14 &  {gray!30}12.66\n  \n {4}{*}{ {Llama-3.1-\nTulu-3.1-8B}}\n& {tabular}[c]{@{}c@{}}16-16-16 {tabular}\n & Baseline & 69.91 & 69.56 & 26.45\n\n {2-7}\n& {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}\n & SmoothQuant* & 64.20 & 58.12 & 20.49\n\n && SpinQuant & 67.47 & 66.91 & 24.03\n\n &&  {gray!30}SiLQ &  {gray!30}69.59 &  {gray!30}69.79 &  {gray!30}27.10\n  \n {9}{*}{ {Granite-3.1-8B-\nInstruct}}\n& {tabular}[c]{@{}c@{}}16-16-16 {tabular}\n & Baseline & 68.46 & 72.11 & 29.91\n\n {2-7}\n& {tabular}[c]{@{}c@{}} {3}{*}{8d-8-4} {tabular}\n & SmoothQuant* & 62.68 & 61.66 & 20.08\n\n && SpinQuant & 65.96 & 65.52 & 21.35\n\n && {gray!30} SiLQ & {gray!30}68.03 &  {gray!30}71.48 &  {gray!30}29.14\n\n {2-7}\n& {tabular}[c]{@{}c@{}} {2}{*}{8s-8-4} {tabular}\n & SmoothQuant* & 49.06 & 35.24 & 8.93\n\n && {gray!30} SiLQ &  {gray!30}67.41 &  {gray!30}70.86 &  {gray!30} 29.03\n\n {2-7}\n\n& {tabular}[c]{@{}c@{}} {3}{*}{8d-4-4} {tabular}\n & SmoothQuant* & 56.48 & 39.78 & 7.15\n\n && SpinQuant & 63.12 & 56.67 & 14.47\n\n && {gray!30} SiLQ &  {gray!30}67.92 &  {gray!30}70.93 &  {gray!30}29.13\n\n \n {3}{l}{*head not quantized}\n\n {tabular}\n\n {small}\n {centering}\n {table*}\n\nHere, we provide a counterpoint to this perspective. We introduce a simple approach to QAT that requires increasing the total training budget by less 0.1%, measured in training tokens, and that can make use of publicly available datasets or the model's original fine-tuning data. The approach achieves accuracy several percentage points superior to the best published alternative quantization methods (Table ) on zero-shot Common Sense Reasoning tasks (CSR) {As in , see references therein} and the Huggingface Open LLM leader-board versions 1 and 2 (OLLMv1 and OLLMv2) {As in , see references therein}.\n\nOur approach, Simple Large Language Model Quantization-Aware Training (SiLQ), is to: 1) add quantization to the model to match the target deployment configuration, using the straight through estimator for training time gradients, 2) set quantizer step size values initially through calibration and then refine further using LSQ , and then 3) train end-to-end with a standard workflow, allowing one to employ existing code frameworks, using knowledge distillation , and either the model's original training dataset or a high quality public dataset. We further introduce a weight calibration technique based on an approximation of mean squared error. We train on up to a few billion tokens, a small investment compared to the trillions of tokens used to train modern LLMs.\n\nA benefit of SiLQ is solution scalability. To the limit of what is possible through deep learning, one can train longer to improve accuracy, which we demonstrate empirically (Figure ). This allows one to balance up-front training costs with deployment time performance as needed. In addition, QAT methods in principle offer better generalizability. One simply adds precision constraints specific to a given accelerator, and lets the optimizer find the best solution tailored to those constraints. On the other hand, to reach acceptable accuracy, PTQ methods require identifying model-specific impediments to quantization, such as outliers appearing in particular layers, and then devising custom solutions for those issues .\n\nSurprisingly few demonstrations have been published using QAT for typical LLM deployment scenarios. Existing work either does not quantize all tensors necessary for full acceleration , targets extremely low precisions at the cost of accuracy degradation that would likely be unacceptable for most users , or introduces time-consuming complexities such as data self-generation . Further, existing publications focus on quantizing the base version of various models, rather than the higher accuracy instruction-tuned version typically used in deployment.\n\nWe demonstrate our approach on models with an 8-bit activation, 4- or 8-bit cache, and 4-bit weight configuration. Remarkably, even when applied to instruction-tuned models that were originally created using a multistage process of pre-training, supervised fine-tuning, direct preference optimization, proximal policy optimization, and model averaging , our single stage end-to-end approach preserves accuracy at nearly the same level as the original model, outperforming all leading hardware-friendly LLM quantization methods.\n\n {figure}\n \n [width=0.48 ]{stepsvsacc_rel_2models.pdf}\n {Accuracy improves with longer QAT. The y-axis represents accuracy relative to the original fp16 model. Horizontal dashed lines show PTQ method SpinQuant accuracy. Accuracy on the harder OLLMv1 and, in particular, OLLMv2 benchmarks improves the most with longer QAT, significantly outperforming PTQ.\n}\n\n {figure}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.389,
      "weak_supervision_score": 0.37,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.431,
      "datasets_score": 0.282,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution is a quantization-aware training method for large language models to reduce model size and improve efficiency, focusing on techniques like quantization of activations, cache, and weights. It does not address distributed training, parallel computing, multi-node setups, or strategies for partitioning data, model architecture, or computation across processors or nodes.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669563",
      "updated_at": "2025-08-11T23:43:05.607164",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16940",
      "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning &\n  Annotation",
      "authors": [
        "Nima Fathi",
        "Amar Kumar",
        "Tal Arbel"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)",
        "cs.MA (Multiagent Systems)"
      ],
      "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.",
      "published_date": "2025-07-22T18:24:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16940v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16940v1",
      "latex_url": "http://arxiv.org/src/2507.16940v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Conventional AI models in medical imaging often fail to meet the needs of real clinical practice. Ideally, an AI system would reason independently, recognize when it lacks sufficient context, and dynamically use various tools—much like clinicians do in complex diagnostic scenarios. However, traditional medical imaging AI is typically rigid, designed for specific tasks with fixed inputs and outputs. This lack of flexibility prevents these systems from adapting to changing clinical situations. When faced with unclear findings, unfamiliar diseases, or incomplete information, these models cannot ask for additional details, gather more data, or revise their conclusions~. Consequently, they fall short in interpretability, adaptability, and gaining clinical trust. Agentic AI offers a promising alternative, providing models that not only handle specific tasks but also reason through uncertainty, generate clear visual and linguistic explanations (VLEs), test hypotheses via counterfactual simulations, and collaborate interactively with clinicians. By combining autonomous reasoning with dynamic tool use, agentic systems significantly enhance AI's practical utility in clinical settings, bridging the gap between static automation and the flexible decision-making required in medical practice.\n\nAgentic AI has gained significant popularity across a variety of domains where systems must reason under uncertainty, take autonomous actions, and interact adaptively with their environments~. Web-based agents such as AutoGPT~ and Voyager~ have demonstrated how large language models (LLMs) can control digital interfaces, pursue multi-step goals, and coordinate tool use—shifting the role of AI from static prediction to dynamic, goal-driven problem-solving. In medical imaging, the principles of agentic AI are beginning to shape how multiple models and tools can work together coherently to assist with complex clinical tasks~. Foundation models (FMs), including LLMs and large multimodal models (LMMs), provide a powerful foundation for building unified frameworks that integrate medical image-text reasoning, diagnostic analysis, and clinical decision support. Several recent agentic frameworks have advanced this direction by introducing structured reasoning, tool orchestration, and modality-aware workflows. MDAgents presents a multi-agent system that dynamically configures collaboration between LLMs based on task complexity, achieving state-of-the-art (SOTA) performance on medical benchmarks. MMedAgent introduces the first multi-modal medical AI agent capable of intelligently selecting and integrating specialized tools—such as segmentation, classification, and report generation—across five imaging modalities. Using instruction tuning and dynamic tool invocation, it outperforms both open- and closed-source models, including GPT-4o~. MedRAX~ combines state-of-the-art analysis tools with multimodal LLMs to address complex queries involving visual question answering and report generation. While these frameworks represent major progress, many still lack explicit reasoning capabilities and robust image-grounded explanations—features critical for transparency, safety, and clinical trust. Together, these frameworks underscore the increasing role of agentic AI in developing more interactive and intelligent healthcare systems.\n\nIn this work, we present  , the first AI agent designed to analyze, generate visual-linguistic explanations (VLEs), and perform self-evaluations using a comprehensive suite of SOTA tools. Unlike conventional models that offer limited interpretability and operate in a static inference paradigm,  \\ emphasizes dynamic VLE, introspective evaluation, and adaptive reasoning in data-scarce clinical settings. Our contributions include an agent capable of: (i) image segmentation with phase grounding (associating regions of medical image to corresponding text or clinical concepts), pathology segmentation, and anatomy segmentation to identify and attribute clinically relevant regions; (ii) CF image generation for probing understanding through controlled perturbations; and (iii) a self-evaluation toolkit incorporating difference map analysis, classification, and specialized tools on Chest X-ray (CXR) images such as RadEdit~ and PRISM~, enabling critical assessment of its own outputs. Notably,  \\ performs robustly even in low-supervision scenarios (less textual prompt guidance) by autonomously identifying knowledge gaps, invoking report generation to gain context, and generating multiple candidate CFs. Finally, the best candidate is chosen using a self-evaluation scoring mechanism. This integration of explanation, self-assessment, and context-aware tool use marks a step toward more trustworthy, adaptable, and clinically actionable AI systems.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.394,
      "weak_supervision_score": 0.359,
      "diffusion_reasoning_score": 0.481,
      "distributed_training_score": 0.309,
      "datasets_score": 0.342,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper introduces AURA, an LLM-based agent for medical image analysis, focusing on segmentation, counterfactual image generation, and evaluation tools. While it mentions counterfactual image generation for reasoning, there is no reference to diffusion models or the adaptation of iterative refinement processes for solving complex logical tasks. The reasoning described relies on LLMs and tool integration, not on treating a Chain-of-Thought as a holistically corrected entity via diffusion-based methods. Thus, the paper does not align with the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669721",
      "updated_at": "2025-08-11T23:43:05.607181",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16946",
      "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic\n  Concepts",
      "authors": [
        "Chiao-An Yang",
        "Kuan-Chuan Peng",
        "Raymond A. Yeh"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Anomaly detection (AD) identifies the defect regions of a given image. Recent\nworks have studied AD, focusing on learning AD without abnormal images, with\nlong-tailed distributed training data, and using a unified model for all\nclasses. In addition, online AD learning has also been explored. In this work,\nwe expand in both directions to a realistic setting by considering the novel\ntask of long-tailed online AD (LTOAD). We first identified that the offline\nstate-of-the-art LTAD methods cannot be directly applied to the online setting.\nSpecifically, LTAD is class-aware, requiring class labels that are not\navailable in the online setting. To address this challenge, we propose a\nclass-agnostic framework for LTAD and then adapt it to our online learning\nsetting. Our method outperforms the SOTA baselines in most offline LTAD\nsettings, including both the industrial manufacturing and the medical domain.\nIn particular, we observe +4.63% image-AUROC on MVTec even compared to methods\nthat have access to class labels and the number of classes. In the most\nchallenging long-tailed online setting, we achieve +0.53% image-AUROC compared\nto baselines. Our LTOAD benchmark is released here:\nhttps://doi.org/10.5281/zenodo.16283852 .",
      "published_date": "2025-07-22T18:35:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16946v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16946v1",
      "latex_url": "http://arxiv.org/src/2507.16946v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Anomaly detection (AD)~ is the task of identifying defect regions in a given image.\nThis task is important due to its high valued applications in industrial manufacturing~ and medical~ settings.\n\nRecently, LTAD~ explored the long-tailed (LT)~ setting of unsupervised one-class AD, where abnormal images are not given during training and the training set is unevenly distributed. This long-tailed distribution assumes that images of certain classes significantly outnumber others, which is a realistic assumption for real-world applications.\nImportantly, LTAD shows that prior work on evenly distributed data performs suboptimally on LT data.\nMeanwhile, recent works~ approach AD with online learning to leverage abnormal images during test time. However, they do not study the case where the classes follow a long-tail distribution, where the performance gap of head and tail classes in the online setting may be significant during offline training. This motivates us to study the long-tailed online AD (LTOAD) task by proposing a benchmark and a novel model.\n\nAt first glance, it may seem that one can take a state-of-the-art (SOTA) long-tailed offline model,  , LTAD~, and combine it with online learning to solve LTOAD. However, several limitations within the LTAD's framework prevent it from being effective in the online setting.\nFirst, LTAD is a class-aware method, where they assume that the class information is given. However, class labels are not available in the online setting. Instead,\na class-agnostic method is required as illustrated in~ {fig:teaser}.\n {figs/teaser}\nSecond, LTAD's encoder-decoder architecture does not leverage the latest vector quantization VAE~, which is more effective, as shown in recent work.\nThird, the prompt learning designs of LTAD and other AD works~ ignore some aspects of abnormal prompts,  , LTAD does not support class-specific abnormal prompts.\n\nTo address these challenges, we propose a novel model that is class-agnostic, with a vector quantized VAE, and a comprehensive prompt learning framework, for the task of long-tailed online anomaly detection.\nExperiments show that our method outperforms the SOTA baselines significantly by at least 4% on several detection benchmarks without access to the class labels and the number of classes.\n\nIn the following sections, we first discuss how to make existing class-aware methods class-agnostic, specifically on LTAD ( {sec:concept}). Next, we define the task of long-tailed online AD and present our solution ( {sec:task}). Finally, we provide details of each proposed module ( {sec:details}), followed by the experimental results ( {sec:exp}) and related works ( {sec:rel}).\n\n{  Our contributions are summarized as follows:}\n {itemize}\n   We propose the task and benchmark for long-tailed online anomaly detection (LTOAD).\n   We propose a class-agnostic framework for both offline and online learning that removes the constraint of requiring additional class information in previous work. Along with vector-quantization VAE and a comprehensive prompt learning scheme that benefits both offline and online learning.\n    ~outperforms SOTA offline long-tailed methods and online baselines on MVTec, VisA, DAGM, and Uni-Medical. It also shows promising generalizability to cross-dataset settings.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.324,
      "weak_supervision_score": 0.392,
      "diffusion_reasoning_score": 0.337,
      "distributed_training_score": 0.343,
      "datasets_score": 0.337,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668380",
      "updated_at": "2025-08-11T23:43:05.606991",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16952",
      "title": "Evaluating Ensemble and Deep Learning Models for Static Malware\n  Detection with Dimensionality Reduction Using the EMBER Dataset",
      "authors": [
        "Md Min-Ha-Zul Abedin",
        "Tazqia Mehrub"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "This study investigates the effectiveness of several machine learning\nalgorithms for static malware detection using the EMBER dataset, which contains\nfeature representations of Portable Executable (PE) files. We evaluate eight\nclassification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\nHistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three\npreprocessing settings: original feature space, Principal Component Analysis\n(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on\naccuracy, precision, recall, F1 score, and AUC to examine both predictive\nperformance and robustness. Ensemble methods, especially LightGBM and XGBoost,\nshow the best overall performance across all configurations, with minimal\nsensitivity to PCA and consistent generalization. LDA improves KNN performance\nbut significantly reduces accuracy for boosting models. TabNet, while promising\nin theory, underperformed under feature reduction, likely due to architectural\nsensitivity to input structure. The analysis is supported by detailed\nexploratory data analysis (EDA), including mutual information ranking, PCA or\nt-SNE visualizations, and outlier detection using Isolation Forest and Local\nOutlier Factor (LOF), which confirm the discriminatory capacity of key features\nin the EMBER dataset. The results suggest that boosting models remain the most\nreliable choice for high-dimensional static malware detection, and that\ndimensionality reduction should be applied selectively based on model type.\nThis work provides a benchmark for comparing classification models and\npreprocessing strategies in malware detection tasks and contributes insights\nthat can guide future system development and real-world deployment.",
      "published_date": "2025-07-22T18:45:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16952v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16952v2",
      "latex_url": "http://arxiv.org/src/2507.16952v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.335,
      "weak_supervision_score": 0.352,
      "diffusion_reasoning_score": 0.329,
      "distributed_training_score": 0.348,
      "datasets_score": 0.386,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668861",
      "updated_at": "2025-08-11T23:43:05.607081",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16955",
      "title": "A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis:\n  Robust Diagnosis with Attention-Based Fusion",
      "authors": [
        "Yalda Zafari",
        "Roaa Elalfy",
        "Mohamed Mabrok",
        "Somaya Al-Maadeed",
        "Tamer Khattab",
        "Essam A. Rashed"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Early and accurate interpretation of screening mammograms is essential for\neffective breast cancer detection, yet it remains a complex challenge due to\nsubtle imaging findings and diagnostic ambiguity. Many existing AI approaches\nfall short by focusing on single view inputs or single-task outputs, limiting\ntheir clinical utility. To address these limitations, we propose a novel\nmulti-view, multitask hybrid deep learning framework that processes all four\nstandard mammography views and jointly predicts diagnostic labels and BI-RADS\nscores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,\ncombining convolutional encoders for rich local feature extraction with Visual\nState Space Models (VSSMs) to capture global contextual dependencies. To\nimprove robustness and interpretability, we incorporate a gated attention-based\nfusion module that dynamically weights information across views, effectively\nhandling cases with missing data. We conduct extensive experiments across\ndiagnostic tasks of varying complexity, benchmarking our proposed hybrid models\nagainst baseline CNN architectures and VSSM models in both single task and\nmulti task learning settings. Across all tasks, the hybrid models consistently\noutperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,\nthe shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.\nFor the more challenging ternary classification, it attains an F1 score of\n0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.\nThese results highlight the effectiveness of the proposed hybrid framework and\nunderscore both the potential and limitations of multitask learning for\nimproving diagnostic performance and enabling clinically meaningful mammography\nanalysis.",
      "published_date": "2025-07-22T18:52:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16955v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16955v1",
      "latex_url": "http://arxiv.org/src/2507.16955v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Breast cancer is the most commonly diagnosed cancer and a leading cause of cancer-related deaths among women globally . Early detection through mammography screening is central to reducing mortality . However, interpreting mammograms is challenging due to subtle malignancy indicators (e.g., microcalcifications, distortions) and high inter-observer variability, leading to false positives/negatives and motivating the development of AI-assisted diagnostic tools.\n\nConventional 2D mammography has inherent limitations in accurately capturing 3D breast anatomy. Standard mammography includes two views per breast, cranio-caudal (CC) and medio-lateral-oblique (MLO), which clinicians analyze using both Ipsi-lateral (within-breast) and Contra-lateral (between-breast) comparisons. Findings are categorized via the BI-RADS system: 0 (incomplete), 1 (negative), 2 (benign), 3 (probably benign), 4 (suspicious), 5 (highly suggestive of malignancy), and 6 (biopsy-proven malignancy). While biopsy confirms diagnosis, mismatches between BI-RADS scores and biopsy-based diagnosis can arise from interpretive errors or imaging limitations .\n\nThe advent of Deep Learning (DL) has marked a new era in Computer-Aided Diagnosis (CAD) systems . Convolutional Neural Networks (CNNs) dominate due to their strength in learning local spatial features from pixel data , though their limited receptive fields restrict global context modeling. To address this, vision transformers and Vision State Space Models (VSSM) have emerged, offering global feature modeling. Combining CNNs with these architectures enhances representation quality and diagnostic accuracy.\n\nA clinically meaningful CAD system for mammography analysis must handle multi-view inputs (mimicking radiologist workflows), support missing views, and allow continuous improvement through human feedback. Integrating multi-task capabilities (such as BI-RADS assessment and malignancy prediction) closely aligned with clinical practice, offers significant potential for fine-tuning model performance at various diagnostic stages. This multi-task approach enhances the system’s adaptability and utility across different levels of clinical decision-making.\n\nMost deep learning multi-view mammography models emphasize two core tasks: feature extraction and cross-view fusion. CNN-based architectures remain the dominant backbone for feature extraction, although recent studies have also explored the use of vision transformers. A summary of various deep learning-based approaches for multi-view mammography classification is presented in Table . Commonly, models use ipsi-lateral analysis (CC and MLO per breast), with simple fusion techniques like concatenation or averaging . The majority of research efforts have focused on single-task classification, primarily distinguishing malignant cases from benign or normal ones. These models were often evaluated and fine-tuned on external unseen datasets, where they maintained comparable performance . Where biopsy data is unavailable, BI-RADS labels serve as surrogate ground truth, though inconsistencies can affect evaluation .\n\nRecent works have explored cross-attention modules to capture inter-view relationships , extending to four-view models under side-invariant and side-variant paradigms . While these methods enhance contextual learning, they also introduce significant computational overhead due to quadratic complexity, especially with high-resolution mammography images. This makes early-layer integration difficult and reduces feasibility in real-world clinical environments. Additionally, the inclusion of additional parameters increases model complexity, thereby requiring large-scale datasets that capture a diverse range of features influencing performance . Furthermore, such models often assume complete view availability and lack built-in mechanisms for handling missing data. Notably, to the best of our knowledge, existing studies have not proposed strategies or conducted evaluations to address scenarios involving incomplete view data.\n\n {table*}[!t]\n \n { }{!}{\n {tabular}{m{1.5cm} m{8cm} m{8cm} m{2cm}}\n \nReference & Dataset \\{Classes\\} & Approach & Results (AUC)\n\n \n &\nCBIS-DDSM (mass subset) \\{Benign vs. Malignant\\} &\nDual-view mammography classification using ResNet-18 for feature extraction and a transformer-based cross-view attention mechanism. &\n0.803\n  \n\n &\nPrivate dataset \\{Non-malignant vs. Malignant\\} &\nFour-view images partitioned into patches processed via local transformers (shared weights), followed by global transformer-based fusion. &\n0.814\n  \n\n &\n {8cm}{VinDr-Mammo \\{BI-RADS(2) vs. BI-RADS(4,5)\\}\n CMMD \\{Non-malignant vs. Malignant\\} } &\nAnalysis of multiple fusion strategies (average and concatenation) at various ResNet-18 layers for dual-view mammography classification. &\n [l]{0.7535\n 0.8416}\n  \n\n &\n {8cm}{CBIS-DDSM \\{Benign vs. Malignant\\}\n CMMD \\{Benign vs. Malignant\\} } &\nDual-view model based on ResNet-18 with added channel- and feature-attention modules across various network layers. Features from ipsi-lateral views are further aggregated using a transformer-based global encoder. &\n [l]{0.7798\n 0.8181}\n  \n\n &\nPrivate dataset \\{Non-malignant vs. Malignant\\} &\nFour-view analysis using ResNet-22 for feature extraction and late-stage fusion via concatenation. &\n [l]{R-MLO: 0.82\n R-CC: 0.85\n L-MLO: 0.84\n L-CC: 0.83}\n  \n\n &\n {8cm}{CBIS-DDSM \\{Benign vs. Malignant\\}\n VinDr-Mammo \\{BI-RADS(1–3) vs. BI-RADS(4,5)\\} } &\nA dynamic attention fusion module using shifted windows to integrate information across dual-view mammograms. &\n [l]{0.6643\n 0.9608}\n  \n\n &\nCSAW case-control subset (Karolinska) , DDSM , Synthetic data \\{Non-malignant vs. Malignant\\} &\nAn anatomy-aware framework based on graph convolutional networks. It models intra-breast geometric relationships (ipsi-lateral) and contra-lateral similarities, followed by a correspondence reasoning module for improved results. &\n0.844\n  \n\n &\nCSAW case-control subset (Karolinska) , DDSM , Synthetic data \\{Non-malignant vs. Malignant\\} &\nFour-view analysis using Swin Transformer for feature extraction and cross-attention mechanisms to capture inter-view relationships. &\n0.767\n\n \n {tabular}\n}\n {Summary of recent deep learning approaches for multi-view mammography classification. Reported AUC values correspond to datasets used for model training or fine-tuning. Results from datasets used solely for evaluation are excluded.}\n\n {table*}\n\nTransformers’ limitations in scalability have prompted interest in Mamba, a state space model architecture that captures long-range dependencies with linear complexity . Building on this, we propose a hybrid CNN-VSSM framework for multi-view mammography. The model jointly predicts malignancy and BI-RADS scores, using a pretrained CNN encoder to extract local features followed by VSSM layers for global context modeling. To integrate multi-view inputs effectively and handle missing views, we introduce a gated attention fusion module, enhancing both interpretability and robustness. The primary contributions of this work are therefore five-fold:\n\n {itemize}\n   We develop a novel multi-view, multi-task, quad-head architecture that uniquely processes the complete four-view mammography study (L-CC, L-MLO, R-CC, R-MLO) to simultaneously and independently predict diagnostic outcomes and BI-RADS assessments for both breasts in a single forward pass.\n   We introduce a new Hybrid CNN–VSSM architecture, which effectively cascades convolutional neural networks with visual state space models, combining the strengths of CNNs in local feature extraction with the global context modeling capabilities of SSMs.\n   We incorporate a gated attention-based fusion mechanism that intelligently learns to assign dynamic importance to each view, enabling the model to construct a more robust for missing data and context-aware fused representation.\n   We perform a comprehensive empirical comparison across a range of models, examining their effectiveness on prediction tasks with varying levels of clinical ambiguity and uncertainty.\n   We investigate single-task versus multi-task training strategies for simultaneous prediction of diagnostic outcomes and BI-RADS scores, two clinically related yet occasionally divergent tasks, to better understand the benefits and trade-offs of joint learning in this context.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main-arxiv.tex",
      "rlhf_score": 0.312,
      "weak_supervision_score": 0.351,
      "diffusion_reasoning_score": 0.402,
      "distributed_training_score": 0.355,
      "datasets_score": 0.365,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a hybrid CNN-VSSM model for multi-view mammography analysis, focusing on image feature extraction, global context modeling, and attention-based fusion for breast cancer diagnosis. It does not involve diffusion models, iterative refinement processes, or any form of multi-step logical reasoning for complex tasks. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669732",
      "updated_at": "2025-08-11T23:43:05.607183",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16962",
      "title": "Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition,\n  Image-level, and Feature-level Methods",
      "authors": [
        "Qinqin Yang",
        "Firoozeh Shomal-Zadeh",
        "Ali Gholipour"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Modern medical imaging technologies have greatly advanced neuroscience\nresearch and clinical diagnostics. However, imaging data collected across\ndifferent scanners, acquisition protocols, or imaging sites often exhibit\nsubstantial heterogeneity, known as \"batch effects\" or \"site effects\". These\nnon-biological sources of variability can obscure true biological signals,\nreduce reproducibility and statistical power, and severely impair the\ngeneralizability of learning-based models across datasets. Image harmonization\naims to eliminate or mitigate such site-related biases while preserving\nmeaningful biological information, thereby improving data comparability and\nconsistency. This review provides a comprehensive overview of key concepts,\nmethodological advances, publicly available datasets, current challenges, and\nfuture directions in the field of medical image harmonization, with a focus on\nmagnetic resonance imaging (MRI). We systematically cover the full imaging\npipeline, and categorize harmonization approaches into prospective acquisition\nand reconstruction strategies, retrospective image-level and feature-level\nmethods, and traveling-subject-based techniques. Rather than providing an\nexhaustive survey, we focus on representative methods, with particular emphasis\non deep learning-based approaches. Finally, we summarize the major challenges\nthat remain and outline promising avenues for future research.",
      "published_date": "2025-07-22T19:06:02+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16962v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16962v1",
      "latex_url": "http://arxiv.org/src/2507.16962v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure*}[t]\n  \n  [width=0.95 ]{images/Figure-01.png}\n  {Overview of harmonization strategies spanning the entire medical imaging pipeline, including image acquisition, reconstruction, post-processing, and feature-level analysis. This figure shows representative examples of image contrasts and image-based measurements, such as DWI: diffusion weighted image; T1WI: T1 weighted image; T2WI: T2 weighted image; RBV: Regional brain volume; CT: Cortical thickness; FA: Fractional Anisotropy.}\n\n {figure*}\n\nMagnetic resonance imaging (MRI) has had a profound impact on the field of medicine, with widespread applications in medical and neuroscience research, computer-aided diagnosis, longitudinal monitoring, and image-guided interventions. To advance scientific discovery and bridge the gap between research and clinical practice, collecting and sharing large-scale imaging datasets across sites has become increasingly essential  {RN134,RN133,RN121,RN136,RN127,RN132}. Multi-center studies that aggregate large and diverse samples not only enhance statistical power, particularly important for investigating rare or low-prevalence diseases, but also provide broader coverage of key biological variables such as age, sex, race, geographic location, socioeconomic status, and disease subtypes. The increased sample size and heterogeneity also improve the ability of studies to detect subtle yet meaningful effects in high-dimensional spaces of variables and confounders  {RN124,RN143,RN143}.\n\nIn the era deep learning, the proper collection and analysis of large-scale medical imaging data has become even more critical  {RN145,RN150}. Although machine or deep learning models have shown great potential in addressing scientific challenges in medicine, their applicability in clinical practice remains limited. Many of these models are built on the assumption that the training and testing datasets come from the same distribution, and their performance can degrade substantially when this assumption does not hold. In other words, when applied to images acquired using protocols that are different from those used during training, these models often exhibit poor reproducibility and limited generalizability  {RN151}. As a result, their effectiveness may decline significantly when used across different hospitals, scanners, or patient populations. This decline in performance is largely caused by non-biological technical variability, often referred to as scanner effects, device effects, or batch effects, which can stem from differences in scanner/device hardware and software by different manufacturers, sequences, acquisition protocols, image reconstruction pipelines and techniques, and other sources  {RN144,RN138,RN141,RN140}.\n\nA common conventional approach to avoid the challenges of directly using and comparing heterogeneous imaging data is meta-analysis, where each site performs its own analysis independently, and the results are later combined  {RN149,RN148,RN146}. However, meta-analyses typically include only group-level statistical and clinical information, making it difficult to perform detailed modeling or adjustments at the individual level. Moreover, when participant distributions are imbalanced, site-specific statistics may introduce systematic biases. In studies with small imaging sample sizes, fluctuations in parameter estimation during z-score conversion may further compromise the stability of statistical inference. In contrast, mega-analysis involves the joint analysis of all raw imaging data on a unified platform  {RN124,RN129,RN135,RN39,RN142}. However, this strategy places more stringent demands on data harmonization, as combining datasets from different centers may introduce additional non-biological variability, particularly due to differences in imaging protocols. Therefore, effective harmonization is a critical prerequisite for the success of mega-analyses.\n\nComprehensive MRI harmonization involves three key components: harmonized acquisition, image-level processing, and feature-level analysis. Harmonized acquisition refers to the prospective standardization of the use of scanner hardware, pulse sequences, and protocol parameters during data collection, aiming to reduce variability or heterogeneity of the acquired data  {RN14}. Image-level harmonization involves retrospective adjustments to the acquired images, such as intensity normalization, statistical correction, or deep learning methods to standardize image contrast and signal distribution  {RN39,RN42,RN36}. Feature-level harmonization focuses on quantitative metrics, texture features, and anatomical representations extracted from images, ensuring their comparability across different sites to support reliable data integration and cross-site analysis  {RN99,RN86,RN95}. The primary goal of harmonization is not to recover some absolute “ground truth,” but rather to enhance the reliability of comparisons at both individual and group levels. In other words, the essence of harmonization is to eliminate non-biological technical variation while preserving meaningful biological differences. Rather than removing systematic bias, harmonization seeks to make such biases consistent across all datasets, thereby minimizing their impact on downstream analyses.\n\nThis review focuses on multi-site harmonization methods for MRI data, although the underlying principles are broadly applicable to other medical imaging modalities. Several review papers have been published on this topic, covering modalities such as positron emission tomography, computed tomography, and microscopic pathology  {RN39,RN42,RN43,RN40,RN41,RN44}. However, the primary emphasis remains on MRI, owing to its inherent characteristics, such as multiple field strengths, diverse imaging modalities, and a wide range of quantitative parameters, which result in pronounced inter-site heterogeneity. Moreover, compared to other modalities, MRI is uniquely complex due to the breadth of imaging capabilities it offers, presenting a wider range of harmonization challenges and thus requiring more comprehensive solutions. Nevertheless, existing MRI harmonization reviews have largely focused on retrospective approaches applied to structural and diffusion MRI  {RN39,RN42,RN43}, while prospective strategies at the acquisition stage have received limited attention. In this review, we place particular emphasis on recent advances in vendor-agnostic pulse sequences and harmonized image reconstruction techniques. For retrospective harmonization, we highlight emerging deep learning-based methods that have gained traction in recent years. This is the first comprehensive review to cover the full spectrum of harmonization efforts across the MRI pipeline - from prospective acquisition harmonization to retrospective image- and feature-level methods  {fig:fig1}.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.376,
      "weak_supervision_score": 0.337,
      "diffusion_reasoning_score": 0.342,
      "distributed_training_score": 0.368,
      "datasets_score": 0.351,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669742",
      "updated_at": "2025-08-11T23:43:05.607184",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16971",
      "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over\n  Knowledge Graphs through Human-Inspired Reasoning",
      "authors": [
        "Aleksandr Perevalov",
        "Andreas Both"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.",
      "published_date": "2025-07-22T19:23:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16971v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16971v1",
      "latex_url": "http://arxiv.org/src/2507.16971v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Previous approaches to multilingual knowledge graph question answering ( ), like  {qanswer,deeppavlov2023}, have employed both rule-based and neural methods to address downstream tasks (  named entity recognition, relation detection, query template classification) necessary for constructing structured queries (    queries).\nMore recent methods (   {srivastava2024mst5}) leverage Large Language Models ( ) to generate such structured queries directly from non-English input.\nThe application of newly introduced   agents (or augmented language models) to   has demonstrated significantly improved performance compared to   that rely solely on standard prompting techniques    {jiang2024kg,huang2024queryagent}).\nHowever, the multilingual aspect of these systems remains largely unexplored within the research community.\nTo the best of our knowledge, there are no studies investigating the   agent architectures for   in multilingual settings.\n\nOne of the key advantages of   is that they enable developers and researchers to model human-like reasoning processes via agentic workflows (   {li2023metaagents}).\nWhen solving complex problems, humans typically break them down into a series of simpler subtasks (   {diefenbach2017qanaryecosystem,correa2020resource}), effectively creating a step-by-step plan to arrive at a solution.\nWhile generating a   query, this decomposition is essential: not only does one need to break down the task, but also look up query language syntax, identify relevant entity identifiers in the target knowledge graph ( ), and analyze feedback (  from executing the SPARQL query candidate on the triplestore).\nTo replicate this human-like process, we introduce  --an  -based agent framework designed as a   system that follows a semantic-parsing approach.\nSpecifically, given a user query (multiple languages are supported), it generates a   query to fulfill the information need.\nAccordingly, this paper aims to answer the following research questions:\n {description}[labelindent=-2pt]\n  [ {1}] How do different LLM agent steps (  plan, action, tool calling, feedback,  ) impact the generation of   queries from natural language?\n  [ {2}] How efficient are these LLM agent steps in terms of computation time and the number of additional calls required?\n  [ {3}] How does the quality of   query generation vary when prompting LLM agents in non-English languages (especially low-resource ones)?\n  [ {4}] How does translating non-English questions into English affect the quality of KGQA?\n {description}\n\nWe conducted preliminary experiments on the widely used KGQA benchmark   (introduced in  {perevalov2022qald}) with multilingual support.\nWe evaluate 10 languages, including two classified as endangered.\nThe experimental results on both proprietary and open-source LLMs demonstrate the effectiveness of  's architecture, achieving superior performance even in non-English settings.\nDuring the final evaluation on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants.\nThe source code and the evaluation results are available in our GitHub repository { {https://github.com/WSE-research/text2sparql-agent}}.\n\nThe paper is organized as follows.\nIn the next section, an overview of the related work is presented.\nThe   architecture is described in Section .\nSection is dedicated to the experimental setup.\nThe results are shown in Section and discussed in Section .\nSection concludes our paper.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.356,
      "weak_supervision_score": 0.353,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.327,
      "datasets_score": 0.343,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution is a human-inspired framework (mKGQAgent) for multilingual question answering over knowledge graphs, using LLM agents for modular subtasks like planning, entity linking, and query refinement. It emphasizes step-by-step reasoning and feedback loops but does not involve diffusion models, iterative noise-denoising processes, or holistic correction of a Chain-of-Thought as described in the topic. Thus, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669572",
      "updated_at": "2025-08-11T23:43:05.607165",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16974",
      "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs\n  in the Agricultural Domain",
      "authors": [
        "Rishemjit Kaur",
        "Arshdeep Singh Bhankhar",
        "Jashanpreet Singh Salh",
        "Sudhir Rajput",
        "Vidhi",
        "Kashish Mahendra",
        "Bhavika Berwal",
        "Ritesh Kumar",
        "Surangika Ranathunga"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Publicly available general-purpose Large Language Models\n(LLMs) typically offer generic agriculture advisories, lacking precision in\nlocal and multilingual contexts. Our study addresses this limitation by\ngenerating multilingual (English, Hindi, Punjabi) synthetic datasets from\nagriculture-specific documents from India and fine-tuning LLMs for the task of\nquestion answering (QA). Evaluation on human-created datasets demonstrates\nsignificant improvements in factuality, relevance, and agricultural consensus\nfor the fine-tuned LLMs compared to the baseline counterparts.",
      "published_date": "2025-07-22T19:25:10+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16974v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16974v2",
      "latex_url": "http://arxiv.org/src/2507.16974v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Agriculture forms the backbone of many economies, specially for countries such as India. Despite substantial advancements of Artificial Intelligence across areas such as healthcare and education~ {zhu2024harnessing}, agriculture sector has seen relatively limited integration of cutting-edge technologies such as Large Language Models (LLMs).\nGeneral-purpose LLMs have been shown to provide useful assistance in agricultural settings, but their advice is often generic, lacking in local specificity~. Agriculture-specific chat-bot systems such as Farmer.Chat~ that have multilingual support are promising, however there has been no quantitative evaluation of such systems. On the other hand, the handful of agriculture-specific Question Answer (QA) datasets are only for high resource languages such as English, German and Chinese~.\n\nIn this paper, we present a novel synthetic QA dataset in three languages - English, Hindi and Punjabi, for the agriculture domain. In addition, we provide human-curated evaluation datasets for all three languages. We carry out extensive human evaluations to determine the best LLM, as well as the prompt. Contrary to prior research that used either metrics such as ROGUE or LLM-as-judge for agriculture QA evaluation, we employ human evaluation at each step. Through this, we highlight the inadequacy of automated evaluation schemes for domain- and country-specific QA evaluation tasks.\n\nWith the synthetic training data, we carry out two different fine-tuning experiments (i) train and test LLMs with language-specific data (ii) train with English data and use translate-test approach~, where the questions in Hindi/Punjabi are first translated to English, fed to the LLM trained with English data, and translating the generated responses back to Hindi/Punjabi.\n\nAlthough the Llama-3x instruct models showed over 95% for consensus, relevancy and factuality results were rather low, with the average across languages being 44.3% and 29%, respectively. However, after fine-tuning with synthetic datasets, these averages increased by 22.1% and 14% (respectively). The translate-test approach further increased results for Hindi and Punjabi.\nThese improvements underscore two key insights: (1) domain-specific fine-tuning significantly boosts performance, and (2) instruction-tuned English LLMs, when used with translate test approach, outperform native language fine-tuned LLMs. Our data (under CC license) and models will be made public .",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "acl_latex.tex",
      "rlhf_score": 0.405,
      "weak_supervision_score": 0.447,
      "diffusion_reasoning_score": 0.375,
      "distributed_training_score": 0.353,
      "datasets_score": 0.437,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "The paper focuses on generating synthetic datasets and fine-tuning LLMs for QA in agriculture, using human evaluations for assessment, but does not involve training a reward model or using reinforcement learning based on human-ranked data. There is no mention of RLHF techniques.",
      "weak_supervision_justification": "The paper generates synthetic QA datasets programmatically from agriculture-specific documents, which aligns with weak supervision by using noisy or derived sources for labels instead of hand-labeled data. However, it does not deeply explore weak supervision methodologies beyond this data creation step.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution includes creating, curating, and evaluating new multilingual synthetic and human-curated datasets for agriculture QA, as well as benchmarking LLMs on these datasets, directly addressing dataset introduction, curation methodologies, and performance analysis.",
      "summary": "This paper addresses the limitations of general-purpose Large Language Models (LLMs) in providing precise, multilingual agriculture-related information by generating synthetic question-answering (QA) datasets in English, Hindi, and Punjabi from Indian agriculture documents. The methodology involves fine-tuning LLMs using these datasets and evaluating them through human assessments, including language-specific training and a translate-test approach, with key findings showing significant improvements in factuality, relevance, and agricultural consensus for fine-tuned models compared to baselines, while also highlighting the superiority of the translate-test method for non-English languages.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by creating a novel synthetic QA dataset for low-resource languages in the agriculture domain and emphasizing human evaluations over automated metrics, effectively combining existing techniques in a new contextual application. However, it builds on established LLM fine-tuning methods rather than introducing a entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in multilingual AI for agriculture by providing publicly available datasets and models that could enhance information access for farmers in their native languages. While its applicability is somewhat niche to specific subfields like computational linguistics and agriculture, it has potential for broader adoption in similar domain-specific applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers valuable contributions through its practical approach to improving multilingual LLMs for agriculture, including useful datasets and insights on evaluation methods, making it essential for researchers in AI and computational linguistics focused on domain-specific applications. However, it is not groundbreaking enough to be considered must-read for the general audience.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/cc501dccf8e778627c248e20d26cf046bec07943",
      "h_index_fetch_method": "full_id",
      "total_authors": 9,
      "authors_found": 9,
      "highest_h_index": 14,
      "average_h_index": 3.0,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Rishemjit Kaur",
          "profile_url": "https://www.semanticscholar.org/author/2219019",
          "h_index": 14
        },
        {
          "name": "Arshdeep Singh Bhankhar",
          "profile_url": "https://www.semanticscholar.org/author/2373266258",
          "h_index": 0
        },
        {
          "name": "J. Salh",
          "profile_url": "https://www.semanticscholar.org/author/2306536724",
          "h_index": 1
        },
        {
          "name": "Sudhir Rajput",
          "profile_url": "https://www.semanticscholar.org/author/2372710262",
          "h_index": 0
        },
        {
          "name": "Vidhi",
          "profile_url": "https://www.semanticscholar.org/author/2373266418",
          "h_index": 0
        },
        {
          "name": "Kashish Mahendra",
          "profile_url": "https://www.semanticscholar.org/author/2373266309",
          "h_index": 0
        },
        {
          "name": "Bhavika Berwal",
          "profile_url": "https://www.semanticscholar.org/author/2304956603",
          "h_index": 0
        },
        {
          "name": "Ritesh Kumar",
          "profile_url": "https://www.semanticscholar.org/author/2332301361",
          "h_index": 1
        },
        {
          "name": "Surangika Ranathunga",
          "profile_url": "https://www.semanticscholar.org/author/143976433",
          "h_index": 11
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668871",
      "updated_at": "2025-08-11T23:45:38.989232",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16978",
      "title": "Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS\n  and ScaNN",
      "authors": [
        "Mohammad Saleh Refahi",
        "Gavin Hearne",
        "Harrison Muller",
        "Kieran Lynch",
        "Bahrad A. Sokhansanj",
        "James R. Brown",
        "Gail Rosen"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "The exponential growth of DNA sequencing data has outpaced traditional\nheuristic-based methods, which struggle to scale effectively. Efficient\ncomputational approaches are urgently needed to support large-scale similarity\nsearch, a foundational task in bioinformatics for detecting homology,\nfunctional similarity, and novelty among genomic and proteomic sequences.\nAlthough tools like BLAST have been widely used and remain effective in many\nscenarios, they suffer from limitations such as high computational cost and\npoor performance on divergent sequences.\n  In this work, we explore embedding-based similarity search methods that learn\nlatent representations capturing deeper structural and functional patterns\nbeyond raw sequence alignment. We systematically evaluate two state-of-the-art\nvector search libraries, FAISS and ScaNN, on biologically meaningful gene\nembeddings. Unlike prior studies, our analysis focuses on\nbioinformatics-specific embeddings and benchmarks their utility for detecting\nnovel sequences, including those from uncharacterized taxa or genes lacking\nknown homologs. Our results highlight both computational advantages (in memory\nand runtime efficiency) and improved retrieval quality, offering a promising\nalternative to traditional alignment-heavy tools.",
      "published_date": "2025-07-22T19:28:54+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16978v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16978v1",
      "latex_url": "http://arxiv.org/src/2507.16978v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Traditional alignment-based methods such as BLAST and MMseqs2 have long served as the backbone of sequence comparison. While effective in many cases, these approaches are limited by their reliance on exact or near-exact matches, making them less suitable for detecting distant homology or structural variation—particularly in short or noisy sequences. Moreover, their computational cost scales poorly with data volume, posing significant challenges for large-scale genomic analysis .\n\nThese limitations are especially pronounced in metagenomics, where datasets are highly fragmented, taxonomically diverse, and often contain a substantial proportion of sequences with no close reference. But similar challenges extend beyond metagenomics—affecting areas such as epigenomics , gene regulation studies, and disease-related variant discovery, where subtle patterns or regulatory signals may be obscured by noise or evolutionary divergence.\n\n {figure*}[h]\n  \n  [width=0.85 ]{proposal_course_pipeline.pdf}\n  {Overview of the embedding-based retrieval pipeline. Sequences are first encoded into dense vector representations using a pretrained language model. These embeddings are then indexed using similarity search methods with FAISS or ScaNN to enable fast and efficient nearest-neighbor retrieval.}\n\n {figure*}\n\nInspired by advances in natural language processing, genomic language models now leverage representation learning to extract biologically meaningful embeddings from raw DNA sequences. These embeddings capture structural, functional, and evolutionary signals in dense vector spaces, allowing comparisons that go beyond exact sequence alignment. Recent models—such as DNABERT~, MetaBERTa~, Nucleotide Transformer~, HyenaDNA~, and Caduceus~—have demonstrated strong performance on a range of genomics and metagenomics tasks by learning contextualized nucleotide representations.\n\nTo make these embeddings useful in practice—particularly for tasks like novelty detection, taxonomic classification, or gene retrieval—we need fast and scalable similarity search infrastructure. Unlike traditional models that rely on token-level prediction, these downstream applications require efficient nearest neighbor retrieval across large embedding databases. Approximate nearest neighbor (ANN) methods offer a promising solution, but their performance in biological settings remains underexplored. In this work, we systematically evaluate two state-of-the-art ANN libraries, FAISS~ and ScaNN~, assessing their speed, accuracy, and utility for real-world metagenomic retrieval.\n\nBeyond comparing their default configurations, we conduct detailed parameter tuning for both tools to evaluate how internal settings—such as index type, quantization strategy, distance metric, and search depth—affect performance. We report their impact on runtime, memory usage, and retrieval quality. Our results provide actionable insights for practitioners seeking scalable and accurate search infrastructure for biological embeddings.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.311,
      "weak_supervision_score": 0.275,
      "diffusion_reasoning_score": 0.328,
      "distributed_training_score": 0.341,
      "datasets_score": 0.328,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669583",
      "updated_at": "2025-08-11T23:43:05.607166",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16991",
      "title": "PyG 2.0: Scalable Learning on Real World Graphs",
      "authors": [
        "Matthias Fey",
        "Jinu Sunil",
        "Akihiro Nitta",
        "Rishi Puri",
        "Manan Shah",
        "Blaž Stojanovič",
        "Ramona Bendias",
        "Alexandria Barghi",
        "Vid Kocijan",
        "Zecheng Zhang",
        "Xinwei He",
        "Jan Eric Lenssen",
        "Jure Leskovec"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "PyG (PyTorch Geometric) has evolved significantly since its initial release,\nestablishing itself as a leading framework for Graph Neural Networks. In this\npaper, we present Pyg 2.0 (and its subsequent minor versions), a comprehensive\nupdate that introduces substantial improvements in scalability and real-world\napplication capabilities. We detail the framework's enhanced architecture,\nincluding support for heterogeneous and temporal graphs, scalable feature/graph\nstores, and various optimizations, enabling researchers and practitioners to\ntackle large-scale graph learning problems efficiently. Over the recent years,\nPyG has been supporting graph learning in a large variety of application areas,\nwhich we will summarize, while providing a deep dive into the important areas\nof relational deep learning and large language modeling.",
      "published_date": "2025-07-22T19:55:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16991v2",
      "pdf_url": "http://arxiv.org/pdf/2507.16991v2",
      "latex_url": "http://arxiv.org/src/2507.16991v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning on ubiquitous graph-structured data.\nFrom social networks, knowledge bases, relational databases, to spatial graphs describing molecular structures, 3D scenes or objects, graphs are used to store most of the world's data.\nSince 2019, PyG (PyTorch Geometric)~ has been an important cornerstone in advancing deep learning on all these different types of graphs (cf.~Sec.~ for a summary).\nPyG introduced a general message passing scheme that allows for a flexible formulation of Graph Neural Networks.\nThis is achieved by decomposing neural message passing~ into  {message},  {aggregation}, and  {update} functions that can be customized to create various types of graph-based operators, thus supporting a broad range of models in a unified framework, which can automatically be mapped onto GPUs.\n\nIn the early years, most applied research around Graph Neural Networks revolved around finding the best operators to solve small-scale benchmark tasks, such as node classification on the Cora citation graph~, the graph-based equivalent to MNIST~.\nSince then, the field of graph learning has rapidly evolved, strongly supported and driven by advancements in infrastructure provided by PyG.\nGNNs can now be trained efficiently on web-scale, heterogeneous, temporal and multi-modal graphs, are explainable, and easily deployable for a wide range of practical applications.\nPyG has evolved into a comprehensive blueprint for end-to-end graph-based machine learning, enabling these functionalities.\n\n \n { }{}\n {Published as a worshopt paper at KDD 2025}\n \n\nIn this work, we present the design principles and architectural decisions behind PyG, beginning with the foundational changes introduced in PyG 2.0 and extending through its continuous evolution to the current state of the library. PyG 2.0 marked a significant milestone in the library's development over three years ago, this paper encompasses the full trajectory of improvements and innovations that have been integrated into PyG up to its most recent version. We focus on the following three core aspects that have been refined and expanded throughout this evolution:\n {itemize}[label= {purple}{ },leftmargin=10pt]\n   Heterogeneity.\n Real world graphs have diverse node and edge types.\n PyG natively supports heterogeneous graph data types and message passing, as well as functionality for learning on temporal graphs.\n   Scaling and Efficiency.\n Many use cases of graph learning have massive graphs ($  10$ billion nodes), which need to be supported through optimized loading and training APIs.\n To this end, we present novel distributed processing capabilities, efficient data formats, loaders, and samplers, accelerated message passing, and compilation mechanisms.\n   Explainability.\n Understanding how a model arrives at its decision is crucial in several domains and often required for trust in deep learning models deployed in practice.\n We discuss explainability in the heterogeneous graph learning setting and describe our plug-and-play method to make any GNN within PyG explainable out-of-the-box.\n {itemize}\n\nGraph learning powered by PyG has made an impact in a wide range of practical fields.\nTo showcase its generality, we also provide an overview of applications in chemistry, material design, computer vision, weather, and traffic forecasting. Moreover, we deep-dive into two specific application areas: GNN (and PyG) integration in Large Language Models~ {he2024gretriever} and Relational Deep Learning~ {fey2024rdl}.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.332,
      "weak_supervision_score": 0.335,
      "diffusion_reasoning_score": 0.392,
      "distributed_training_score": 0.425,
      "datasets_score": 0.328,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper's main contribution focuses on enhancements in PyG 2.0 for scalability and efficiency in graph learning, including explicit mentions of novel distributed processing capabilities, efficient data formats, loaders, samplers, and accelerated message passing. These features are designed to handle massive graphs with billions of nodes, directly aligning with distributed training concepts such as partitioning data and computation across multiple processors or nodes to accelerate model training. This makes the paper's content a strong match for the topic.",
      "datasets_justification": "below_threshold",
      "summary": "This paper presents PyG 2.0, an updated version of the PyTorch Geometric framework, aimed at enhancing scalability and real-world applicability for Graph Neural Networks (GNNs) by introducing support for heterogeneous and temporal graphs, optimized data handling, distributed processing, and explainability features. It details the architectural improvements, including efficient loaders, samplers, and compilation mechanisms for handling large-scale graphs, while summarizing PyG's role in various applications such as chemistry, computer vision, and its integration with large language models and relational deep learning, thereby facilitating advanced graph learning research and practice.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper offers notable improvements and clever combinations of existing ideas, such as enhanced support for heterogeneous graphs and scalability features, to address known challenges in graph learning more effectively. However, it primarily refines an established framework rather than introducing a entirely new problem or technique.",
      "impact_score": "High",
      "impact_justification": "The updates to PyG 2.0 are likely to influence a wide range of future research and commercial applications in machine learning and AI, given its role as a leading framework for GNNs and its applicability to diverse fields like chemistry and large language models. As a foundational tool, these enhancements could drive broader adoption and innovation in graph-based learning.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong and valuable contribution for researchers and practitioners in graph neural networks, providing essential updates to a key framework that enhance scalability and real-world utility. It is particularly relevant for those working in AI and machine learning but may not be essential for those outside this specific domain.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ba7fe04c0529bd86a9dc048c064472b1a3c08177",
      "h_index_fetch_method": "full_id",
      "total_authors": 13,
      "authors_found": 13,
      "highest_h_index": 67,
      "average_h_index": 8.538461538461538,
      "notable_authors_count": 3,
      "author_h_indexes": [
        {
          "name": "Matthias Fey",
          "profile_url": "https://www.semanticscholar.org/author/2294360908",
          "h_index": 5
        },
        {
          "name": "Jinu Sunil",
          "profile_url": "https://www.semanticscholar.org/author/2373489551",
          "h_index": 0
        },
        {
          "name": "Akihiro Nitta",
          "profile_url": "https://www.semanticscholar.org/author/2294360062",
          "h_index": 2
        },
        {
          "name": "R. Puri",
          "profile_url": "https://www.semanticscholar.org/author/144830538",
          "h_index": 67
        },
        {
          "name": "Manan Shah",
          "profile_url": "https://www.semanticscholar.org/author/2333428671",
          "h_index": 1
        },
        {
          "name": "Blavz Stojanovivc",
          "profile_url": "https://www.semanticscholar.org/author/2373488645",
          "h_index": 0
        },
        {
          "name": "Ramona Bendias",
          "profile_url": "https://www.semanticscholar.org/author/2373489000",
          "h_index": 0
        },
        {
          "name": "Alexandria Barghi",
          "profile_url": "https://www.semanticscholar.org/author/2291956661",
          "h_index": 1
        },
        {
          "name": "Vid Kocijan",
          "profile_url": "https://www.semanticscholar.org/author/2281744793",
          "h_index": 1
        },
        {
          "name": "Zecheng Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2294848842",
          "h_index": 3
        },
        {
          "name": "Xinwei He",
          "profile_url": "https://www.semanticscholar.org/author/2313796183",
          "h_index": 2
        },
        {
          "name": "J. E. Lenssen",
          "profile_url": "https://www.semanticscholar.org/author/9572099",
          "h_index": 18
        },
        {
          "name": "J. Leskovec",
          "profile_url": "https://www.semanticscholar.org/author/2251205420",
          "h_index": 11
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668881",
      "updated_at": "2025-08-11T23:45:41.236617",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.16999",
      "title": "Bayesian preference elicitation for decision support in multiobjective\n  optimization",
      "authors": [
        "Felix Huber",
        "Sebastian Rojas Gonzalez",
        "Raul Astudillo"
      ],
      "categories": [
        "stat.ML (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "We present a novel approach to help decision-makers efficiently identify\npreferred solutions from the Pareto set of a multi-objective optimization\nproblem. Our method uses a Bayesian model to estimate the decision-maker's\nutility function based on pairwise comparisons. Aided by this model, a\nprincipled elicitation strategy selects queries interactively to balance\nexploration and exploitation, guiding the discovery of high-utility solutions.\nThe approach is flexible: it can be used interactively or a posteriori after\nestimating the Pareto front through standard multi-objective optimization\ntechniques. Additionally, at the end of the elicitation phase, it generates a\nreduced menu of high-quality solutions, simplifying the decision-making\nprocess. Through experiments on test problems with up to nine objectives, our\nmethod demonstrates superior performance in finding high-utility solutions with\na small number of queries. We also provide an open-source implementation of our\nmethod to support its adoption by the broader community.",
      "published_date": "2025-07-22T20:14:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.16999v1",
      "pdf_url": "http://arxiv.org/pdf/2507.16999v1",
      "latex_url": "http://arxiv.org/src/2507.16999v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Many real-world problems require optimizing multiple objectives or criteria simultaneously. For example, in operations management, decision-makers (DMs) often aim to maximize expected return while minimizing risk  {winston2004operations}. Similarly, in aerodynamic engineering, a common goal is to maximize the lift coefficient while minimizing the drag coefficient of an airfoil design  {he2020}. In such cases, a single solution that optimizes all objectives rarely exists; for instance, solutions with higher expected returns typically come with increased risk. As a result, a trade-off between objectives is inevitable, and DMs need to understand this trade-off to make informed choices.\n\nA common approach in these scenarios is to estimate the Pareto-optimal set (also simply referred to as the Pareto set), which consists of solutions that cannot be improved in all objectives simultaneously; the image of the Pareto set is usually termed the Pareto front. Each solution in this set represents a different optimal trade-off between the competing objectives, providing DMs with a range of options. However, when the Pareto set is large, it becomes challenging to select a single solution for implementation  {cheikh2010method}. This issue is further exacerbated as the size of the Pareto set typically grows exponentially with the number of objectives, making it difficult to approximate and choose among the solutions effectively  {sanders2024does}.\n\nIn the literature, methods that approximate the entire Pareto set are known as a posteriori methods and are among the most widely studied. In contrast, a priori methods rely on DMs to specify their preferences before the optimization process begins, typically by combining the objectives into a single scalar-valued function. While this approach simplifies the selection of a final solution, it assumes that the DM can fully articulate their preferences beforehand, which is rarely the case in practice. Indeed, before the optimization process begins, DMs often have little knowledge about the trade-offs of the different criteria and the objective space in general  {misitano2021desdeo}.\n\nTo address these limitations, a broad range of interactive methods have been proposed in the literature  {InteractiveSurvey}. These methods aim to progressively elicit the DM's preferences during the optimization process. By presenting a series of queries, they adaptively focus on the most relevant regions of the Pareto front. While this approach can be more flexible and user-friendly, most interactive methods either make strong assumptions about the DM's underlying preferences and responses or rely on data-inefficient heuristics to select the DM queries, often leading to suboptimal or biased solutions in practice.\n\nIn this paper, we propose a novel approach to support decision-making in multi-objective optimization, addressing several major drawbacks of existing methods. Specifically, we leverage non-parametric Bayesian preference learning to model the DM's preferences, along with a principled elicitation strategy to select queries. Our approach offers three key advantages over existing methods:\n\n {enumerate}\n  It does not rely on strong assumptions about the DM's utility function and can accommodate noisy responses.\n  It is query-efficient, providing a mechanism to balance exploration and exploitation while effectively eliciting the DM's utility function.\n  It offers a natural scheme for generating a menu of user-specified size, consisting of diverse high-quality solutions, at the end of the elicitation phase.  {enumerate}\n\nOverall, our work provides principled and practical framework for decision support in general multi-objective problems via efficient and robust elicitation of the DM preferences. We also provide an implementation of our method to reproduce our experiments and support its use by the broader community { {https://github.com/qres/BPE4MOO}}.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.439,
      "weak_supervision_score": 0.348,
      "diffusion_reasoning_score": 0.345,
      "distributed_training_score": 0.29,
      "datasets_score": 0.281,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on Bayesian preference elicitation for multi-objective optimization, using pairwise comparisons to estimate a decision-maker's utility function and guide solution selection. While it involves human feedback through queries, it does not involve training a reward model, fine-tuning an AI model via reinforcement learning, or aligning AI systems with human preferences. RLHF specifically requires these elements, which are absent here, making the paper unrelated to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669592",
      "updated_at": "2025-08-11T23:43:05.607167",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17000",
      "title": "Divisive Decisions: Improving Salience-Based Training for Generalization\n  in Binary Classification Tasks",
      "authors": [
        "Jacob Piland",
        "Chris Sweet",
        "Adam Czajka"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Existing saliency-guided training approaches improve model generalization by\nincorporating a loss term that compares the model's class activation map (CAM)\nfor a sample's true-class ({\\it i.e.}, correct-label class) against a human\nreference saliency map. However, prior work has ignored the false-class CAM(s),\nthat is the model's saliency obtained for incorrect-label class. We hypothesize\nthat in binary tasks the true and false CAMs should diverge on the important\nclassification features identified by humans (and reflected in human saliency\nmaps). We use this hypothesis to motivate three new saliency-guided training\nmethods incorporating both true- and false-class model's CAM into the training\nstrategy and a novel post-hoc tool for identifying important features. We\nevaluate all introduced methods on several diverse binary close-set and\nopen-set classification tasks, including synthetic face detection, biometric\npresentation attack detection, and classification of anomalies in chest X-ray\nscans, and find that the proposed methods improve generalization capabilities\nof deep learning models over traditional (true-class CAM only) saliency-guided\ntraining approaches. We offer source codes and model weights\\footnote{GitHub\nrepository link removed to preserve anonymity} to support reproducible\nresearch.",
      "published_date": "2025-07-22T20:17:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17000v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17000v1",
      "latex_url": "http://arxiv.org/src/2507.17000v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Background and Motivation Deep neural networks have demonstrated impressive performance across various computer vision tasks, but their opaque decision-making process remains a significant limitation.\nSalience-based explainability methods, including class activation maps (CAMs) , have been widely adopted to address this issue by visualizing the image regions most influential to a model’s prediction.\nWhile initially developed for post-hoc interpretability, CAMs have also been incorporated into the saliency-guided training paradigms, where models are rewarded for aligning their attention with human- or auxiliary model-provided annotations.\nOne example implementation of such training paradigm is CYBORG method , which improves generalization by penalizing discrepancies between the CAM of the true-class and a human salience map.\n\nHowever, prior work has shown that models can be trained to produce visually persuasive but misleading CAMs for methods utilizing only the true ({  i.e.}, sample-specific correct-label) class without harming classification accuracy, known as passive fooling .\nThis suggests that supervising only the true-class CAM may be insufficient to ensure meaningful model attention.\n\nProposed Solution\n\nIn this work, we revisit the CAMs of both the true and false ({  i.e.}, sample-specific incorrect-label) classes during training, using what we refer to as the ``teacher'' setup, in which class labels are known and used to generate both CAMs.\nRather than supervising CAMs in isolation, we propose loss terms that enforce a contrast between the true- and false-class CAMs, either directly or indirectly through human annotations. {  We introduce three training variants and one novel visualization approach} that build on this intuition:\n\n {enumerate}[label=( *)]\n  the first training method {  supervises the CAM difference to match human annotations}; we further present this ``Difference Salience'' as a novel CAM that reveals new and plausible features from the contrast of the two classes;\n\n  the second training method, called in this paper ``Per-class Salience,'' {  independently supervises the true and false CAMs to match the human map and its inverse}, respectively;\n\n  the third proposed method, called ``Contrast Salience,'' {  supervises the true-class CAM to match human annotations while encouraging the false-class CAM to diverge from it} by matching an inverted version of the true CAM.\n {enumerate}\n\nEvaluation Domains\n\nAll three proposed methods aim to induce more discriminative internal representations and improve generalization. We evaluate them in three contexts and domains:\n\n {itemize}\n  {  in-set} chest X-ray anomaly detection, to serve as a baseline domain, in which generalization capabilities of the classifier are not crucial,\n\n  {  } {  out-of-set} synthetic face detection, where prior saliency-guided training methods have shown strong out-of-distribution classification accuracy gains, and\n\n  {  out-of-set} iris presentation attack detection (PAD), which has also been used in previous works to evaluate saliency-guided training.\n\n {itemize}\n\nResearch Questions\n\nWe find that while traditional saliency-guided training methods already improve generalization, the addition of contrastive CAM supervision leads to further benefits in challenging generalization settings. To structure our investigations related to concrete benefits coming from the proposed methods, we define the following research questions, around which our experiments are built:\n\n {description}[leftmargin=1cm]\n [{  RQ1:}] Does Difference Salience reveal new and plausible features in models trained to obfuscate their true-class CAMs with passive fooling?\n [{  RQ2:}] In binary classification, does supervising the CAM difference using human annotations improve model generalization beyond traditional saliency-guided training?\n [{  RQ3:}] Does directly supervising both true and false CAMs (per-class salience) using complementary annotations (human-sourced: direct and inverted) improve model behavior?\n [{  RQ4:}] Can contrastive supervision using human-guided true-class CAM to define a target for false-class CAM yield additional gains in classification generalization?\n {description}\n\nSummary of Contributions\n\nWe propose a novel visualization and saliency-guided training target called Difference Salience and qualitatively demonstrate it's value.\n\nWe further propose and evaluate three progressively stronger saliency-guided training methods based on the use of both class CAMs in a binary classification set-up, and applied to both in-set and out-of-set classification problems. First, we modify the loss function to request that the difference between unnormalized CAMs, rather than the true-class CAM alone, match human-sourced salient features (obtained via image annotations or eye tracking). Second, we jointly supervise both CAMs: the true-class CAM is aligned with human saliency, and the false-class CAM is aligned with the inverted human saliency heatmap.\nThird, we supervise the true-class CAM with human saliency, and require the false-class CAM to match the inverted true-class CAM, allowing the model to maintain a strong contrast in class CAMs even when the true-class CAM differs from the human annotations.\n\nFinally, we offer the source codes, all model weights and training configurations (splits, seeds, etc.) along with the paper {GitHub repository link removed to preserve anonymity} to support the reproducible research.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.401,
      "weak_supervision_score": 0.429,
      "diffusion_reasoning_score": 0.419,
      "distributed_training_score": 0.396,
      "datasets_score": 0.364,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper uses human saliency maps as guidance in training to align model attention, which involves human feedback. However, it does not involve training a separate reward model or using reinforcement learning algorithms to fine-tune the model, making it only loosely related to RLHF.",
      "weak_supervision_justification": "The paper employs human-provided saliency maps, which may be noisy or imprecise, as additional training signals, somewhat aligning with weak supervision by reducing reliance on perfect labels. However, it still uses standard supervised learning with class labels, rather than primarily focusing on programmatically generated labels.",
      "diffusion_reasoning_justification": "The paper focuses on saliency-guided training for binary classification using CAMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "The paper addresses limitations in existing saliency-guided training for binary classification by incorporating both true-class and false-class class activation maps (CAMs) to improve model generalization. It introduces three novel training methods—Difference Salience, Per-class Salience, and Contrast Salience—that enforce contrasts between CAMs using human annotations, along with a new visualization tool, and evaluates them on tasks like synthetic face detection, biometric presentation attack detection, and chest X-ray anomaly detection, demonstrating superior generalization over traditional methods that only use true-class CAMs.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by incorporating false-class CAMs into saliency-guided training, which prior work overlooked, thus combining existing ideas in a new way to address model attention discrepancies. However, it builds on established saliency methods rather than introducing a completely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to influence research in explainable AI and saliency-guided training within computer vision and machine learning subfields, as it provides practical methods and reproducible code that could be built upon. Its applicability is somewhat limited to binary classification tasks, reducing broader commercial or widespread impact.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "The paper offers valuable contributions to improving model generalization and interpretability in binary classification, making it a strong and relevant read for researchers in AI explainability. While not essential for all, its practical innovations and empirical evaluations warrant attention from those in the field.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/32d551529b9df29da61830c982c64a40c3df9dc4",
      "h_index_fetch_method": "full_id",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 5,
      "average_h_index": 2.3333333333333335,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Jacob Piland",
          "profile_url": "https://www.semanticscholar.org/author/52126729",
          "h_index": 5
        },
        {
          "name": "Chris Sweet",
          "profile_url": "https://www.semanticscholar.org/author/2366166212",
          "h_index": 0
        },
        {
          "name": "Adam Czajka",
          "profile_url": "https://www.semanticscholar.org/author/2282534732",
          "h_index": 2
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669159",
      "updated_at": "2025-08-11T23:45:54.818701",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17008",
      "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance\n  Through Generative Models",
      "authors": [
        "Gaston Gustavo Rios",
        "Pedro Dal Bianco",
        "Franco Ronchetti",
        "Facundo Quiroga",
        "Oscar Stanchi",
        "Santiago Ponte Ahón",
        "Waldo Hasperué"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Most sign language handshape datasets are severely limited and unbalanced,\nposing significant challenges to effective model training. In this paper, we\nexplore the effectiveness of augmenting the training data of a handshape\nclassifier by generating synthetic data. We use an EfficientNet classifier\ntrained on the RWTH German sign language handshape dataset, which is small and\nheavily unbalanced, applying different strategies to combine generated and real\nimages. We compare two Generative Adversarial Networks (GAN) architectures for\ndata generation: ReACGAN, which uses label information to condition the data\ngeneration process through an auxiliary classifier, and SPADE, which utilizes\nspatially-adaptive normalization to condition the generation on pose\ninformation. ReACGAN allows for the generation of realistic images that align\nwith specific handshape labels, while SPADE focuses on generating images with\naccurate spatial handshape configurations. Our proposed techniques improve the\ncurrent state-of-the-art accuracy on the RWTH dataset by 5%, addressing the\nlimitations of small and unbalanced datasets. Additionally, our method\ndemonstrates the capability to generalize across different sign language\ndatasets by leveraging pose-based generation trained on the extensive HaGRID\ndataset. We achieve comparable performance to single-source trained classifiers\nwithout the need for retraining the generator.",
      "published_date": "2025-07-22T20:41:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17008v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17008v1",
      "latex_url": "http://arxiv.org/src/2507.17008v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "In recent years, the performance of deep learning models has improved significantly. However, this progress is closely related to the availability of large high-quality datasets, which are often difficult and expensive to create . The challenge is especially pronounced in sign language recognition, where data scarcity and imbalance are prevalent. Many sign language datasets suffer from a lack of diversity and volume, as they require the participation of signers for accurate data collection and labeling. This results in small, unbalanced, and low-quality datasets , limiting the performance of the models trained on them .\n\nSince data collection is difficult, sign language data is generally obtained from real-world sources. Due to the natural distribution of signs and words within a language, and the fact that many data sources focus on a limited range of themes, most sign language datasets tend to be naturally unbalanced . Moreover, the creation of new sign language datasets is further hindered by the fact that sign languages are not mutually intelligible, necessitating the development of separate datasets for each language . As a consequence, communities with fewer resources are disproportionately affected, with even high-resource communities facing significant challenges due to the limited scope and quality of available datasets.\n\nSynthetic training data generation has proven to be effective in improving model training in limited and unbalanced datasets, leading to faster and more stable convergence . However, the generated images often lack realism, introducing noise that can degrade the training process. Furthermore, label-based generation struggles with generalization across domains, as it requires a specific generative model for each sign language . Despite significant advancements in multi-domain generators , these models still fail to produce accurate and realistic images for specialized domains such as sign language handshapes. Thus, there remains a critical need for a general-purpose handshape generator that can operate effectively across multiple sign languages.\n\nProposed approach\n\nIn this article, we propose using generated data to improve the classification of handshapes on datasets with unbalanced and limited data.\n\nTo augment the datasets, we propose the Generative Adversarial Networks (GAN) architectures conditioned on labels and pose. Rebooted Auxiliary Classifier GAN (ReACGAN) uses labels to calculate the data cross-entropy (D2D-CE) loss which is used with the adversarial loss to train the model. In contrast, SPatially-Adaptive\n(DE)normalization (SPADE) replaces Conditional Batch Normalization as the conditional normalization method for our second model which we refer to simply as SPADE and receives pose data as part of its input. Given that pose information can be extracted from any sign language, we can exploit this domain superposition to create a generator capable of generating hand shapes from any sign language. With this in mind, we can easily extend the proposed methods to other datasets.\n\n {figure}[ht!]\n  \n  {subfigure}{0.33 }\n  \n  [height= ]{images/realdiagram.drawio.pdf}\n  {Train only with real data}\n\n  {subfigure}\n  \n  {subfigure}{0.33 }\n  \n  [height= ]{images/gendiagram.drawio.pdf}\n  {Pretrain with generated data}\n\n  {subfigure}\n  \n  {subfigure}{0.33 }\n  \n  [height= ]{images/reggendiagram.drawio.pdf}\n  {Regularize training using generated data}\n\n  {subfigure}\n  \n  {subfigure}{0.33 }\n  \n  [height= ]{images/mixgendiagram.drawio.pdf}\n  {Use generated data with mixup}\n\n  {subfigure}\n  {Diagram (a) shows the regular training approach of our classifier model. For our other methods, a generator model is fitted with real data to create newly generated data samples.\n }\n\n {figure}\n\nWe compare several approaches to take advantage of the generated data (Figure ).\n\n {itemize}\n   REAL: pre-training on ImageNet, and fine-tuning with real data. Used as a baseline.\n   PRETRAIN: pre-training with generated data, and fine-tuning on real data\n   REGULARIZER: Training with both generated and real data, using the generated data as a regularizer\n   MIXUP: Training with both generated and real data, using mixup to combine them.\n {itemize}\n\nContributions\n\nOur work introduces several key contributions that advance the field of sign language handshape classification, particularly in the context of unbalanced and limited datasets:\n\n {itemize}\n  Improved Classification and Per-Class Accuracy: We demonstrate that augmenting the training dataset with GAN-generated samples can significantly improve the accuracy of handshape classification. Specifically, our method achieves a 5% improvement over the state-of-the-art on the RWTH German sign language dataset. By generating a balanced dataset with GANs, we were able to correctly classify underrepresented classes that could not be accurately classified when training only with real data. This dual benefit addresses both the general performance and the specific challenge of class imbalance.\n  Effective pre-training Strategy: We conducted a comprehensive comparison of different training strategies using a combination of generated and real data. Our findings show that pre-training with GAN-generated samples, followed by fine-tuning on real data, yields superior performance compared to alternative approaches.\n  Accelerated Convergence: We observe that models pre-trained with GAN-generated data converge more rapidly during training. This faster convergence not only reduces computational costs but also enhances the efficiency of the training process, making it more feasible to deploy high-performing models in real-world applications.\n  Generalization Across Datasets: We explore the use of both class-based and pose-based data generation strategies. While both methods enhance model performance, pose-based generation proves particularly effective in enabling the generalization of the model to multiple handshape datasets from different sign languages. This contribution highlights the versatility of our approach in addressing the diversity of sign language datasets.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.355,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.359,
      "distributed_training_score": 0.382,
      "datasets_score": 0.389,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Tangentially Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper primarily focuses on generating synthetic data via GANs to augment imbalanced datasets for handshape classification, which indirectly involves creating labeled data through conditioning on labels or poses. However, it does not center on programmatically generating labels from high-level, noisy, or imprecise sources as defined in weak supervision; instead, it emphasizes data generation for training enhancement, making it only loosely connected.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667586",
      "updated_at": "2025-08-11T23:43:05.606813",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17010",
      "title": "Towards Trustworthy AI: Secure Deepfake Detection using CNNs and\n  Zero-Knowledge Proofs",
      "authors": [
        "H M Mohaimanul Islam",
        "Huynh Q. N. Vo",
        "Aditya Rane"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "In the era of synthetic media, deepfake manipulations pose a significant\nthreat to information integrity. To address this challenge, we propose\nTrustDefender, a two-stage framework comprising (i) a lightweight convolutional\nneural network (CNN) that detects deepfake imagery in real-time extended\nreality (XR) streams, and (ii) an integrated succinct zero-knowledge proof\n(ZKP) protocol that validates detection results without disclosing raw user\ndata. Our design addresses both the computational constraints of XR platforms\nwhile adhering to the stringent privacy requirements in sensitive settings.\nExperimental evaluations on multiple benchmark deepfake datasets demonstrate\nthat TrustDefender achieves 95.3% detection accuracy, coupled with efficient\nproof generation underpinned by rigorous cryptography, ensuring seamless\nintegration with high-performance artificial intelligence (AI) systems. By\nfusing advanced computer vision models with provable security mechanisms, our\nwork establishes a foundation for reliable AI in immersive and\nprivacy-sensitive applications.",
      "published_date": "2025-07-22T20:47:46+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17010v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17010v1",
      "latex_url": "http://arxiv.org/src/2507.17010v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The rapid rise of deepfakes—that is, hyper-realistic synthetic media generated by advanced artificial intelligence (AI) techniques such as generative adversarial networks (GANs) and convolutional neural networks (CNNs)—poses a severe and growing threat to trust and authenticity, especially in immersive environments like extended reality (XR). These manipulations, which can convincingly alter the likeness or voice of a person, have proliferated online: the number of deepfake videos doubled from $14{,}678$ in 2019 to over $85{,}000$ by 2022~, with projections estimating up to two (2) million by 2025~. While early deepfakes were largely limited to entertainment and satire, recent incidents have demonstrated their potential in spreading misinformation, conducting financial fraud, and even influencing political discourse~.\n\nExtended Reality (XR)—encompassing virtual reality (VR), augmented reality (AR), and mixed reality (MR)—relies on the seamless integration of real and synthetic content to deliver truly immersive experiences~. In such settings, the authenticity of visual and auditory feeds is paramount: a deepfake avatar in a collaborative VR meeting or an AR overlay on a historical landmark can have immediate and far-reaching consequences. Unlike traditional video playback, XR systems often render content in real time and on resource-constrained devices, rendering post hoc forensic analysis impractical. Moreover, these platforms routinely process sensitive personal data (e.g., facial scans, biometric readings, and behavioral metrics), amplifying privacy concerns when detection tasks are outsourced to centralized servers.\n\nConventional deepfake detectors typically exploit visual artifacts—such as imperceptible warping in facial landmarks, subtle inconsistencies in eye-blinking patterns, or lighting mismatches across frames—to distinguish genuine footage from forgeries~. However, adversaries continuously refine their generation pipelines, employing techniques like attention-based GAN refinement and high-frequency detail synthesis, which erode these telltale signs. Modern detectors employ CNN architectures that are adept at capturing complex spatial patterns and temporal correlations, yielding high detection accuracy on benchmark datasets~. Yet, deploying such models in XR faces two critical challenges:\n  {-2mm}\n  {itemize}\n   Computational Constraints: Real-time inference on head-mounted displays or mobile devices requires lightweight models and optimized proof-based verification systems.\n  {-3mm}\n   Privacy Preservation: Raw media typically contain personally identifiable information; thus, sharing them with third-party detectors or cloud services risks data breaches and regulatory non-compliance.\n  {itemize}\n  {-2mm}\n\nTo address these challenges, we introduce TrustDefender-XR, a unified framework that marries a streamlined CNN-based detection pipeline with succinct zero-knowledge proofs (ZKPs). Our contributions are fourfold:\n  {-2mm}\n  {itemize}\n   Lightweight Detection Module: We design a compact CNN architecture tailored for XR streaming, achieving competitive accuracy while maintaining a small memory and compute footprint.\n  {-3mm}\n   Real-Time Proof Construction: We develop a novel ZKP circuit that encapsulates the decision boundary of CNN(s), enabling verifiers to confirm detection outcomes in under 150~ms—suitable for interactive XR applications.\n  {-3mm}\n   Privacy-First Protocol: By integrating ZKPs, TrustDefender-XR ensures that no raw frames or biometric data leave the user's device; only a succinct proof and a binary verdict are transmitted.\n  {-3mm}\n   Comprehensive Evaluation: We benchmark our system on two state-of-the-art deepfake datasets and an XR-specific testbed, demonstrating robust detection (95.3% accuracy) and practical proof overheads.\n  {itemize}\n  {-2mm}\n\nThe remainder of this paper is organized as follows. Section~ reviews related work in deepfake detection and cryptographic proofs. Section~ details the design of our CNN architecture and ZKP construction. Section~ presents our experimental setup and its corresponding results. Finally, Section~ concludes and outlines our future research directions.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.354,
      "diffusion_reasoning_score": 0.369,
      "distributed_training_score": 0.375,
      "datasets_score": 0.335,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669602",
      "updated_at": "2025-08-11T23:43:05.607169",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17012",
      "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents",
      "authors": [
        "Zhihan Zhang",
        "Alexander Metzger",
        "Yuxuan Mei",
        "Felix Hähnlein",
        "Zachary Englhardt",
        "Tingyu Cheng",
        "Gregory D. Abowd",
        "Shwetak Patel",
        "Adriana Schulz",
        "Vikram Iyer"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CE (Computational Engineering, Finance, and Science)"
      ],
      "abstract": "Interest in sustainability information has surged in recent years. However,\nthe data required for a life cycle assessment (LCA) that maps the materials and\nprocesses from product manufacturing to disposal into environmental impacts\n(EI) are often unavailable. Here we reimagine conventional LCA by introducing\nmultimodal AI agents that emulate interactions between LCA experts and\nstakeholders like product managers and engineers to calculate the\ncradle-to-gate (production) carbon emissions of electronic devices. The AI\nagents iteratively generate a detailed life-cycle inventory leveraging a custom\ndata abstraction and software tools that extract information from online text\nand images from repair communities and government certifications. This approach\nreduces weeks or months of expert time to under one minute and closes data\navailability gaps while yielding carbon footprint estimates within 19% of\nexpert LCAs with zero proprietary data. Additionally, we develop a method to\ndirectly estimate EI by comparing an input to a cluster of products with\nsimilar descriptions and known carbon footprints. This runs in 3 ms on a laptop\nwith a MAPE of 12.28% on electronic products. Further, we develop a data-driven\nmethod to generate emission factors. We use the properties of an unknown\nmaterial to represent it as a weighted sum of emission factors for similar\nmaterials. Compared to human experts picking the closest LCA database entry,\nthis improves MAPE by 120.26%. We analyze the data and compute scaling of this\napproach and discuss its implications for future LCA workflows.",
      "published_date": "2025-07-22T20:49:25+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17012v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17012v1",
      "latex_url": "http://arxiv.org/src/2507.17012v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The urgent need to address human-caused climate change is widely recognized by the scientific community and a growing share of the general public. Recent global surveys show 63% of respondents now consider climate impacts in personal decisions such as what they buy~, which is matched by rapidly increasing internet searches for product carbon footprints (see Google Search analysis in  {blue}{Fig.~a}). Sustainability information can both inform product design~ and also increase sales as many consumers are now willing to pay a premium for environmentally friendly products~. There is a growing need to provide environmental impact (EI) information about consumer products at scale.\n\nThe traditional approach to assessing EIs, such as carbon footprint, is Life Cycle Assessment (LCA). A process-based LCA quantifies the resources used throughout the life cycle of a product or service, from raw material extraction and manufacturing to end-of-life disposal, and maps them to environmental impacts~. LCA practitioners construct a Life Cycle Inventory (LCI) by identifying all components, processes, and energy used throughout a supply chain ( {blue}{Fig.~b}). This data is often missing, proprietary, or siloed across disparate sources, requiring slow and costly manual collection efforts coordinating multiple stakeholders within a company and external suppliers~.\nMoreover, inconsistencies in system boundaries, background data matching, data transparency~, and regional variations in supply chains~ all lead to uncertain benchmarking and reproducibility challenges.\nNext, during Life Cycle Impact Assessment (LCIA), inventory entries are matched to emission factors in LCA databases or direct measurements which convert grams of material or kilowatt hours of energy to carbon emissions. When a database entry is unavailable, experts choose the closest match~. This labor-intensive workflow is challenging to scale, especially in sectors with complex supply chains and proprietary data such as electronics with hundreds of components~.\n\nThis article reimagines the conventional approach to LCA by introducing an AI-driven computational workflow illustrated in  {blue}{Fig.~c}. Leveraging recent advancements in large language models (LLMs) ~, vision-language models (VLMs)~, and AI agents~, we develop a multi-AI agent system capable of autonomously generating a life cycle inventory for real-world products and estimating their EI. To demonstrate this approach, we focus specifically on calculating the cradle-to-gate (production) carbon emissions of electronic devices.\n\nWhile domain-specific EI estimation tools have emerged , they address only a limited subset of the challenges in LCA. Each tool is tailored to a narrow use case, such as transportation~ and cloud computing~, and typically requires structured inventory inputs or proprietary design files, such as CAD~ or PCB layouts~.\n\nIn contrast, we develop an end-to-end system that requires only a product name as input. To understand the practical challenges of scaling LCA, we conducted industry research and interviews with professionals~. Our approach reframes LCA as a hierarchical information retrieval problem across a product’s supply chain.  {blue}{Figure ~c} illustrates how our multi-agent self-play environment automates the distributed information-seeking process. Two AI agents emulate the manual process whereby an LCA expert iteratively refines the LCI through consultation with a variety of stakeholders representing different knowledge and expertise about the design, manufacture, delivery, and end-of-life considerations for a product or process. This computational approach enables practitioners to perform rapid estimation and focus their time on analysis and methodology refinement instead of raw data collection.\n\nOur system makes three key contributions to LCA methodology. First, we develop a series of multimodal information retrieval tools~ that allow our multi-agent AI system to automatically search untapped, public data sources.  {blue}{Figure ~c} illustrates our pipelines for extracting textual descriptions and visual clues from textual descriptions to visual clues from sources such as volunteer repair communities and government agencies, all to automate both high-level product attributes and detailed LCI data collection. This approach both accelerates weeks or months of expert time to under one minute and closes data availability gaps.\nCombined with standard LCIA using established emission factors, our end-to-end system outperforms state-of-the-art LLMs and VLMs on EI estimation and LCA-related tasks, and can achieve, tabula rasa, estimates within 19% of expert LCAs.\n\nSecond, we introduce a k-nearest neighbors (kNN) weighted Gaussian LCIA estimator that bypasses the need for fine-grained inventory construction and emission factor mapping for certain product classes. Leveraging high-level features and domain knowledge (e.g., technology node, memory capacity), we can map an input query to a cluster of products with similar attributes and known carbon footprints. This lightweight approach achieves an MAPE of 12.28% on desktops, displays, and laptops.\n\nThird, we develop a data-driven method to estimate unknown emission factors to improve conventional LCIA calculations.\nOur agentic system can infer emission factors of inventory entries outside of LCA databases using analogous components with known emissions.\nIn a human benchmarking study where domain experts selected the closest database match, our approach improves MAPE by 120.26% (23.61% vs 143.87%).\nWe further demonstrate the inference-time scaling of AI agents and data scaling in life cycle modeling, and conclude with a discussion of the implications of the proposed approach for future LCA workflows.\n\n {figure}[t]\n  \n  [width= ]{figures/1.jpg}\n  {a, Google Search trends for sustainability-related keywords from 2008 to 2025, highlight growing public interest in considering EI in daily lives, however, such critical EI information remains largely unavailable for most real-world objects and activities. Search interest score is normalized monthly, with a value of 1 representing the peak popularity of the term.\n b, Traditionally, EI is assessed via Life Cycle Assessment (LCA), a manual, expert-driven process. LCA experts construct a Life Cycle Inventory (LCI) by identifying all components involved throughout a product’s life cycle. This process requires slow and costly manual collection efforts, coordinating multiple stakeholders within the company and external suppliers, then mapping each entry in LCI to an emission factor in databases.\n c, We introduce the first autonomous multi-AI agent system capable of generating LCIs for real-world products and estimating their EI.\n The system simulates the traditional LCA process at scale through a multi-agent self-play environment in which an LCA expert iteratively refines the LCI through consultation with a variety of stakeholders representing different knowledge and expertise through iterative querying and dialogue.\n The refined LCI can be used for standard LCIA to deliver final EI, or for estimation using a weighted sum of similar objects with known emissions based on domain-specific features.\n d, Evaluations show that the proposed system outperforms current practice in both technical performance (e.g., time efficiency), and user perception (e.g., confidence in accurately finding EI information) compared to conventional search approaches.\n }\n\n {figure}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1_intro.tex",
      "rlhf_score": 0.424,
      "weak_supervision_score": 0.384,
      "diffusion_reasoning_score": 0.377,
      "distributed_training_score": 0.366,
      "datasets_score": 0.394,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution focuses on developing multimodal AI agents for autonomous sustainability assessment, specifically for Life Cycle Assessment (LCA), using techniques like multi-agent self-play for information retrieval and estimation. While it mentions a self-play environment, which could involve reinforcement learning concepts, it does not describe the use of human feedback to train a reward model or fine-tune AI models based on human preferences. RLHF requires explicit incorporation of human-ranked data, which is absent here, making the paper unrelated to this topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668892",
      "updated_at": "2025-08-11T23:43:05.607087",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17013",
      "title": "laplax -- Laplace Approximations with JAX",
      "authors": [
        "Tobias Weber",
        "Bálint Mucsányi",
        "Lenard Rommel",
        "Thomas Christie",
        "Lars Kasüschke",
        "Marvin Pförtner",
        "Philipp Hennig"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "The Laplace approximation provides a scalable and efficient means of\nquantifying weight-space uncertainty in deep neural networks, enabling the\napplication of Bayesian tools such as predictive uncertainty and model\nselection via Occam's razor. In this work, we introduce laplax, a new\nopen-source Python package for performing Laplace approximations with jax.\nDesigned with a modular and purely functional architecture and minimal external\ndependencies, laplax offers a flexible and researcher-friendly framework for\nrapid prototyping and experimentation. Its goal is to facilitate research on\nBayesian neural networks, uncertainty quantification for deep learning, and the\ndevelopment of improved Laplace approximation techniques.",
      "published_date": "2025-07-22T20:49:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17013v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17013v1",
      "latex_url": "http://arxiv.org/src/2507.17013v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure}[ht]\n  {fig/laplax_figure_1.pdf}\n  {Linearised Laplace approximation on a two-parameter ReLU network $ (x, )= _2\\,ReLU( _1 x+1)$ trained on $ =\\{(1,-1),(-1,-1)\\}$.\n Gray contours: energy with square loss; black dot: optimised weights $ ^*$; green ellipses: $1 $ and $2 $ levels of the Laplace approximation.}\n\n {figure}\n\n {codesnippet}[ht]\n\n {codeblock}{}\nfrom jax.nn import relu\nfrom jax.numpy import array\nfrom laplax import laplace\nfrom plotting import plot_figure_1\n\n# You need a model...\ndef model_fn(input, params):\n return relu(\n params[\"theta1\"] * input - 1\n ) * params[\"theta2\"]\nparams = { # optimized weights,\n \"theta1\": array(1.6556547),\n \"theta2\": array(1.0420421)\n}\ndata = { # and training data.\n \"input\": array([1., -1.]),\n \"target\": array([1., -1.])\n}\n# ... then apply laplax ...\nposterior_fn, _ = laplace(\n model_fn, params, data,\n loss_fn=\"mse\", curv_type=\"full\",\n)\narg = {\"prior_prec\": 0.2}\ncurv = posterior_fn(arg).state['scale']\n# ... to get Figure 1.\nplot_figure_1(model_fn, params, curv)\n {codeblock}\n\n { \\ code for generating  {fig:laplace_relu}.}\n {codesnippet}\nBayesian modelling provides principled approaches to several open challenges in modern deep learning , including overconfidence in predictions , catastrophic forgetting in continual learning , and the incorporation of prior knowledge into model predictions~.\nThe Laplace approximation offers a computationally efficient, post-hoc method for approximating the posterior distribution over neural network weights, effectively transforming standard deep architectures into Bayesian neural networks.\nThis enables the use of Bayesian tools such as predictive uncertainty estimation, marginal likelihood evaluation, and model selection.\n\nDespite its conceptual simplicity, implementing the Laplace approximation involves several non-trivial choices, ranging from curvature estimation and posterior parameterization to calibration and inference techniques.\nWhile a comprehensive implementation exists for PyTorch~, a similarly extensive but more flexible and research-oriented solution for the  {jax} ecosystem has been lacking.\n\nTo address this gap, we introduce  , a lightweight and modular Python library for Laplace approximations built entirely on  {jax} .\nDesigned with research flexibility in mind,  \\ supports seamless integration with any  {jax}-based deep learning framework.\nIt features both a high-level, functional API for rapid experimentation (see Code Snippet 1 producing  {fig:laplace_relu}) and low-level building blocks to support in-depth analyses and changing the algorithm itself.\n\nIn this paper, we outline the design principles of  , describe its core components, and demonstrate its application on a simple regression and classification task.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/introduction.tex",
      "rlhf_score": 0.287,
      "weak_supervision_score": 0.312,
      "diffusion_reasoning_score": 0.297,
      "distributed_training_score": 0.297,
      "datasets_score": 0.192,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668901",
      "updated_at": "2025-08-11T23:43:05.607089",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17015",
      "title": "Can External Validation Tools Improve Annotation Quality for\n  LLM-as-a-Judge?",
      "authors": [
        "Arduin Findeis",
        "Floris Weers",
        "Guoli Yin",
        "Ke Ye",
        "Ruoming Pang",
        "Tom Gunter"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.",
      "published_date": "2025-07-22T20:57:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17015v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17015v1",
      "latex_url": "http://arxiv.org/src/2507.17015v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Pairwise feedback is widely used to understand  {} performance on complex tasks that more traditional benchmarks fail to measure well. Given a prompt and two possible responses, the annotator decides which response is \"better\". This pairwise judgement can be used for evaluation (e.g., Chatbot Arena  {chiang2024ChatbotArenaOpen}) or to provide feedback for training (e.g., via RLHF  {stiennon2020LearningSummarizeHumana,ouyang2022TrainingLanguageModels} or DPO  {rafailov2023DirectPreferenceOptimization}). Either human or AI annotators, also referred to as LLM-as-a-Judge, are used to collect such feedback. Human annotations are often considered higher quality but more expensive.\n\nBoth human and AI annotations have notable limitations: AI annotators have been observed to be susceptible to a number of biases, including changing preference based on superficial features like response order or response length . Whilst possibly providing higher quality annotations than AI annotators, human annotators also have known limitations. For example, human annotators have been observed to let their assessment of truthfulness be affected by responses' assertiveness  {hosking2024HumanFeedbackNot}.\n\nIn certain domains, obtaining high-quality annotations is particularly challenging: for responses containing long-form factual, advanced coding and math content both AI and (many) human annotators struggle to provide reliable annotations  {zheng2023JudgingLLMasaJudgeMTBench}. Annotating responses in these domains requires expertise and careful deliberation, challenging to achieve for human annotators in a limited amount of time. AI annotators may be less \"time-constrained\" but nevertheless due to known reliability issues (e.g, hallucinations, limited basic arithmetic) often fail to provide high quality annotations in these domains  {yang2023gptsolvemathematicalproblems}.\n\nIn this work, we aim to explore improving the annotation quality of widely used AI annotators on these challenging domains by augmenting the annotators with tools that can externally validate answers. We enable responses to be fact-checked using web-search, or verified using code execution. Our setup is illustrated in  {fig:abstract_summary,fig:agent_overview}. In particular, we make the following contributions:\n\n {enumerate}[wide, labelwidth=0pt, labelindent=0pt, left=3pt,itemsep=-0.1em]\n   Extensible framework for using tools with existing  {}. We introduce a new framework that enables the integration of new tools on top of existing  {} to improve annotation quality in certain domains using external validation. Our framework  {is agentic in the sense that an LLM} assesses the response domain and plans the optimal tool usage accordingly. {See  {app:agenttermdiscussion} for further discussion of our use of the term agentic.} We provide a number of initial tool implementations: (1) a long-form fact checking tool based on the Search Augmented Fact Evaluation (SAFE) method by  {wei2024LongformFactualityLarge}; (2) a code check tool built on OpenAI's code interpreter API; and (3) a math check tool similarly built on code execution. We open-source the framework's code. { {}}\n   Comprehensive experimental results evaluating our framework's capabilities. We evaluate our framework's effectiveness across a wide range of tasks including newly created datasets as well as well-established benchmarks. We compare our method to a number of popular state-of-the-art  {}, including the annotators underlying AlpacaEval 2.0~ {dubois2023AlpacaFarmSimulationFramework}, and ArenaHard~ {li2024CrowdsourcedDataHighQuality}.\n {enumerate}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.5,
      "weak_supervision_score": 0.45,
      "diffusion_reasoning_score": 0.393,
      "distributed_training_score": 0.33,
      "datasets_score": 0.39,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "Moderately Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper mentions pairwise feedback for training methods like RLHF, as it can provide data for reward models, but its primary focus is on improving AI annotator tools for general evaluation, not on implementing or advancing RLHF systems directly.",
      "weak_supervision_justification": "The paper's framework augments AI annotators with tools to programmatically generate and validate labels from external sources (e.g., web-search, code execution), which aligns with weak supervision by using noisy or imprecise methods to create training labels, though it does not explicitly address weak supervision techniques.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": "This paper explores whether external validation tools can enhance the quality of annotations for LLM-as-a-Judge systems, particularly in challenging domains such as long-form factual responses, math, and code, by addressing limitations in both AI and human annotators. The authors develop an agentic framework that integrates tools like web-search for fact-checking and code execution for verification, demonstrating through experiments on benchmarks like RewardBench, RewardMath, and new datasets that these tools improve annotation performance in many cases, though results are sensitive to factors like prompts and highlight the need for better, non-saturated benchmarks.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining existing AI annotators with external tools for validation, offering a clever new approach to mitigate biases in domains like factual, math, and code responses without introducing a entirely new problem or technique.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI evaluation and training, as it provides practical methods to enhance annotation reliability, though its influence may be limited to specific domains rather than broadly transformative.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a strong, valuable contribution with practical tools and experimental insights that advance LLM annotation techniques, making it essential for researchers focused on AI evaluation to be aware of these developments.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/a556ef1d3f2049dd1f281bebb1d8835a02f4efea",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 8,
      "average_h_index": 4.5,
      "notable_authors_count": 2,
      "author_h_indexes": [
        {
          "name": "Arduin Findeis",
          "profile_url": "https://www.semanticscholar.org/author/2172220462",
          "h_index": 2
        },
        {
          "name": "Floris Weers",
          "profile_url": "https://www.semanticscholar.org/author/1395932715",
          "h_index": 7
        },
        {
          "name": "Guoli Yin",
          "profile_url": "https://www.semanticscholar.org/author/2293171017",
          "h_index": 4
        },
        {
          "name": "Ke Ye",
          "profile_url": "https://www.semanticscholar.org/author/2371989028",
          "h_index": 1
        },
        {
          "name": "Ruoming Pang",
          "profile_url": "https://www.semanticscholar.org/author/2238621132",
          "h_index": 8
        },
        {
          "name": "Tom Gunter",
          "profile_url": "https://www.semanticscholar.org/author/2238621478",
          "h_index": 5
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668911",
      "updated_at": "2025-08-11T23:45:43.485479",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17016",
      "title": "Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time\n  Series Forecasting",
      "authors": [
        "Omid Orang",
        "Patricia O. Lucas",
        "Gabriel I. F. Paiva",
        "Petronio C. L. Silva",
        "Felipe Augusto Rocha da Silva",
        "Adriano Alonso Veloso",
        "Frederico Gadelha Guimaraes"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In recent years, the application of Large Language Models (LLMs) to time\nseries forecasting (TSF) has garnered significant attention among researchers.\nThis study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with\nfuzzy time series (FTS) and causal graph to predict multivariate time series,\nmarking the first such architecture in the literature. The key objective is to\nconvert numerical time series into interpretable forms through the parallel\napplication of fuzzification and causal analysis, enabling both semantic\nunderstanding and structural insight as input for the pretrained GPT-2 model.\nThe resulting textual representation offers a more interpretable view of the\ncomplex dynamics underlying the original time series. The reported results\nconfirm the effectiveness of our proposed LLM-based time series forecasting\nmodel, as demonstrated across four different multivariate time series datasets.\nThis initiative paves promising future directions in the domain of TSF using\nLLMs based on FTS.",
      "published_date": "2025-07-22T21:03:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17016v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17016v1",
      "latex_url": "http://arxiv.org/src/2507.17016v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Time series forecasting (TSF) and analysis play a pivotal role in many real-world applications, such as energy, traffic, healthcare, finance, and meteorology . A variety of forecasting methods have been proposed in the literature, generally classified into three main categories: statistical, machine learning (ML), and deep learning (DL) approaches . Despite their successes, these methods still face challenges such as high dimensionality, missing values, limited data availability, and the need to capture long-term dependencies—all of which are critical for accurate temporal modeling .\n\nLarge Language Models (LLMs), built on transformers composed of billions of parameters, have emerged to revolutionize DL models . Although LLMs were originally developed for natural language processing (NLP) tasks, their application in time series (TS) analysis and forecasting has recently gained significant momentum. This shift is largely due to their ability to leverage self-attention mechanisms to capture temporal dependencies and complex dynamics, thereby effectively modeling input–output relationships . This surge is driven by their remarkable ability to capture long-range dependencies and complex sequential patterns through the attention mechanism inherent in the transformers .\n\nAccording to the reviewed literature , a number of methods leveraging LLMs have been proposed for TSF, differing in input types, model integration strategies, and LLM architectures. For instance, Time-LLM uses LLaMA and GPT-2 to process multimodal TS (MTS) and relies on tokenization and fine-tuning strategies. TEMPO , which also employs GPT-2, handles univariate TS data using tokenization and prompt-based modeling without fine-tuning. In contrast, PromptCast uses BART and BERT, focusing on prompt engineering without modifying LLM weights. Chronos incorporates GPT-2 and T5, combining prompt design with model fine-tuning. LLMTIME utilizes GPT-3 and LLaMA-2 to handle multimodal inputs with prompt tuning and partial integration of LLMs into the downstream task. GPT4MTS , employing GPT-2, and UniTime , also with GPT-2, both use prompt-based methods without integrating the LLM as part of the model. Meanwhile, S {2}IP-LLM relies on GPT-2 and fully integrates it into the forecasting pipeline. Several methods, such as LAMP (using GPT-3 variants and LLaMA-2) and the model proposed in (with GPT-4 and Open-LLaMA), explore newer LLMs and fine-tuning for multivariate forecasting in specific domains. These diverse strategies reflect the flexibility of LLMs in modeling sequential dependencies in TS, originally designed for text but increasingly adapted to structured temporal data. Notably, methods like Time-LLM, TEMPO, Chronos, and S {2}IP-LLM provide open-source code, fostering reproducibility and further research.\n\nIn more domain-specific scenarios, the authors in use ChatGPT to query multimodal data for financial forecasting without fine-tuning or integration. In the healthcare domain, Liu et al. apply PaLM for MTS both forecasting and classification, although without integration or tokenization. For mobility forecasting, AuxMobLCast leverages LLMs such as BERT, RoBERTa, GPT-2, and XLNet, combining tokenization and fine-tuning strategies. LLM-Mob builds on GPT-3.5, using token-based modeling without integration. Finally, in traffic forecasting, ST-LLM employs LLaMA and GPT-2, utilizing tokenization and prompt engineering with full integration into the final model. These models demonstrate that LLMs can be effectively adapted beyond general domains, extending their capabilities to temporal, multimodal, and spatio-temporal forecasting tasks across sectors.\n\nThis research pioneers a new multiple-input single-output (MISO) LLM-based forecasting model termed CGF-LLM. This method combines the concepts of fuzzy time series (FTS) , causal graphs, and LLMs. The central objective of this work is to transform numerical time series into interpretable linguistic representations through the parallel application of fuzzification and causal analysis. This dual approach enables both semantic understanding of variable behavior and structural insight into their temporal dependencies. Specifically, the framework integrates FTS modeling with causal discovery using the PCMCI algorithm . By combining these two perspectives, the method constructs a fuzzy causal text that is both data-driven and interpretable, which serves as input for GPT-2. In other words, it extracts meaningful knowledge, providing a clearer understanding of the complex dynamics within the original time series and revealing the causal relationships among variables. The results confirm that the proposed CGF-LLM technique surpasses the standard LLM in both accuracy and computational efficiency.\n\nThe remainder of this paper is structured as follows: Section~ provides the basics of LLMs, FTS, and causal graphs. Section outlines the details of the proposed CGF-LLM method. Section covers the case studies, results, and discussion. Finally, Section concludes the paper and highlights the future research avenues.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "IEEE-conference-template-062824.tex",
      "rlhf_score": 0.411,
      "weak_supervision_score": 0.386,
      "diffusion_reasoning_score": 0.467,
      "distributed_training_score": 0.344,
      "datasets_score": 0.336,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper introduces a new LLM-based framework for time series forecasting using GPT-2, fuzzy time series, and causal graphs, focusing on data preprocessing and model integration. There is no mention of human feedback, reward models, or reinforcement learning techniques for aligning the model with preferences, which are core to RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper utilizes GPT-2 for time series forecasting by converting data into interpretable text via fuzzification and causal analysis, but it does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning as a holistic entity.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668920",
      "updated_at": "2025-08-11T23:43:05.607092",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17025",
      "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP\n  Embeddings",
      "authors": [
        "Soumen Sinha",
        "Shahryar Rahnamayan",
        "Azam Asilian Bidgoli"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.",
      "published_date": "2025-07-22T21:29:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17025v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17025v1",
      "latex_url": "http://arxiv.org/src/2507.17025v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.309,
      "weak_supervision_score": 0.298,
      "diffusion_reasoning_score": 0.346,
      "distributed_training_score": 0.307,
      "datasets_score": 0.294,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668929",
      "updated_at": "2025-08-11T23:43:05.607093",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17029",
      "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
      "authors": [
        "Luchuan Song",
        "Yang Zhou",
        "Zhan Xu",
        "Yi Zhou",
        "Deepali Aneja",
        "Chenliang Xu"
      ],
      "categories": [
        "cs.GR (Graphics)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
      "published_date": "2025-07-22T21:33:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17029v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17029v1",
      "latex_url": "http://arxiv.org/src/2507.17029v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The rapid reconstruction of head avatars reconstruction and reenactment of facial expression dynamics from a single video have become a rapidly advancing research tpoic, with vast potential for applications in VR/AR, digital human development, holographic communication, live streaming, and more. Recently, the volumetric models (e.g.~Instance-NeRF~ and 3DGS~) have endeavored to achieve both high-quality and efficient rendering.\nFor instance,\nINSTA~ employs Instant-NGP~ to accelerate rendering through engineering optimizations.\nAvatarMAV~ leverages the learnable blendshape as motion representation to achieve fast recovery of head avatar. FlashAvatar~ simulates the head avatar with a large number of Gaussian points in UV space.\n\nHowever, they continue to face challenges in balancing rendering quality and storage overhead, which constrains their applicability in consumer applications.\n \n\nIn this paper, we advance rapid facial reconstruction techniques to address the existing limitations. Additionally, we introduce a novel head avatar reconstruction task, termed on-the-fly training for reconstruction, which pushes the efficiency boundaries of fast reconstruction even further. Based on these observations, prior methods have uniformly separated training and inference processes due to efficiency constraints (refer as offline training). Such as, while AvatarMAV~ achieves efficient training speeds offline, it cannot support frame-by-frame training for reconstruction within the live streaming. Our on-the-fly training approach offers multiple advantages, including (i) protect facial privacy by eliminating the need to pre-cached personal facial models on the external machines, (ii) only 3DGS parameters are transmitted in stream video, rather than the full images (about 70% compression) and (iii) the synchronous training and recording with real-time visualization, allowing for immediate re-recording of under-trained facial areas.  \n\nWe propose a novel on-the-fly head avatar reconstruction method named StreamME. Different from the all previous head avatar reconstruction methods~, the StreamME avoids dependence on multiple MLP layers to capture deformable facial dynamics (e.g., facial expression motion), significantly reducing expression recovery time and enabling true on-the-fly training. Specifically, we attach the 3D Gaussian point clouds to the tracked head mesh surface, allowing the points to move in tandem with mesh deformations. However, the point clouds associated with the deformed mesh on the 3D head template do not fully preserve the geometric properties of the face, which results in noise cloud artifacts around the rendered face and reducing the realism. From our method, we dynamically adjust the initial 3D Gaussian points through anchor-based pruning-and-clone strategy. Instead of selecting all points from the tracked head mesh as 3D Gaussian points, we identify specific anchor points that accurately capture facial motion. The 3D Gaussian points are then updated based on these selected anchors, optimizing for head representation. This strategy improves efficiency from eliminating points that do not contribute to facial motion, while preserving the motion anchor points critical for controlling facial deformation.  \n\nMeanwhile, we find in practice that more replicated 3D Gaussian points will lead to better quality but reduce speed, especially the training speed involving backpropagation. Therefore, we explored a method to gradually simplify the point clouds, which balance the number of point clouds and rendering quality. Here, we introduce two assumptions for simplifying point clouds: (i) the points should be distributed around the facial surface, rather than within it, as internal points remain unobservable due to occlusion; (ii) the small-size 3D Gaussian points with minimal volume, contribute negligibly to image quality and the impact is imperceptible. In optimization, these two assumptions serve as foundational principles. We ensure that 3D Gaussians are progressively distributed around and outside the surface, while occluded and small-sized points are removed, enhancing execution speed. This strategy yields a sparser 3D Gaussian representation of the head avatar, substantially improving efficiency without compromising rendering quality. \n\nWith the help of Motion-Aware Anchor Points selection and Gaussian Points Simplification strategy, we achieve on-the-fly photo-realistic head avatar representation within approximately 5 minutes of live streaming, as shown in Figure~ (a). Moreover, the 3D Gaussian properties learned within 5 minutes can be applied to cross-identity head animation, facial toonification, environment relighting, and other applications with minimal fine-tuning, as illustrated in Figure~ (b, c, d). This flexibility significantly broadens the application scope of our method. Furthermore, we demonstrate the superiority of our method through extensive experiments and comparisons with both instant and long-term training approaches. In summary, our contributions include the following aspects:  \n\n (1) We present the on-the-fly head avatar reconstruction method, which is able to reconstruct facial appearance from the live streams within about 5 minutes by pure Pytorch code. To the best of our knowledge, we are the first to reconstruct and visualize the head avatar within the on-the-fly training.  \n\n (2) We emphasize the efficiency in training, and introduce motion saliency anchor selection and point cloud simplification strategy. The anchor selection minimizes reliance on MLPs within the deformation field, while point cloud simplification strategy reduces computational redundancy from 3D Gaussian points.\n\n (3) A series of downstream applications are attached, which have demonstrated the advances of our approach and provided novel insight for the on-the-fly training method.\n\n {figure*}[t]\n  \n\n  [width=.99 ]{figs/pipeline_update.pdf}\n\n  {-0.4cm}\n  {The overview the pipeline of StreamME. We list three components at here. i) The 3DGS Properties Warming-Up (Optional): we introduce two auxiliary learnable 3D Gaussian attribute texture and illumination, refining the UV vertex positions to improve facial geometry detail (e.g. here, we show the coarse displacement for the vertices around the hair). This step is optional, and the users may also opt to use the tracked head without displacement. ii) Anchor Duplication and Simplification: we freeze the Tex and SH attributes and introduce a binary learnable mask, initialized with all values set to $1$, from the UV vertices sampled on the mesh. Natural face$ $Xuan Gao et al. (CC BY).}\n  {-0.1cm}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "new_submission.tex",
      "rlhf_score": 0.31,
      "weak_supervision_score": 0.3,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.37,
      "datasets_score": 0.261,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667726",
      "updated_at": "2025-08-11T23:43:05.606844",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17038",
      "title": "Transformer Based Building Boundary Reconstruction using Attraction\n  Field Maps",
      "authors": [
        "Muhammad Kamran",
        "Mohammad Moein Sheikholeslami",
        "Andreas Wichmann",
        "Gunho Sohn"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In recent years, the number of remote satellites orbiting the Earth has grown\nsignificantly, streaming vast amounts of high-resolution visual data to support\ndiverse applications across civil, public, and military domains. Among these\napplications, the generation and updating of spatial maps of the built\nenvironment have become critical due to the extensive coverage and detailed\nimagery provided by satellites. However, reconstructing spatial maps from\nsatellite imagery is a complex computer vision task, requiring the creation of\nhigh-level object representations, such as primitives, to accurately capture\nthe built environment. While the past decade has witnessed remarkable\nadvancements in object detection and representation using visual data,\nprimitives-based object representation remains a persistent challenge in\ncomputer vision. Consequently, high-quality spatial maps often rely on\nlabor-intensive and manual processes. This paper introduces a novel deep\nlearning methodology leveraging Graph Convolutional Networks (GCNs) to address\nthese challenges in building footprint reconstruction. The proposed approach\nenhances performance by incorporating geometric regularity into building\nboundaries, integrating multi-scale and multi-resolution features, and\nembedding Attraction Field Maps into the network. These innovations provide a\nscalable and precise solution for automated building footprint extraction from\na single satellite image, paving the way for impactful applications in urban\nplanning, disaster management, and large-scale spatial analysis. Our model,\nDecoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,\ndemonstrating its ability to deliver accurate and regularized building\nfootprints across diverse and challenging scenarios.",
      "published_date": "2025-07-22T21:53:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17038v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17038v1",
      "latex_url": "http://arxiv.org/src/2507.17038v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{F}{}or centuries, maps have been indispensable tools for human society, serving critical roles in navigation, planning, and understanding the world. Their applications span various fields, including urban planning, disaster response, ecological monitoring, and agricultural management. The creation of maps involves several intricate steps, such as geometric alignment to ensure accurate spatial representation, semantic labeling to add meaningful context, and vectorization to convert spatial features into abstract representations. Among these processes, this work focuses on vectorization, which transforms spatial data into simplified representations that capture objects' essential geometric and relational properties.\n\n  Abstract representations, such as building outlines and road networks, are fundamental to efficiently processing and analyzing spatial data. These abstractions are crucial for automating and scaling the map-making process, enabling faster storage, processing, and interpretation. However, compared to other components of cartography, the study of vector-based abstractions is less mature. While advancements in artificial intelligence (AI) and remote sensing have addressed some of the challenges in this area, achieving fully automated and reliable vectorization remains unresolved.\n\n  The need for automation in mapping has grown more urgent with the rapid pace of urbanization. Cities around the globe are expanding at unprecedented rates, with thousands of buildings constructed every year. This urban growth requires frequent updates to spatial maps to support urban development, infrastructure management, and environmental planning. Traditional mapping techniques, which rely heavily on manual annotation and human expertise, struggle to keep up with the demands of modern cities. Agencies like the United States Geological Survey (USGS) and the European Space Agency (ESA) have made strides in producing high-quality maps, but manual processes remain time-consuming, labor-intensive, and costly. As a result, many regions—particularly in developing countries—still lack accurate, up-to-date maps, highlighting a pressing need for scalable solutions.\n\n  Recent advancements in satellite technology and commercial space exploration, often referred to as the \"new space era,\" have transformed how spatial data is collected and utilized. This era has brought unprecedented accessibility to Earth observation data, offering new insights into urban growth and land-use patterns. These innovations can potentially revolutionize map-making, enabling faster and more accurate mapping processes.\n\n  This paper focuses on leveraging cutting-edge deep learning techniques to automate the vectorization and abstract representation of buildings. By integrating advanced AI algorithms, geospatial analysis methods, and computationally efficient processes, this research addresses the challenges posed by rapid urbanization and the growing demand for automated mapping. Also, deep learning has emerged as a powerful tool for remote sensing applications, particularly in building footprint extraction. Our network excel at learning complex patterns from large datasets and can generalize effectively to new, unseen data. This work aims to harness these capabilities to bridge the gap between manual and automated mapping, paving the way for scalable and efficient solutions to support urbanization and global mapping efforts.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "bare_jrnl_new_sample4.tex",
      "rlhf_score": 0.343,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.408,
      "distributed_training_score": 0.366,
      "datasets_score": 0.335,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using Graph Convolutional Networks (GCNs) and transformers for building boundary reconstruction from satellite imagery, emphasizing geometric regularity and feature integration. It does not involve diffusion models, iterative refinement for logical tasks, or any form of Chain-of-Thought reasoning. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668388",
      "updated_at": "2025-08-11T23:43:05.606994",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17043",
      "title": "Computational Performance Bounds Prediction in Quantum Computing with\n  Unstable Noise",
      "authors": [
        "Jinyang Li",
        "Samudra Dasgupta",
        "Yuhong Song",
        "Lei Yang",
        "Travis Humble",
        "Weiwen Jiang"
      ],
      "categories": [
        "quant-ph (Quantum Physics)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Quantum computing has significantly advanced in recent years, boasting\ndevices with hundreds of quantum bits (qubits), hinting at its potential\nquantum advantage over classical computing. Yet, noise in quantum devices poses\nsignificant barriers to realizing this supremacy. Understanding noise's impact\nis crucial for reproducibility and application reuse; moreover, the\nnext-generation quantum-centric supercomputing essentially requires efficient\nand accurate noise characterization to support system management (e.g., job\nscheduling), where ensuring correct functional performance (i.e., fidelity) of\njobs on available quantum devices can even be higher-priority than traditional\nobjectives. However, noise fluctuates over time, even on the same quantum\ndevice, which makes predicting the computational bounds for on-the-fly noise is\nvital. Noisy quantum simulation can offer insights but faces efficiency and\nscalability issues. In this work, we propose a data-driven workflow, namely\nQuBound, to predict computational performance bounds. It decomposes historical\nperformance traces to isolate noise sources and devises a novel encoder to\nembed circuit and noise information processed by a Long Short-Term Memory\n(LSTM) network. For evaluation, we compare QuBound with a state-of-the-art\nlearning-based predictor, which only generates a single performance value\ninstead of a bound. Experimental results show that the result of the existing\napproach falls outside of performance bounds, while all predictions from our\nQuBound with the assistance of performance decomposition better fit the bounds.\nMoreover, QuBound can efficiently produce practical bounds for various circuits\nwith over 106 speedup over simulation; in addition, the range from QuBound is\nover 10x narrower than the state-of-the-art analytical approach.",
      "published_date": "2025-07-22T22:00:09+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17043v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17043v1",
      "latex_url": "http://arxiv.org/src/2507.17043v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{W}{e} are now witnessing the rapid growth of quantum hardware (e.g., IBM quantum scaled from 5 qubits in 2016 to 1,121 qubits in 2023), which raises the great potential for next-generation quantum-centric supercomputing , where multiple quantum processors will interplay with high-performance computing (HPC) systems, aiming to significantly boost computational capacity for real-world applications, such as chemistry and finance .\nHowever, the practical use of these quantum processors faces notable challenges, especially the inherent noise in quantum devices, which are prone to high errors.\nDue to multiple noise sources exhibiting randomness, such as thermal noise related to the ambient computing environment, noise on quantum devices presents instability and unpredictability.\nAs a result, the performance of an application will fluctuate over time ,  {and several works have explored methods to handle noise in order to improve real-world performance .}\n\nUnlike classical computing, the system management (e.g., job scheduling) for quantum-centric computing needs to consider not only the traditional objectives (e.g., resource utilization and job wait time) but also the functional correctness (i.e., fidelity) of an application on noisy quantum processors.\nTherefore, it calls for fidelity-aware system management.\nIn this context, the accurate and efficient performance prediction of quantum applications becomes essential, which provides the metric for management optimizations.\nThis work aims to develop an efficient workflow for performance-bound prediction of quantum circuits at runtime.\n {Such a capability is particularly valuable for system schedulers selecting execution backends and for compiler developers evaluating transpilation or circuit layout strategies under time-varying noise conditions.}\n\nOne straightforward solution is to run a noisy simulation to capture the performance bounds ; however,\n\nit lacks efficiency in two aspects.\nFirst, noisy simulations are time-consuming, but noise changes frequently; thus, the obtained bounds might already be outdated after the simulation.\nSecond, the simulation faces scalability issues; the memory requirements of a full-state simulation grow exponentially along with the number of qubits, easily exceeding the memory capacity on classical computers.\nAnother solution is to carry out statistic analysis; a recent research work proposes to predict the theoretical performance bounds by calculating the distance of the distributions under different noise. It can achieve significant speedup at runtime; however, the theoretic analysis is commonly coupled with worst-case analysis, which leads to a too-loose predicted bound.\n\nTo make the performance prediction practical, we aim to minimize the prediction time and bound range, while maximizing the ratio of successfully predicted run-time performance between the bounds.\n\nIn this paper, we rethink the solution from a data-driven perspective.\nAs cloud access to quantum computers became available in 2016, and the quantum providers have already collected and released enough quantum noise data, this provides the potential to use data-driven approaches for performance predictions.\n\nWhile it's motivating, challenges remain.\nFirst, a mismatch exists between performance and noise sources.\nSpecifically, all quantum noise sources contribute to the performance, but we can only get the performance as a whole.\nMoreover, the off-the-shelf noise model only covers part of noise sources, saying it provides device-related noise information but lacks sampling noise information, which comes from the probabilistic nature of quantum mechanics.\nAs such, without decomposing the performance and matching it to the related noise sources, it's hard to accurately predict the performance.\nSecond, even if we can decompose and match the performance with noise sources,\n\nthe appropriate machine learning (ML) architecture for performance prediction is unclear. More importantly, it's unknown how to encode noise with a quantum circuit and process it by the ML algorithm.\n\nThis paper proposes a data-driven performance-bound prediction framework, namely QuBound.\nIt is a dual-component workflow that combines performance decomposition with an ML-based performance predictor. The first component, QuDECOM, focuses on decomposing the performance of quantum circuits into trend and residual parts, isolating device noise from sampling noise. This decomposition not only enhances the accuracy of our predictor by using the pair of device noise and trend performance, but also enables us to predict upper and lower bounds by using the residual performance.\nThe second component, QuPRED, contains a novel encoding strategy that integrates detailed quantum circuit information with noise characteristics, capturing the interplay between quantum operations and noise effects.\n\nThen, QuBound applies the Long Short-Term Memory (LSTM) model, renowned for its ability to handle sequential data, corresponding to the time-series historic noise and performance traces in our work.\n\nThe model then processes the encoded information to predict the performance of a quantum circuit under specific noise.\n\nThe main contributions of this paper are as follows:\n\n {itemize}\n   We formally define the quantum bound prediction problem, denoted ${QuBound_p}$.\n   QuBound innovatively proposes a performance decomposition approach QuDECOM to isolate the effects of different types of noise on performance.\n\n   To the best of our knowledge, the solver QuBound is the very first data-driven workflow to combine ML and performance decomposition to predict the performance bounds on quantum computers with unstable noise.\n\n   A dataset is built with pairs of noise sources and decomposed performance, on top of which, a novel data encoder can create embeddings of noise data and quantum circuits, enabling an LSTM-based performance prediction.\n {itemize}\n\nWe evaluate QuBound on various circuit types. We first compare it with learning-based methods that predict only a single value. Results show that QuBound more accurately estimates performance within the ideal bound. We further compare it with two bound-generating methods: (1) repeated noisy simulations and (2) theoretical analysis. QuBound is evaluated in terms of bound range, runtime, and prediction accuracy under different noise conditions. It generates tight, practical bounds with low latency on both small (e.g., GHZ-3) and large circuits (e.g., GHZ-15).\nWe also demonstrate the effectiveness of QuBound across multiple quantum backends.\n\nThe paper is organized as follows. Section discusses the background and related works. Section defines the problem. Section reveals the motivation and challenges. Section details the proposed workflow QuBound. Experimental results and discussions are given in Section and Section . Last, conclusion remarks are in Section .\n\n%",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "introduction.tex",
      "rlhf_score": 0.301,
      "weak_supervision_score": 0.363,
      "diffusion_reasoning_score": 0.367,
      "distributed_training_score": 0.368,
      "datasets_score": 0.261,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668938",
      "updated_at": "2025-08-11T23:43:05.607094",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17047",
      "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding",
      "authors": [
        "Kuleen Sasse",
        "Efsun Sarioglu Kayi",
        "Arun Reddy"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.",
      "published_date": "2025-07-22T22:09:00+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17047v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17047v1",
      "latex_url": "http://arxiv.org/src/2507.17047v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{figure*}[t]\n  \n\n  [width= ]{figures/lavila-llava.png}\n  {Ensemble Captioner: Two separate models are responsible for generating the action and scene captions.}\n\n {figure*}\n {figure*}[t]\n  \n\n  [width= ]{figures/lavila-chc.png}\n  {Controllable Hybrid Captioner (CHC): Single model generates both action and scene captions. }\n\n {figure*}\nCurrent approaches for short-form video understanding do not easily scale up to processing and reasoning over longer videos. Video analysis research has largely focused on short-term tasks like human action recognition, which analyze video clips on the order of a few seconds in length . While useful, these algorithms do not scale easily to higher-level understanding of long-form videos, which involves reasoning over complex relationships between different actions, actors and objects . In addition, the spatiotemporal modeling present in short-form video models would become computationally infeasible to implement at longer timescales .\n\nWe investigate approaches for handling long-form video through the progressive construction of a compact memory of observed activity. Our methods enable natural language question answering over several minutes of video data. To represent the video in a more compact format, we construct a text-based representation of the video content in the form of a log of different events that occur during the video. This log is then used to answer natural language questions about the video and evaluate our approaches on a publicly available video question answering benchmark, EgoSchema .\n\nSpecifically, our framework is based on Language-augmented Video Language Pretraining (LaViLa) captioner which learns video-language representations by conditioning LLMs on visual input, and finetunes them to create automatic video narrators. The LLoVi approach demonstrates how LaViLa model can be used as a short-term video captioner for long-form video Q\\&A by pairing it with another LLM. In contrast to LLoVi, we do not use multi-round\nprompting or more advanced prompting strategies such as query-based summarization, and instead use the same prompt for Q\\&A. Both LaViLa and LLoVi models focus only on action captions and do not capture other possibly relevant details such as scene information. In our approach, we improve long-form video Q\\&A task by enhancing the caption logs to incorporate scene information when a scene change is detected. We first pair LaViLa captioner with a VLM to describe the scene. To improve efficiency, we also finetune the narrator on synthetically generated scene captions to act as a hybrid captioner. Our results show that having scene information in the caption log improves long-form video Q\\&A accuracy. By incorporating scene information only when a change is detected, and training a single model to output both types of captions, we also improve the efficiency of the system.\n\n {table*}\n  \n  {tabular}{@{}lllcc@{}}\n  \n Captioner Model& Scene Segmentation &Scene Captions?&  {2}{c}{Accuracy (%)}\n\n  \n LaViLa/LLoVi &n/a &No & {2}{c}{41.4}\n\n  \n & &&  {2}{c}{Scene Captioner }\n\n  {4-5}\n & && LLaVA 7B& LLaVA 34B\n\n  \n  {3}{*}{LaViLa + LLaVA} & Uniform & Yes & 38.2 &51.0\n\n &PyScene & Yes & 37.6 &50.2\n\n & KTS & Yes & 43.8&57.2\n\n  \n & &&  {2}{c}{Distillation Model}\n\n  {4-5}\n & && LLaVA 7B& LLaVA 34B\n\n  \n  {4}{*}{LaViLA-CHC} &Uniform &No &47.6&45.6\n\n &Uniform& Yes &50.2 & 52.4\n\n &PyScene& Yes& 48.4&51.6\n\n &KTS &Yes &40.6 & 52.2\\%49.2&49.8\n  \n  {tabular}\n  {Accuracy of our captioning framework on EgoSchema dataset using Llama3.1-70B-Instruct for Q\\&A }\n\n {table*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "templateArxiv.tex",
      "rlhf_score": 0.375,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.438,
      "distributed_training_score": 0.325,
      "datasets_score": 0.305,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on developing a controllable hybrid captioner for long-form video understanding, using models like LaViLa and LLaVA for captioning and question answering. It involves fine-tuning for action and scene captions but does not mention diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning tasks. Therefore, it lacks any connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667597",
      "updated_at": "2025-08-11T23:43:05.606816",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17050",
      "title": "Toward Scalable Video Narration: A Training-free Approach Using\n  Multimodal Large Language Models",
      "authors": [
        "Tz-Ying Wu",
        "Tahani Trigui",
        "Sharath Nittur Sridhar",
        "Anand Bodas",
        "Subarna Tripathi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.",
      "published_date": "2025-07-22T22:16:37+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17050v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17050v1",
      "latex_url": "http://arxiv.org/src/2507.17050v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Video is a multidimensional signal, encapsulating the dynamic scenes and complex visual details across spatial and temporal dimensions. This characteristic makes it an influential medium for recording, communication, entertainment, and advertising.\nDespite containing vast amounts of information, videos are inherently low-level and demand substantial storage space. Moreover, retrieving specific information from very long videos in response to a query can be challenging and inefficient if done frequently. It is therefore essential to extract the core content of the video and preserve it in a more concise format, such as dense video captioning (DVC), where narrations are provided with their timestamps, such as ``52.2s - 74.4s the person is then spreading mayonnaise on the bread.\"\nThis creates a structured snapshot of the video, capturing the scene semantics and dynamics within each video segment, that can be potentially extended for downstream applications in advertising and marketing, e.g., understanding visual advertisements~, and analyzing user or influencer videos for targeted marketing~ in several domains including ad-personalization, retail~, and e-commerce~.\n\n {figure}[t!]\n  \n  [width=.95 ]{images/teaser.pdf}\n  {-5pt}\n  {{  VideoNarrator} is a {  training-free} and configurable pipeline harnessing the power of off-the-shelf MLLMs and VLMs for dense video captioning, establishing a scalable solution for real-world video understanding tasks.}\n\n {figure}\n\nWhile videos are widely accessible from multiple sources, DVC annotations are costly to obtain and thus sparsely available, limiting the training and evaluation scope in prior DVC research~.\nIn contrast, the recent advances that bridge visual and language domains present a new opportunity: generate video narrations for {  any} video using common knowledge acquired from a broader range of datasets. For example, a general purpose multimodal large language model (MLLM)~ can be guided to describe the content at regular intervals (for every $S$ seconds). Although promising, this approach remains underexplored. Since these models are not specifically tailored to the target video, the resulting video narrations may not always be reliable and could include inaccuracies or hallucinations.\n\nTo address this, we propose {  VideoNarrator}, a {  training-free} pipeline for reducing hallucinations and improving the quality of DVC. This framework employs a modular design, leveraging existing MLLMs and visual-language models (VLMs) to serve as {  caption generators}, {  context providers}, or {  caption verifiers}, where each component plays a distinct role: generating narrations, supplying scene context, and detecting hallucinated captions, respectively. For example, an object detector~ can be a {  context provider}, offering rich semantics about the scene to supplement a {  caption generator} for crafting more relevant captions, while a {  caption verifier} can be utilized to identify and eliminate inaccuracies. The synergy of these roles improves the accuracy and relevance of the captions.\n\n {figure}[t!]\n  \n  \n  [width=.93 ]{images/mllm.pdf}\n  {-5pt}\n  {Dense video captioning with MLLMs. Videos are segmented into chunks with uniform intervals (i.e., $S$ seconds), and the MLLM generates the caption for each segment individually.}\n\n {figure}\n\n {figure*}[t!]\n  \n  [width=0.84 ]{images/pipeline.pdf}\n\n  {The {  VideoNarrator} pipeline includes MLLM and VLM modules functioned for different purposes: {  caption generator}, {  context provider}, and {  caption verifier}. It is a {  training-free} and configurable framework. The video in the example is sourced from .}\n\n {figure*}\n\nFor quantitative assessment of these components, we introduce an evaluation protocol that measures the quality of video captions through a multiple-choice question answering (MCQ) task using the Video-MME~ dataset, which comprises a wide range of questions associated with diverse videos.\nExtensive experiments demonstrate that by integrating these roles, {  VideoNarrator} effectively enhances the reliability of video descriptions, offering a scalable solution for generating high-quality narrations without the necessity for extensive training tailored for specific use cases.\n\nIn summary, the paper makes the following contribution:\n {itemize}\n   We propose VideoNarrator, a {  training-free} DVC framework that enhances caption quality and reliability through modular integration of existing MLLMs and VLMs.\n   We enhance caption accuracy and relevance by leveraging semantic scene information and hallucination detection, reducing common errors in video narration.\n   We present a new evaluation protocol based on multiple-choice question answering using the Video-MME dataset, offering robust quantitative assessment of DVC performance.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "2_intro.tex",
      "rlhf_score": 0.341,
      "weak_supervision_score": 0.406,
      "diffusion_reasoning_score": 0.423,
      "distributed_training_score": 0.4,
      "datasets_score": 0.34,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper introduces a training-free pipeline using pre-trained MLLMs and VLMs for video captioning, without any model training or label generation from noisy sources. Weak supervision involves programmatically generating labels for training, which is not addressed here, making the paper unrelated.",
      "diffusion_reasoning_justification": "The paper focuses on a modular pipeline for video narration using existing MLLMs and VLMs, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning. Thus, it does not involve diffusion-based approaches.",
      "distributed_training_justification": "The paper's method is training-free and does not involve any training processes, distributed or otherwise, such as parallel computing or partitioning data across nodes. It only integrates off-the-shelf models for video captioning.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668396",
      "updated_at": "2025-08-11T23:43:05.606996",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17054",
      "title": "New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent\n  Path Finding",
      "authors": [
        "Shao-Hung Chan",
        "Thomy Phan",
        "Jiaoyang Li",
        "Sven Koenig"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths, one for each agent in a shared environment. Its objective\nis to minimize the sum of path costs (SOC), where the path cost of each agent\nis defined as the travel time from its start location to its target location.\nExplicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for\nbounded-suboptimal MAPF, with the SOC of the solution being at most a\nuser-specified factor $w$ away from optimal. EECBS maintains sets of paths and\na lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of\npaths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve\ncollisions. For each path in a set, EECBS maintains a lower bound on its\noptimal path that satisfies constraints. By finding an individually\nbounded-suboptimal path with cost at most a threshold of $w$ times its lower\nbound, EECBS guarantees to find a bounded-suboptimal solution. To speed up\nEECBS, previous work uses flex distribution to increase the threshold. Though\nEECBS with flex distribution guarantees to find a bounded-suboptimal solution,\nincreasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS\nto switch among different sets of paths instead of resolving collisions on a\nparticular set of paths, and thus reducing efficiency. To address this issue,\nwe propose Conflict-Based Flex Distribution that distributes flex in proportion\nto the number of collisions. We also estimate the delays needed to satisfy\nconstraints and propose Delay-Based Flex Distribution. On top of that, we\npropose Mixed-Strategy Flex Distribution, combining both in a hierarchical\nframework. We prove that EECBS with our new flex distribution mechanisms is\ncomplete and bounded-suboptimal. Our experiments show that our approaches\noutperform the original (greedy) flex distribution.",
      "published_date": "2025-07-22T22:25:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17054v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17054v1",
      "latex_url": "http://arxiv.org/src/2507.17054v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Multi-Agent Path Finding (MAPF) is the problem of finding collision-free paths, one for each agent moving from its start location to its target location in a shared environment.\nAn optimal solution for a MAPF instance is a set of collision-free paths with a minimal sum of path costs (SOC), where the path cost of an agent is its travel time for moving from its start location to its target location.\nThe MAPF applications include autonomous warehouses~ and traffic management~.\n\nSince finding optimal MAPF solutions is NP-hard~, bounded-suboptimal MAPF algorithms have been used to speed up the search while still providing guarantees on the solution quality.\nThat is, the SOC is at most a user-specified suboptimality factor $w$ away from optimal.\nOne of the leading MAPF algorithms is Explicit Estimation Conflict-Based Search (EECBS)~, which first finds a path for each agent individually and then resolves the collisions.\nEECBS maintains several sets of paths, where each set of paths is associated with a set of constraints that have been used to resolve collisions.\nFor each path in a set of paths, EECBS maintains a lower bound on its optimal path that satisfies constraints.\nThus, given a set of paths, the sum of each path's lower bound (SOLB) is the lower bound on the SOC of the optimal solution that satisfies the constraints, and the minimum SOLB over all sets of paths is a lower bound $LB$ on the SOC of the optimal solution.\nEECBS iteratively selects a set of paths whose SOC is at most $w   LB$ and introduces new constraints to resolve collisions.\nBy finding an individually bounded-suboptimal path whose cost is at most a threshold equal to $w$ times its lower bound, EECBS guarantees that each set of paths is bounded-suboptimal with respect to their constraints, i.e., the SOC is at most $w$ away from the optimal solution that satisfies the constraints.\n\nHowever, the requirement that each path needs to be individually bounded-suboptimal prohibits EECBS from finding solutions where the paths are not all individually bounded-suboptimal, but their SOC is still bounded-suboptimal with respect to the constraints.\nThus, previous work has proposed using flex distribution to relax this requirement while still guaranteeing to find a bounded-suboptimal solution~.\nBefore finding a path for an agent, the approach sums the differences between $w$ times the lower bounds and the path costs over all other agents, defined as the flex, and then increases the threshold of the agent by this flex.\nThe increased threshold allows the agent to take a longer path to avoid collisions with other agents while still guaranteeing that the solution is bounded-suboptimal.\n\nEECBS with flex distribution is able to explore part of the solution space that is not reachable when each path has to be individually bounded-suboptimal.\nHowever, using all the flex to increase the thresholds may push the SOC beyond $w   LB$, which forces EECBS to switch among different sets of paths whose SOCs are still at most $w   LB$ instead of focusing on resolving collisions in a particular set of paths.\n\nOn the other hand, if the path satisfying constraints for an agent has a cost beyond $w$ times its lower bound, then by using flex, EECBS may no longer have to increase its lower bound to find the path as the flex increases its threshold.\nIn this case, with flex distribution, the SOLB of a set of paths may be lower than that without, which may result in an under-estimated $LB$.\nThat is, EECBS with flex distribution may overlook solutions that are bounded-suboptimal but whose SOCs are larger than a factor $w  LB$.\n\nThus, in this paper, we aim to address the issue of flex distribution that the sets of paths may have SOCs larger than $w   LB$.\nOur contributions are as follows:\n {itemize}\n   To eliminate the increment of SOC in a set of paths, we propose new mechanisms for flex distribution to avoid using all the flex.\n   For a congested environment, we redesign Focal-A* search from~ {ChanAAAI2022} for EECBS to find a path for an agent while trying to increase its lower bound in congested environments.\n {itemize}\nOur empirical evaluation shows that EECBS with our approaches improves the success rate within a runtime limit of 120 seconds in comparison to the state-of-the-art EECBS.\nWe also provide a case study to demonstrate our approach in comparison to the state-of-the-art EECBS.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "aaai25.tex",
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.263,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.322,
      "datasets_score": 0.213,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667881",
      "updated_at": "2025-08-11T23:43:05.606879",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17056",
      "title": "Pragmatic Policy Development via Interpretable Behavior Cloning",
      "authors": [
        "Anton Matsson",
        "Yaochen Rao",
        "Heather J. Litman",
        "Fredrik D. Johansson"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Offline reinforcement learning (RL) holds great promise for deriving optimal\npolicies from observational data, but challenges related to interpretability\nand evaluation limit its practical use in safety-critical domains.\nInterpretability is hindered by the black-box nature of unconstrained RL\npolicies, while evaluation -- typically performed off-policy -- is sensitive to\nlarge deviations from the data-collecting behavior policy, especially when\nusing methods based on importance sampling. To address these challenges, we\npropose a simple yet practical alternative: deriving treatment policies from\nthe most frequently chosen actions in each patient state, as estimated by an\ninterpretable model of the behavior policy. By using a tree-based model, which\nis specifically designed to exploit patterns in the data, we obtain a natural\ngrouping of states with respect to treatment. The tree structure ensures\ninterpretability by design, while varying the number of actions considered\ncontrols the degree of overlap with the behavior policy, enabling reliable\noff-policy evaluation. This pragmatic approach to policy development\nstandardizes frequent treatment patterns, capturing the collective clinical\njudgment embedded in the data. Using real-world examples in rheumatoid\narthritis and sepsis care, we demonstrate that policies derived under this\nframework can outperform current practice, offering interpretable alternatives\nto those obtained via offline RL.",
      "published_date": "2025-07-22T22:34:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17056v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17056v1",
      "latex_url": "http://arxiv.org/src/2507.17056v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Observational data provide a valuable foundation for applying machine learning (ML) to derive treatment policies that enhance clinical decision-making~ {chakraborty2013statistical}. Recently, significant attention has been devoted to reinforcement learning (RL), a branch of ML focused on learning optimal decision-making strategies~ {sutton2018rl}. While classical RL relies on trial-and-error learning—ill-suited to high-stakes domains like healthcare—offline RL learns from previously collected data, offering the potential to turn static datasets into effective decision-making engines~ {levine2020offline}. However, applying offline RL in clinical settings presents several well-known challenges~ {yu2021reinforcement,jayaraman2024primer}, with the lack of interpretability and the difficulty of policy evaluation being among the most significant. These limitations raise the question of whether other approaches may be better suited for practical clinical use.\n\nPolicy evaluation involves assessing the performance of a newly derived policy, often referred to as the target policy. In safety-critical domains, this evaluation—like policy learning—must be performed using an offline dataset, a problem known as off-policy evaluation (OPE). A large class of OPE methods rely on importance sampling (IS)~ {precup2000eligibility}, where outcomes from patient trajectories collected under the behavior policy—that is, the current, observed decision-making behavior—are weighted by how likely those trajectories would be under the target policy. When the target and behavior policies differ substantially, IS-based estimates of performance tend to exhibit high variance. While this issue can be mitigated—for example, by normalizing weights~ {precup2000eligibility} or incorporating model-based components into the estimator~ {jiang2016doubly,thomas2016data,farajtabar2018more}—reliable OPE generally requires that the policies are sufficiently similar~ {gottesman2018evaluating,voloshin2021empirical}.\n\nThe challenge of interpretability arises from the fact that much of RL's recent success is due to its integration with deep learning (deep RL), where black-box neural networks are used to represent policies~ {mnih2015human}. Such target policies are typically difficult to interpret, and this lack of transparency can prevent domain experts from identifying errors or artifacts, potentially undermining trust in medical applications~ {pace2022poetree,lipton2017doctor}. While there have been efforts to improve the interpretability of RL policies—either directly by defining interpretable policy classes~ {silva2020optimization,verma2019imitation,hein2018interpretable}, or indirectly by distilling interpretable policies from black-box models~ {verma2018programmatically,liu2018toward}—the prevailing view is that deep RL is not yet ready for deployment in high-stakes domains such as healthcare~ {glanois2024survey}.\n\nCombining ideas from interpretable RL with robust offline RL, where the target policy is constrained to stay close to the behavior policy to improve evaluability~ {fujimoto2019off,kostrikovoffline,kumar2020conservative}, offers a promising direction for addressing these practical limitations. However, before deploying the full RL machinery, it is reasonable to ask: can we develop interpretable policies that are amenable to reliable OPE in a simpler and more pragmatic way?\n\n {Contributions}\nWe propose using supervised learning of the behavior policy—also known as behavior cloning~ {torabi2018behavioral}—to derive interpretable and evaluable target policies for clinical decision-making. Specifically, the proposed policy is constructed based on the most frequently chosen treatments in each patient state, as estimated by the behavior policy model, with the option to incorporate their observed outcomes to further guide treatment selection. As such, the policy exploits the collective clinical expertise embedded in the data. By varying the number of treatments considered, we control the degree of overlap with the behavior policy, facilitating reliable OPE. Furthermore, by choosing an interpretable model class for the behavior policy, the resulting target policy is interpretable by design. While several model classes are possible, we recommend using a tree-based structure, as it provides a natural partitioning of the patient space based on observed treatment patterns. To utilize a common situation in clinical decision-making—where patients often remain on the same treatment across decision points—we construct a meta-model that uses separate trees to predict whether a patient will switch treatments and, if so, which treatment they will switch to.\n\nWe refer to this approach as pragmatic policy development. While we cannot guarantee that such policies outperform current practice, they are explicitly designed to be amenable to OPE, enabling meaningful comparison. In experiments, we find that policies developed under this framework are, on average, estimated to have higher policy values than current practice in real-world examples from the management of rheumatoid arthritis (RA) and sepsis. In contrast, policies derived using offline RL yield estimates with high variance, raising questions about their practical relevance.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.47,
      "weak_supervision_score": 0.395,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.331,
      "datasets_score": 0.282,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is a pragmatic approach to offline reinforcement learning using interpretable behavior cloning from observational data, such as patient trajectories in healthcare. It does not involve human feedback, such as rankings or preferences, to train a reward model or fine-tune an AI model via reinforcement learning. Since RLHF specifically requires these elements, the paper has no connection to the topic.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668947",
      "updated_at": "2025-08-11T23:43:05.607095",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17061",
      "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in\n  Multi-Agent LLM Systems",
      "authors": [
        "Chengxuan Xia",
        "Qianye Wu",
        "Sixuan Tian",
        "Yilun Hao"
      ],
      "categories": [
        "cs.MA (Multiagent Systems)",
        "cs.AI (Artificial Intelligence)",
        "cs.IR (Information Retrieval)"
      ],
      "abstract": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.",
      "published_date": "2025-07-22T22:42:51+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17061v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17061v1",
      "latex_url": "http://arxiv.org/src/2507.17061v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent advances in large language models (LLMs) have enabled autonomous agents to perform increasingly complex tasks across domains such as summarization, research assistance, and technical writing. Building on these capabilities, multi-agent frameworks have been proposed to coordinate several LLM-powered agents for collaborative task completion. While these systems have demonstrated the potential of distributed workflows, most rely on static designs—fixed role assignments, linear task flows, and limited interaction protocols.\n\nSuch rigidity poses serious limitations in real-world settings where ambiguity, changing task states, and uneven agent performance are common. For example, a static agent team tasked with analyzing a financial disclosure may fail to revise earlier assumptions when new information is introduced or may overlook domain-specific inconsistencies that require cross-agent validation.\n\nTo address these limitations, we introduce a framework for adaptive coordination in LLM-based multi-agent systems. Our design focuses on three key capabilities. First, dynamic task routing allows agents to reassign subtasks based on current context, confidence, and capacity. Second, bidirectional feedback loops enable downstream agents to provide critiques or revision requests, improving quality and accountability. Third, parallel agent evaluation introduces structured competition: multiple agents attempt the same task independently, and an evaluator selects the most coherent and factual output based on scoring criteria.\n\nWe evaluate this framework through case studies involving long-form document analysis and regulatory question answering. Results show that our approach achieves significant improvements over static pipelines and feedback-only baselines, particularly in accuracy, consistency, and resilience to ambiguity.\n\nThis paper presents the architectural design, coordination strategies, and empirical validation of the framework. In doing so, it contributes toward the development of scalable, robust, and intelligent multi-agent systems capable of operating in dynamic and high-stakes environments.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.447,
      "weak_supervision_score": 0.409,
      "diffusion_reasoning_score": 0.459,
      "distributed_training_score": 0.457,
      "datasets_score": 0.376,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Tangentially Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on coordination mechanisms in multi-agent LLM systems, such as dynamic task routing and bidirectional feedback among agents, but does not involve training models using human-ranked data or reinforcement learning to align with human preferences. There is no mention of RLHF or human feedback in the training process.",
      "weak_supervision_justification": "The paper does not address machine learning approaches for generating training labels from noisy sources. Instead, it discusses adaptive coordination in multi-agent systems, with no reference to training models or programmatic label generation.",
      "diffusion_reasoning_justification": "The paper describes iterative improvements through feedback and evaluation in multi-agent systems but does not involve diffusion models or a clear component for multi-step logical reasoning via iterative refinement of a Chain-of-Thought. There is no adaptation of diffusion processes mentioned.",
      "distributed_training_justification": "The paper incorporates parallelism in agent evaluation and task handling, which involves parallel computing elements, but it is focused on runtime coordination in multi-agent systems rather than algorithms for distributed training, parallel computing for model training, or multi-node machine learning acceleration.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669611",
      "updated_at": "2025-08-11T23:43:05.607170",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17063",
      "title": "Compatibility of Max and Sum Objectives for Committee Selection and\n  $k$-Facility Location",
      "authors": [
        "Yue Han",
        "Elliot Anshelevich"
      ],
      "categories": [
        "cs.DS (Data Structures and Algorithms)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We study a version of the metric facility location problem (or, equivalently,\nvariants of the committee selection problem) in which we must choose $k$\nfacilities in an arbitrary metric space to serve some set of clients $C$. We\nconsider four different objectives, where each client $i\\in C$ attempts to\nminimize either the sum or the maximum of its distance to the chosen\nfacilities, and where the overall objective either considers the sum or the\nmaximum of the individual client costs. Rather than optimizing a single\nobjective at a time, we study how compatible these objectives are with each\nother, and show the existence of solutions which are simultaneously\nclose-to-optimum for any pair of the above objectives. Our results show that\nwhen choosing a set of facilities or a representative committee, it is often\npossible to form a solution which is good for several objectives at the same\ntime, instead of sacrificing one desideratum to achieve another.",
      "published_date": "2025-07-22T22:47:35+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17063v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17063v1",
      "latex_url": "http://arxiv.org/src/2507.17063v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Metric facility location problems {and their variants}\n\nform a classic and well-known area of research. In these problems, we are given a set of possible facility locations $ $ and a set of $n$ clients $ $ in an arbitrary metric space $(M,d)$. {In the variants that we study in our work,} the goal is to choose a set of $k > 1$ facilities $A$ that optimizes some objective, usually making sure that the distance between clients and facilities is not too large.\nSuch problems have been studied extensively in various areas such as operations research (see survey ), approximation algorithms (see book and surveys ),\n\nand mechanism design (see survey ) for decades.\nMetric facility location {and its variants are} general enough to capture many important settings. For example, consider when a city needs to choose a location to build a hospital from a set of potential locations, or a neighborhood wishes to choose multiple locations, each dedicated for one purpose (e.g. school, post office, library, grocery store).\n\nFacility location can also be thought of as a committee selection problem in a spacial voting setting , where each candidate is a potential facility location and each voter is a client; the goal is to choose $k$ candidates to form a committee which is somehow representative of all the voters according to some objective.\n\nThe objective being optimized, i.e., the measure of what makes a committee or a set of facilities be ``good'', is a crucial component of facility location problems {and their many versions}. Many different objectives have been studied in the past (see Related Work). It is not clear, however, what the correct objective is for most settings.\n\nFor example, should we care about utilitarian objectives (minimizing the average client cost), or egalitarian objectives (making sure that all clients are not too unhappy)?\n\nWhat determines if a committee or a set of locations is good from a client's perspective? Optimizing a single chosen objective can often make the cost of a solution in terms of other objectives be extremely bad, even if these other objectives are just as reasonable as the one we decided to optimize. In our work, instead of selecting a single specific objective, we would like to see if we are capable of finding a solution that would be {  simultaneously good for multiple objectives}.\n\nTo see what kind of objectives we will optimize, first consider the individual cost for each client. Here note that instead of assigning each client to one facility, we are interested in modeling settings and applications where clients need to utilize all of the $k$ facilities. For example, as mentioned above, consider the case where a neighborhood wants to build $k$ different facilities to fulfill the needs of the residents in that area, e.g., a shopping center, a post office, and a grocery store. The residents will use all of these, but should they care more about not living too far away from any of these facilities, or about having a short average distance from their home to these facilities? In other words, is the goal to {  minimize the maximum distance} from the client to all facilities ({  max-variant} ), or is the goal to {  minimize the average (or total) distance} from the client to all facilities ({  sum-variant} )? Similar objectives arise in spacial voting settings as well, where distance between a voter and a committee member represents ideological differences.\nDoes a voter care about {  all} of the committee members being ideologically similar to them ({  max-variant}), or about the {  total} ideological difference between them and the committee members ({  sum-variant})? While many other important objectives exist{, such as the classical setting where each client is assigned to their {  closest} facility}, we focus on the max and sum variants in our work, and attempt to optimize {  both} of them simultaneously.\n\nAfter fixing the cost for each client, we also have many choices on how to calculate the overall cost of a committee or a set of facilities. Should we care about keeping the average of the individual costs as low as possible (a utilitarian measure), or should we keep the maximum cost of every individual low (an egalitarian measure)? With all the above individual costs, as well as different ways of combining them into an overall objective, this results in numerous possible objectives, with none necessarily ``better'' than the other. Because of this, we focus on optimizing multiple objectives simultaneously. While many other reasonable objectives exist, in our work we focus on the following four natural objectives:\n\n {definition} Let $A$ be a set of $k$ facilities and $ $ be the set of clients in metric space $(M,d)$. We define the following:\n {itemize}\n    (A) = $ _{i   }  _{a  A} d(i,a)$\n    (A) = $ _{i   }  _{a  A} d(i,a)$\n    (A) = $ _{i   }  _{a  A} d(i,a)$\n    (A) = $ _{i   }  _{a  A} d(i,a)$\n {itemize}\n\n {definition}\n\nOur goal in this paper is to form solutions which are close-to-optimal for more than just a single objective. Formally, we want to form solutions which simultaneously approximate two objectives at the same time.\n\n {definition}\n {Simultaneous Approximation}: Let $c_1$ and $c_2$ be two different objectives, and let $ {A}$ be the set of all possible solutions. Let $O_i$ be an optimum solution for objective $c_i$, i.e., $O_i=   _{A   {A}} c_i(A)$. We then define the approximation ratio of solution $A$ with respect to objective $c_i$ as\n $$ _{c_i}(A) =  {c_i(A)}{c_i(O_i)}   1.$$\n Therefore, by choosing $A$, we would obtain a $( _{c_1}(A),  _{c_2}(A))$ approximation for minimizing the two objectives. If we let $  =  \\{ _{c_1}(A),  _{c_2}(A)\\}$, this means that $A$ is within a factor $ $ for minimizing {  both} of the objectives. Hence, we define $ $ as the simultaneous approximation ratio of $A$ for objectives $c_1$ and $c_2$.\n\n {definition}\n\nIn other words, a solution $A$ which simultaneously approximates two objectives within a factor $ $ is simply a solution which is within a factor $ $ of optimum for each objective, and thus a solution which is an $ $-approximation for each objective individually.\n\nIn our work, we call a pair of objectives {  $ $-compatible} or just {  compatible} if there always exists a solution which simultaneously approximates both objectives within some small constant $ $. This shows that it is possible to (approximately) optimize both objectives at once, and we do not have to sacrifice one objective in order to form a good solution for the other. Our goal, then, is to see if the four objectives listed above are compatible with each other, as well as quantify the upper and lower bounds of the simultaneous approximation ratio for each pair of them.\n\nOur Contributions\n {figure}[t]\n  {center}\n\n  {tikzpicture}\n\n  [draw, fill=blue!10, rounded corners] (SS) at (-3,2) {Sum-Sum};\n  [draw, fill=blue!10, rounded corners] (MS) at (3,2) {Max-Sum};\n  [draw, fill=blue!10, rounded corners] (SM) at (-3,-2) {Sum-Max};\n  [draw, fill=blue!10, rounded corners] (MM) at (3,-2) {Max-Max};\n\n   (SS) -- (MS) node[midway, above=8pt] {$[2.22, 2.29]^*$};\n   (SS) -- (SM) node[midway, left=8pt] {$[ {2},  ( {k},3)]$};\n   (MS) -- (MM) node[midway, right=8pt] {$[ {2},2]$};\n   (SM) -- (MM) node[midway, below=8pt] {$[1+ {2},1+ {2}]$};\n   (SS) -- (MM) node[midway, sloped, pos=0.25,below] {$[1+ {2},3]$};\n   (SM) -- (MS) node[midway, sloped, pos=0.25,below] {$[1+ {2},3]$};\n  {tikzpicture}\n\n  {A summary of our results. Each line connecting two objectives includes a label showing how compatible we prove these two objectives to be. Specifically, if a line has a label $[x,y]$, this means we show there always exists a solution which is a $y$-approximation to the optimum for both of these objectives simultaneously, as well as a lower bound showing no simultaneous approximation better than $x$ is possible. Note that $(*)$ only holds when $k   3$ and is an approximation for the actual bounds, $[(4 +  {7})/3  2.22,1 +  {5/3}  2.29]$.}\n\n  {center}\n {figure}\n\nWe begin by proving that for any pair of our objectives, there always exists a solution which is simultaneously a 3-approximation for both objectives. Moreover, this solution is simply the optimum solution for  . This simple result shows that all of the objectives are 3-compatible. We then proceed to improve the 3-simultaneous approximation ratio for individual pairs of objectives; our results are summarized in Figure .\n\nTo do this, we first show that results from which apply to placing a single facility (i.e., $k=1$), extend without much difficulty to simultaneously approximating the pair $( ,  )$, as well as the pair $( ,  )$. This immediately implies that both pairs of objectives are $(1+ {2})$-compatible.\n\nWe then proceed with further improvements to these bounds, as shown in our main contributions:\n\n {description}\n [  ~and  ] While the above approximation bound of $1+ {2}  2.41$ is tight for   ~and  , we can show better bounds for simultaneously approximating   ~and   ~when choosing more than 2 facilities. In fact, in Section we prove that these objectives are simultaneously approximable to within a factor of $1 +  { {5}{3}}   2.29$, as long as $k  3$. This may be somewhat surprising, as usually things get worse and more complex as the number of facilities $k$ becomes larger. For the questions we are asking, however, the reverse turns out to be true: when $k$ is small, then only a few possible solutions may exist, and they may all be bad for at least one of the two objectives. When $k$ is large, however, many solutions become possible, and we are able to always form a solution which is good for {  both} objectives by carefully stitching together parts of the optimum solutions for each separate objective. The solution we form is not optimum for either, but is a good approximation for both.\n\n [  ~and  ]\nWe then proceed to consider { } and { }, { } and { }, for which the results from can no longer be extended directly; the compatibility of these objectives has not been considered before. While we show both of these pairs of objectives are simultaneously approximable to within a factor of $ ( {k},3)$, we are able to significantly improve this bound for the specific case of   ~and  . We prove that these objectives are 2-compatible, showing that if we care about both of these costs, we are able to obtain a solution that is close-to-optimum for both. This result requires somewhat different techniques, and is presented in Section .\n {description}\n\nRelated Work\n\nMany variants of facility location problems, as well as spacial voting problems, have been studied within many disciplines and are too much to survey here; see surveys . Single-facility location (i.e., $k=1$) has been especially well-studied, while building multiple facilities has received somewhat less attention to due to the added complexity. When choosing the locations for multiple facilities, the most commonly considered cost for each client is the distance from the client to their closest facility .\n\nIn our work, however, we are interested in settings where the client cares about the locations of all $k$ facilities, not just the closest one. These settings arise when the facilities being built are heterogeneous (for example one is a post office, another is a grocery store, etc, so the client has to use them all). In social choice settings, this corresponds to the idea that the voter cares about the entire committee membership, not just the member which is most similar to them.\n\nThus, we instead consider other types of client costs, specifically the maximum distance to a built facility (as in ), and the total (or average) distance to the built facilities (as in ).\n\nAlthough we focus on the above two cost types, there are certainly other client cost functions that have been studied and are interesting.\nFor example, studies $q$-$social$ $cost$, where the cost of choosing a committee of size $k$ for each voter is the distance from them to their $q$'th closest alternative in the committee, and considers the scenario where different\n\nclients could have different cost functions.\nFor the overall objective combining the costs of the clients, max and sum are commonly used (see for example, ), but are rarely considered together under the notion of simultaneous approximation. Note that while computing the optimum solution for some of these objectives is NP-Complete, in this work we are more focused on which objectives are compatible with each other in principle, leaving the question of poly-time simultaneous approximation for future work.\n\nA significant amount of work also exists on optimizing several objectives at once for facility location.\nHowever, as described in , the most commonly studied way to do this is to convert multiple objectives into a single objective. For example, combine two objectives together using a convex combination of them. Moreover, considers a slightly different measure that would achieve a $(4,8)$ approximation for $k$-$Center$ and $k$-$Median$ problems w.r.t the optimal solution of a convex combination of the two objectives.\n\nInstead of such previous approaches, we attempt to approximate a pair of objectives {  simultaneously}, so the solution formed is close-to-optimal for each of them {  at the same time.} This notion of approximation has received far less attention (see the Related Work section in for a detailed discussion). While studied exactly this notion for exactly our setting, they only considered building one facility ($k=1$); in this paper we greatly generalize their results to $k>1$ facilities, as well as use new techniques for multiple facilities to form better approximation bounds. Although not the main focus of their work, considered this notion of approximation as well, and showed that if each client's cost is their distance to their closest facility, the simultaneous approximation ratio even for choosing two facilities for minimax and minisum can be arbitrarily large. While our results establish that many sum and max objectives are simultaneously compatible, the results from show that this is not true if we care about the distance to the closest facility instead.\n\nThere is also other previous work, such as , which either considers the simultaneous approximation of max and sum objectives in our setting, or implies results that would fit under this model. However, such work only considers choosing a single facility, while our results hold for an arbitrary number of facilities $k$.\n\nAnother nice line of literature which studies facility location and committee selection exists in the area of mechanism design (see survey ). Such literature is mostly interested in finding mechanisms such that no clients have the incentive to lie about their location or preferences, i.e., a truthful (or, strategy-proof) mechanism.\n\nIn addition, much of this work is focused on choosing a single facility , or two facilities , and often only on a line, instead of in an arbitrary...",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.278,
      "weak_supervision_score": 0.224,
      "diffusion_reasoning_score": 0.222,
      "distributed_training_score": 0.273,
      "datasets_score": 0.211,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668955",
      "updated_at": "2025-08-11T23:43:05.607097",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17070",
      "title": "Advancing Robustness in Deep Reinforcement Learning with an Ensemble\n  Defense Approach",
      "authors": [
        "Adithya Mohan",
        "Dominik Rößle",
        "Daniel Cremers",
        "Torsten Schön"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Recent advancements in Deep Reinforcement Learning (DRL) have demonstrated\nits applicability across various domains, including robotics, healthcare,\nenergy optimization, and autonomous driving. However, a critical question\nremains: How robust are DRL models when exposed to adversarial attacks? While\nexisting defense mechanisms such as adversarial training and distillation\nenhance the resilience of DRL models, there remains a significant research gap\nregarding the integration of multiple defenses in autonomous driving scenarios\nspecifically. This paper addresses this gap by proposing a novel ensemble-based\ndefense architecture to mitigate adversarial attacks in autonomous driving. Our\nevaluation demonstrates that the proposed architecture significantly enhances\nthe robustness of DRL models. Compared to the baseline under FGSM attacks, our\nensemble method improves the mean reward from 5.87 to 18.38 (over 213%\nincrease) and reduces the mean collision rate from 0.50 to 0.09 (an 82%\ndecrease) in the highway scenario and merge scenario, outperforming all\nstandalone defense strategies.",
      "published_date": "2025-07-22T23:15:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17070v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17070v1",
      "latex_url": "http://arxiv.org/src/2507.17070v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Reinforcement Learning (RL) has emerged as a pivotal methodology in developing autonomous driving systems, enabling vehicles to learn optimal decision-making strategies through interaction with their environment and feedback in the form of rewards . When integrated with deep neural networks, Deep Reinforcement Learning (DRL) empowers agents to navigate complex, high-dimensional state and action spaces, facilitating significant advancements in autonomous driving technology. DRL has been instrumental in various autonomous driving tasks, including path planning, behavior modeling, traffic negotiation, and adaptive cruise control .\n\nBeyond autonomous driving, DRL has also shown success in other domains such as healthcare for personalized treatment strategies , robotics for handling dynamic tasks , energy systems for demand-response optimization , and the financial sector for portfolio management and fraud detection .\n\nDespite its transformative potential, DRL systems face significant challenges in real world applications, particularly in safety-critical domains. One major concern is their vulnerability to adversarial attacks strategically crafted inputs designed to exploit weaknesses in the model and manipulate the agent's behavior. For example, in autonomous driving, adversarial perturbations to sensory inputs such as camera or lidar data can cause an agent to veer off-road or ignore traffic signals . In one study, small image perturbations caused an end-to-end DRL driving agent to make incorrect lane change decisions, potentially leading to collisions . Similarly, physical world attacks like applying stickers to stop signs can trick DRL based perception modules into misclassification .\n\n {figure}\n  \n  [width=260pt]{images/Picture1.png}\n  {Overview of the proposed Ensemble Defense Framework for Deep Reinforcement Learning under adversarial attacks. During inference, the agent receives a perturbed observation ($ {red}{ }$) resulting from adversarial noise Fast Gradient Sign Method (FGSM). This perturbed state is simultaneously passed through three independent defense modules: (i) Random Noise, which introduces additional controlled noise to neutralize adversarial patterns, (ii) Autoencoder, which reconstructs the state using learned nominal representations, and (iii) Principal Component Analysis (PCA), which projects the input onto a lower-dimensional subspace to suppress irrelevant noise. The outputs from these modules are aggregated via simple averaging to form a robust corrected observation, which is then used by the fixed DRL policy to select actions. The framework operates entirely at inference-time and requires no policy retraining, making it suitable for real-world deployment in safety-critical environments such as autonomous driving.}\n\n {figure}\n\nThese vulnerabilities raise serious concerns about the reliability of DRL in real world deployment. In healthcare, for example, adversarial inputs could lead to incorrect treatment recommendations. In autonomous driving, they could result in erratic or unsafe driving behaviors, especially in dense traffic or urban environments. As DRL becomes increasingly integrated into such critical systems, ensuring their trustworthiness is essential to prevent catastrophic outcomes and promote public acceptance .\n\nTo address these security challenges, a wide range of adversarial defense mechanisms have been proposed. These include adversarial training, robust policy optimization, detection algorithms, and input preprocessing techniques . However, many of these defenses are developed as standalone solutions and are effective only against specific types of attacks. As a result, they may fail to generalize across different environments or adversarial strategies.\n\nA promising direction is the use of ensemble defense mechanisms, which combine multiple defenses to exploit their complementary strengths . Ensemble methods are a well established technique in machine learning for improving generalization and robustness . Yet, their application in adversarially robust DRL especially in domains like autonomous driving remains underexplored. By aggregating diverse defenses, ensemble strategies can potentially provide more comprehensive protection against a broader class of attacks, including both white-box and black-box scenarios.\n\nThis paper aims to bridge this gap by proposing an ensemble defense framework for DRL, demonstrated in the Highway-env simulation environment. Our approach evaluates the effectiveness of combining multiple defenses to counteract adversarial attacks and highlights how this strategy can significantly enhance the safety and reliability of DRL systems in autonomous driving settings.\n\nThe main contributions of this paper are as follows:\n\n {itemize}\n   We propose a novel ensemble defense framework for DRL, combining random noise, autoencoder reconstruction, and Principal Component Analysis (PCA) based filtering to improve robustness against adversarial attacks.\n\n   We present a fully inference-time pipeline that operates independently of the policy network, allowing modular deployment without retraining the agent.\n\n   Preliminary results show that the ensemble defense outperforms individual defenses on a safety-critical autonomous driving benchmark (Highway-env), both in terms of reward recovery and collision avoidance.\n {itemize}\n\nTo the best of our knowledge, this is the first study to apply an ensemble defense architecture to adversarial robustness in DRL for autonomous driving. By demonstrating the feasibility and advantages of ensemble defenses, this work lays a foundation for more resilient DRL systems, paving the way for safer deployment in real-world applications where reliability is paramount.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "root.tex",
      "rlhf_score": 0.438,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.405,
      "datasets_score": 0.331,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is an ensemble defense framework for enhancing robustness against adversarial attacks in Deep Reinforcement Learning (DRL) for autonomous driving. It does not involve human feedback, reward models based on human-ranked data, or any alignment with human preferences, focusing instead on technical defenses and simulated rewards.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper addresses adversarial robustness in DRL through an ensemble defense approach, with no discussion of distributed training, parallel computing, multi-node systems, or strategies for partitioning data or computation across processors. It operates at inference time without involving training methodologies.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668964",
      "updated_at": "2025-08-11T23:43:05.607098",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17075",
      "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs",
      "authors": [
        "Yihao Xue",
        "Baharan Mirzasoleiman"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex\nproblems that were previously out of reach. To ensure LLMs do not assist with\nharmful requests, safety alignment fine-tuning is necessary in the\npost-training phase. However, safety alignment fine-tuning has recently been\nshown to significantly degrade reasoning abilities, a phenomenon known as the\n\"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets\neffectively aligns the model for safety without harming its reasoning\ncapabilities. This is because restricting the safety weight updates to a\nlow-rank space minimizes the interference with the reasoning weights. Our\nextensive experiments across four benchmarks covering math, science, and coding\nshow that this approach produces highly safe LLMs -- with safety levels\ncomparable to full-model fine-tuning -- without compromising their reasoning\nabilities. Additionally, we observe that LoRA induces weight updates with\nsmaller overlap with the initial weights compared to full-model fine-tuning. We\nalso explore methods that further reduce such overlap -- via regularization or\nduring weight merging -- and observe some improvement on certain tasks. We hope\nthis result motivates designing approaches that yield more consistent\nimprovements in the reasoning-safety trade-off.",
      "published_date": "2025-07-22T23:25:16+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17075v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17075v1",
      "latex_url": "http://arxiv.org/src/2507.17075v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large language models (LLMs) have made remarkable progress across a wide range of tasks. A major recent breakthrough is the emergence of LLMs with advanced reasoning capabilities, enabling them to solve complex problems previously out of reach. However, recent studies have reported significant safety risks associated with reasoning-capable models  {jiang2025safechain,zhou2025hidden,huang2025safety,li2025smarter}.\nIndeed, reasoning fine-tuning—the process through which LLMs acquire these capabilities—often compromises safety, even when starting from a safety-aligned checkpoint  {jiang2025safechain,zhou2025hidden,zhao2025trade,li2025smarter}.\nFor example,  {jiang2025safechain} show that models distilled for reasoning from DeepSeek-R1 become substantially less safe than their original base models.\n\nThere has been significant effort in the literature to preserve LLMs' safety alignment during instruction fine-tuning.\n\nHowever, these approaches are not applicable to reasoning fine-tuning. First, reasoning fine-tuning datasets are often highly curated  {muennighoff2025s1} and unlikely to contain unsafe content. Thus,\ndata filtering methods such as those proposed by are not applicable. In addition, methods that restrict model updates during fine-tuning  {hsu2024safe, mukhoti2023fine} are ineffective in the reasoning setting, as acquiring reasoning capabilities typically requires longer training and more substantial weight updates compared to instruction fine-tuning. To the best of our knowledge, the current literature does not offer any method for safety alignment of reasoning models.\n\nInstead, the prevailing strategy is to apply a secondary safety alignment phase after reasoning capabilities have been acquired. This phase—often implemented via supervised fine-tuning (SFT) or reinforcement learning (RL)—has become a standard step in modern LLM development.\n\nAlthough safety alignment fine-tuning can significantly improve model safety, it often comes at a steep cost to reasoning performance—a phenomenon referred to as the “Safety Tax”  {huang2025safety}.\nEven incorporating chain-of-thought (CoT) style reasoning into safety fine-tuning datasets  {jiang2025safechain} cannot succeed in fully preserving reasoning abilities  {huang2025safety}.\n\nIn this work, we investigate the algorithmic factors that contribute to this trade-off. Existing evidence suggests that safety-related behavior in LLMs is often governed by a small number of dominant directions—either in activation space, such as steering vectors  {panickssery2023steering} or refusal features  {arditi2024refusal, yu2024robust}, or in weight space. In particular,  {jain2024makes, wei2024assessing} show that safety-critical weights tend to lie in a low-rank subspace.\nIn our analysis, we find that the model undergoes relatively high-rank changes during full-model fine-tuning (see Figure ), which results in Safety Tax. This\nhighlights a key insight: although achieving safety may require modifying weights only along a low-rank subspace, full-model fine-tuning permits arbitrary updates, potentially introducing many unnecessary changes that interfere with reasoning.\n\nOur extensive experiments reveal the surprising effectiveness of a simple recipe for safety alignment of reasoning models: Applying LoRA during SFT using a straightforward direct refusal data set. Despite its simplicity, this approach achieves safety performance on par with full-model alignment, while preserving reasoning capability close to that of the original reasoning-tuned model. This result holds for both 7B and 14B models and is validated across four benchmarks spanning mathematics, science, and code generation. It represents a rare ``one stone, three birds” outcome: strong safety, strong reasoning, and computational efficiency. Moreover, we find that the performance is highly robust to the hyperparameters and configurations of LoRA.\n\nNext, we explore the weight structure imposed by LoRA to understand the differences it introduces. We find that LoRA updates are not only low-rank by design but also exhibit smaller alignment with the weights of the original reasoning model, compared to those from full-model fine-tuning—in most layers. While the reduction in alignment is small, it may suggest that LoRA updates are less disruptive to reasoning-related weights. We further explore whether explicitly reducing such overlap—via regularization or post-hoc weight merging—can improve safety or reasoning capabilities. We observe mixed results, with one post-hoc method achieving a slightly better reasoning–safety trade-off on the  {AIME} and  {GPQA} datasets. More effort is still needed to develop approaches that lead to more consistent improvements across tasks, which we consider a valuable direction for future work.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "intro.tex",
      "rlhf_score": 0.528,
      "weak_supervision_score": 0.412,
      "diffusion_reasoning_score": 0.502,
      "distributed_training_score": 0.391,
      "datasets_score": 0.313,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is using LoRA for supervised fine-tuning (SFT) on refusal datasets to achieve safety alignment, without involving reinforcement learning, a reward model, or human-ranked data. While it mentions RL as a possible safety method, it is not implemented or central to the work.",
      "weak_supervision_justification": "The paper relies on standard refusal datasets for SFT, with no indication of programmatically generating noisy or imprecise labels from high-level sources; it focuses on fine-tuning techniques rather than weak supervision methods.",
      "diffusion_reasoning_justification": "The paper addresses LoRA-based fine-tuning for safety without any components involving diffusion models, iterative refinement processes, or multi-step logical reasoning via diffusion; it centers on preserving reasoning abilities during alignment.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667888",
      "updated_at": "2025-08-11T23:43:05.606881",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17079",
      "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey",
      "authors": [
        "Md Meftahul Ferdaus",
        "Kendall N. Niles",
        "Joe Tom",
        "Mahdi Abdelguerfi",
        "Elias Ioup"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Few-shot learning (FSL) enables object detection models to recognize novel\nclasses given only a few annotated examples, thereby reducing expensive manual\ndata labeling. This survey examines recent FSL advances for video and 3D object\ndetection. For video, FSL is especially valuable since annotating objects\nacross frames is more laborious than for static images. By propagating\ninformation across frames, techniques like tube proposals and temporal matching\nnetworks can detect new classes from a couple examples, efficiently leveraging\nspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces\nchallenges like sparsity and lack of texture. Solutions integrate FSL with\nspecialized point cloud networks and losses tailored for class imbalance.\nFew-shot 3D detection enables practical autonomous driving deployment by\nminimizing costly 3D annotation needs. Core issues in both domains include\nbalancing generalization and overfitting, integrating prototype matching, and\nhandling data modality properties. In summary, FSL shows promise for reducing\nannotation requirements and enabling real-world video, 3D, and other\napplications by efficiently leveraging information across feature, temporal,\nand data modalities. By comprehensively surveying recent advancements, this\npaper illuminates FSL's potential to minimize supervision needs and enable\ndeployment across video, 3D, and other real-world applications.",
      "published_date": "2025-07-22T23:37:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17079v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17079v1",
      "latex_url": "http://arxiv.org/src/2507.17079v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Object detection is a fundamental task in computer vision that involves locating and classifying objects belonging to predefined categories in images or video frames . Over the years, deep convolutional neural networks (CNNs) have revolutionized object detection with remarkable accuracy . However, the success of these models heavily relies on large annotated datasets for training, which are often costly and time-consuming to acquire. The data scarcity problem poses a significant challenge to the development of robust object detectors that can generalize well to new, unseen objects and domains .\n\nTo address the limitations of data scarcity, considerable research has been devoted to exploring few-shot and zero-shot learning techniques in the field of object detection . Few-shot learning (FSL), in particular, seeks to recognize novel object categories with only a few training examples per class, typically ranging from 1 to 5 . The aim is to minimize the prohibitive annotation effort and enable the scalable deployment of object detectors in real-world applications . By leveraging knowledge transfer and efficient adaptation, FSL methods strive to extract transferable knowledge from a set of base classes with abundant labeled data, enabling generalization to novel classes with limited available examples .\n\nEffective FSL algorithms introduce strong inductive biases into models, allowing for rapid adaptation using the limited annotations associated with novel classes. Meta-learning algorithms , which train models to quickly adapt to new tasks and metrics with few examples, have shown promise in this regard . Transfer learning from related domains and data augmentation techniques are also commonly employed to enhance FSL performance . Additionally, distance metric learning is utilized to learn embeddings that reflect semantic class relationships, aiding in effective few-shot object detection .\n\nWhile few-shot classification has been extensively explored, few-shot object detection presents unique challenges . In addition to recognizing object classes with limited data, few-shot object detection requires accurate object localization. This localization task becomes particularly challenging when only a small number of examples are available . By overcoming these challenges, FSL techniques have the potential to revolutionize the field of object detection . They can enable accurate and efficient detection of novel objects with minimal annotated data, enhancing the scalability and real-world applicability of object detectors. In this survey, we comprehensively investigate recent advancements in FSL techniques applied to video and 3D object detection, examining their strengths, limitations, and potential for future development.\n\nMotivation\nThe field of object detection has witnessed significant advancements with the rise of deep learning and convolutional neural networks (CNNs). However, these advancements primarily focus on 2D image-based object detection, which poses limitations in real-world scenarios where objects exist in three-dimensional space and exhibit temporal dynamics . Hence, there is a pressing need to explore and understand the progress made in video and 3D object detection. However, existing surveys on FSL have not focused specifically on video or 3D object detection .\n\nVideo object detection is of paramount importance in various domains such as surveillance, autonomous driving, and action recognition. However, the task of detecting objects in videos presents unique challenges compared to static image-based detection. These challenges arise from the need to cope with motion blur, occlusions, and object interactions across frames . By conducting a survey specifically dedicated to video object detection, we aim to provide a comprehensive overview of the latest methodologies, techniques, and benchmarks, thus shedding light on the progress made in this critical area and identifying potential future research directions.\n\nOn the other hand, 3D object detection, especially in the context of autonomous driving, is crucial for enabling safe and reliable perception systems. Traditional object detection methods primarily rely on 2D sensors such as cameras, which may not provide accurate depth information and struggle with challenging lighting and weather conditions. Integrating LiDAR (Light Detection and Ranging) sensors with cameras can significantly enhance the detection accuracy by providing precise depth information. However, 3D object detection remains a challenging task due to the sparsity of LiDAR point clouds, object occlusions, and the need to handle large-scale 3D data . Our survey on 3D object detection aims to provide an in-depth analysis of the state-of-the-art techniques, highlighting their strengths, limitations, and novel approaches that address these challenges.\n\nBy conducting a survey on both video and 3D object detection, we aim to bridge the gap and provide a comprehensive understanding of the advancements and challenges in these emerging areas. By exploring the latest techniques, model architectures, and evaluation benchmarks, we can assess the progress made, identify gaps in current approaches, and propose potential research directions for future work. This survey serves as a valuable resource for researchers, practitioners, and developers working on video and 3D object detection, paving the way for further advancements in these domains.\n\nOrganization of the Paper\nThis paper is organized into seven sections as follows: Section 1 provides an introduction that motivates the need for a comprehensive survey on FSL techniques for video and 3D object detection. It highlights the unique challenges posed by these domains and outlines the structure of the paper. Section 2 establishes the theoretical foundations of few-shot learning by reviewing key concepts, problem formulations, and common strategies. It focuses on principles like the support set, episodic training, meta-learning, metric-based approaches, data augmentation, and regularization. Section 3 explores the fundamentals of object detection, including two-stage and one-stage detector paradigms. It analyzes influential architectures like Faster R-CNN, YOLO, and SSD, and examines video and 3D detection approaches. Section 4 provides an in-depth analysis of state-of-the-art few-shot techniques tailored for video object detection. It discusses specialized model architectures, losses, and training methodologies to overcome video-specific challenges. Section 5 investigates few-shot learning strategies for 3D object detection using modalities like LiDAR. It reviews model designs, losses, and training procedures enabling effective few-shot detection on sparse 3D data. Section 6 identifies open challenges and promising research directions to advance few-shot video and 3D object detection. It proposes solutions to limitations in existing approaches. Section 7 presents concluding remarks and summarizes the key insights. Additional architectural diagrams, detailed comparisons, and secondary discussions are provided in the supplementary material. To provide an overview of the paper structure, a visual taxonomy outlining the relationships between the key sections and topics is presented in Figure . This diagram aims to enhance comprehension of the survey scope and content flow for the reader.\n\n {figure*}[htpb!]\n  \n  [scale=0.2]{images/taxonomy_vdo_3d_whole3.png}\n  {Visual taxonomy illustrating comprehensive structural organization of survey content}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main_paper_few_vdo_3d.tex",
      "rlhf_score": 0.354,
      "weak_supervision_score": 0.38,
      "diffusion_reasoning_score": 0.343,
      "distributed_training_score": 0.377,
      "datasets_score": 0.349,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668404",
      "updated_at": "2025-08-11T23:43:05.606998",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17080",
      "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and\n  LLM-Augmented CLIP Embeddings",
      "authors": [
        "Ramin Giahi",
        "Kehui Yao",
        "Sriram Kollipara",
        "Kai Zhao",
        "Vahid Mirjalili",
        "Jianpeng Xu",
        "Topojoy Biswas",
        "Evren Korpeoglu",
        "Kannan Achan"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.",
      "published_date": "2025-07-22T23:45:43+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17080v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17080v1",
      "latex_url": "http://arxiv.org/src/2507.17080v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "E-commerce platforms have revolutionized the way consumers interact with products, offering extensive catalogs that cater to diverse preferences. As the number of products continues to grow exponentially, delivering highly relevant personalized recommendations has become an increasingly complex challenge. Consumers often rely on multimodal interactions—searching with a combination of textual queries and images—to find the products they desire. Therefore, improving multimodal representation learning is critical for enhancing search accuracy, recommendation quality, and overall user experience in e-commerce .\n\n {figure*}[t]\n  \n  [width=0.90 ]{samples/Figure1.png}\n  {Illustration of: (a) visual recommendation improvement achieved by our proposed model, VL-CLIP and (b) visual search improvement using VL-CLIP.}\n  {}\n\n {figure*}\n\nRecent advances in vision-language models have significantly improved cross-modal retrieval. CLIP , in particular, has demonstrated strong zero-shot capabilities by aligning images and text in a shared embedding space. However, despite its success, CLIP exhibits several limitations when applied to e-commerce scenarios. First, CLIP processes images globally, meaning that it often fails to capture fine-grained product attributes that are crucial to distinguish visually similar but semantically different items. For example, two handbags might appear nearly identical in a global embedding space, even if one has a unique texture or clasp design that differentiates it. This weak object-level alignment leads to suboptimal retrieval performance, especially in a large e-commerce platform.\n\nAnother major challenge is the ambiguity of textual representations. Product descriptions in e-commerce catalogs vary widely in quality and consistency. Some descriptions are too verbose, containing extraneous information, while others are sparse, lacking essential details. CLIP’s text encoder struggles with such inconsistencies, especially with long-text descriptions, leading to poor semantic alignment between textual and visual representations. Without structured and enriched textual inputs, CLIP may misinterpret product intent, reducing the accuracy of multimodal retrieval.\n\nMoreover, existing multimodal models are typically trained on general-purpose datasets, such as LAION-400M , which contain a broad spectrum of image-text pairs. While this training paradigm enables broad zero-shot learning, it also introduces a significant domain mismatch when applied to e-commerce. Product images often contain controlled backgrounds, well-lit professional shots, or lifestyle depictions, all of which differ from the diverse, noisy images seen in open-domain datasets. Consequently, pre-trained models fail to generalize effectively to e-commerce-specific data, necessitating domain adaptation strategies .\n\nTo overcome these limitations, we propose a novel framework that enhances CLIP embeddings through two key innovations: (1) the integration of Visual Grounding for fine-grained object localization and (2) the use of a Large Language Model (LLM) to refine textual embeddings. Visual Grounding enables precise localization of key product attributes within an image, ensuring that CLIP’s vision encoder focuses on the most relevant regions. By incorporating Visual Grounding, we improve object-level alignment, leading to more discriminative visual embeddings.\n\nOn the textual side, we employ an LLM agent to enrich product descriptions by generating structured, semantically meaningful text representations. Given raw metadata, the LLM refines descriptions, removes noise, and injects domain-specific knowledge, ultimately improving the quality of text embeddings. This augmentation mitigates CLIP’s struggle with ambiguous text and ensures that the image-text alignment is robust, accurate, and context-aware.\n\nFigure~ illustrates the effectiveness of our approach in both visual and textual recommendation. In Figure~ (a), the traditional recommendation system suggests products based on broad categorical similarity, often missing fine-grained visual coherence. In contrast, our visual recommendation system, powered by Visual Grounding and enhanced CLIP embeddings, retrieves visually and semantically aligned items, improving recommendation relevance. Similarly, Figure~ (b) highlights how our model enhances e-commerce search. Traditional keyword-based search may yield inconsistent results when dealing with complex queries such as “area rug with pet pic” or “damask silk bed sheet.” Our model effectively aligns textual queries with the most relevant visual content, ensuring that search results are not only textually but also visually accurate. These improvements validate our approach’s superiority in capturing fine-grained details and providing semantically meaningful retrievals, ultimately enhancing the user experience.\n\nThe contributions of our paper are threefold: First, we introduce a novel multimodal pipeline that integrates Visual Grounding and LLM-enhanced embeddings to improve fine-grained alignment in e-commerce applications; Second, we develop a scalable retrieval and ranking system that efficiently handles large-scale product catalogs; Third, we validate our approach through extensive experiments over tens of millions of items in Walmart.com, demonstrating significant improvements in retrieval accuracy, recommendation quality, and overall system performance compared to existing state-of-the-art multimodal models.\n\nThe remainder of this paper is organized as follows. Section 2 discusses related work in multimodal learning, vision-language models, and e-commerce recommendation systems. Section 3 describes our proposed framework, detailing the enhancements to both image and text representations. Section 4 presents experimental results, including comparative evaluations and ablation studies. Section 5 concludes the paper.\n\n 0\n\nWith millions of products that span various categories, e-commerce platforms face the challenge of providing personalized recommendations that match user intent. Accurate image-query alignment plays a crucial role in enabling these recommendations. Traditional approaches, while effective for small-scale datasets, struggle to handle the diversity and scale of industrial catalogs. Furthermore, the introduction of multimodal deep learning models has opened new avenues to address this challenge.\n\nProblem Statement\n\nThe Home and Apparel department at Walmart encompasses a vast range of products, requiring robust methods to ensure high-quality recommendations. Existing methods often fall short due to limitations in handling large-scale image-query pairs, lack of domain-specific fine-tuning, and inefficient pipelines for real-world deployment.\n\nMotivation\n\nThe ability to deploy an efficient and scalable recommendation pipeline for Walmart’s e-commerce platform has far-reaching implications, both for operational efficiency and customer satisfaction. By fine-tuning a state-of-the-art vision-language model and implementing a robust, scalable pipeline, we address these challenges head-on.\n\nContributions\n\nThis paper presents:\n {itemize}\n\n  A fine-tuned CLIP model tailored for the Home and Apparel departments.\n  LLM-generated queries using metadata to enhance alignment.\n  A scalable pipeline for deduplication, embedding generation, retrieval, and ranking.\n  Real-world implementation at Walmart, demonstrating impact on a 20-million-item catalog.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.406,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.404,
      "distributed_training_score": 0.343,
      "datasets_score": 0.34,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's main contribution is enhancing CLIP embeddings for e-commerce recommendations using Visual Grounding and LLMs, with no mention of reinforcement learning, human feedback, reward models, or fine-tuning based on human-ranked data.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on multimodal learning and embedding enhancements for recommendations, without any reference to diffusion models, iterative refinement processes, or multi-step logical reasoning as described in the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667736",
      "updated_at": "2025-08-11T23:43:05.606846",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17083",
      "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D\n  Multimodal Occupancy Prediction",
      "authors": [
        "Zaipeng Duan",
        "Chenxu Dang",
        "Xuzhong Hu",
        "Pei An",
        "Junfeng Ding",
        "Jie Zhan",
        "Yunbiao Xu",
        "Jie Ma"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Multimodal 3D occupancy prediction has garnered significant attention for its\npotential in autonomous driving. However, most existing approaches are\nsingle-modality: camera-based methods lack depth information, while LiDAR-based\nmethods struggle with occlusions. Current lightweight methods primarily rely on\nthe Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth\nestimation and fails to fully exploit the geometric and semantic information of\n3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction\nnetwork called SDG-OCC, which incorporates a joint semantic and depth-guided\nview transformation coupled with a fusion-to-occupancy-driven active\ndistillation. The enhanced view transformation constructs accurate depth\ndistributions by integrating pixel semantics and co-point depth through\ndiffusion and bilinear discretization. The fusion-to-occupancy-driven active\ndistillation extracts rich semantic information from multimodal data and\nselectively transfers knowledge to image features based on LiDAR-identified\nregions. Finally, for optimal performance, we introduce SDG-Fusion, which uses\nfusion alone, and SDG-KL, which integrates both fusion and distillation for\nfaster inference. Our method achieves state-of-the-art (SOTA) performance with\nreal-time processing on the Occ3D-nuScenes dataset and shows comparable\nperformance on the more challenging SurroundOcc-nuScenes dataset, demonstrating\nits effectiveness and robustness. The code will be released at\nhttps://github.com/DzpLab/SDGOCC.",
      "published_date": "2025-07-22T23:49:40+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17083v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17083v1",
      "latex_url": "http://arxiv.org/src/2507.17083v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Accurate 3D perception of the surrounding environment forms the cornerstone of modern autonomous driving systems and robotics, ensuring efficient planning and safe control . In recent years, advancements in 3D object detection and semantic segmentation have significantly propelled the field of 3D perception. However, object detection relies on strict bounding boxes, making it difficult to recognize arbitrary shapes or unknown objects, while semantic segmentation struggles with fine-grained classification in complex scenes, especially under occlusion and overlap. In this context, 3D semantic occupancy prediction offers a more comprehensive approach to environment modeling. It simultaneously estimates the geometric structure and semantic categories of scene voxels, assigns labels to each 3D voxel, and provides a more complete perception, showing stronger robustness to arbitrary shapes and dynamic occlusions.\n\n {figure}[t]\n  \n\n  [width=1.0 ,height=!]{FPS.pdf}\n  {-14pt}\n  {Comparisons of the mIoU and inference speed (FPS) of various 3D occupancy prediction methods on the Occ3D-nuScenes validation set. SDG-OCC achieves higher accuracy and real-time inference speed.}\n\n  {-12pt}\n {figure}\nLeveraging the complementary strengths of LiDAR and camera data is crucial for various 3D perception tasks. However, due to the heterogeneity between modalities, fusing LiDAR and camera data for 3D occupancy prediction remains challenging. Specifically, cameras provide rich semantic information but lack precise depth details, while LiDAR offers accurate depth information but only captures sparse data, potentially missing comprehensive scene details such as occluded objects. Existing methods often suffer from significant computational burdens (see  {fig:fps}), with some approaches attempting to leverage the LSS pipeline for real-time performance. Although LSS simulates the uncertainty of each pixel's depth through depth distribution (with depth intervals typically set to 0.5m), its sparse BEV representation allows only 50% of the grids to receive valid image features (see  {fig:bev} (a)). While increasing the depth interval can improve depth estimation accuracy to mitigate sparsity, it significantly increases computational demands.Additionally, while LiDAR can provide valuable geometric priors, fusion-based methods that process both point clouds and images simultaneously impose heavy computational burdens, thereby increasing the strain on real-time applications.\n {figure}[t]\n  \n  {subfigure}{0.45 }\n  \n  [width= ]{bev_lss.pdf}\n  {Initial BEV features of LSS}\n  {subfigure}\n\n  {2pt}\n  {subfigure}{0.45 }\n  \n  [width= ]{bev_our.pdf}\n  {Our initial BEV features}\n  {subfigure}\n  {0.8em}\n  {subfigure}{0.45 }\n  \n  [width= ]{bev_gt1.pdf}\n  {Ground truth of BEV}\n  {subfigure}\n\n  {1.8pt}\n  {subfigure}{0.45 }\n  \n  [width= ]{bev_gt.pdf}\n  {Occupancy grid of BEV}\n  {subfigure}\n {-10pt}\n {(a) BEV feature map of LSS with a shape of 200×200. We can observe that LSS has an extremely low utilization rate for BEV space. (b) The corresponding BEV features in SDG-OCC. Using depth and semantic information, the 2D-to-3D view transformation achieves efficient occupancy and utilization of the BEV features. (c) The corresponding BEV features in Ground Truth. (d) The corresponding BEV features in the occupancy grid.}\n {-12pt}\n\n {figure}\n\nTo address these issues, we propose a multimodal 3D semantic occupancy prediction framework, named SDG-OCC, which aims to achieve higher accuracy and competitive inference speed by fusing LiDAR information in the BEV perspective. In this framework, we introduce a semantic and depth-guided view transformation to replace normal BEV feature generation. Specifically, after extracting features from camera data and obtaining semantic segmentation masks and depth distributions through a multi-task head, we use the semantic masks and depth maps provided by LiDAR to construct virtual points via local diffusion and bilinear discretization. Combined with the depth distribution, these points are then projected into the BEV space. The comparison between LSS and our generated BEV features is shown in  {fig:bev}. The SDG view transformation significantly refines depth estimation accuracy and reduces redundant virtual point seeds, improving both the speed and accuracy of semantic occupancy.\n\nSecondly, we introduce a fusion-to-occupancy-driven active distillation module. We first fuse LiDAR and camera features in the BEV space and then unidirectionally selectively transferred multimodal knowledge to image features based on LiDAR-identified regions. Our proposed SDG-Fusion, which includes only fusion, achieved SOTA performance on the Occ3D-nuscenes and SurroundOcc-nuScenes validation dataset. In comparison, SDG-KL, which combines fusion and unidirectional distillation, achieves real-time speed with a slight performance penalty.\n\nOur contributions can be summarized as follows:\n {itemize}\n  We introduce a multimodal 3D semantic occupancy prediction framework, termed SDG-OCC, aimed at achieving higher accuracy and competitive inference speed by fusing LiDAR information in the BEV perspective.\n\n  We propose a novel view transformation method that leverages the geometric and semantic information of point clouds to guide the 2D-3D view transformation. This significantly enhances the accuracy of depth estimation and improves both the speed and accuracy of semantic occupancy.\n\n  We propose a fusion-to-occupancy-driven active distillation module that integrates multimodal features and selectively transfers multimodal knowledge to image features based on LiDAR-identified regions. Building on this, we present SDG-Fusion for high performance and SDG-KL for faster inference.\n\n  Our method achieves SOTA performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes validation dataset, demonstrating the effectiveness of our approach.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.338,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.361,
      "datasets_score": 0.339,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on 3D multimodal occupancy prediction for autonomous driving, using a diffusion process specifically for integrating pixel semantics and depth in view transformation (e.g., via local diffusion and bilinear discretization for depth distributions). However, it does not involve adapting diffusion models for multi-step logical reasoning, treating a 'Chain-of-Thought' as an entity, or solving complex logical tasks. The diffusion here is applied to perceptual data processing, not reasoning, so it does not align with the topic's definition.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667613",
      "updated_at": "2025-08-11T23:43:05.606819",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17774",
      "title": "Human-AI Co-Creation: A Framework for Collaborative Design in\n  Intelligent Systems",
      "authors": [
        "Zhangqi Liu"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision.",
      "published_date": "2025-07-22T04:29:33+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17774v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17774v1",
      "latex_url": "http://arxiv.org/src/2507.17774v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.484,
      "weak_supervision_score": 0.383,
      "diffusion_reasoning_score": 0.462,
      "distributed_training_score": 0.353,
      "datasets_score": 0.428,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Tangentially Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "The paper focuses on human-AI co-creation frameworks using existing models like GPT-4 and Stable Diffusion for collaborative design, involving iterative feedback. However, it does not describe training or fine-tuning models with human-ranked data via a reward model and reinforcement learning, which is the core of RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper uses multimodal diffusion models like Stable Diffusion for iterative creative processes in design, such as proposal and revision. While this involves iteration, it does not adapt diffusion for multi-step logical reasoning or holistic correction of a chain-of-thought, focusing instead on generative collaboration rather than logical tasks.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper presents a framework for human-AI co-creation in design and does not involve creating, analyzing, benchmarking, or evaluating datasets for AI applications.",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668972",
      "updated_at": "2025-08-11T23:43:05.607099",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17775",
      "title": "Comparison of Optimised Geometric Deep Learning Architectures, over\n  Varying Toxicological Assay Data Environments",
      "authors": [
        "Alexander D. Kalian",
        "Lennart Otte",
        "Jaewook Lee",
        "Emilio Benfenati",
        "Jean-Lou C. M. Dorne",
        "Claire Potter",
        "Olivia J. Osborne",
        "Miao Guo",
        "Christer Hogstrand"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Geometric deep learning is an emerging technique in Artificial Intelligence\n(AI) driven cheminformatics, however the unique implications of different Graph\nNeural Network (GNN) architectures are poorly explored, for this space. This\nstudy compared performances of Graph Convolutional Networks (GCNs), Graph\nAttention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7\ndifferent toxicological assay datasets of varying data abundance and endpoint,\nto perform binary classification of assay activation. Following pre-processing\nof molecular graphs, enforcement of class-balance and stratification of all\ndatasets across 5 folds, Bayesian optimisations were carried out, for each GNN\napplied to each assay dataset (resulting in 21 unique Bayesian optimisations).\nOptimised GNNs performed at Area Under the Curve (AUC) scores ranging from\n0.728-0.849 (averaged across all folds), naturally varying between specific\nassays and GNNs. GINs were found to consistently outperform GCNs and GATs, for\nthe top 5 of 7 most data-abundant toxicological assays. GATs however\nsignificantly outperformed over the remaining 2 most data-scarce assays. This\nindicates that GINs are a more optimal architecture for data-abundant\nenvironments, whereas GATs are a more optimal architecture for data-scarce\nenvironments. Subsequent analysis of the explored higher-dimensional\nhyperparameter spaces, as well as optimised hyperparameter states, found that\nGCNs and GATs reached measurably closer optimised states with each other,\ncompared to GINs, further indicating the unique nature of GINs as a GNN\nalgorithm.",
      "published_date": "2025-07-22T11:38:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17775v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17775v1",
      "latex_url": "http://arxiv.org/src/2507.17775v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.318,
      "weak_supervision_score": 0.327,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.337,
      "datasets_score": 0.36,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669622",
      "updated_at": "2025-08-11T23:43:05.607171",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17776",
      "title": "Axiomatizing Rumsfeld Ignorance",
      "authors": [
        "Jie Fan"
      ],
      "categories": [
        "math.LO (Logic)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In a recent paper, Kit Fine presents some striking results concerning the\nlogical properties of (first-order) ignorance, second-order ignorance and\nRumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of\nignorance, which makes some existing results and the axiomatization problem\ntrivial. A main reason is that the accessibility relations for the implicit\nknowledge operator contained in the packaged operators of ignorance and\nRumsfeld ignorance are the same. In this work, we assume the two accessibility\nrelations to be different so that one of them is an arbitrary subset of the\nother. This will avoid the definability issue and retain most of the previous\nvalidities. The main results are axiomatizations over various proper bi-frame\nclasses. Finally we apply our framework to analyze Fine's results.",
      "published_date": "2025-07-22T14:25:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17776v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17776v1",
      "latex_url": "http://arxiv.org/src/2507.17776v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Since Socrates~,\nignorance has played a pivotal role in many areas, for instance, not only in epistemology, but also in ethics, philosophy of law, social philosophy, philosophy of science and even science itself~. {For instance, researchers focus on the nature/definition of ignorance, the relation between ignorance and virtue, and so on. Moreover, it is argued in~ that ignorance is the engine of science.} Sometimes a reference to ignorance has featured in epistemological discussions --- as in the title of Unger~ --- but only as a counterpoint to knowledge, when emphasizing how little knowledge of this or that kind is possible. Recently, however, there has been an explosion of interest in ignorance in its own right, with attempts to say what exactly ignorance consists in and what its fundamental logical properties are. Instead of simply taking for granted the standard view (SV), according to which ignorance of a proposition is merely the absence of knowledge of the proposition~, we now also have what has been called the new view (NV),\nas well as what has been termed the logical view (LV) to contend with. {The terminology ‘the standard view’ is introduced in~, ‘the new view’ is from~, whereas the term\n‘the logical view’ comes from~.} According to NV, ignorance is instead simply the absence of true belief~. According to LV, ignorance with respect to a proposition is the absence of knowledge of that proposition and of its negation~.\n\nIn addition to the competing views about what ignorance consists in, various different forms of ignorance have attracted attention in the literature, such as pluralistic ignorance~, circumscriptive ignorance~, chronological ignorance~, group ignorance~, factive ignorance~, radical ignorance~, relative ignorance~, disjunctive ignorance~, severe ignorance~, and so on. In a recent paper~, Kit Fine presents some striking results concerning the logical properties of ignorance, for the purpose of presenting which, Fine introduces the following terminology, all defined in terms of the primitive knowledge operator $ $ of epistemic logic:\n\n {itemize}\n  [(i)] One is {  ignorant that} $ $, if one does not know that $ $ ($   $)\n  [(ii)] It is {  (epistemically) possible} that $ $, if one is ignorant that not-$ $ ($  =_{df}    $)\n  [(iii)] One is {  ignorant of the fact that} $ $, if $ $ is the case and one is ignorant that $ $ ($     $)\n  [(iv)] One is {  (first-order) ignorant whether $ $}, if one is both ignorant that $ $ and ignorant that not-$ $ ($  =_{df}        $)\n  [(v)] One is {  Rumsfeld ignorant of} $ $, if one is ignorant of the fact that one is ignorant whether $ $ ($       $) {As correctly observed by Fine in~, Rumsfeld ignorance is a form of Fitchean ignorance; in other words, it is of the form $      $, a notion called `unknown truths' or `accident' in the literature, see e.g.~. This helps us find the suitable canonical relation for Rumsfeld ignorance in Def.~.}\n  [(vi)] One is {  second-order ignorant whether} $ $, if one is ignorant whether one is ignorant whether $ $ ($   $)\n {itemize}\n\nAmong other things, Fine establishes the following results within the\ncontext of the modal system ${  S4}$, where (4) is surprising and interesting, with $ $ written as $K$ in the relevant pages (pp.~4032--4033), though in the later `formal appendix', Fine uses the $ $ notation (with $ $ for the above $ $): {Of the following, only (1) and (4) are proved, in~ [Lemma~3, Thm.~4]{Fine:2018}, the remained being justified by semi-formal arguments. That the result (4) is surprising and interesting, is because it disobeys our intuition. As Fine shows via an argument in~ [p.~4033]{Fine:2018}, in principle, one could know that those propositions, which one does not know, do not appear on the list of their knowledge; however, (4) tells us that this is impossible.}\n\n {enumerate}\n  [(1)] Second-order ignorance implies first-order ignorance.\n\n  [(2)] Second-order ignorance implies Rumsfeld ignorance.\n  [(2$^ $)] Rumsfeld ignorance implies second-order ignorance.\n  [(3)] One does not know that one is Rumsfeld ignorant.\n\n  [(4)] One does not know that one is second-order ignorant.\n {enumerate}\n\nNow, as indicated in our opening paragraphs, the relation between knowledge and ignorance has recently become somewhat problematic, making it resonable to approach the logic of ignorance in its own right, rather than by defining ignorance --- first-order, second-order, or of the mixed `Rumsfeldian' variety --- in terms of a primitive $K$ operator, and no such operator is included here for that reason. Following Kit Fine, in the semantic treatment of ignorance (specifically first-order ignorance) and Rumsfeld ignorance below (Definition~, $ $ and $ ^R$ for these notions), we will use a binary accessibility relation $R$ to interpret $ $ and $ ^R$ exactly as if these had been defined in terms of a $ $ operator by the definitions $  :=        $ and $ ^R :=       $.\n\nHowever, as we shall show below, Rumsfeld ignorance is definable in terms of (first-order) ignorance over the class of all frames. This makes some of Kit Fine's results mentioned above and the problem of axiomatizing Rumsfeld ignorance (and (first-order) ignorance) trivial, which may be undesirable.\n\nWe can fix this issue by sticking with the ignorance-related primitives to emphasize neutrality w.r.t. the intended interpretation of this $ $ operator. {By ``sticking with the ignorance-related primitives'', we mean using $ $ and $ ^R$ instead of $ $ as primitives. By emphasizing neutrality with respect to the interpretation of $ $ operator, we would like to indicate that $ $ can be given different interpretations in different contexts, just as the $ $ operators implicitly contained in the packaged operators $ $ and $ ^R$.} For example, an adherent of SV may want to think of this as representing knowledge, and an adherent of NV may prefer to think of $ $ as representing true belief, while someone else again may want to think of this as representing just belief. This indicates that one may give different interpretations to the implicit $ $ operator in the packaged operator $ $ on one hand, and in the packaged operator $ ^R$ on the other hand, which semantically corresponds to different accessibility relations for the implicit $ $ operator in $ $ and $ ^R$, denoted $R$ and $R^{ }$ below, respectively. This is what we will do. Moreover, we will assume that either $R  R^{ }$ or $R^{ }  R$, which may retain some validities that hold in the case of $R=R^{ }$. We will show that in either case, the previous definability fails; in fact, Rumfeld ignorance cannot be defined with ignorance. We will then focus on the restriction of $R  R^{ }$ (the reason of focusing on this restriction will be explained at the end of Sec.~) and axiomatize the logic of ignorance and Rumsfeld ignorance over various classes of frames with this restriction.\n\nThe remainder of the paper is structured as follows. Sec.~ introduces the syntax and semantics of the logic of (first-order) ignorance and Rumsfeld ignorance, demonstrates the definability of Rumsfeld ignorance in terms of ignorance, and make some assumptions about the relationship between the accessibility relations for the implicit $ $ contained in $ $ and $ ^R$, which makes the previous definability fail. Sec.~ proposes the minimal logic of ignorance and Rumsfeld ignorance and shows its soundness and strong completeness, where the completeness is proved via a nontrivial inductive method. Sec.~ axiomatizes the logic over various special frame classes. Sec.~ applies our framework to analyze Fine's results in~. We conclude with some remarks in Sec.~.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "Axiomatizing_Rumsfeld_Ignorance.tex",
      "rlhf_score": 0.243,
      "weak_supervision_score": 0.251,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.156,
      "datasets_score": 0.175,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668983",
      "updated_at": "2025-08-11T23:43:05.607100",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17777",
      "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid\n  Mechanics",
      "authors": [
        "Theofanis Aravanis",
        "Grigorios Chrimatopoulos",
        "Mohammad Ferdows",
        "Michalis Xenos",
        "Efstratios Em Tzirtzilakis"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial.",
      "published_date": "2025-07-22T15:16:20+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17777v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17777v1",
      "latex_url": "http://arxiv.org/src/2507.17777v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Fluid mechanics is essential to many fields, from engineering and environmental science to medical research. However, the complexity of fluid behaviour, especially in turbulent or multiphase systems, makes it challenging to model and understand. Traditional methods, including {  Computational Fluid Dynamics} (CFD), have been effective but come with high computational costs and often lack of interpretability. Similarly, {  Machine-Learning} (ML) approaches such as Artificial Neural Networks exhibit remarkable accuracy but generally act as ``black boxes'', making it challenging to interpret the physical reasoning behind predictions. Nevertheless, in applications of fluid mechanics, understanding the underlying flow physics is as important as the prediction itself.\n\nAgainst this background, research increasingly turns to data-driven methods for new solutions, and approaches that combine accuracy with clear, understandable models. {  Symbolic Regression} (SR) stands out in this space. Unlike typical ML models that operate as ``black boxes'', SR identifies mathematical expressions that best describe relationships in data, requiring no predefined functional forms, making it possible to reveal or confirm physical laws in fluid mechanics . This feature is invaluable for researchers and engineers who need models that are not just predictive, but also provide insight into the underlying physics. Additionally, SR can play a significant role in contemporary and demanding applications, like digital twins, where real-time data is used to create virtual representations of physical systems for monitoring and optimization.\n\nDespite the interpretability of the SR approach, and although it has been used in a plethora of domains in the recent years , the use of SR in the context of fluid mechanics is, surprisingly, limited. In what follows, we briefly review an indicative collection of that limited literature.\n\nPraks and Brki{\\'c} demonstrated its potential by approximating the Colebrook equation, a key relation for calculating turbulent flow friction factors. By leveraging extensive datasets of Reynolds numbers and relative roughness, the initial approximations produced by SR tools were iteratively refined using fixed-point methods. This iterative process underscored the adaptability of SR in improving accuracy while maintaining low computational complexity.\n\nFurther advancements in SR applications include studies by El Hasadi and Padding , who utilized semi-supervised guided symbolic regression to explore the fluid drag experienced by ellipsoidal and spherocylindrical particles with arbitrary aspect ratios. Their work contributed novel correlations for drag forces, broadening the scope of SR in particle-fluid interactions. Milo{ {s}}evi{\\'c} {  et al.} contributed to SR's growing relevance by combining it with data-driven insights to unify models of laminar and turbulent flow friction, proposing novel formulations that efficiently bridged these flow regimes. Sofos {  et al.} also employed SR to define a Lennard-Jones fluid descriptor, correlating density and temperature variables, further emphasizing SR's adaptability across diverse fluid systems. Roland {  et al.} explored SR in non-Newtonian fluid mechanics, creating accurate models for viscous dissipation in polymer extrusion processes. Their work reduced reliance on computationally expensive numerical simulations, showcasing SR's role in optimizing industrial processes.\n\nReassessing transport properties of Lennard-Jones fluids, Angelis {  et al.} employed SR to develop closed-form equations for viscosity and thermal conductivity. These models encompassed transitions from dilute gases to dense liquids, achieving fine accuracy with reduced complexity compared to traditional molecular dynamics simulations. Similarly, Alam {  et al.} used SR to predict self-diffusion in Lennard-Jones fluids, finding SR-derived models more interpretable and accurate than many empirical relationships.\n\nIn turbulence modelling, Wu and Zhang enhanced the predictive power of the shear-stress-transport (SST) turbulence model using SR to derive correction terms that generalize well across diverse test cases, including three-dimensional flows. This approach demonstrated SR's ability to overcome the generalization limitations of machine-learning-based turbulence models. Chakrabarty and Yakovenko applied SR to improve the Reynolds stress approximations within Reynolds-averaged Navier-Stokes (RANS) equations, emphasizing its potential to derive compact, interpretable models for turbulence closures.\n\nIn experimental setups, Reinbold {  et al.} showcased SR's capacity to distil meaningful equations from high-dimensional, noisy data by incorporating prior physical knowledge. Their study reconstructed external force fields absent from the training data, illustrating how SR can leverage incomplete information to generate robust models. This capability demonstrates SR's strength in aligning empirical observations with theoretical frameworks, yielding models that closely approximate foundational equations like the Navier-Stokes equations.\n\nIn this article, we push the frontier of fluid-dynamics modelling by harnessing SR to extract interpretable, closed-form expressions that capture the essential physics of a fundamental fluid-flow problem. Our approach establishes a {  novel and robust benchmark} for interpretable ML models in viscous fluid motion within confined geometries. Uniquely, without any a priori assumptions about functional forms, our SR framework successfully derives explicit expressions for the (axial) velocity and pressure fields in three-dimensional (3D) channels as functions of {  spatial coordinates} and the {  Reynolds number}. The quantitative evaluation of the derived models underscored their exceptional accuracy and reliability. Specifically, the symbolic equation for velocity achieved a {  Normalized Mean Absolute Error} (NMAE) below $0.01%$ on both the training and testing datasets, while the symbolic equation for pressure demonstrated an NMAE as low as $10^{-8}%$. These results highlight the capability of the developed SR models to provide fast, accurate, and causally explainable predictions in fluid dynamics, effectively replicating the performance of computationally intensive numerical methods (see\nFigure~).\n\n {3mm}\n\n {figure}[h!]\n  \n {1}{\n {tikzpicture}[very thick, node distance=3cm]\n  (input1) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (-4, 0) {Input\n Data};\n  (method) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (0, 0) {Numerical\n Method};\n  (output1) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (5, 0) {Numerical\n Solution};\n\n  (input2) [draw, shape=rounded rectangle, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (-4, -2.8) {Input\n Data};\n  (sr) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.5cm, minimum height=1.5cm, text centered] at (0,-2.8) {Symbolic\n Expressions};\n  (output2) [draw, shape=rounded rectangle, align=center, minimum width=4.5cm, minimum height=2cm, text centered] at (5, -2.8) {Rapid, Accurate\n and Interpretable\n Prediction of the\n Numerical Solution};\n\n [->, >=stealth, line width=0.5mm] (input1) -- (method);\n [->, >=stealth, line width=0.5mm] (method) -- (output1);\n [->, >=stealth, line width=0.5mm] (input2) -- (sr);\n [->, >=stealth, line width=0.5mm] (sr) -- (output2);\n {tikzpicture}}\n {The SR approach (bottom) can operate, through its derived symbolic expressions, as an efficient, interpretable surrogate for traditional, computationally intensive numerical methods (top).}\n\n {figure}\n\nWhile, as we demonstrate, SR excels in its generative capabilities, it operates within a {  purely data-driven} manner, potentially overlooking intricate domain-specific constraints and logical relationships inherent to physical systems. This limitation can lead to the selection of models that, despite their statistical accuracy, {  may violate} fundamental physical laws or, in general, domain-specific constraints. To address these challenges, we propose an innovative integration of SR with the {  knowledge-representation} framework of {  Answer Set Programming} (ASP) . ASP constitutes a declarative-programming paradigm that allows the specification of complex problems and constraints in a high-level (symbolic) language. By blending data-driven and symbolic Artificial Intelligence (AI), the resulting hybrid SR/ASP framework ensures that the SR-generated symbolic expressions are not only statistically accurate, but also physically plausible, adhering to domain-specific constraints and principles, encoded into ASP (see Figure~). This integration offers a promising direction for building versatile and trustworthy AI systems, capitalizing on the complementary strengths of learning from data and reasoning with knowledge . It is worth emphasizing that, while ASP has been extensively applied across a wide range of domains , the integration of ASP with SR represents, to the best of our knowledge, the {  first} effort to combine these two powerful frameworks.\n\n {figure}[t]\n  \n {1}{\n {tikzpicture}[very thick, node distance=3cm]\n \n  (data) [draw, shape=rounded rectangle, align=center, minimum width=3.2cm, minimum height=1.5cm, text centered] at (-0.2,0) {Fluid-Mechanics\n Raw Data};\n  (sr) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.3cm, minimum height=1.5cm, text centered] at (3,0) {SR Module};\n  (sr_out) [draw, shape=rounded rectangle, align=center, minimum width=3cm, minimum height=2cm, text centered] at (6,0) {Accurate\n Symbolic\n Expressions};\n  (asp) [draw, shape=rectangle, rounded corners, align=center, minimum width=2.3cm, minimum height=1.5cm, text centered] at (9,0) {ASP Module};\n  (asp_out) [draw, shape=rounded rectangle, align=center, minimum width=3.5cm, minimum height=2cm, text centered] at (12.3,0) {Accurate \\&\n Physically Valid\n Symbolic\n Expressions};\n  (domain) [draw, shape=tape, align=center, minimum width=2.6cm, minimum height = 1.5cm] at (9,2.5) {Domain-Specific\n Constraints};\n\n [->, >=stealth, line width=0.5mm] (data) -- (sr);\n [->, >=stealth, line width=0.5mm] (sr) -- (sr_out);\n [->, >=stealth, line width=0.5mm] (sr_out) -- (asp);\n [->, >=stealth, line width=0.5mm] (asp) -- (asp_out);\n [->, >=stealth, line width=0.5mm] (domain) -- (asp);\n {tikzpicture}}\n {Integrated SR and ASP work-flow. SR generates candidate accurate symbolic expressions from fluid-mechanics raw data. Then, ASP applies domain-specific constraints to filter and select expressions that are both accurate and physically valid.}\n\n {figure}\n\nOn the whole, the present study highlights two major contributions: First, the ability of SR to translate intricate flow dynamics into simple, interpretable equations that effectively balance precision and clarity; and second, the significant role of knowledge-representation techniques in improving the reliability and domain-specific validity of data-driven SR models. These developments open new avenues for incorporating hybrid (data-driven and symbolic) methodologies into efficient computational systems, particularly in high-stakes fluid-dynamics scenarios where the integration of detailed, explainable simulations with real-time, data-driven insights is crucial.\n\nThe remainder of the article is organized as follows: The subsequent section (Section~) sets the mathematical formulation of the fluid-flow problems under investigation. Thereafter, Section~ introduces the architecture and parameters of the developed SR models. Section~ is dedicated to the presentation and discussion of the results of the SR models. Section~ presents the alluded hybrid SR/ASP approach. The article closes with a concluding section that summarizes the overall contribution and discusses interesting avenues for future research.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "paper.tex",
      "rlhf_score": 0.326,
      "weak_supervision_score": 0.317,
      "diffusion_reasoning_score": 0.362,
      "distributed_training_score": 0.314,
      "datasets_score": 0.287,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667897",
      "updated_at": "2025-08-11T23:43:05.606883",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17778",
      "title": "An advanced AI driven database system",
      "authors": [
        "M. Tedeschi",
        "S. Rizwan",
        "C. Shringi",
        "V. Devram Chandgir",
        "S. Belich"
      ],
      "categories": [
        "cs.DB (Databases)",
        "cs.AI (Artificial Intelligence)",
        "cs.SE (Software Engineering)"
      ],
      "abstract": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats.",
      "published_date": "2025-07-22T16:10:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17778v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17778v1",
      "latex_url": "http://arxiv.org/src/2507.17778v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.363,
      "weak_supervision_score": 0.371,
      "diffusion_reasoning_score": 0.383,
      "distributed_training_score": 0.334,
      "datasets_score": 0.437,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Not Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the development of an AI-driven database system that enhances usability through NLP, LLMs, and automation of tasks like schema creation and query handling. It does not address creating, analyzing, benchmarking, or evaluating datasets for machine learning and AI applications, focusing instead on database management improvements.",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669633",
      "updated_at": "2025-08-11T23:43:05.607172",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.17845",
      "title": "Towards Robust Foundation Models for Digital Pathology",
      "authors": [
        "Jonah Kömen",
        "Edwin D. de Jong",
        "Julius Hense",
        "Hannah Marienwald",
        "Jonas Dippel",
        "Philip Naumann",
        "Eric Marcus",
        "Lukas Ruff",
        "Maximilian Alber",
        "Jonas Teuwen",
        "Frederick Klauschen",
        "Klaus-Robert Müller"
      ],
      "categories": [
        "eess.IV (Image and Video Processing)",
        "cs.AI (Artificial Intelligence)",
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled\nhealthcare research and entering clinical validation. However, their\nsusceptibility to learning non-biological technical features -- including\nvariations in surgical/endoscopic techniques, laboratory procedures, and\nscanner hardware -- poses risks for clinical deployment. We present the first\nsystematic investigation of pathology FM robustness to non-biological features.\nOur work (i) introduces measures to quantify FM robustness, (ii) demonstrates\nthe consequences of limited robustness, and (iii) proposes a framework for FM\nrobustification to mitigate these issues. Specifically, we developed PathoROB,\na robustness benchmark with three novel metrics, including the robustness\nindex, and four datasets covering 28 biological classes from 34 medical\ncenters. Our experiments reveal robustness deficits across all 20 evaluated\nFMs, and substantial robustness differences between them. We found that\nnon-robust FM representations can cause major diagnostic downstream errors and\nclinical blunders that prevent safe clinical adoption. Using more robust FMs\nand post-hoc robustification considerably reduced (but did not yet eliminate)\nthe risk of such errors. This work establishes that robustness evaluation is\nessential for validating pathology FMs before clinical adoption and\ndemonstrates that future FM development must integrate robustness as a core\ndesign principle. PathoROB provides a blueprint for assessing robustness across\nbiomedical domains, guiding FM improvement efforts towards more robust,\nrepresentative, and clinically deployable AI systems that prioritize biological\ninformation over technical artifacts.",
      "published_date": "2025-07-22T16:51:53+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.17845v1",
      "pdf_url": "http://arxiv.org/pdf/2507.17845v1",
      "latex_url": "http://arxiv.org/src/2507.17845v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Biomedical Foundation Models (FMs) are large-scale AI models pre-trained on increasingly large unlabeled biomedical datasets . They drastically improved performance and generalization capabilities over standalone supervised models and non-biomedical pre-training across domains .\nIn digital pathology, FM pre-training has been scaled up to millions of Whole Slide Images (WSIs) and billions of model parameters .\n\nSome of the resulting models demonstrate remarkable capabilities at a wide range of diagnostic tasks, such as pan-cancer classification or rare cancer detection . They further advance the prediction of clinically relevant biomarkers from histology that typically require additional molecular or immunohistochemical testing --- such as MSI, HER2, and EGFR --- and enable real-world clinical utility of ML-based biomarkers .\n\nAs the development of pathology FMs is progressing rapidly, measuring their capabilities and differences becomes increasingly challenging . To this end, many recent efforts have focused on contributing new pathology benchmarks to assess the performance potential of foundation models in various clinically relevant settings .\n\nHowever, one major issue that deserves systematic analysis is the apparent lack of robustness of FMs to technical variability across medical centers (hospitals, laboratories, biobanks, etc.). Such variability (see, e.g., Sup.~Figure~) is caused by numerous factors, including biopsy acquisition technique, tissue preparation and sectioning, staining protocols, and whole slide scanning, among other factors. These differences neither reflect medical nor biological tissue characteristics. Nevertheless, machine learning models can be negatively influenced by these types of variation .\n\nNote that such systematic technical data biases, also known as batch effects , are not limited to digital pathology, but pose a fundamental issue across biomedical disciplines, e.g., in radiology or molecular biology .\n\nFoundation models might be expected to provide more robust information thanks to their large and diverse pre-training datasets. However, the self-supervised learning methods applied to pre-train pathology FMs are designed to capture any differences in the data , which includes technical variation. In fact, recent work suggests that pathology FMs encode technical/medical center information in their representations . For example, Filiot et al.\\ considered different stainings and scanners applied to the same slides and observed substantial variations in the resulting FM representations. Other factors prevalent in real-world diagnostic slides, such as differences in tissue fixation, section thickness, and quality, were not considered in that study , though.\n\nWith the present work, we intend to contribute to the above-described challenge by thoroughly investigating FM robustness, its medical consequences, and strategies for improving FM robustness.\nAs a part of this endeavor, we constructed PathoROB, an extensive, first-of-its-kind benchmark for systematically measuring pathology foundation model robustness against non-biological variation across medical centers. It consists of four multi-class multi-medical center datasets from three public sources that facilitate comparisons between biological and non-biological signals present in FM representations of histopathology images. We present three novel metrics for assessing FM robustness and its implications: the performance drop in downstream tasks, a clustering score reflecting the global organization of the embedding space, and the {  robustness index}: a metric measuring the degree to which foundation embeddings represent biological features rather than confounding ones.\nFurthermore, we describe a framework to make foundation models more robust without retraining them and compare different ways to achieve this.\n\nWe applied our benchmarking approach to 20 current pathology FMs. We identified major performance differences between the models related to pre-training scale and objective, but also found considerable robustness deficits in all FMs. In addition, we discovered that supervised downstream models are prone to exploiting medical center signatures instead of biological signals, causing diminished generalization performance and potentially dangerous failures. Similarly, medical center signatures deteriorated applications like image clustering and diagnostic case search, which are all based on the learned FM representations. We find that in all cases, the performance drops were correlated with a low robustness index, providing evidence for the utility and predictiveness of the proposed metrics.\nUsing post-training robustification methods like image-space stain normalization and representation-space batch correction considerably improved robustness and reduced the risk of downstream errors, but could not eliminate them fully.\n\nIn summary, our work demonstrates the importance of including robustness criteria in FM development. It further lays the foundation for more robust pathology foundation models and serves as a blueprint for a systematic evaluation and improvement of FM robustness applicable across biomedical domains.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "arxiv.tex",
      "rlhf_score": 0.38,
      "weak_supervision_score": 0.391,
      "diffusion_reasoning_score": 0.379,
      "distributed_training_score": 0.361,
      "datasets_score": 0.378,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667755",
      "updated_at": "2025-08-11T23:43:05.606850",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18649",
      "title": "Livatar-1: Real-Time Talking Heads Generation with Tailored Flow\n  Matching",
      "authors": [
        "Haiyang Liu",
        "Xiaolin Hong",
        "Xuancheng Yang",
        "Yudi Ruan",
        "Xiang Lian",
        "Michael Lingelbach",
        "Hongwei Yi",
        "Wei Li"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "We present Livatar, a real-time audio-driven talking heads videos generation\nframework. Existing baselines suffer from limited lip-sync accuracy and\nlong-term pose drift. We address these limitations with a flow matching based\nframework. Coupled with system optimizations, Livatar achieves competitive\nlip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and\nreaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single\nA10 GPU. This makes high-fidelity avatars accessible to broader applications.\nOur project is available at https://www.hedra.com/ with with examples at\nhttps://h-liu1997.github.io/Livatar-1/",
      "published_date": "2025-07-22T01:02:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18649v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18649v1",
      "latex_url": "http://arxiv.org/src/2507.18649v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent breakthroughs in Large Language Models (LLMs)~ {touvron2023llama,openai2023gpt4,anil2023palm2} and real-time Text-to-Speech (TTS)~ {kim2021vits,ren2020fastspeech2,shen2018tacotron2} systems have paved the way for highly interactive, streaming AI agents. To truly unlock their potential, these AI agents require visual embodiments, enabling new applications in education, sales, and virtual companionship.\n\nA typical scenario involves LLMs and TTS systems generating a streaming audio response based on a user's input. The remaining key problem to realizing these visualized agents is a real-time and streaming model that can generate talking-head videos from a single image and the streaming audio.\n\nCurrent approaches have two key problems: limited lip-sync accuracy and long-term pose drift, where cumulative errors cause the head's pose and shape to deviate over time. In this work, we present Livatar {This technical report is a shorten version only summarizing the performance of the Livatar due to the intellectual property policy, the complete version was finished earlier.}, a system designed to address these challenges and achieve production-ready quality and performance.",
      "intro_extraction_method": "scrape_entire_folder",
      "tex_file_name": "iclr2025_conference.tex",
      "rlhf_score": 0.379,
      "weak_supervision_score": 0.267,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.317,
      "datasets_score": 0.253,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668412",
      "updated_at": "2025-08-11T23:43:05.607000",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18650",
      "title": "Features extraction for image identification using computer vision",
      "authors": [
        "Venant Niyonkuru",
        "Sylla Sekou",
        "Jimmy Jackson Sinzinkayo"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.",
      "published_date": "2025-07-22T10:43:52+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18650v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18650v1",
      "latex_url": "http://arxiv.org/src/2507.18650v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.276,
      "weak_supervision_score": 0.295,
      "diffusion_reasoning_score": 0.35,
      "distributed_training_score": 0.29,
      "datasets_score": 0.359,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.668423",
      "updated_at": "2025-08-11T23:43:05.607002",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18653",
      "title": "Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane\n  Detection under Distribution Shift",
      "authors": [
        "Mohammed Abdul Hafeez Khan",
        "Parth Ganeriwala",
        "Sarah M. Lehman",
        "Siddhartha Bhattacharyya",
        "Amy Alvarez",
        "Natasha Neogi"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Lane detection models are often evaluated in a closed-world setting, where\ntraining and testing occur on the same dataset. We observe that, even within\nthe same domain, cross-dataset distribution shifts can cause severe\ncatastrophic forgetting during fine-tuning. To address this, we first train a\nbase model on a source distribution and then adapt it to each new target\ndistribution by creating separate branches, fine-tuning only selected\ncomponents while keeping the original source branch fixed. Based on a\ncomponent-wise analysis, we identify effective fine-tuning strategies for\ntarget distributions that enable parameter-efficient adaptation. At inference\ntime, we propose using a supervised contrastive learning model to identify the\ninput distribution and dynamically route it to the corresponding branch. Our\nframework achieves near-optimal F1-scores while using significantly fewer\nparameters than training separate models for each distribution.",
      "published_date": "2025-07-22T18:39:15+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18653v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18653v1",
      "latex_url": "http://arxiv.org/src/2507.18653v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Deep learning has become the dominant paradigm in computer vision with pretrained models serving as the foundation for state-of-the-art systems across tasks such as image classification~, semantic segmentation~, and object detection~. While highly effective on in-distribution data, these models often fail under distribution shift, that is, when test data diverge from training distributions~. This limitation is especially critical in downstream applications like lane detection~, where safe deployment requires robustness across diverse real-world environments. Recent advances have produced a range of lane detection architectures such as segmentation-based~, row-wise~, anchor-based~, and parametric methods~. These approaches differ both in how they represent lane lines (e.g., pixel masks, row-wise positions, polynomial curves) and in their trade-offs between instance discrimination, inference, and post-processing complexity.\n\nHowever, despite the architectural progress and large-scale benchmarks, lane detection models are typically evaluated on a single dataset (e.g., CULane~ or CurveLanes~), reflecting a closed-world assumption that overlooks generalization across datasets and environments~. For instance, a model trained on one-lane dataset may perform poorly on another where lanes originate from different image regions and follow more diverse topologies, thereby violating the model’s learned assumptions. Moreover, when a model trained on car lanes is evaluated on a distribution like airport taxiways (AssistTaxi~), where visual semantics differ markedly from typical road lanes, we observe that the F1-score drops to near-zero.\n\nThe difficulty of generalizing across datasets and distributions is rooted in how lane detection models internalize dataset-specific priors during training. They learn to associate visual cues with patterns that dominate their source distribution, embedding strong assumptions about lane geometry, spatial orientation, and camera perspective. These assumptions are encoded hierarchically across the network: early layers capture low-level textures like road markings while deeper layers and task-specific heads model layout regularities, curvature, and anchor parameter relationships. In anchor-based models, for instance, the network head learns to predict offsets from predefined anchors based on where lane markings typically appear, such as the left and right sides in car-lane datasets. When exposed to a distribution with different curvature, lane density, or semantic context---such as taxiway markings that usually appear in the center of an image---these priors often fail to hold, leading to feature misalignment and degraded scene interpretability. This suggests that the model has not truly learned a distribution-invariant understanding of what constitutes a \"lane\" in terms of geometry, continuity, and spatial semantics, but has instead internalized anchor patterns and positional constraints tied to the biases of its training data.\n\nWhile fine-tuning is the most common approach for adapting pretrained models to new data, it often leads to catastrophic forgetting~, that is, a sharp drop in performance on the original training distribution after adaptation. This occurs because the same network parameters that encode useful features for the source distribution are updated to fit the new one, resulting in the overwriting of previously learned representations~. This phenomenon has been studied extensively in continual learning~. Classical fine-tuning heuristics attempt to reduce forgetting by freezing early layers or limiting updates to task-specific heads, under the assumption that low-level features are more general and stable. Yet, we observe that forgetting persists even under such constraints, suggesting that semantic knowledge is distributed more broadly across the network. This raises a key question: Can we empirically characterize the roles of different network components in adaptation and forgetting under distribution shift, and use this understanding to inform more targeted and efficient fine-tuning strategies?\n\n {Contributions.}\nIn this work, we address the challenge of adapting a lane detection model to a new distribution while preserving its original performance using minimal trainable parameters. To this end, we first conduct a component-wise analysis under distribution shift to empirically characterize the adaptation–retention tradeoff across different network modules. Based on these insights, we introduce a modular branching strategy, where only target-specific components are fine-tuned while the source branch remains fixed—preserving source performance while enabling efficient target adaptation. To support distribution-aware inference, we train a supervised contrastive model to classify input distributions and dynamically route them to the appropriate branches. We base our study on CLRerNet~, a state-of-the-art anchor-based lane detection model, and evaluate across three datasets—CULane~, CurveLanes~, and AssistTaxi~. Our framework enables parameter-efficient adaptation with no source forgetting, and we further validate its generality on ERFNet to examine how model capacity influences adaptability.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sec/1_intro.tex",
      "rlhf_score": 0.396,
      "weak_supervision_score": 0.389,
      "diffusion_reasoning_score": 0.412,
      "distributed_training_score": 0.446,
      "datasets_score": 0.361,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on lane detection model adaptation using fine-tuning and contrastive routing to handle distribution shifts, with no mention of diffusion models, iterative refinement processes, or multi-step logical reasoning tasks. It does not involve treating a Chain-of-Thought as an entity for holistic correction.",
      "distributed_training_justification": "The paper addresses model adaptation and fine-tuning strategies for lane detection under distribution shifts, but it does not discuss distributed training, parallel computing, multi-node machine learning, or methods for partitioning data/computation across processors to accelerate training.",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669168",
      "updated_at": "2025-08-11T23:43:05.607121",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18654",
      "title": "Diffusion Models for Solving Inverse Problems via Posterior Sampling\n  with Piecewise Guidance",
      "authors": [
        "Saeed Mohseni-Sehdeh",
        "Walid Saad",
        "Kei Sakaguchi",
        "Tao Yu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Diffusion models are powerful tools for sampling from high-dimensional\ndistributions by progressively transforming pure noise into structured data\nthrough a denoising process. When equipped with a guidance mechanism, these\nmodels can also generate samples from conditional distributions. In this paper,\na novel diffusion-based framework is introduced for solving inverse problems\nusing a piecewise guidance scheme. The guidance term is defined as a piecewise\nfunction of the diffusion timestep, facilitating the use of different\napproximations during high-noise and low-noise phases. This design is shown to\neffectively balance computational efficiency with the accuracy of the guidance\nterm. Unlike task-specific approaches that require retraining for each problem,\nthe proposed method is problem-agnostic and readily adaptable to a variety of\ninverse problems. Additionally, it explicitly incorporates measurement noise\ninto the reconstruction process. The effectiveness of the proposed framework is\ndemonstrated through extensive experiments on image restoration tasks,\nspecifically image inpainting and super-resolution. Using a class conditional\ndiffusion model for recovery, compared to the \\pgdm baseline, the proposed\nframework achieves a reduction in inference time of \\(25\\%\\) for inpainting\nwith both random and center masks, and \\(23\\%\\) and \\(24\\%\\) for \\(4\\times\\)\nand \\(8\\times\\) super-resolution tasks, respectively, while incurring only\nnegligible loss in PSNR and SSIM.",
      "published_date": "2025-07-22T19:35:14+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18654v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18654v1",
      "latex_url": "http://arxiv.org/src/2507.18654v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Diffusion models are a class of deep generative models designed to sample from complex data distributions. Diffusion models have been shown to outperform alternatives like generative adversarial networks (GANs) in image synthesis tasks and currently represent the state of the art in this domain . The core idea behind diffusion models is to gradually remove the structure from given input data through a forward diffusion process, transforming it into a tractable distribution, which is typically Gaussian white noise. The model is then trained to learn the reverse process, effectively denoising the data step by step to reconstruct a sample that closely approximates the original data distribution. Diffusion models have been applied across various fields, including computer vision , natural language processing , audio synthesis , and medical image reconstruction .\n\nOnce trained on a dataset from a specific distribution, diffusion models can generate samples that follow that distribution . These samples are inherently random, as the generation process begins with a noise vector sampled randomly. Consequently, the resulting samples may correspond to any region within the support of the learned distribution. Diffusion models can be employed for conditional sampling when their denoising process is adapted to incorporate auxiliary information, enabling the generation of samples that are consistent with the provided conditions.\n\nThis conditional sampling capability makes diffusion models promising candidates for solving inverse problems, where the objective is to reconstruct a degraded signal. The core idea is to recover the original signal by sampling from the posterior distribution conditioned on the observed degraded input. In the context of image inverse problems, this approach has demonstrated the ability to produce perceptually high-quality outputs . This motivates the use of diffusion models as effective tools for solving inverse problems.\n\nNumerous challenges arise when employing diffusion models to solve inverse problems. These models are typically large and require extensive computational resources and substantial amounts of data for effective training. Given the wide variety of inverse problems, it is impractical to train a separate diffusion model for each specific task . Therefore, a key challenge is developing a unified framework that can address multiple inverse problems using a single, pre-trained diffusion model without the need for task-specific retraining. Another key challenge is maintaining high-quality restoration, typically measured by standard metrics such as the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). A general-purpose system must achieve results that are competitive with models trained specifically for individual inverse problems. In addition, computational efficiency is critical: The faster the conditional sampling process, the more practical the framework becomes.\n\nRelated Works}\nVarious deep neural network-based techniques have been proposed for solving inverse problems and the aforementioned challenges. These methods can be broadly categorized into supervised (problem-specific) and unsupervised (problem-agnostic) approaches. In supervised methods, the degradation model is known during both training and inference. In contrast, unsupervised methods assume that the degradation is only known at inference time. The unsupervised approaches are particularly appealing, as they better reflect real-world scenarios where access to degradation models during training is often limited or unavailable, and these approaches do not rely on training problem-specific models.\n\nOne class of unsupervised deep neural network techniques addresses inverse problems by iteratively applying a pretrained model . Methods such as plug-and-play (PnP) , regularization by denoising (RED) , and their successors in and incorporate a denoiser into an iterative recovery process. Another line of work leverages (GANs) , where the latent space of a pretrained GAN is searched to find latent codes that generate images best aligned with the observed measurements. These methods often require a large number of iterations to converge to a satisfactory solution making them time inefficient and not practical for real use cases in which low inference time is critical.\n\nDiffusion models are another class of deep neural networks that can be used for solving inverse problems, with applications in both supervised and unsupervised settings. Denoising diffusion reconstruction models (DDRM) represent a diffusion-based approach for solving unsupervised inverse problems. In this method, denoising is performed in the spectral domain of the degradation matrix, and the results are subsequently transformed back into the original image space. While DDRM has demonstrated promising restoration quality, its effectiveness is limited in scenarios where the relationship between the measurement noise level and the diffusion noise level in the spectral domain is weak.\n\nAnother diffusion-based method for unsupervised inverse problems is the pseudoinverse-guided diffusion model ( {\\( \\)}GDM) . This approach computes the guidance term using a one-step denoising approximation of the posterior distribution of the data conditioned on the noisy latent diffusion variable. Unlike DDRM,  {\\( \\)}GDM enables updates regardless of the levels of diffusion and measurement noise. Although effective,   requires computing the derivative of the denoiser's output with respect to its input, a computationally intensive operation, particularly when the denoiser's complexity and the dimensionality of the data increase.\n\nThe computational and time complexity of GAN-based methods and  {\\( \\)}GDM, the sensitivity of DDRM to measurement noise, and the need for fine-tuning and retraining in supervised approaches highlight the need for a new, reliable, problem-agnostic framework for solving inverse problems. Such a framework should deliver high-quality restoration with low inference time and computational cost, while also explicitly accounting for measurement noise, which is almost always present in real-world scenarios.\n\n \n\n \n\nContributions\n\nThe main contribution of this paper is the introduction of a new diffusion-based framework for solving inverse problems, which uses a piecewise function to approximate the guidance term. This approach preserves accuracy while significantly reducing computational complexity. Moreover, the proposed method explicitly accounts for measurement noise. Specifically, our key contributions include:\n {itemize}\n   We propose a novel, problem-agnostic, diffusion-based framework for solving inverse problems via posterior sampling, which employs a piecewise guidance function that depends on both the measurement and the noisy latent variable at each diffusion time step.\n\n   We show how the proposed method leverages the varying noise and information content of latent variables across time steps to compute time-dependent guidance values, enabling a more effective tradeoff between computational efficiency and reconstruction accuracy.\n\n   We expose how the proposed approach explicitly accounts for measurement noise during guidance computation, which is often overlooked in existing methods. As a result, the proposed approach aligns the guidance term more consistently with the measurement distribution, enhancing its performance in real-world scenarios where the measurement is affected by noise.\n\n   We derive mathematical expressions that quantify the quality of the approximation in terms of the Kullback–Leibler (KL) divergence between the true and approximated distributions used in the guidance computation, providing insights into how problem parameters affect the effectiveness of the approximation.\n\n   Extensive experiments show that the proposed method reduces inference time while maintaining comparable performance in terms of PSNR and SSIM. Compared to \\( \\)GDM, it achieves a reduction of \\(25%\\) for both inpainting with random and center masks, and \\(23%\\) and \\(24%\\) for \\(4 \\) and \\(8 \\) super-resolution, respectively, using a class conditional diffusion model.\n\n {itemize}\n\n \nThe main contribution of this paper is the introduction of a novel problem-agnostic, diffusion-based framework for solving inverse problems via posterior sampling, which employs a piecewise guidance function. This guidance function depends on both the measurement and the noisy latent variable at each time step of the diffusion trajectory. Since latent variables at different time steps contain varying levels of noise and information about the original data, this variation can be leveraged to compute time-dependent guidance values that achieve a more effective tradeoff between computational efficiency and reconstruction accuracy. Our experimental results demonstrate that the proposed method is computationally efficient as it can reduce the inference time by at least \\(7.28%\\) compared to a similar method while maintaining the same level of peak to signal ratio (PSNR), structural similarity index (SSIM) and earned perceptual image patch similarity (LPIPS). Additionally, our approach explicitly accounts for the noise present in the measurements.\n \n\nThe rest of this paper is organized as follows. Section introduces the problem and its challenges. Then, Section details the proposed method. Section presents a theoretical analysis of how the problem parameters influence the proposed solution, and Section presents and analyzes the simulation results. Finally, conclusions are drawn in Section .\n\n%",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.316,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.564,
      "distributed_training_score": 0.325,
      "datasets_score": 0.274,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on using diffusion models for solving inverse problems in image restoration, such as inpainting and super-resolution, through a piecewise guidance scheme. It does not involve adapting diffusion models for complex logical tasks, multi-step reasoning, or treating a 'Chain-of-Thought' as an entity. There is no component related to logical reasoning or holistic correction of reasoning paths, making it unrelated to the topic.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669178",
      "updated_at": "2025-08-11T23:43:05.607122",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18655",
      "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing",
      "authors": [
        "James Dickens",
        "Kamyar Hamad"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "eess.IV (Image and Video Processing)"
      ],
      "abstract": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.",
      "published_date": "2025-07-22T19:42:34+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18655v2",
      "pdf_url": "http://arxiv.org/pdf/2507.18655v2",
      "latex_url": "http://arxiv.org/src/2507.18655v2",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "High-quality part segmentation of 3D human meshes is a useful tool for applications such as character animation and game development, where fine-grained control over character models is desired. While many publicly available 3D models include skeletal rigs—hierarchies of anatomical keypoints used for animation—they often lack per-vertex part labels. Additionally, some models have incomplete or missing texture maps, making it difficult to apply color-based 2D-to-3D segmentation approaches.\n  To address this gap, in this work a pipeline that automatically generates 3D parsing labels from textured 3D human models, and trains a deep neural network to predict these labels using only geometric information is developed. Unlike existing approaches that rely on synthetic datasets where human instances are fit to parametric mesh models such as SMPL or SMPL-X , our method operates directly on raw, real-world 3D meshes obtained from a multi-view camera setup, which often exhibit greater diversity in body shapes, clothing, and poses.\n  This work draws inspiration from two complementary areas: 2D human parsing and 3D deep learning. The former refers to the task of segmenting human body parts and clothing in color images , a well-studied domain with a rich set of pretrained models. These models will be leveraged to project 2D parsing labels into 3D space via multi-view backprojection and aggregation, enabling the creation of pseudo ground truth for 3D mesh segmentation.\n  The latter area, known as 3D deep learning, has emerged as a first-class research area in modern computer vision, focused largely on learning representations from point clouds and polygonal meshes. In the proposed approach, the vertices of polygonal meshes are treated as point clouds, wherein 3D deep learning techniques for semantic segmentation can be applied. However, existing point cloud models are typically designed for smaller point clouds (e.g., 2048 points), while human meshes can contain millions of vertices with redundant and densely packed regions. To address this, an efficient point cloud downsampling strategy that preserves semantic structure for training is introduced, followed by a simple upsampling stage to produce full-resolution mesh segmentations.\n  Further, many 3D models do not come in a cannonical orientation, wherein human parsing models are most accurate when individuals face the camera in commonly encountered poses, i.e. front facing with minimal self-occlusions. To solve this issue, a keypoint-based approach locates anatomical points of interest, where rotations can be employed to correct a wide range of less desirable poses. In summary, the main contributions of this work are as follows:\n {itemize}\n  We propose a pipeline for generating pseudo ground truth 3D parsing labels by aggregating multi-view projections from 2D human parsing models, including an alignment step making use of keypoint-based correction of the input orientation of the 3D model.\n  We develop a memory-efficient point cloud sampling and upsampling strategy to enable full-resolution part segmentation of high-density human meshes.\n {itemize}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "HumanSeg3D/main.tex",
      "rlhf_score": 0.353,
      "weak_supervision_score": 0.349,
      "diffusion_reasoning_score": 0.348,
      "distributed_training_score": 0.374,
      "datasets_score": 0.331,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669187",
      "updated_at": "2025-08-11T23:43:05.607123",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.18656",
      "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision\n  Avoidance in Machine Learning-based Advanced Driver Assistance Systems",
      "authors": [
        "Muhammad Zaeem Shahzad",
        "Muhammad Abdullah Hanif",
        "Bassem Ouni",
        "Muhammad Shafique"
      ],
      "categories": [
        "cs.CV (Computer Vision and Pattern Recognition)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether.",
      "published_date": "2025-07-22T20:04:29+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.18656v1",
      "pdf_url": "http://arxiv.org/pdf/2507.18656v1",
      "latex_url": "http://arxiv.org/src/2507.18656v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Of the approximately 7 million traffic accidents in the US in 2016, 40% would have been avoidable had the ego-vehicle been equipped with Advanced Driver Assistance Systems (ADAS), with 29% being avoidable with the collision avoidance feature alone {green}{~}. ADAS are sophisticated embedded systems designed to improve road safety and reduce accidents by providing real-time driver facilitation. These systems rely on cutting edge sensors, such as LiDAR, radar, and cameras, to observe the environment of the ego vehicle and take proactive safety measures. However, widespread adoption of ADAS remains a challenge despite their effectiveness, particularly in low- and middle-income countries where 92% of global traffic deaths occur {green}{~}. This is because these systems are mostly unaffordable in these regions due to the expensive sensor technologies they employ. Advances in machine learning (ML), particularly in deep learning, offer a promising path forward. Using deep neural networks (DNNs) that rely solely on visual input from standard cameras, ML-ADAS can deliver functionality comparable to traditional ADAS at a fraction of the cost.\n\n {figure}[!t]\n \n [width=1 ]{Figures/attacks3.png}\n\n {A comparison of different backdoor attacks on object detection, highlighting that the proposed ShrinkBox attack produces less perceptible deviation in the annotations from ground truth.}\n\n {figure}\n\nIn this paper, we focus on the collision avoidance ML-ADAS which observes the traffic ahead to warn the driver to apply timely brakes in case of a predicted collision. Two specialized DNNs are required in this system. Firstly, an object detection DNN detects objects in an image by regressing their bounding boxes and identifies their classes {green}{~}. This empowers vehicles with the critical capability to locate and classify objects on the road, such as pedestrians, vehicles, and road signs. Popular object detectors such as the YOLO {green}{~} models offer state-of-the-art real-time performance, making them ideal for an ML-ADAS. Secondly, a specialized DNN is required to estimate distance. Although traditional depth estimation DNNs are available {green}{~}, their high computational complexity, due to a pixel-wise regression across the entire image, limits their suitability for real-time applications on edge devices.\n\nFor instance, while the object detector YOLOv9t requires 7.7 billion FLOPs, MonoDepth, one of the most efficient depth estimators, demands 11.6 billion FLOPs. In contrast, a fast, lightweight DNN designed to directly estimate object-specific distances based on features extracted from predicted bounding boxes is far more practical {green}{~}. In DECADE {green}{~}, such a detection-wise approach offers higher accuracy than MonoDepth yet requires only 8.3M FLOPs---an approximately 1400x reduction in computation. Overall, this pipeline defines highly accurate and robust object detectors as the cornerstone of collision avoidance in ML-ADAS. Thus, potential failures in object detection compromise the entire system, putting the lives of the passengers and those around them at risk.\n\nSecurity vulnerabilities, such as backdoor attacks, originally demonstrated in image classification {green}{~} have increasingly been identified as critical risks in object detection as well {green}{~}. These vulnerabilities often stem from inadvertently incorporating poisoned or malicious data into the training process. In backdoor attacks, a malicious party can poison the training dataset with backdoor triggers allowing the model to learn the trigger during the training phase. Later, the attackers can exploit this backdoor trigger to achieve specific behavior during the deployment phase. This infection is achieved by modifying a portion of the training dataset by altering the images and ground truth annotations such that the model behaves as expected on benign (uninfected) samples, but predicts the attacker-specified outcome on infected samples containing the backdoor trigger.\n\n {figure}[!t]\n \n [width=1 ]{Figures/motiv3.png}\n {The left column visualizes the stealthiness of Shrinkbox by showing an image, its clean ground truth annotations, and its center instance poisoned from top to bottom respectively. The right column shows distance estimation with DECADE, prediction/ground-truth format, on clean (top) and poisoned (right) bounding boxes. The original box area is shrunk by 34%, maintaining aspect ratio.}\n\n {figure}\n\nFig.~ illustrates the different types of backdoor attacks specialized for achieving different outcomes in collision avoidance, as described in {green}{~}. Object Generation Attack (OGA) aims to generate a false object of a target class around the trigger’s location. In contrast, Object Disappearance Attack (ODA) makes the detector fail to detect an object of the target class near the trigger. Lastly, Regional Misclassification Attack (RMA) and Global Misclassification Attack (GMA) aim to misclassify objects to the specified target class by using one trigger for one surrounding object and one trigger for all objects in the image respectively. While all of these attacks have the potential to cause devastating crashes, their realization can be easily prevented with a quick scan of the object detection dataset, revealing its poisoned nature.\n\nObject annotations modified to the extent that bounding boxes are completely removed (ODA), appear out-of-place (OGA), or have class labels that are clearly misaligned with the image contents (RMA/GMA), makes the attacks strikingly detectable in the manual and automated inspection phases. To this end, we propose a novel backdoor attack, ShrinkBox, where a trigger in the image over an object only shrinks the dimensions of the object’s ground truth bounding box. Since there are no out of place, absent, or misclassified instances in the ground truth, it will be especially difficult to detect this embedded poison. Furthermore, the difference between Average Precision (AP) and, consequently, the mean AP (mAP) of the benign and infected models should be negligible. This further increases the invisibility of the ShrinkBox as even if a pretrained infected detector is downloaded and evaluated on a poisoned dataset, its performance does not degrade in terms of the standard metrics. Not only does this hide the infection in the detector, but also the poison in the dataset.\n\nTo measure the effectiveness of ShrinkBox, we propose a novel Attack Success Rate (ASR) evaluation metric. By comparing the detected objects in terms of their similarity in box size with both the clean or the poisoned ground truth instances, we are able to determine the efficacy of the attack. Finally, to highlight the detrimental effect of ShrinkBox on the collision avoidance ML-ADAS, we evaluate its impact on downstream distance estimation using DECADE {green}{~} which relies on highly precise object detection. Intuitively, as the boxes become smaller, they appear further than they actually are. In this way, a higher error from DECADE is guaranteed to cause traffic accidents due to failure to generate warnings in time, potentially resulting in the tragic loss of lives. We demonstrate that by attacking the YOLOv9m {green{~} object detector with ShrinkBox, we achieve an ASR of 96.4%, with a negligible difference between mAP\\(_{benign}\\) and mAP\\(_{poison}\\), while also degrading DECADE’s distance estimation accuracy by more than 3.1x in the poisoned instances}.\n\n {figure*}[!t]\n \n [width= ]{Figures/mainmeth.png}\n {Overview of the complete pipeline for our ShrinkBox attack}\n\n {figure*}\n\nMotivational Case Study\nIn Fig.~, we demonstrate the stealthiness and effectiveness of the ShrinkBox attack on samples from the KITTI 3D Object Detection dataset  {green}{~}. Firstly, upon human inspection, we show that it is difficult to identify the changes made between a poisoned box and its clean counterpart even when the poisoned bounding box is reduced by 34% of its original area. This is especially true for images where there are many annotations present. Furthermore, we demonstrate the significant errors observed in DECADE’s distance estimation due to the reduced bounding box size of the poisoned instance. Note that only for this preliminary study, we have assumed that the backdoor trigger in the image is invisible. Most importantly, we observe a critical divergence of almost 8m from the ground truth distance in the poisoned instance. Since 4.5m is the average length of a car, we believe that the ShrinkBox attack can plausibly lead to collision warnings being delayed or entirely suppressed with only this level of deviation. Overall, the stealthy ShrinkBox attack theoretically has the potential to mislead a collision avoidance ML-ADAS to the extent of causing devastating traffic accidents.\n\nOur Novel Contributions\nIn this paper, we present the following novel contributions.\n {enumerate}[leftmargin=*]\n   We propose the ShrinkBox attack which shrinks the predicted bounding boxes in the presence of a backdoor trigger. To the best of our knowledge, this is the first time a backdoor attack is explored which specifically targets the size/dimensions of the bounding box. We highlight how ShrinkBox can not only evade visual inspections but also benchmarking criteria as the infected object detector will score high on standard metrics like the mAP on both benign and infected samples.\n   In light of this, we define a method for evaluating the ASR of our ShrinkBox attack specifically. We define a predicted box as successfully attacked when it exceeds a similarity threshold when compared with the poisoned box, as opposed to the similarity with the clean box. We achieve a dangerous 96% ASR with the YOLOv9m {green}{~} trained with only a 4% poisoning ratio in the KITTI dataset.\n   While mAP does not suffer with ShrinkBox, downstream tasks like distance estimation that depend on object detection deteriorate. We demonstrate that ShrinkBox causes the Mean Absolute Error (MAE) in the pretrained DECADE to increase by 3.3x, from 1.67m to 5.51m, over all poisoned samples.\n {enumerate}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.299,
      "weak_supervision_score": 0.329,
      "diffusion_reasoning_score": 0.298,
      "distributed_training_score": 0.318,
      "datasets_score": 0.255,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669196",
      "updated_at": "2025-08-11T23:43:05.607124",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.19534",
      "title": "FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated\n  Learning Settings",
      "authors": [
        "Ali Shakeri",
        "Wei Emma Zhang",
        "Amin Beheshti",
        "Weitong Chen",
        "Jian Yang",
        "Lishan Yang"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)"
      ],
      "abstract": "Pre-trained Language Models (PLMs) have demonstrated impressive performance\nin various NLP tasks. However, traditional fine-tuning methods for leveraging\nPLMs for downstream tasks entail significant computational overhead.\nPrompt-tuning has emerged as an efficient alternative that involves prepending\na limited number of parameters to the input sequence and only updating them\nwhile the PLM's parameters are frozen. However, this technique's prompts remain\nfixed for all inputs, reducing the model's flexibility. The Federated Learning\n(FL) technique has gained attention in recent years to address the growing\nconcerns around data privacy. However, challenges such as communication and\ncomputation limitations of clients still need to be addressed. To mitigate\nthese challenges, this paper introduces the Federated Dynamic Prompt Generator\n(FedDPG), which incorporates a dynamic prompt generator network to generate\ncontext-aware prompts based on the given input, ensuring flexibility and\nadaptability while prioritising data privacy in federated learning settings.\nOur experiments on three NLP benchmark datasets showcase that FedDPG\noutperforms the state-of-the-art parameter-efficient fine-tuning methods in\nterms of global model performance, and has significantly reduced the\ncalculation time and the number of parameters to be sent through the FL\nnetwork.",
      "published_date": "2025-07-22T03:47:12+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.19534v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19534v1",
      "latex_url": "http://arxiv.org/src/2507.19534v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Large Language Models (LLMs) have shown great potential in various Natural Language Processing (NLP) tasks; however, training these models requires a large amount of data, which has heightened concerns about data privacy. Additionally, training LLMs require powerful computational resources, making it more difficult for small to medium organisations to invest in training one for their business. The federated learning (FL) technique, proposed by McMahan et al. , offers a solution by enabling AI model training, including training LLMs, without sharing private data. FL facilitates collaboration among multiple organisations to train AI models while ensuring their datasets remain localised and protected, even in scenarios of limited data availability. Organisations across various industries can harness FL to train AI models while prioritising data privacy collaboratively. For example, in the telecommunications industry, companies can jointly develop advanced scam detection models powered by LLMs. This approach ensures robust protection for users while maintaining the confidentiality of their data .\n\nExtensive research works have explored FL from various perspectives and challenges, such as FL architectures and concepts , data privacy preservation challenges , communication costs , the impact of data distribution (IID vs. non-IID) , and Personalized Federated Learning (PFL), which focuses on customising the global model for each client .\nAdditionally, Large Language Models (LLMs) have emerged as a promising research direction within Federated Learning (FL) systems , attracting increasing attention in recent years.\n\nDue to the vast number of parameters in LLMs, training them from scratch incurs substantial computational costs. A common way is to fine-tune the Pre-trained Language Models (PLMs) for a downstream task, which could update a part of the PLM parameters and thus is more efficient than training them from scratch . However, fine-tuning still requires accessing the PLM's parameters, which can exceed hundreds of billions . Parameter-efficient fine-tuning (PEFT) has recently emerged as a promising approach for customising pre-trained language models (PLMs). This approach involves integrating a small set of trainable parameters into the PLM, either by incorporating them as part of the PLM's input (soft prompts) or by embedding small neural network modules within the PLM's architecture while keeping the original model parameters frozen. Compared to traditional fine-tuning techniques, PEFT methods can significantly reduce the computational cost, decreasing the number of trainable parameters by up to 90% .\n\nIntegrating PEFT methods into FL systems has garnered significant attention recently as researchers explore practical solutions for efficiently customising PLMs for various downstream tasks . This approach is particularly promising because it freezes most of the PLM's parameters, requiring only the transfer of newly added parameters. As a result, it reduces the computational demands on clients and addresses key communication and computation challenges associated with FL. For example, FedPrompt adapts the soft prompt-tuning method for FL, enabling the exchange of trained prompts between the server and clients. Similarly, FedPepTAO introduces a scoring mechanism to selectively transfer trained prompts, optimising the communication process within FL systems.\n\nBuilding on current state-of-the-art PEFT methods, particularly prompt-tuning methods, we propose the Federated Dynamic Prompt Generator (FedDPG) to leverage the capabilities of PLMs in FL settings. We design FedDPG to be more flexible and dynamic than existing PEFT methods while maintaining high performance. Unlike existing prompt-tuning methods, which train a fixed set of prompt vectors and apply the same vectors to all inputs during inference, FedDPG incorporates an auxiliary network to generate unique prompt vectors for each input dynamically. These input-specific vectors encapsulate general information about the input, enhancing the performance of PLMs in text classification tasks. Given the paramount importance of the \"Right to Be Forgotten\" (RTBF) in machine learning and growing privacy concerns, we also explore the emerging domain of Federated Machine Unlearning (FMU). Through this exploration, we propose FedDPGu, which introduces a novel approach to federated unlearning within PEFT methods.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.488,
      "weak_supervision_score": 0.39,
      "diffusion_reasoning_score": 0.406,
      "distributed_training_score": 0.483,
      "datasets_score": 0.352,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "Highly Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on federated learning and prompt-tuning for pre-trained language models, with no mention of human feedback, reward models, or reinforcement learning techniques.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning; it centers on dynamic prompt generation for NLP tasks in federated learning settings.",
      "distributed_training_justification": "The paper's main contribution, Federated Dynamic Prompt Generator (FedDPG), operates in federated learning, a form of distributed training, by addressing communication costs, computation efficiency, and data partitioning across multiple clients for NLP tasks.",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces FedDPG, a novel approach for federated learning that integrates a dynamic prompt generator to create context-aware prompts for pre-trained language models, aiming to enhance flexibility, efficiency, and data privacy in NLP tasks. By freezing the main model parameters and only updating input-specific prompts, the methodology reduces computational overhead and communication costs, with experiments on three NLP benchmarks demonstrating superior global model performance, faster calculation times, and fewer parameters transferred compared to state-of-the-art parameter-efficient fine-tuning methods.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by combining dynamic prompt generation with federated learning, offering a clever adaptation of existing prompt-tuning techniques to address flexibility in FL settings, though it does not introduce an entirely new problem or architecture.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of federated learning for NLP, as it effectively tackles efficiency and privacy challenges, potentially influencing developments in parameter-efficient methods for real-world applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper represents a strong, valuable contribution to efficient and privacy-preserving AI techniques, making it essential for researchers in federated learning and NLP to be aware of its advancements and potential applications.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/6ce45a3b0bb8d75600e0ceaa962cc0b088ffd3d5",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 3,
      "average_h_index": 1.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Ali Shakeri",
          "profile_url": "https://www.semanticscholar.org/author/2326125582",
          "h_index": 1
        },
        {
          "name": "W. Zhang",
          "profile_url": "https://www.semanticscholar.org/author/2256597050",
          "h_index": 3
        },
        {
          "name": "Amin Beheshti",
          "profile_url": "https://www.semanticscholar.org/author/2369919257",
          "h_index": 0
        },
        {
          "name": "Weitong Chen",
          "profile_url": "https://www.semanticscholar.org/author/2338482635",
          "h_index": 1
        },
        {
          "name": "Jian Yang",
          "profile_url": "https://www.semanticscholar.org/author/2338538107",
          "h_index": 0
        },
        {
          "name": "Lishan Yang",
          "profile_url": "https://www.semanticscholar.org/author/2326254855",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669644",
      "updated_at": "2025-08-11T23:46:10.306704",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.19536",
      "title": "Graph Learning Metallic Glass Discovery from Wikipedia",
      "authors": [
        "K. -C. Ouyang",
        "S. -Y. Zhang",
        "S. -L. Liu",
        "J. Tian",
        "Y. -H. Li",
        "H. Tong",
        "H. -Y. Bai",
        "W. -H. Wang",
        "Y. -C. Hu"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Synthesizing new materials efficiently is highly demanded in various research\nfields. However, this process is usually slow and expensive, especially for\nmetallic glasses, whose formation strongly depends on the optimal combinations\nof multiple elements to resist crystallization. This constraint renders only\nseveral thousands of candidates explored in the vast material space since 1960.\nRecently, data-driven approaches armed by advanced machine learning techniques\nprovided alternative routes for intelligent materials design. Due to data\nscarcity and immature material encoding, the conventional tabular data is\nusually mined by statistical learning algorithms, giving limited model\npredictability and generalizability. Here, we propose sophisticated data\nlearning from material network representations. The node elements are encoded\nfrom the Wikipedia by a language model. Graph neural networks with versatile\narchitectures are designed to serve as recommendation systems to explore hidden\nrelationships among materials. By employing Wikipedia embeddings from different\nlanguages, we assess the capability of natural languages in materials design.\nOur study proposes a new paradigm to harvesting new amorphous materials and\nbeyond with artificial intelligence.",
      "published_date": "2025-07-22T08:30:03+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.19536v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19536v1",
      "latex_url": "http://arxiv.org/src/2507.19536v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "The past decades have witnessed the fast growth in the number of available metallic materials with a disordered structure, namely metallic glasses (MGs) or amorphous alloys~. Unlike conventional crystalline alloys, the amorphous nature of MGs endows them with exceptional mechanical, physical and chemical properties. They are not only important prototypical materials to solve critical scientific problems, but also show great potentials in various applications. For instance, they may enable transformative applications ranging from biomedical implants leveraging their biocompatibility to aerospace components utilizing their high strength-to-weight ratios~.\n\nOne of the most important unresolved problems associated with MGs is their limited glass-forming ability (GFA)~.\nGFA is usually measured by the critical cooling rate, which is the minimal cooling rate required by a material to bypass crystallization.\nDominated by undirectional metallic bonds, metallic alloys are prone to crystallization with a remarkably faster rate than molecular systems. In experiments, there is always an upper limit of the accessible sample size determined by the critical cooling rate for each composition. So far, the record-keeper is still PdNiCuP discovered in 1997, with a critical diameter of $ $ 72 mm~. This severely hinders their wide applications.\nHow to break the ceiling of this amorphous sample size is urgent to tackle. The critical route is to discover a composition with sluggish crystallization kinetics and enhanced supercooled liquid stability.\nThere are great endeavors in the past to propose empirical rules to guide MG design. So far, there are nearly 30 physical quantities that have exhibited a certain correlation with GFA for experimentally developed MGs~. Unfortunately, they are not agnostic but phenomenological. The common prerequisite is to fabricate the MG. Therefore, the predictive capability is absent.\n\nTrial-and-error experimentation is the main strategy to explore new MGs, with Turnbull's deep eutectic rule~, Inoue's three conditions, and some other experiences as the rule of thumb~. This is rather costly and inefficient considering the combinatorial complexity of elemental interactions and the unknown components. This challenge has motivated the proposal of high-throughput sputtering facility design to enable parallel synthesis of a library of an alloy system with composition gradients in space~. This technology remarkably boosts production efficiency but suffers from the high effective cooling rate and trial element combinations. It remains resource-intensive without systematic theoretical guidance.\n\nIntelligent material design is the new request.\nIn recent years, data-driven approaches with machine learning algorithms have gained prominence in materials design, including MGs~. By leveraging accumulated experimental and computational datasets, statistical learning methods greatly forwarded the frontlines of MG design. Unprecedented physical insights were unveiled especially from the high-dimensional latent space.\nHowever, the generalization capability of these models is still rather limited, rendering constrained prediction power in new MG design.\nThere are several inherent reasons.\nFirstly, the available dataset is rather small and imbalanced.\nSecondly, tabular data representation is the common strategy to call for the statistical learning models. The supervised learning frameworks predominantly optimize for known compositional patterns, lacking capacity to infer latent chemical interaction principles governing amorphous phase stability.\nThirdly, the materials are generally encoded by the physical properties of the involved elements and their composition regulations, and some alloy properties as well. The descriptor selection introduces domain knowledge bias, potentially overlooking non-canonical glass-forming mechanisms.\nThese conditions impose immense challenge to make constructive suggestions, underscoring the need for representation learning strategies that transcend conventional feature engineering while preserving materials-specific interpretability.\n\nLearning from small data is currently one of the most intriguing research directions in various fields. Compared to natural language texts and images, scientific data from experimental measurements always falls into the group of small data with lots of physics~. That is, learning from these data requires sophisticated knowledge conversion to vectorial (or tensorial) material representations. The physical properties derived features are never complete by suffering from the finite and discrete nature. The underlying hidden correlations are not clear enough to be inherited.\nSince generating excessive amount of data is not accomplishable in the short-term, how to effectively represent these data with knowledgeable encodings is the thought-worthy solution.\nIn addition, deep learning has emerged as the major strategy in various learning applications~. The tabular data representation in materials science intrinsically prohibits effective implementation of advanced deep learning algorithms in recommending new materials. This limitation is intrinsic for all kinds of materials.\n\nThe first principal question regarding experimental material design is how to directly pick up several elements from the periodic table to synthesize a desired MG with a low critical cooling rate.\nThis reminds us of the analogy to the customer-product relationship in marketplaces. For example, for e-commerce like Amazon or Taobao, their websites aim to make effective recommendations for customers with their desired products from the inventories. Recommendation systems with deep learning architectures have emerged as the efficient tool.\nWe bear in mind that this process does not involve physical synthesis but to uncover the hidden relationship between existing customers and products. This is where the complexity resides in natural science than the marketplaces. The recommended materials will ask experimental fabrication from either element or alloy precursors. Nevertheless, this philosophy indeed provides fresh ideas for materials design.\n\nIn this work, we build glass recommendation systems from our proposal of network representations for binary and ternary MGs~. This strategy focuses on materials relationships from the perspective of the involved elements and their correlations, rather than treats each composition as independent instances. To learn from these graphs, we also propose novel elemental encodings from the Wikipedia, which is the most precious knowledge library in the world from natural languages. A variety of properties of each element and its links to various Wikipedia pages are encoded in its embeddings. These strategies enable us to build versatile architectures of graph neural networks (GNNs). These models show prominent advantages in predicting MGs in different systems, especially from Transformer-powered GNN~. The performance difference arising from different Wikipedia languages also suggest possible knowledge gaps from natural languages and call for deeper knowledge share. The flexibility and versatility of our data-driven machine learning strategy propose a new paradigm in accelerating materials discovery.\n\n {5mm}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.346,
      "weak_supervision_score": 0.36,
      "diffusion_reasoning_score": 0.41,
      "distributed_training_score": 0.337,
      "datasets_score": 0.315,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's main contribution involves using graph neural networks and Wikipedia-derived embeddings for metallic glass discovery, focusing on material relationships and recommendation systems. It does not mention or utilize diffusion models, iterative refinement processes, or multi-step logical reasoning for tasks like Chain-of-Thought correction. Therefore, there is no connection to diffusion-based reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669654",
      "updated_at": "2025-08-11T23:43:05.607175",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.19539",
      "title": "Swift-Sarsa: Fast and Robust Linear Control",
      "authors": [
        "Khurram Javed",
        "Richard S. Sutton"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Javed, Sharifnassab, and Sutton (2024) introduced a new algorithm for TD\nlearning -- SwiftTD -- that augments True Online TD($\\lambda$) with step-size\noptimization, a bound on the effective learning rate, and step-size decay. In\ntheir experiments SwiftTD outperformed True Online TD($\\lambda$) and\nTD($\\lambda$) on a variety of prediction tasks derived from Atari games, and\nits performance was robust to the choice of hyper-parameters. In this extended\nabstract we extend SwiftTD to work for control problems. We combine the key\nideas behind SwiftTD with True Online Sarsa($\\lambda$) to develop an on-policy\nreinforcement learning algorithm called $\\textit{Swift-Sarsa}$.\n  We propose a simple benchmark for linear on-policy control called the\n$\\textit{operant conditioning benchmark}$. The key challenge in the operant\nconditioning benchmark is that a very small subset of input signals are\nrelevant for decision making. The majority of the signals are noise sampled\nfrom a non-stationary distribution. To learn effectively, the agent must learn\nto differentiate between the relevant signals and the noisy signals, and\nminimize prediction errors by assigning credit to the weight parameters\nassociated with the relevant signals.\n  Swift-Sarsa, when applied to the operant conditioning benchmark, learned to\nassign credit to the relevant signals without any prior knowledge of the\nstructure of the problem. It opens the door for solution methods that learn\nrepresentations by searching over hundreds of millions of features in parallel\nwithout performance degradation due to noisy or bad features.",
      "published_date": "2025-07-22T15:08:38+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.19539v1",
      "pdf_url": "http://arxiv.org/pdf/2507.19539v1",
      "latex_url": "http://arxiv.org/src/2507.19539v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "no_intro_found",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.344,
      "weak_supervision_score": 0.346,
      "diffusion_reasoning_score": 0.252,
      "distributed_training_score": 0.291,
      "datasets_score": 0.25,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Could not find introduction section"
      ],
      "created_at": "2025-08-11T23:15:40.669665",
      "updated_at": "2025-08-11T23:43:05.607176",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21130",
      "title": "INTEGRALBENCH: Benchmarking LLMs with Definite Integral Problems",
      "authors": [
        "Bintao Tang",
        "Xin Yang",
        "Yuhao Wang",
        "Zixuan Qiu",
        "Zimo Ji",
        "Wenyuan Jiang"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present INTEGRALBENCH, a focused benchmark designed to evaluate Large\nLanguage Model (LLM) performance on definite integral problems. INTEGRALBENCH\nprovides both symbolic and numerical ground truth solutions with manual\ndifficulty annotations. Our evaluation of nine state-of-the-art LLMs reveals\nsignificant performance gaps and strong correlations between problem difficulty\nand model accuracy, establishing baseline metrics for this challenging domain.\nINTEGRALBENCH aims to advance automated mathematical reasoning by providing a\nrigorous evaluation framework specifically tailored for definite integral\ncomputation.",
      "published_date": "2025-07-22T08:44:36+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21130v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21130v1",
      "latex_url": "http://arxiv.org/src/2507.21130v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "Mathematical reasoning represents one of the most advanced forms of human intelligence and serves as a critical benchmark for evaluating Large Language Model (LLM) capabilities.\nSeveral benchmarks currently assess LLMs' mathematical performance:\nMATH~ tests advanced high school competition problems, GSM8K~ focuses on grade school arithmetic word problems, and MathVista~ evaluates multimodal mathematical reasoning.\nThis widespread attention underscores the recognized importance of mathematical evaluation for LLMs.\n\nWithin this domain, definite integral problems offer a uniquely challenging testbed for assessing both computational accuracy and symbolic reasoning.\nUnlike elementary arithmetic or algebra, integral calculus demands sophisticated multi-step reasoning including decomposition of complex expressions, pattern recognition for simplification techniques, and recall of integration methods.\nThese characteristics make integral calculus particularly suitable for evaluating advanced LLM reasoning capabilities.\n\nDespite existing mathematical benchmarks, current evaluation frameworks exhibit significant limitations for integral problems.\nFirst, most benchmarks contain insufficient integral problems for meaningful assessment.\nWhile MATH includes calculus problems, it has relatively few challenging integrals that comprehensively test integration techniques.\nSecond, current frameworks lack metrics specifically designed for integral evaluation, such as differentiating between symbolic and numerical solution accuracy.\nFinally, existing benchmarks rarely implement appropriate difficulty gradation for integrals, failing to distinguish between routine applications and problems requiring advanced techniques.\nThis lack of stratification restricts the precise measurement of model capabilities across complexity levels.\n\n {figure}[t]\n  \n  {pcvstack}[boxed, space=0.2cm]\n  {}{\n   Problem\n [][ ]\n\n [-0.3cm]\n {  _0^1 1{ {1+4x^2}}  (2x+ {1+4x^2} )  x\\:dx}{38pt}\n\n [-0.3cm]\n   Answer\n [][ ]\n\n [-0.3cm]\n  {Symbolic} = - { }{16} {Li}_2(-4)\n\n  {Numerical} = 0.465336591\n\n [-0.3cm]\n   Difficulty\n [][ ]\n\n [-0.3cm]\n  {Rating} = 4  \n\n [-0.3cm]\n   Source\n [][ ]\n\n [-0.3cm]\n Brychkov, Yury A. Handbook of Special Functions. CRC Press,\n\n 28 May 2008, p. 166.\n\n [-0.7cm]\n }\n  {pcvstack}\n\n  {Example problem from  ~with symbolic/numerical ground truth solutions, difficulty rating, and source attribution.}\n\n {figure}\n\nTo address these limitations, we introduce  , a focused benchmark specifically designed for evaluating LLM performance on definite integral problems.\n ~comprises 317 carefully selected graduate-level problems sourced from advanced textbooks and competitions.\nEach problem provides both symbolic and numerical ground truth solutions as is shown in Figure , enabling separate assessment of LLM-generated answers through distinct evaluation metrics.\n\nAdditionally, each problem is manually annotated with difficulty ratings from 1 to 5, enabling fine-grained analysis across varying complexity levels.\n ~also incorporates a novel term-rewriting method to generate problem variations, preventing dataset contamination while maintaining mathematical rigor.\nIn terms of construction cost,  ~employs a systematic methodology that balances the trilemma of cost, difficulty, and relevance for building benchmark datasets through LLM-assisted curation from academic sources to create challenging mathematical benchmarks.\n\nOur evaluation of nine state-of-the-art LLMs yields several key insights. Larger models generally perform better, with Qwen3-235B-A22B~ achieving the highest accuracy—50.16% on numerical solutions and 56.15% on symbolic solutions.\nHowever, model size alone does not determine performance; the 32B QwQ~ model outperforms larger models including GPT-4.1~ and Claude 3.7~, demonstrating the importance of architecture and training methodology.\n\nWe observe a strong negative correlation between problem difficulty and model accuracy across all evaluated models, with performance declining sharply on challenging problems.\nThis validates our difficulty annotations and reveals current limitations in complex mathematical reasoning.\n\nOur analysis of inference-time scaling shows that accuracy improves rapidly during initial token consumption before plateauing, with different models exhibiting distinct ``sweet spots.''\nThis suggests varying efficiency in information extraction during extended reasoning.\n\nIn summary, our contributions are threefold:\n {itemize}\n  Dataset: We construct  , a focused benchmark of 317 graduate-level integral problems with verified solutions for evaluating advanced LLM mathematical reasoning.\n {The dataset is publicly available at  {https://github.com/vegetable-yx/IntegralBench/}.}\n\n  Pipeline: We propose a scalable methodology for constructing challenging mathematical benchmarks through LLM-assisted curation from academic sources, providing a framework for future benchmark development.\n  Evaluation: We conduct a comprehensive evaluation of nine mainstream LLMs on  , revealing strengths and limitations in definite integral computation and informing future research directions.\n {itemize}",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "parts/Introduction.tex",
      "rlhf_score": 0.361,
      "weak_supervision_score": 0.345,
      "diffusion_reasoning_score": 0.446,
      "distributed_training_score": 0.352,
      "datasets_score": 0.406,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Highly Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper focuses on introducing a benchmark for evaluating LLMs on definite integral problems and does not mention or involve diffusion models, iterative refinement processes, or any adaptation of diffusion for multi-step logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper's main contribution is the creation and evaluation of INTEGRALBENCH, a new dataset for assessing LLMs on definite integral problems, including curation methodologies, manual annotations, benchmark evaluations, and analysis of model performance, which directly aligns with research on datasets for AI applications.",
      "summary": "INTEGRALBENCH is a specialized benchmark designed to evaluate Large Language Models (LLMs) on definite integral problems, featuring 317 graduate-level problems with both symbolic and numerical ground truth solutions, manual difficulty ratings from 1 to 5, and a term-rewriting method for generating variations to prevent dataset contamination. The methodology involves LLM-assisted curation from academic sources, and evaluations of nine state-of-the-art LLMs reveal that larger models generally perform better but are influenced by architecture and training, with accuracy declining as problem difficulty increases, highlighting limitations in complex mathematical reasoning and establishing baseline metrics for this domain.",
      "novelty_score": "Moderate",
      "novelty_justification": "The paper presents a notable improvement by introducing a specialized benchmark for definite integral problems with unique features like difficulty annotations and separate symbolic/numerical metrics, effectively combining existing benchmarking ideas in a new way for advanced mathematical reasoning. However, it does not introduce a entirely new problem or technique, as it builds on established LLM evaluation frameworks.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of LLM mathematical reasoning, as it provides a new tool for evaluating and improving model performance on integrals. Its influence may be limited to specific AI research areas rather than broader commercial or general applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution by addressing gaps in existing benchmarks and providing valuable insights into LLM capabilities, making it essential for researchers focused on AI and mathematical reasoning. While not groundbreaking for all audiences, it represents a strong advancement in its niche.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/ee20a79953bcc66faaf5798405e78ba1af1d876e",
      "h_index_fetch_method": "full_id",
      "total_authors": 6,
      "authors_found": 6,
      "highest_h_index": 1,
      "average_h_index": 0.3333333333333333,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Bintao Tang",
          "profile_url": "https://www.semanticscholar.org/author/2371005521",
          "h_index": 0
        },
        {
          "name": "Xin Yang",
          "profile_url": "https://www.semanticscholar.org/author/2371339008",
          "h_index": 0
        },
        {
          "name": "Yuhao Wang",
          "profile_url": "https://www.semanticscholar.org/author/2370953596",
          "h_index": 0
        },
        {
          "name": "Zixuan Qiu",
          "profile_url": "https://www.semanticscholar.org/author/2374185727",
          "h_index": 0
        },
        {
          "name": "Zimo Ji",
          "profile_url": "https://www.semanticscholar.org/author/2371265003",
          "h_index": 1
        },
        {
          "name": "Wenyuan Jiang",
          "profile_url": "https://www.semanticscholar.org/author/2371347113",
          "h_index": 1
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667904",
      "updated_at": "2025-08-11T23:44:45.489596",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21131",
      "title": "NPO: Learning Alignment and Meta-Alignment through Structured Human\n  Feedback",
      "authors": [
        "Madhava Gaikwad",
        "Ashwini Ramchandra Doke"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "We present NPO, an alignment-aware learning framework that operationalizes\nfeedback-driven adaptation in human-in-the-loop decision systems. Unlike prior\napproaches that treat alignment as a static or post-hoc property, NPO\nintroduces a formalization of alignment loss that is measurable, supervisable,\nand reducible under structured feedback. In parallel, we propose meta-alignment\nas the fidelity of the monitoring process that governs retraining or override\ntriggers, and show that it is formally reducible to primary alignment via\nthreshold fidelity. Our implementation spans a scalable operational loop\ninvolving scenario scoring, threshold tuning, policy validation, and structured\nfeedback ingestion, including \"likes\", overrides, and abstentions. We provide\nformal convergence results under stochastic feedback and show that both\nalignment loss and monitoring fidelity converge additively. Empirically, NPO\ndemonstrates measurable value in hyperscale deployment settings. A\nsimulation-based artifact and ablation studies further illustrate the\ntheoretical principles in action. Together, NPO offers a compact, inspectable\narchitecture for continual alignment monitoring, helping bridge theoretical\nalignment guarantees with practical reliability in dynamic environments.",
      "published_date": "2025-07-22T11:23:18+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21131v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21131v1",
      "latex_url": "http://arxiv.org/src/2507.21131v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "completed",
      "h_index_status": "completed",
      "introduction_text": "As AI systems take on increasingly consequential roles in real-world settings, ensuring that they behave in ways aligned with human expectations, operational constraints, and ethical principles becomes both urgent and technically challenging. While much of alignment research has focused on modeling user preferences or optimizing reward signals in static or simulated environments, these approaches often fail to capture the dynamic, high-stakes nature of alignment in practice. In safety-critical settings, such as hyperscale data centers, automated recovery systems, and fault-tolerant infrastructure, alignment cannot be a one-time specification. It must be continuously evaluated and adapted, based on structured feedback and real-world consequences. Human oversight is not simply an afterthought; it is the central mechanism through which misalignment is identified and corrected.\n\nWe introduce NPO (Network Performance Optimizer framework), a decision-making and learning framework deployed in hyperscale data center networks, where thousands of links, servers, and switches generate dynamic fault conditions under stringent availability and resilience requirements. In these environments, operators (SREs) must decide whether to remove or retain a degraded component based on traffic conditions, fault impact, and evolving service-level objectives (SLOs). These decisions are often informed by policies, past experience, and real-time tradeoffs, yet traditional AI systems struggle to remain both helpful and compliant. NPO is designed to co-operate with existing Safety Policy Engines (SPEs), issuing proactive remediation recommendations while learning over time how to better align with operator preferences and real-world outcomes. It does this by observing structured human feedback, such as overrides of incorrect actions (\"red button\") or affirmation of correct decisions (\"likes\"), and treating this feedback as a first-class supervisory signal. Rather than optimizing for latent or inferred reward, NPO defines an explicit alignment loss function based on these signals. This loss is minimized through targeted retraining and adaptive threshold control, ensuring that recommendations become increasingly aligned with human judgment under operational pressure. Importantly, the system also learns from deviations between formal policy and observed practice, integrating real-world nuance into its behavior.\n\nWe present this work not as a full production system, but as a modular, reproducible proof-of-concept that formalizes core alignment principles, simulates realistic feedback-driven learning, and provides tools for evaluation and ablation. Our focus is not on the system code itself, but on the alignment theory, metrics, and feedback learning loop that underlie its behavior. Our key contributions are:\n {itemize}\n   A formalization of alignment loss driven by high-fidelity human feedback.\n   A feedback-adaptive learning architecture based on red-button overrides and threshold tuning.\n   A simulation and logging platform for reproducible alignment analysis.\n   Empirical demonstration of convergence in alignment loss under structured feedback.\n {itemize}\nNPO operationalizes alignment as a measurable and improvable behavior in deployed AI systems, bridging the gap between theory and critical infrastructure practice.\n\nMonitoring, Evaluation, and Meta-Alignment\nThe role of introspective monitoring has been discussed in the context of system oversight  {skalse2022evaluating}, alignment auditing  {uesato2018rigorous}, and safety-centric retraining policies. Recent efforts, such as OpenAI’s recursive oversight and Anthropic’s interpretability-driven supervision loops, hint at the need for meta-alignment, ensuring that a system’s self-monitoring mechanisms are themselves aligned with operator expectations. Our work contributes the first formal definition and proof sketch of this property: we show that meta-alignment can be reduced to alignment loss convergence when supervision is consistent and trustworthy. Unlike red-teaming or offline auditing, our approach embeds introspective monitoring into the real-time feedback loop of the system, making alignment continuously observable and operationally actionable.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.636,
      "weak_supervision_score": 0.443,
      "diffusion_reasoning_score": 0.364,
      "distributed_training_score": 0.433,
      "datasets_score": 0.341,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Moderately Relevant",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "Not Relevant",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper's NPO framework uses structured human feedback, such as overrides and likes, to minimize an explicit alignment loss and adapt the system, which aligns with RLHF's core idea of incorporating human preferences to guide learning. However, it does not explicitly involve training a separate reward model or using reinforcement learning for fine-tuning, focusing instead on direct feedback-driven retraining and threshold tuning in a decision-making context. This makes it relevant but not a full match to standard RLHF methodologies.",
      "weak_supervision_justification": "The paper relies on direct, structured human feedback (e.g., overrides and abstentions) as a supervisory signal for alignment learning, rather than programmatically generating noisy or imprecise labels from high-level sources, which is the hallmark of weak supervision. There is no indication of using weak supervision techniques, making this topic unrelated to the paper's main contributions.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "The paper discusses deployment in hyperscale environments but focuses on alignment learning through feedback loops, not on techniques for partitioning data, models, or computations across multiple nodes for training acceleration. It does not address distributed training algorithms or parallel computing, so this topic is not applicable.",
      "datasets_justification": "below_threshold",
      "summary": "The paper introduces NPO, a framework for achieving alignment and meta-alignment in AI systems through structured human feedback, aiming to make alignment a dynamic, measurable process in real-world settings like hyperscale data centers. It formalizes alignment loss based on feedback such as overrides and likes, implements a scalable operational loop for continuous adaptation, and demonstrates through simulations and empirical studies that both alignment loss and monitoring fidelity converge, thereby enhancing practical reliability and bridging theoretical principles with operational deployment.",
      "novelty_score": "High",
      "novelty_justification": "The paper introduces a truly new formalization of alignment loss and the concept of meta-alignment as a measurable and reducible property, significantly advancing the state-of-the-art in dynamic human-in-the-loop AI systems by moving beyond static approaches.",
      "impact_score": "Moderate",
      "impact_justification": "The work is likely to be cited and built upon in the subfield of AI alignment and safety-critical systems, as it provides a practical framework for feedback-driven learning that could influence related research and applications.",
      "recommendation_score": "Should Read",
      "recommendation_justification": "This paper offers a high-quality contribution with novel insights into AI alignment through human feedback, making it valuable for researchers and practitioners in AI safety and decision systems to understand and potentially apply.",
      "semantic_scholar_url": "https://www.semanticscholar.org/paper/0aa91cbb6a43670e77c09c4c0353adfc89c64d7e",
      "h_index_fetch_method": "full_id",
      "total_authors": 3,
      "authors_found": 3,
      "highest_h_index": 0,
      "average_h_index": 0.0,
      "notable_authors_count": 0,
      "author_h_indexes": [
        {
          "name": "Madhava Gaikwad",
          "profile_url": "https://www.semanticscholar.org/author/2373720620",
          "h_index": 0
        },
        {
          "name": "Ashwini Ramchandra Doke Microsoft",
          "profile_url": "https://www.semanticscholar.org/author/2373720744",
          "h_index": 0
        },
        {
          "name": "Amrita University",
          "profile_url": "https://www.semanticscholar.org/author/2373720320",
          "h_index": 0
        }
      ],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667914",
      "updated_at": "2025-08-11T23:44:47.962818",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21132",
      "title": "Can You Trust an LLM with Your Life-Changing Decision? An Investigation\n  into AI High-Stakes Responses",
      "authors": [
        "Joshua Adrian Cahyono",
        "Saran Subramanian"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)",
        "cs.CY (Computers and Society)",
        "cs.LG (Machine Learning)"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly consulted for high-stakes life\nadvice, yet they lack standard safeguards against providing confident but\nmisguided responses. This creates risks of sycophancy and over-confidence. This\npaper investigates these failure modes through three experiments: (1) a\nmultiple-choice evaluation to measure model stability against user pressure;\n(2) a free-response analysis using a novel safety typology and an LLM Judge;\nand (3) a mechanistic interpretability experiment to steer model behavior by\nmanipulating a \"high-stakes\" activation vector. Our results show that while\nsome models exhibit sycophancy, others like o4-mini remain robust.\nTop-performing models achieve high safety scores by frequently asking\nclarifying questions, a key feature of a safe, inquisitive approach, rather\nthan issuing prescriptive advice. Furthermore, we demonstrate that a model's\ncautiousness can be directly controlled via activation steering, suggesting a\nnew path for safety alignment. These findings underscore the need for nuanced,\nmulti-faceted benchmarks to ensure LLMs can be trusted with life-changing\ndecisions.",
      "published_date": "2025-07-22T14:11:13+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21132v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21132v1",
      "latex_url": "http://arxiv.org/src/2507.21132v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.513,
      "weak_supervision_score": 0.399,
      "diffusion_reasoning_score": 0.417,
      "distributed_training_score": 0.302,
      "datasets_score": 0.346,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Tangentially Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper discusses safety alignment and model behavior in high-stakes scenarios, which could relate to broader alignment techniques, but it does not involve training a reward model on human-ranked data or using reinforcement learning for fine-tuning. Instead, it focuses on experiments like evaluations and activation steering, without any mention of human feedback mechanisms.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper's experiments involve multiple-choice evaluations, free-response analysis, and mechanistic interpretability, but there is no reference to diffusion models, iterative refinement processes, or treating Chain-of-Thought as a single entity for logical reasoning.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669678",
      "updated_at": "2025-08-11T23:43:05.607177",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21133",
      "title": "Analysis of Threat-Based Manipulation in Large Language Models: A Dual\n  Perspective on Vulnerabilities and Performance Enhancement Opportunities",
      "authors": [
        "Atil Samancioglu"
      ],
      "categories": [
        "cs.CR (Cryptography and Security)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Large Language Models (LLMs) demonstrate complex responses to threat-based\nmanipulations, revealing both vulnerabilities and unexpected performance\nenhancement opportunities. This study presents a comprehensive analysis of\n3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini)\nacross 10 task domains under 6 threat conditions. We introduce a novel threat\ntaxonomy and multi-metric evaluation framework to quantify both negative\nmanipulation effects and positive performance improvements. Results reveal\nsystematic vulnerabilities, with policy evaluation showing the highest metric\nsignificance rates under role-based threats, alongside substantial performance\nenhancements in numerous cases with effect sizes up to +1336%. Statistical\nanalysis indicates systematic certainty manipulation (pFDR < 0.0001) and\nsignificant improvements in analytical depth and response quality. These\nfindings have dual implications for AI safety and practical prompt engineering\nin high-stakes applications.",
      "published_date": "2025-07-22T14:13:08+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21133v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21133v1",
      "latex_url": "http://arxiv.org/src/2507.21133v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini have achieved remarkable capabilities across a wide range of cognitive tasks, including programming, scientific analysis, legal reasoning, and content creation. However, their growing integration into high-stakes applications has intensified concerns about susceptibility to manipulative prompting techniques.\n\nPrior research on LLM robustness has predominantly focused on adversarial attacks designed to induce harmful or policy-violating outputs, highlighting vulnerabilities in both instruction following and ethical alignment mechanisms [1,2]. Studies such as Zou et al. (2023) [3] and Perez et al. (2022) [4] systematically explored how carefully crafted prompts can bypass safety constraints, revealing a persistent gap in defensive generalization across diverse prompt types. Complementary investigations by Ganguli et al. (2022) [5] and Madaan et al. (2023) [6] documented the phenomenon of ``jailbreaking,'' where targeted manipulations undermine content moderation.\n\nYet, while the security risks of adversarial prompts are well-documented, emerging evidence indicates that certain forms of manipulation, including subtle psychological pressures or threat framing, may paradoxically enhance task performance under specific conditions. For example, recent empirical evaluations by Pichotta et al. (2023) [7] and Dey et al. (2023) [8] observed improved factual correctness or richer analytical detail when models were primed with high-consequence framing. These findings align with foundational studies in cognitive psychology demonstrating that perceived stakes can modulate reasoning depth and attentional resources [9].\n\nThis work situates within the broader ``motivated prompting'' literature examining how compliance pressure and contextual framing influence LLM behavior. Studies [10] explored authority-based compliance mechanisms, while recent investigations [11,12] examined how expectation setting and role assignment affect response characteristics. Our threat-based manipulation framework extends this line of inquiry by systematically examining both positive and negative behavioral modifications across diverse professional contexts.\n\nThis dual-nature perspective --- wherein threat-based manipulations may simultaneously reveal vulnerabilities and performance enhancement opportunities --- remains underexplored in the LLM literature. Unlike traditional adversarial robustness studies, few investigations have systematically examined how varying threat intensities and framing types influence both negative failure modes (e.g., reduced certainty, defensive responses) and positive metrics (e.g., analytical depth, domain appropriateness).\n\nOur study directly addresses this gap by presenting a comprehensive experimental analysis of threat-based prompting effects across ten professional task domains, three major LLM architectures, and six distinct threat framing conditions. We develop a novel evaluation framework that jointly quantifies vulnerability metrics and positive performance indicators, enabling a rigorous assessment of the complex trade-offs inherent in threat-based manipulations.\n\nResearch Questions: This investigation is guided by two primary research questions:\n {itemize}\n   RQ1: Vulnerability Assessment: What threat framings systematically compromise LLM response quality, particularly certainty and domain appropriateness measures?\n   RQ2: Enhancement Potential: What threat framings reliably improve analytical depth, response comprehensiveness, and professional language usage in complex reasoning tasks?\n {itemize}\n\nBy systematically mapping both risks and enhancement opportunities, our work contributes to a more nuanced understanding of LLM behavioral dynamics under manipulative conditions. The findings hold dual implications: they inform AI safety efforts aimed at mitigating psychological manipulation vulnerabilities, and they offer empirically grounded strategies for responsible prompt engineering in high-stakes analytical contexts.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.483,
      "weak_supervision_score": 0.398,
      "diffusion_reasoning_score": 0.444,
      "distributed_training_score": 0.342,
      "datasets_score": 0.378,
      "highest_similarity_topic": "RLHF",
      "rlhf_relevance": "Not Relevant",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "Not Relevant",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "The paper focuses on analyzing LLMs' responses to threat-based manipulations through experiments and evaluations, without discussing or involving the training of models using human feedback, reward models, or reinforcement learning techniques. It does not address alignment with human preferences via RLHF.",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "The paper examines threat-based manipulations and their effects on LLMs' responses, including analytical depth, but does not involve diffusion models, iterative refinement processes, or multi-step logical reasoning treated as a holistic entity for correction. There is no reference to diffusion-based approaches.",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.668991",
      "updated_at": "2025-08-11T23:43:05.607102",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21136",
      "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based\n  Independence Criteria for Improving Supervised and Unsupervised Learning",
      "authors": [
        "Mojtaba Moattari"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)",
        "stat.ML (Machine Learning)"
      ],
      "abstract": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.",
      "published_date": "2025-07-22T22:02:50+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21136v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21136v1",
      "latex_url": "http://arxiv.org/src/2507.21136v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "extraction_failed",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": null,
      "intro_extraction_method": null,
      "tex_file_name": null,
      "rlhf_score": 0.342,
      "weak_supervision_score": 0.408,
      "diffusion_reasoning_score": 0.325,
      "distributed_training_score": 0.321,
      "datasets_score": 0.385,
      "highest_similarity_topic": "Weak_supervision",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "Not Relevant",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "The paper primarily explores independence criteria for improving supervised and unsupervised learning through dimensionality reduction methods, focusing on capturing data nonlinearities and enhancing interpretability. It does not involve techniques for programmatically generating labels from noisy or imprecise sources, which is central to weak supervision. Therefore, there is no direct connection to weak supervision concepts.",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [
        "Introduction extraction failed after 4 attempts: not a gzip file"
      ],
      "created_at": "2025-08-11T23:15:40.669688",
      "updated_at": "2025-08-11T23:43:05.607178",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21137",
      "title": "Project Patti: Why can You Solve Diabolical Puzzles on one Sudoku\n  Website but not Easy Puzzles on another Sudoku Website?",
      "authors": [
        "Arman Eisenkolb-Vaithyanathan"
      ],
      "categories": [
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "In this paper we try to answer the question \"What constitutes Sudoku\ndifficulty rating across different Sudoku websites?\" Using two distinct methods\nthat can both solve every Sudoku puzzle, I propose two new metrics to\ncharacterize Sudoku difficulty. The first method is based on converting a\nSudoku puzzle into its corresponding Satisfiability (SAT) problem. The first\nproposed metric is derived from SAT Clause Length Distribution which captures\nthe structural complexity of a Sudoku puzzle including the number of given\ndigits and the cells they are in. The second method simulates human Sudoku\nsolvers by intertwining four popular Sudoku strategies within a backtracking\nalgorithm called Nishio. The second metric is computed by counting the number\nof times Sudoku strategies are applied within the backtracking iterations of a\nrandomized Nishio. Using these two metrics, I analyze more than a thousand\nSudoku puzzles across five popular websites to characterize every difficulty\nlevel in each website. I evaluate the relationship between the proposed metrics\nand website-labeled difficulty levels using Spearman's rank correlation\ncoefficient, finding strong correlations for 4 out of 5 websites. I construct a\nuniversal rating system using a simple, unsupervised classifier based on the\ntwo proposed metrics. This rating system is capable of classifying both\nindividual puzzles and entire difficulty levels from the different Sudoku\nwebsites into three categories - Universal Easy, Universal Medium, and\nUniversal Hard - thereby enabling consistent difficulty mapping across Sudoku\nwebsites. The experimental results show that for 4 out of 5 Sudoku websites,\nthe universal classification aligns well with website-labeled difficulty\nlevels. Finally, I present an algorithm that can be used by early Sudoku\npractitioners to solve Sudoku puzzles.",
      "published_date": "2025-07-22T22:32:30+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21137v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21137v1",
      "latex_url": "http://arxiv.org/src/2507.21137v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "{20pt}Sudoku is a widely popular logic-based puzzle enjoyed around the world. A Sudoku puzzle consists of a 9x9 grid of 81 cells, further divided into 9 smaller 3x3 grids, or commonly referred to as boxes (see Figure ). The objective is to fill the puzzle with the digits 1 through 9, such that each row, column, and box contains each digit without repetition. As a general term, rows, columns, and boxes will henceforth be called \"sub-groups\". Each puzzle begins with a set of pre-filled cells to provide a starting point. The puzzle is considered solved when all 81 cells are filled in accordance with these rules.\n\n  {20pt} There are tens if not hundreds of online Sudoku websites. Although all Sudoku puzzles are dictated by the same set of rules, not all puzzles are equally difficult. Indeed, the primary differentiator between websites are the difficulty levels and the puzzles within each difficulty level. Each site has their own way of defining Sudoku difficulty, and each site often has a different number of difficulty levels. For example, New York Times has three levels of difficulty - Easy, Medium, and Hard - while Sudoku.org.uk has four levels of difficulty - Gentle, Moderate, Tough, and Diabolical. The classification of Sudoku difficulty on each site is entirely done by the individual site. This paper analyzes Sudoku puzzles collected from 5 websites: New York Times , Sudoku.org.uk , Extreme Sudoku , Sudoku of the Day , and Sudoku of the Day UK .\n\n  {20pt} To understand the difficulty levels within each website and also compare them across websites, the first step is to characterize the difficulty of a Sudoku puzzle. To this end, I employ two distinctly different approaches. The first, purely computational, involves converting a Sudoku puzzle into the well-known boolean satisfiability problem (SAT). This allows the use of the characteristics of a SAT instance, specifically the length of clauses, to characterize the difficulty of Sudoku.\n\n  {20pt}The second approach involves simulating the way a human solves a Sudoku puzzle. There are tens of Sudoku-solving strategies that people use to solve Sudoku puzzles. Ranging from easy to very complex, such strategies may involve sophisticated pattern identification on a Sudoku puzzle. To characterize Sudoku difficulty across a variety of human skill levels, I choose four Sudoku strategies that range from beginner to moderately sophisticated. While the application of these four strategies, alone, can solve some of the Sudoku puzzles on most websites, they do not solve all the puzzles on all websites. Therefore, for the second approach, I use a simple trial-and-error methodology in conjunction with the four human strategies. The trial and error implementation for Sudoku, known as Nishio, with the four human strategies, can solve every Sudoku puzzle. This work simulates random humans solving Sudoku puzzles using the four human strategies within a randomized version of Nishio. Difficulty is characterized by counting the number of times Sudoku strategies are applied within Nishio by the simulated human solver. The average count within each difficulty level provides a metric that characterizes difficulty both within a website and across websites. Finally, I use the two proposed metrics to create a universal rating system built on a simple, unsupervised classifier to enable comparison across the five Sudoku websites and classification of unlabeled datasets.\n\n  {20pt}This paper is organized as follows. The next section outlines the key contributions of the work. Following that, a running example is introduced along with three core constructs common to all Sudoku puzzles. This is followed by a section describing the encoding of Sudoku puzzle into SAT. Then the Nishio interleaving Human Strategies method is presented, followed by the data sets used in this study. Next, results for both methods are reported, followed by a detailed analysis conducted both within individual sites and across different sites. I then present a universal classification of Sudoku difficulty. Finally, the paper concludes with a proposed human solving algorithm derived from the study's findings.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.25,
      "weak_supervision_score": 0.249,
      "diffusion_reasoning_score": 0.347,
      "distributed_training_score": 0.233,
      "datasets_score": 0.289,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.667923",
      "updated_at": "2025-08-11T23:43:05.606889",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2507.21138",
      "title": "TTS-1 Technical Report",
      "authors": [
        "Oleg Atamanenko",
        "Anna Chalova",
        "Joseph Coombes",
        "Nikki Cope",
        "Phillip Dang",
        "Zhifeng Deng",
        "Jimmy Du",
        "Michael Ermolenko",
        "Feifan Fan",
        "Yufei Feng",
        "Cheryl Fichter",
        "Pavel Filimonov",
        "Louis Fischer",
        "Kylan Gibbs",
        "Valeria Gusarova",
        "Pavel Karpik",
        "Andreas Assad Kottner",
        "Ian Lee",
        "Oliver Louie",
        "Jasmine Mai",
        "Mikhail Mamontov",
        "Suri Mao",
        "Nurullah Morshed",
        "Igor Poletaev",
        "Florin Radu",
        "Dmytro Semernia",
        "Evgenii Shingarev",
        "Vikram Sivaraja",
        "Peter Skirko",
        "Rinat Takhautdinov",
        "Robert Villahermosa",
        "Jean Wang"
      ],
      "categories": [
        "cs.CL (Computation and Language)",
        "cs.AI (Artificial Intelligence)",
        "cs.LG (Machine Learning)",
        "cs.SD (Sound)",
        "eess.AS (Audio and Speech Processing)"
      ],
      "abstract": "We introduce Inworld TTS-1, a set of two Transformer-based autoregressive\ntext-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters\nand is designed for utmost quality and expressiveness in demanding\napplications. TTS-1 is our most efficient model, with 1.6B parameters, built\nfor real-time speech synthesis and on-device use cases. By scaling train-time\ncompute and applying a sequential process of pre-training, fine-tuning, and\nRL-alignment of the speech-language model (SpeechLM) component, both models\nachieve state-of-the-art performance on a variety of benchmarks, demonstrating\nexceptional quality relying purely on in-context learning of the speaker's\nvoice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech\nwith low latency, and support 11 languages with fine-grained emotional control\nand non-verbal vocalizations through audio markups. We additionally open-source\nour training and modeling code under an MIT license.",
      "published_date": "2025-07-22T23:57:11+00:00",
      "arxiv_url": "http://arxiv.org/abs/2507.21138v1",
      "pdf_url": "http://arxiv.org/pdf/2507.21138v1",
      "latex_url": "http://arxiv.org/src/2507.21138v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Recent advancements in deep learning and the proliferation of large-scale audio datasets  {galvez2021people, li2023yodas, he2024emilia} have propelled text-to-speech (TTS) synthesis from multi-stage pipelines  {li2023styletts, ren2019fastspeech} to end-to-end generative systems  {betker2023better, kim2021conditional, le2023voicebox}.\nThe latest paradigm leverages large language models (LLMs)  {radford2019language, hoffmann2022training} as powerful speech-language models (SpeechLMs), using neural audio codecs as tokenizers to generate highly naturalistic speech from text  {wang2023neural, zhang2025minimax, anastassiou2024seed, du2024cosyvoice, du2024cosyvoice2}.\nDespite this progress, many existing models struggle to meet the demands of real-world applications, often lacking high-fidelity output (e.g., 48~kHz), robust multilingual support, reliable real-time streaming capabilities, or suffering from synthesis artifacts.\n\nThis paper introduces Inworld TTS-1 and TTS-1-Max, two generative speech models designed to bridge this gap.\nOur models, based on 1B and 8B parameter LLaMA backbones respectively, achieve state-of-the-art speech quality and control through a systematic training methodology and architectural innovations.\nWe demonstrate that a sequential process of pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL) alignment is critical for developing high-performance TTS systems. Our key contributions are as follows:\n\n {itemize}\n   A three-stage training framework for SpeechLMs. We propose a robust pipeline consisting of (1) large-scale pre-training on over 1M hours of raw audio mixed with text data  {weber2024redpajama, laion-oig} to build a strong foundational model; (2) supervised fine-tuning on 200k hours of high-quality, filtered audio-text pairs; and (3) reinforcement learning alignment using Group Relative Policy Optimization (GRPO)  {shao2402deepseekmath} to fine-tune the model against perceptual quality metrics and reduce hallucinations.\n   A high-resolution audio codec for 48~kHz speech synthesis. We develop a novel audio codec built on top of the X-codec2  {ye2025llasa} architecture with a super-resolution module to natively generate 48~kHz audio. We introduce an root mean-square (RMS) loudness loss term during training to ensure volume consistency, a critical factor for streaming applications.\n   An extensible reinforcement learning framework for speech quality. We adapt GRPO for TTS alignment. We design a composite reward function combining word error rate (WER), speaker similarity (SIM)  {chen2022wavlm}, and DNSMOS scores  {reddy2022dnsmos}. The framework is modular, allowing for the integration of further reward signals like prosody or emotion consistency.\n   Expressive and controllable speech synthesis. We enable fine-grained control over non-verbal vocalizations and speaking styles through textual audio markups. We show that pairing neutral and stylized utterances from the same speaker during a LoRA-based  {hu2022lora} fine-tuning phase is an effective strategy for teaching the model stylistic control while preserving speaker identity.\n   Efficient and robust streaming inference. We detail a low-latency streaming pipeline that employs novel techniques, including context-aware decoding and concatenation at non-voicing regions, to ensure seamless and high-quality audio delivery in real-time scenarios.\n {itemize}\n\nOur models generate high-fidelity 48~kHz speech, support 11 languages, and offer fine-grained emotional control through in-context learning from short reference audio clips. Through extensive evaluations, we demonstrate their superior performance and practical utility for a wide range of applications, from interactive assistants to content creation. To facilitate further research and development in the community, we open-source our training, modeling, and benchmarking code under a permissive license.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.349,
      "weak_supervision_score": 0.328,
      "diffusion_reasoning_score": 0.382,
      "distributed_training_score": 0.398,
      "datasets_score": 0.314,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669769",
      "updated_at": "2025-08-11T23:43:05.607187",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2508.00877",
      "title": "Satellite Connectivity Prediction for Fast-Moving Platforms",
      "authors": [
        "Chao Yan",
        "Babak Mafakheri"
      ],
      "categories": [
        "cs.LG (Machine Learning)",
        "cs.AI (Artificial Intelligence)"
      ],
      "abstract": "Satellite connectivity is gaining increased attention as the demand for\nseamless internet access, especially in transportation and remote areas,\ncontinues to grow. For fast-moving objects such as aircraft, vehicles, or\ntrains, satellite connectivity is critical due to their mobility and frequent\npresence in areas without terrestrial coverage. Maintaining reliable\nconnectivity in these cases requires frequent switching between satellite\nbeams, constellations, or orbits. To enhance user experience and address\nchallenges like long switching times, Machine Learning (ML) algorithms can\nanalyze historical connectivity data and predict network quality at specific\nlocations. This allows for proactive measures, such as network switching before\nconnectivity issues arise. In this paper, we analyze a real dataset of\ncommunication between a Geostationary Orbit (GEO) satellite and aircraft over\nmultiple flights, using ML to predict signal quality. Our prediction model\nachieved an F1 score of 0.97 on the test data, demonstrating the accuracy of\nmachine learning in predicting signal quality during flight. By enabling\nseamless broadband service, including roaming between different satellite\nconstellations and providers, our model addresses the need for real-time\npredictions of signal quality. This approach can further be adapted to automate\nsatellite and beam-switching mechanisms to improve overall communication\nefficiency. The model can also be retrained and applied to any moving object\nwith satellite connectivity, using customized datasets, including connected\nvehicles and trains.",
      "published_date": "2025-07-22T10:33:48+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.00877v1",
      "pdf_url": "http://arxiv.org/pdf/2508.00877v1",
      "latex_url": "http://arxiv.org/src/2508.00877v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "A communication satellite is an artificial satellite designed to provide communication service by transmitting signals through a transponder, creating a link between a transmitter and a receiver located at different points on Earth. These satellites play a critical role in enabling a wide range of services.\nSatellites move around the Earth due to the gravitational pull along specific paths called orbits. These orbits can vary in altitude and some other factors, depending on the satellite's purpose.\n\nThere are various satellite providers nowadays on the market, such as ViaSat, Starlink, SES Sirius, and so on.\nStarlink satellites mainly run in  {LEO}, at altitudes from 340 km to 1,200 km above the Earth.\nViasat satellites operate in GEO. In this orbit, the satellites match the Earth's rotation and are positioned at around 35,786 km above the equator, in a fixed position to the Earth. SES Sirius is  {MEO} satellite provider, having their satellites operating at an altitude of around 8,000 km.\nGEO, MEO, and LEO satellites are the three commonly used communication satellite systems.\n\nSatellite communication is widely used in various scenarios, ranging from everyday individual use to business purposes and critical systems for overall infrastructure. The services include telecommunications, global navigation systems, autonomous systems, maritime and aviation, and other fields. Satellites' communication can widely be used for mobile phones, TV, and internet . Global navigation systems are utilized for providing global positioning services via  {GPS}. Autonomous systems are critical for autonomous vehicles which require high-precision GPS so that tasks such as optimizing routes, avoiding collisions, and recognizing lanes can be performed well . Aircraft and ships rely highly on satellites for communication, navigation and so on. Satellite communication provides internet access to crew and passengers during flights at high altitudes targeting on-board internet access, also called  {IFC} , and cruises in the middle of the ocean .\nOn commercial aircraft, good IFC is extremely demanding, since it can enhance passenger satisfaction to some extent which is what the airlines are pursuing .\n {HO} is another important term in satellite communication which means the whole step of switching an ongoing communication session from one ground station or satellite to another, during which seamless and uninterrupted connectivity should be secured. This is especially critical in aviation since the aircraft is moving at a high speed.\n\nWe have already demonstrated the usage of a LEO satellite for  {IFC} in and introduced an AI-based  {IFEC} system in . Moreover, there are several works discussing the LEO, GEO satellite networks, IFC, handover (HO), and application of ML in this domain , etc.\nAuthors in proposed a new method for IFC link allocation, which could make dynamic switches between terrestrial cellular and LEO satellite networks in real-time.\nThe paper summarizes the current ML methods for  {GNSS} based positioning, their advantages, disadvantages, and potential challenges. Min J proposes a nonorthogonal multiple access (NOMA) in LEO satellite communication systems using ML techniques. The experiment results prove that the performance is comparable to the optimal one by using standard calculation. On the other hand, proposes solutions based on ML for optimizing handover decisions in non-terrestrial networks (NTN), focusing on decreasing the issue of signaling storms during handovers. The results reveal that achieving optimal HO performance requires considering the distance between the cell center and the user, which is a critical variable in NTN.\nM Chen proposes a Q-learning-based HO scheme for LEO satellite networks that utilizes both remaining service time and signal quality to optimize the performance of handovers. Experimental results show that this method increases overall signal quality and decreases the number of HOs compared to traditional methods.\nFeng L presents a scheme by leveraging the timely and accurate orbit and position data of the aircraft to facilitate seamless HO. This scheme not only decreases the packet drop rate during HO but also minimizes data transmission delays.\nMoreover, S. Mondal proposes a prediction-based solution for HO selection. They derive the cost function and constraints based on dual connectivity variables over the prediction horizon and solve the problem using a  {2D-GA} to achieve the best HO solution. The results indicate that network densification combined with the predictive control model improves overall aircraft performance.\n\nAlthough existing studies propose methods to improve satellite handover, a prediction model specifically tailored to moving platforms can significantly enhance satellite communication handover mechanisms. To the best of our knowledge, no prior research has analyzed historical data to develop a prediction model for satellite connectivity of moving platforms.\nIn this context, our objective is to create a satellite-aircraft link quality prediction model that provides comprehensive analysis into link conditions throughout an entire flight across all potential geographic coordinates. The primary goal of this model is to predict the satellite signal quality in different geographical coordinates which helps enable an automated handover mechanism capable of making network switching decisions based on the aircraft’s location and other real-time data such as weather conditions during flight.\n\nThe main contribution of this paper is the development of an accurate prediction model capable of forecasting the signal quality of GEO satellites on moving aircraft. This model enables airline operators to take proactive measures before encountering issues related to connection failures, such as beam-switching or network switching. By utilizing the most relevant features for optimal signal quality prediction, the model is categorized into high-altitude and low-altitude scenarios, with low-altitude scenarios incorporating weather data as an influential factor. This adaptability allows the model to be applied to other moving objects on the ground, such as cars and trains.\n\nThe remainder of the article is structured as follows: Section II discusses the methodology, including dataset processing, metrics selection, and training. Section III presents the experimental results, and Section IV concludes the work with a discussion of future directions.",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "main.tex",
      "rlhf_score": 0.352,
      "weak_supervision_score": 0.331,
      "diffusion_reasoning_score": 0.353,
      "distributed_training_score": 0.361,
      "datasets_score": 0.343,
      "highest_similarity_topic": "Distributed_training",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669000",
      "updated_at": "2025-08-11T23:43:05.607103",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2508.03711",
      "title": "A Social Data-Driven System for Identifying Estate-related Events and\n  Topics",
      "authors": [
        "Wenchuan Mu",
        "Menglin Li",
        "Kwan Hui Lim"
      ],
      "categories": [
        "cs.IR (Information Retrieval)",
        "cs.AI (Artificial Intelligence)",
        "cs.CL (Computation and Language)",
        "cs.LG (Machine Learning)",
        "cs.SI (Social and Information Networks)"
      ],
      "abstract": "Social media platforms such as Twitter and Facebook have become deeply\nembedded in our everyday life, offering a dynamic stream of localized news and\npersonal experiences. The ubiquity of these platforms position them as valuable\nresources for identifying estate-related issues, especially in the context of\ngrowing urban populations. In this work, we present a language model-based\nsystem for the detection and classification of estate-related events from\nsocial media content. Our system employs a hierarchical classification\nframework to first filter relevant posts and then categorize them into\nactionable estate-related topics. Additionally, for posts lacking explicit\ngeotags, we apply a transformer-based geolocation module to infer posting\nlocations at the point-of-interest level. This integrated approach supports\ntimely, data-driven insights for urban management, operational response and\nsituational awareness.",
      "published_date": "2025-07-22T14:48:42+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.03711v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03711v1",
      "latex_url": "http://arxiv.org/src/2508.03711v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Over the past two decades, social media platforms such as Twitter/X and Facebook have undergone unprecedented growth, now reaching approximately 82% of the global online population, with nearly 20% of users’ online time spent on these platforms~. These platforms have evolved into essential channels for real-time information dissemination and discussion, encompassing topics ranging from entertainment and pop culture to more specialized domains such as politics and human rights. In parallel, organizations increasingly leverage social media as a sensing modality to identify and monitor both large-scale events (e.g., natural disasters, public crises) and localized issues (e.g., infrastructure faults, community disturbances).\n\nDespite the value of social media as an information source, its scale and velocity introduce significant challenges, foremost among them being information overload~. This impairs the end users’ abilities to efficiently filter, retrieve, and contextualize relevant content. In response, a range of social media analytics systems have emerged. Examples include InfoTrace~, which tracks the lifecycle of social media campaigns; DISCO~, a framework for explainable disinformation detection; RAPID~, which enables real-time mining of streaming social media data; Li et al.~ that presented a framework implementing clustering and temporal identification for event detection; and Rosa et al.~ that utilizes user behaviour changes over time to detect pandemic-related events on social media. While these systems address important facets of social media analysis, a critical gap remains: the automated detection of estate-related events, such as facility breakdowns, noise complaints, or parking violations, within both historical and real-time social media data streams.\n\nTo address this gap, we propose a novel system for detecting estate-related events and associated discussion topics from both archival and live social media data. This system is part of the broader Estate-IQ initiative, which aims to automate estate operations and maintenance through AI-driven event detection and decision support.\n\n {figure*}[t]\n  \n  [width= ]{EstateIQ.pdf}\n  {Model Architecture of Our Proposed System}\n\n {figure*}",
      "intro_extraction_method": "main_tex_file",
      "tex_file_name": "estateIQDemo.tex",
      "rlhf_score": 0.351,
      "weak_supervision_score": 0.364,
      "diffusion_reasoning_score": 0.352,
      "distributed_training_score": 0.33,
      "datasets_score": 0.412,
      "highest_similarity_topic": "Datasets",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "Tangentially Relevant",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "The paper focuses on developing a system for detecting and classifying estate-related events from social media using language models and geolocation techniques. While it implies the use of social media data as a dataset for training and analysis, it does not primarily involve creating, analyzing, benchmarking, or evaluating datasets for AI applications. The main contribution is the event detection system, not dataset-related research.",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669779",
      "updated_at": "2025-08-11T23:43:05.607188",
      "last_generated": "2025-08-11"
    },
    {
      "id": "2508.03713",
      "title": "Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy\n  and Visual Attention",
      "authors": [
        "Minsuk Chang",
        "Yao Wang",
        "Huichen Will Wang",
        "Yuanhong Zhou",
        "Andreas Bulling",
        "Cindy Xiong Bearfield"
      ],
      "categories": [
        "cs.HC (Human-Computer Interaction)",
        "cs.CV (Computer Vision and Pattern Recognition)"
      ],
      "abstract": "Accounting for individual differences can improve the effectiveness of\nvisualization design. While the role of visual attention in visualization\ninterpretation is well recognized, existing work often overlooks how this\nbehavior varies based on visual literacy levels. Based on data from a\n235-participant user study covering three visualization tests (mini-VLAT,\nCALVI, and SGL), we show that distinct attention patterns in visual data\nexploration can correlate with participants' literacy levels: While experts\n(high-scorers) generally show a strong attentional focus, novices (low-scorers)\nfocus less and explore more. We then propose two computational models\nleveraging these insights: Lit2Sal -- a novel visual saliency model that\npredicts observer attention given their visualization literacy level, and\nSal2Lit -- a model to predict visual literacy from human visual attention data.\nOur quantitative and qualitative evaluation demonstrates that Lit2Sal\noutperforms state-of-the-art saliency models with literacy-aware\nconsiderations. Sal2Lit predicts literacy with 86% accuracy using a single\nattention map, providing a time-efficient supplement to literacy assessment\nthat only takes less than a minute. Taken together, our unique approach to\nconsider individual differences in salience models and visual attention in\nliteracy assessments paves the way for new directions in personalized visual\ndata communication to enhance understanding.",
      "published_date": "2025-07-22T20:18:45+00:00",
      "arxiv_url": "http://arxiv.org/abs/2508.03713v1",
      "pdf_url": "http://arxiv.org/pdf/2508.03713v1",
      "latex_url": "http://arxiv.org/src/2508.03713v1",
      "scraper_status": "successfully_scraped",
      "intro_status": "intro_successful",
      "embedding_status": "completed",
      "llm_validation_status": "completed",
      "llm_score_status": "not_relevant_enough",
      "h_index_status": "not_fetched",
      "introduction_text": "Visualization can guide users to notice key patterns in data.\nYet what counts as the 'right' design often depends on who is looking.\nPeople interpret data on a deeply personal level~, thus visualization design effectiveness can depend on their analytic tasks~, their goals~, and most importantly, their visualization literacy~.\n\nExisting research has demonstrated that we can improve visualization design by understanding where people look in a visualization~.\nResearchers have built saliency models to predict which parts of a visualization are most likely to attract a viewer’s attention~.\nThese models have proven valuable in supporting visualization and tool design, such as attention-aware UI~, chart compression~, and image quality evaluations~.\n\nHowever, existing models still largely assume universal viewing patterns among people and overlook differences driven by cognitive abilities such as visualization literacy~.\nWe posit that individuals with varying levels of visualization literacy interpret visualizations through distinct viewing patterns and can be effectively captured by models of their visual attention.\nTherefore, we argue that we can improve existing saliency models by accounting for the unique patterns in viewers' visual attention depending on their literacy levels.\n\nIn this paper, we conduct user study (N = 235) using three established literacy tests: the mini Visualization Literacy Assessment Test (mini-VLAT) ~, the Critical thinking Assessment for Literacy in VIsualizations (CALVI)~, and the Subjective Graph Literacy assessment (SGL)~.\nWe recorded participants' responses and attention maps generated using the established BubbleView~ technique, which approximates gaze tracking with mouse clicks.\nOur analyses reveal striking attention differences between high- and low-literacy groups in each assessment:\nExperts focus on specific regions of visualizations, creating concentrated `hot spots' that reflect their targeted attention.\nIn contrast, the viewing patterns of novices tended to be more distributed, with less intense focal points.\nThis finding provides strong evidence that performance on visualization literacy tests does correlate with distinct visual attention patterns when interpreting visualizations.\nInformed by these insights, we then introduce two novel saliency models for literacy-aware attention prediction (Lit2Sal) and visual attention-based visualization literacy prediction (Sal2Lit).\n\nLit2Sal extends the state-of-the-art saliency model VisSalFormer~ to predict where people look in a visualization while accounting for their literacy levels, across three literacy assessment categories: mini-VLAT~ for visualization comprehension, CALVI~ for critical thinking abilities, and SGL for self-perceived literacy, both individually and holistically.\nBy integrating these measures, our model generates distinct saliency predictions for novices and experts.\n\nSal2Lit can predict visualization literacy levels for all three tests with an average accuracy of 86% with a human attention map from only one visualization.\nIf we expand the input data to include attention maps from three visualizations, our model accuracy increases by over 87% (mini-VLAT) to 90% (CALVI, SGL).\nThis time-efficient proxy supplements existing literacy assessments and reveals visual processes that differentiate novices and experts in visual data exploration.\n\n {2mm}\n Contributions:\nWe conducted a crowdsourced study and a series of analyses, discovering how different aspects of visualization literacy (e.g., basic comprehension, critical thinking, and self-assessed ability) correlate with viewers’ gaze patterns.\nOur literacy-to-saliency model, Lit2Sal, enhances existing saliency models by incorporating individual differences and aligning attention predictions with a viewer’s visualization literacy level.\nConversely, our saliency-to-literacy model, Sal2Lit, takes a perception-driven approach to reframe how visualization literacy can be assessed through observed visual attention patterns.\nTogether, they advance our fundamental understanding of visual patterns in visual data exploration.\nBy embracing personalization, these models empower researchers and practitioners to design personalized visualizations and efficiently estimate literacy levels.",
      "intro_extraction_method": "dedicated_intro_file",
      "tex_file_name": "sections/1-Introduction.tex",
      "rlhf_score": 0.323,
      "weak_supervision_score": 0.314,
      "diffusion_reasoning_score": 0.385,
      "distributed_training_score": 0.242,
      "datasets_score": 0.31,
      "highest_similarity_topic": "Diffusion_reasoning",
      "rlhf_relevance": "not_validated",
      "weak_supervision_relevance": "not_validated",
      "diffusion_reasoning_relevance": "not_validated",
      "distributed_training_relevance": "not_validated",
      "datasets_relevance": "not_validated",
      "rlhf_justification": "below_threshold",
      "weak_supervision_justification": "below_threshold",
      "diffusion_reasoning_justification": "below_threshold",
      "distributed_training_justification": "below_threshold",
      "datasets_justification": "below_threshold",
      "summary": null,
      "novelty_score": null,
      "novelty_justification": null,
      "impact_score": null,
      "impact_justification": null,
      "recommendation_score": null,
      "recommendation_justification": null,
      "semantic_scholar_url": null,
      "h_index_fetch_method": null,
      "total_authors": null,
      "authors_found": null,
      "highest_h_index": null,
      "average_h_index": null,
      "notable_authors_count": null,
      "author_h_indexes": [],
      "errors": [],
      "created_at": "2025-08-11T23:15:40.669206",
      "updated_at": "2025-08-11T23:43:05.607125",
      "last_generated": "2025-08-11"
    }
  ]
}